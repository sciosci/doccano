{"section": "Abstract", "text": "Although individuals born at extremely low birth weight (ELBW; \ud97b\udf59 1,000 g) are the most vulnerable of all preterm survivors, their risk for mental health problems across the life span has not been systematically reviewed. The primary objective of this systematic review and meta-analysis was to ascertain whether the risk for mental health problems is greater for ELBW survivors than their normal birth weight (NBW) peers in childhood, adolescence, and adulthood. Forty-one studies assessing 2,712 ELBW children, adolescents, and adults and 11,127 NBW controls were reviewed. Group differences in mental health outcomes were assessed using random effects meta-analyses. The impacts of birthplace, birth era, and neurosensory impairment on mental health outcomes were assessed in subgroup analyses. Children born at ELBW were reported by parents and teachers to be at significantly greater risk than NBW controls for inattention and hyperactivity, internalizing, and externalizing symptoms. ELBW children were also at greater risk for conduct and oppositional disorders, autistic symptoms, and social difficulties. Risks for parent-reported inattention and hyperactivity, internalizing, and social problems were greater in adolescents born at ELBW. In contrast, ELBW teens self-reported lower inattention, hyperactivity, and oppositional behavior levels than their NBW peers. Depression, anxiety, and social difficulties were elevated in ELBW survivors in adulthood. Group differences were robust to region of birth, era of birth, and the presence of neurosensory impairments. The complex needs faced by children born at ELBW continue throughout development, with long-term consequences for psychological and social well-being.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Developmental Stages and Change", "text": "Relatively few data exist on the developmental trajectory of mental health problems within ELBW individuals over time. How- ever, some evidence suggests that preterm survivors manifest a distinct behavioral phenotype characterized by attention deficits, anxiety, and social problems (e.g., Johnson & Marlow, 2011). In the extremely preterm population, the type and course of emotional and behavioral problems has yet to be elucidated. The develop- ment of appropriate interventions and the optimal timing of their application require accurate information about the onset and course of these problems ( Szatmari et al., 1993). Systematically gathering, quantifying, and summarizing this information are some of the goals of this review.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Accessibility and Changes in Health Care", "text": "Intensive neonatal care is a developing entity that may differ by region and has changed over time. In the developed world, some countries provide universal health care for their citizens and others require payment by insurance or other means. Free and timely access to health care in the neonatal period and beyond may have a major influence on the lifelong trajectory of mental health in highly vulnerable infants, as can the prevailing social circum- stances in the environments in which these individuals develop (Hack & Klein, 2006). Therefore, we wished to examine whether participants' region of birth had any influence on mental health outcomes. Technological developments have also significantly altered neo- natal intensive care practices over the last half century. Between the 1960s and the 1980s, intensive care of preterm newborns was limited to intravenous fluid administration, assisted ventilation (by endotracheal intubation), and oxygen monitoring (Hack & Costello, 2008). The 1990s saw widespread adoption of surfactant to treat or prevent severe respiratory distress syndrome in preterm neonates, and prenatal administration of synthetic glucocorticoids, to maximally develop lung function in fetuses at risk (e.g., Ferrara et al., 1991;G\u00f6pel et al., 2015;Hoekstra, Ferrara, Couser, Payne, & Connett, 2004;Pelkonen, Hakulinen, Turpeinen, & Hallman, 1998;Verder et al., 1994). Concomitantly, survival of preterm infants increased during the 1990s in the United States (Fanaroff et al., 1995;Hoekstra et al., 2004), Britain (Riley, Roth, Sellwood, & Wyatt, 2008), and Australia, (Doyle, 2006;Doyle, & the Victorian Infant Collaborative Study Group, 2004). This improved infant survival was largely attributed to the adoption of surfactant (Fanaroff et al., 1995;Horbar, Wright, & Onstad, & The Members of the National Institute of Child Health and Human Development Neonatal Research Network, 1993;Jobe, Mitchell, & Gunkel, 1993;Schwartz, Luby, Scanlon, & Kellogg, 1994), and steroid therapies (Victorian Infant Collective Study Group, 1997), as well as birth in large hospital centers (Hoekstra et al., 2004), and an increased willingness to treat the tiniest infants (Victorian Infant Collective Study Group, 1997). Experimental trials of surfactant were undertaken in the later 1980s, with 35 randomized controlled trials reported between 1985 and 1992 ( Jobe et al., 1993). In 1990, a meta-analysis of random- ized controlled trials of antenatal corticosteroids was published (Crowley, Chalmers, & Keirse, 1990), demonstrating improved survival and reduced morbidity in infants born preterm. This was followed in 1994 by a National Institutes of Health recommenda- tion that all women at risk of preterm delivery between 24 and 34 weeks be given antenatal steroids, only to be superseded six years later by a recommendation to restrict repeated administration of corticosteroids to women enrolled in randomized control trials, until it was determined that this now-common practice was both safe and effective (Bonanno & Wapner, 2012). Although developments in neonatal intensive care continued in the 2000s, (e.g., positive airway pressure, widespread use of caf- feine, decreased use of endotracheal intubation, protein enhance- ment of parenteral feeding, and treatments via the mother, such as antibiotic therapy and cesarean sections; Hack & Costello, 2008), there was no further dramatic increase in survival rates like those of the previous decades (Doyle, Roberts, & Anderson, & the Victorian Infant Collaborative Study Group, 2011;Field, Dorling, Manktelow, & Draper, 2008). As a result, we wished to compare mental health outcomes in cohorts born before and after surfactant and steroid therapies became widely available. To examine the question of whether these therapies may have resulted in improved mental health outcomes, we proposed to examine mental health in those born before and after their introduction, making a division at the year 1990.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Neurosensory Impairment (NSI)", "text": "A small but significant proportion of individuals born at ELBW have significant NSI. Although NSI may not strictly constitute an exclusion criterion for investigations of psychiatric functioning (Szatmari et al., 1993), higher rates of neurological disability (e.g., visual and hearing impairment, cerebral palsy, intellectual disabil- ity) may profoundly affect the risk for psychopathology in ELBW survivors, particularly depression and anxiety (Johnson & Wolke, 2013). On the other hand, when the goal of a study is to identify the consequences of low birth weight separately from any effects of major neurological damage (Breslau, 1995), then children born extremely preterm who have significant NSI may be excluded. We deemed it valuable to compare the outcomes from these two strategies, by contrasting rates and types of mental health problems in studies from which NSI participants were excluded versus those where they were included.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Informant Effects", "text": "Reports from different informants can provide converging evi- dence, but informants do not always agree, and this variance requires interpretation. First, discrepancies may be informative with respect to the different roles of informants (De Los Reyes, 2011). As the primary adults in their children's lives, parents may be particularly sensitive to their children's emotional problems or behavior, and they are likely to have had many more opportunities to observe them than have other significant adults such as teachers (Pharoah, Stevenson, Cooke, & Stevenson, 1994). Alternatively, discrepancies may accurately represent behavioral adaptations that are appropriate for different contexts, such as the home environ- ment versus the classroom. While attentional and behavioral de- mands may be relaxed in the informal milieu of the home, atten- tional and behavioral control is required in situations of formal learning. Second, discrepant information may represent legitimate differences in informants' perspectives. Where a parent or teacher might see particular behaviors as problematic, the child or adoles- cent in the situation may not, or vice versa. Third, subjective views about mental health and social functioning are important primary sources of information for older children, adolescents, and adults, given that agentic potential over one's own well-being increases with age. To obtain a comprehensive picture of mental health outcomes in ELBW, it is therefore important to compare informa- tion from multiple informants, including parents and teachers, and where possible, children.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Developmental Origins of Health and Disease (DOHaD) Hypothesis and Developmental Psychopathology", "text": "There is much clinical and scientific interest in the etiology of mental health problems that appear to develop disproportionately in children born at ELBW. Following the evolutionary principle of biological fitness, the DOHaD hypothesis posits that fetal adapta- tion to environmental influences encountered during gestation may increase susceptibility to chronic health problems later in life. These later-arriving consequences may be viewed as the down side of a necessary trade-off, that is, as distal sequelae of the adapta- tions that ensured fetal survival during an earlier, more difficult stage of development (Ellison, 2005;Gluckman, Hanson, & Buklijas, 2010;Nederhof & Schmidt, 2012). However, contrary to the view that prenatal conditions lead inexorably to chronic health problems in later life, the DOHaD framework suggests only that the risk for these outcomes may be increased, as a direct result of the early processes involved in adaptation. While being born at ELBW may increase the risk for chronic health problems, conver- gence of multiple risk factors over time may be necessary to precipitate actual manifestations of disease or disorder. The degree of causality between developmental processes dur- ing gestation and chronic conditions in later life is of great interest to researchers of mental as well as physical health disorders. Because causality with respect to development cannot ethically be tested by experiment, researchers are left to extrapolate from observational data provided by intriguing \"experiments of na- ture\"-one of which is preterm birth-to aid in answering the question of causation. The DOHaD hypothesis might be best tested by prematurity in its most extreme form, in individuals born at the lower limits of viability, either extremely preterm or small for gestational age (SGA). If early, adverse, environmental conditions are likely to affect later psychological functioning, then infants born extremely preterm are more likely than their NBW peers to experience mental health difficulties during the course of devel- opment.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Objectives", "text": "To our knowledge, no meta-analytic review has attempted to examine mental health problems in children, adolescents, and adults born at ELBW/EP. The present review had six objec- tives: (1) to identify the mental health problems presenting the greatest risks for individuals born at ELBW/EP by qualitatively and quantitatively summarizing the existing literature on these individuals and age-matched controls; (2) to describe these risks by developmental stage (e.g., childhood, adolescence, adulthood); (3) to explore nonrandom variability across studies by determining whether mental health outcomes in this popu- lation varied by region of birth, birth era, or NSI; (4) to examine informant effects on these findings; (5) to use these findings to guide recommendations for future research in the field; and (6) to test the DOHaD hypothesis with respect to mental health outcomes in this unique population.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Method", "text": "This systematic review was registered with the PROSPERO Inter- national prospective register of systematic reviews CRD42014015491 (Dobson, Van Lieshout, Schmidt, Mathewson, Chow). The Preferred Reporting items for Systematic Reviews and Meta-Analyses statement served as a guideline for its preparation (Moher, Liberati, Tetzlaff, & Altman, & the PRISMA Group, 2009).", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Study Selection Criteria", "text": "Observational designs such as case-control studies, and prospec- tive longitudinal and cross-sectional studies (excluding reviews) were eligible for this systematic review. The primary exposure of interest was ELBW, defined as a group-wise mean birth weight of \ud97b\udf591,000 g (2.2 lbs). Eligible studies contained participants with a mean age \ud97b\udf595 years at the time of outcome assessment. Second- ary exposures included extremely preterm birth (EP; i.e., with a group-wise mean gestational age \ud97b\udf5927 \ud97b\udf597 weeks). Three studies ( Hille et al., 2001;Natalucci et al., 2013;Sternqvist & Svenningsen, 1999) included cohorts whose mean gestational ages were \ud97b\udf5928 \ud97b\udf597 weeks, but these were retained because they identi- fied an extremely premature group on the basis of birth weight (\ud97b\udf591,000 g; Hille et al., 2001;Natalucci et al., 2013), or because the mean gestational age (27.1 week) would have met criteria but for a wide standard deviation (1.03;Sternqvist & Svenningsen, 1999). Other eligibility criteria stipulated that individuals born at ELBW/EP be compared with a control group of participants born at or near normal birth weight (mean weight \ud97b\udf592,500 g), or at, or near full term, but in any case, \ud97b\udf5937 weeks' gestational age. Finally, the control group could not constitute an entire population, as this would have biased our meta-analytic models. Outcomes of primary interest were levels of symptoms of psy- chological problems (means, standard deviations), or the risk of developing a clinically significant psychological disorder in ELBW survivors (e.g., odds ratios, risk ratios). Where sufficient studies were available, we also examined group differences in specific behavioral and social problems, including conduct disor- der, oppositional defiant disorder, and autism spectrum disorder. With respect to the possibility of Type I error, the level for significance was set at p \ud97b\udf59 .05.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Information Sources and Search Strategy", "text": "A systematic search was performed of electronic databases (MEDLINE, EMBASE, PsycINFO, Web of Science, ERIC, and CINAHL), from their inceptions until May 2016. The list was complemented by hand-searches of the references from relevant reviews and the 41 selected studies. The electronic search strategy included medical subject heading terms (MeSH) in MEDLINE, where keywords and text words were combined. The electronic search was based on five concepts: (1) term(s) related to ELBW: \"extremely low birth weight\"; (2) terms related to SGA: \"small for gestational age\"; (3) terms related to preterm birth: \"premature birth,\" \"very premature birth,\" \"extremely pre- mature birth,\" \"preterm birth,\" \"very preterm birth,\" \"extremely preterm birth\"; and (4) terms related to mental health: \"mental disorders,\" \"personality development,\" \"human development\"; and (5) those related to observational study designs: \"cohort anal- ysis,\" \"longitudinal study,\" \"epidemiology,\" \"prospective studies,\" \"retrospective studies,\" \"case-control,\" \"cross-sectional studies.\" Details of the search strategies used for each source are available in the online supplemental material. The search strategy was developed by the reviewers in consul- tation with a university research librarian. A similar search strategy was used for each database with appropriate modification of the search terms. There were no search language restrictions. Case studies, abstracts, conference presentations, editorials, unpublished studies (e.g., theses and dissertations), and grey literature (e.g., government reports) were included in the search. In addition, we conducted searches of several databases to help with identifying grey literature using (i.e., ERIC, Open Grey, The Health Care Management Information Consortium (HMIC) database, The National Technical Information Service, and PsycEXTRA). The progress of abstracts and papers or publications presented at con-ferences was tracked, but no eligible unpublished studies were identified. Hand searches of key journals within the field were also carried out, based on the reference lists from the published articles that were deemed eligible for inclusion in the study. ", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Study Selection", "text": "The screening process was completed independently by three reviewers (CHTC, KGD, KJM) on the basis of the study inclusion criteria outlined above (see Figure 1). Initially, titles and abstracts of each study were screened, at which point duplicate and nonrel- evant studies were removed. The full text of potentially relevant studies was further examined to determine whether inclusion cri- teria were met. In the case of disagreements, a fourth reviewer (RVL) was brought in to aid in resolution. All reviewers met and agreed on the final inclusion of studies (n \ud97b\udf59 41). All of the included studies were published between January 1990 and May, 2016.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Data Extraction", "text": "A data extraction form was specifically developed for this review. The form was pilot-tested using two randomly selected studies that met inclusion criteria, and refined if additional studies presented new information not covered by the pilot- tested version. The information extracted from each study in- cluded: (a) study methods, (b) sample demographics (e.g., year of birth, location, NSI, socioeconomic status), (c) outcome assessment details (i.e., types of mental disorders, scales used), (d) outcome data (e.g., means and standard deviations or odds ratios) and v) risk of bias assessments. Two reviewers (EIP, KGD) extracted data and information that were independently verified by one other reviewer (KJM).", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Risk of Bias in Individual Studies", "text": "The risk of bias in the included studies was assessed using the Newcastle-Ottawa Scale (NOS) for cohort studies (Wells et al., 2000). Two independent reviewers (EIP, KJM) scored each of the 41 studies, and any discrepancies between reviewers were resolved by consensus reached after discussion, with the pos- sibility of consulting a third reviewer if necessary. The median score given by each reviewer was 8, and agreement between the reviewers was calculated as 75.6%. Overall, the distribution of NOS scores ranged from 5 to 9 out of a possible maximum score of 9, with one study scoring 5, one scoring 6, six studies scoring 7, 28 studies scoring 8, and five studies scoring 9. The most common sources of bias in the studies related to the use of subjective reports to assess outcomes, followed distantly by drawing the control cohort from a different source, or inade- quate follow-up of cohorts.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Data Synthesis", "text": "Meta-analyses. In some cases, multiple studies were pub- lished on the same cohort during the same developmental stage. To avoid inappropriate double-counting of participants which may have influenced study weighting, only one study per age category was entered in any meta-analysis. When a choice was required, those studies with the best profile, for example, the largest sample sizes and broadest concept coverage, were selected for inclusion in meta-analyses. Other criteria included the presentation of mental health outcomes using means (and standard deviations) or odds ratios for each group (ELBW, NBW), along with the sample sizes for each group, to enable the calculation of standardized mean differences. Because the eligible studies varied in many respects, differences in mental health outcomes between ELBW and NBW were as- sessed using random effects meta-analyses, an approach that as- sumes the studies included in the analyses are random samples from a larger population of studies, and likely to exhibit different effect sizes (Borenstein, Hedges, Higgins, & Rothstein, 2009). We calculated mean effect sizes and their confidence intervals, along with prediction intervals derived from the samples to represent the dispersion of effects among the included studies (i.e., the range in which the effect size for any new study is likely to be found; Borenstein et al., 2009;Neyeloff, Fuchs, & Moreira, 2012). As the included studies assessed psychiatric problems with a variety of different measurement scales, we elected to use the standardized mean difference (SMD) from every study as the effect estimate in our meta-analyses (see Table 1). These effect sizes represent pooled estimates of the differences between ELBW and NBW participants. For studies presenting dichotomous out- comes (e.g., frequencies, odds ratios), SMDs were calculated using the following formula, SMD \ud97b\udf59 \ud97b\udf593/\ud97b\udf59 \ud97b\udf59 ln(OR). The standard error was then calculated by multiplying the SMD by \ud97b\udf593/\ud97b\udf59. Statistical formulae for converting dichotomous data into SMDs were sug- gested by Deeks, Higgins, and Altman (2011). These estimates were then converted to d for each symptom type, and adjusted to remove any bias accruing from the use of small samples, produc- ing estimates of Hedges' g ( Borenstein et al., 2009). Standardized mean differences were entered into Review Manager (RevMan 5.3) software as g values and their standard errors, for ELBW and NBW groups separately, to compare group differences across studies. Each of the meta-analyses, subgroup, and sensitivity anal- yses analyzed estimates of g from the mental health outcomes data eligible for analysis. We examined symptoms of the following emotional and behav- ioral problems in separate analyses: attention problems (e.g., symptoms of the hyperactive/impulsive, inattentive, and combined subtypes of ADHD), internalizing, externalizing, conduct disorder, oppositional disorder, social problems and autism spectrum disor- der. The three ADHD subtypes are distinct entities, with the combined subtype incorporating symptoms of both hyperactivity/ impulsivity and inattention (Barkley, 1997). Internalizing prob- lems were defined by the internalizing scales of the appropriate instruments (e.g., Child Behavior Checklist [CBCL]; the Strengths and Difficulties Questionnaire [SDQ]), and typically represent a composite of anxiety, withdrawal, and depressive symptoms (Achenbach, 1991;Goodman, 2001). Externalizing is character- ized by measures of disruptive and aggressive behavior (e.g., Achenbach, 1991;Goodman, 2001). Hyperactivity that is func- tionally aligned with attention deficits is thought to be distinct from delinquency and aggression, and so was examined separately from externalizing problems. Given their identity as distinct and meaningful Diagnostic and Statistical Manual (DSM)-based clin- ical syndromes (Achenbach, Bernstein, & Dumenci, 2005), and because they were reported separately from externalizing problems in many of the studies in this review, conduct problems and oppositional defiant disorder difficulties were also examined in separate meta-analyses. The reasons for performing separate analyses by problem-type were several-fold. First, there may have been heterotypic continu- ity in this population with respect to some disorders, and homo- typic continuity for others. Second, each of the specific disorders is separated diagnostically in authoritative references such as the DSM and International Classification of Diseases because they are thought to represent different entities, both theoretically and clin- ically. Therefore, they were examined separately because we were concerned that combining them could obscure important effects and differences. Third, and perhaps most importantly, we wished this review to not only inform theory, but also include information on specific diagnoses that can guide clinical detection and treat- ment decision-making for clinicians, survivors, and families, and provide anticipatory guidance for these groups. Understanding the specific problems faced by individuals born preterm and their families can also serve as the basis for resource allocation by health care systems. For each category of emotional functioning, data were analyzed from as many studies as possible (ranging from 2 to 13), with each unique cohort represented only once in any given analysis. As- sessment measures were deemed eligible for meta-analysis by the comparability of their items and concepts. Comparability was based on careful examination of the individual items in each scale from each study, to determine whether measures from different studies were functionally equivalent. Analyses were stratified and examined by age group (child, adolescent, adult), to account for changes related to development. In addition, because some cohorts were represented by studies from more than one developmental period, some studies would have been eliminated from any of the analyses if we had we combined studies from all age groups. For the eligible studies, the period of childhood spanned an age range of age 5 to 13 years, adolescents included youth aged 14 to 18, and adulthood was defined as 19 years or older. Adult outcomes were not meta-analyzed because five of the six available adult studies represented the same cohort, and the data provided for the remain- ing cohort did not permit calculation of standardized mean differ- ences. Analyses were also stratified by informant (parent, teacher, or self-report), to examine possible informant discrepancies that have been reported in some instances (e.g., Szatmari et al., 1993). Where an analysis included 10 or more studies, funnel plots were created to assess the risk of small-study effects among the studies in that analysis (Sterne, Egger, & Moher, 2011). As the degree of funnel plot asymmetry (representing the extent to which the find- ings are biased) is difficult to assess visually, the funnel plots were evaluated using Egger's test for bias (Egger, Davey Smith, Schneider, & Minder, 1997). Subgroup analyses. In addition to meta-analysis, tests of non- random variability across studies are critical to the interpretation of the results, because they may indicate the reliability of an effect. A major purpose of the review was to examine how important sociodemographic, secular treatment, and physical health-related factors affected risk for adverse mental health outcomes. Accord- ingly, outcomes were analyzed by geographical region (i.e., Aus- tralia, Europe, North America), by birth era (e.g., birth prior to 1990 vs. in 1990 or later), and by the inclusion vs. exclusion of ELBW individuals with significant NSI in each study. We also compared reports of mental health outcomes completed by differ- ent participating informants, to ascertain whether any discrepan- cies were meaningful and important. Finally, we performed a series of sensitivity analyses to assess whether studies at risk of greater or lesser bias generated different findings.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Study Sample Characteristics", "text": "The 41 studies of mental health outcomes suitable for inclusion represented a total of 13,839 participants, 2,712 of whom were born at ELBW, and 11,127 who were born at NBW. (Each par- ticipant who contributed data was counted once across all studies). Sample sizes for the ELBW groups ranged from 21 to 408 indi- viduals (Mdn \ud97b\udf59 83), and for NBW controls, from 30 to 4,000 individuals (Mdn \ud97b\udf59 108). Mean birth weight for ELBW partici- pants was 840 (92) g, and for NBW controls, 3,343 (387) g. Mean gestational age was 26.6 (1.0) weeks for participants born at ELBW, versus 39.7 (0.7) weeks for NBW controls. Controls were matched with cases on at least one demographic variable, usually age at the time of a childhood assessment. Con- trols were also matched on sex, race, geographic region of birth or birth hospital, and/or familial socioeconomic status, in many stud- ies. Mean ELBW age by category was 8.4 (3.9) years across all child studies, 16.7 (3.7) years for adolescent studies, and 24.7 (8.9) years across all adult studies. ELBW sex distributions by category were 47 (0.9)% males across all child studies, 37 (0.9)% males for studies of adolescents, and 40 (0.4)% males across all adult stud- ies. Demographic characteristics and information on study mea- sures are presented in Table 1. Forty-one studies reported on 24 unique samples born in Aus- tralia (n \ud97b\udf59 3), New Zealand (n \ud97b\udf59 1), North America (Canada, n \ud97b\udf59 3; United States, n \ud97b\udf59 5), and Europe (n \ud97b\udf59 12), (including Germany [n \ud97b\udf59 3], Switzerland [n \ud97b\udf59 1], the Netherlands [n \ud97b\udf59 1], the United Kingdom [n \ud97b\udf59 2] and the Nordic countries [n \ud97b\udf59 5]). Nineteen studies assessed cohorts born before 1990, and 22 assessed cohorts born after 1990. Sixteen of the 41 studies provided analyses that included individuals with significant NSI, 15 studies provided analyses from which individuals with significant NSI were ex- cluded, and 10 studies reported both. Meta-analytic results are presented separately below by age group (children, adolescents, adults), informant (parent, teacher, self), and condition (e.g., at- tention, internalizing, externalizing, conduct problems, opposi- tional problems, social problems, and autistic symptoms). Twenty- five studies of children (aged 5 to 12 years), 10 studies of adolescents (aged 14 to 18 years), and six studies of adults (\ud97b\udf5919 years) were included in this systematic review.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Meta-Analytic Results", "text": "Childhood. Results from random effects meta-analyses for children (including summary effect sizes and their 95% CIs, the variance among the study effect sizes (T 2 ), the ratio of observed variance to within-study error (Q), the proportion of the ob- served variance that is due to between-study differences in effect sizes (I 2 ), and the dispersion of the effect sizes (95% prediction intervals) are presented in Table 2. Effect sizes were interpreted as small (\ud97b\udf590.30), moderate (0.30 to 0.60), or large (\ud97b\udf590.60; Borenstein et al., 2009). Childhood attention problems. Parents reported significantly higher scores for all three types of ADHD in ELBW children relative to NBW controls. Summary effect sizes across ADHD types were moderate-to-large, being greatest for combined ADHD (g \ud97b\udf59 0.68, 95% CI \ud97b\udf59 0.56 to 0.80), followed by inattentive ADHD (g \ud97b\udf59 0.58, 95% CI \ud97b\udf59 0.39 to 0.77), and hyperactive ADHD (g \ud97b\udf59 0.46, 95% CI \ud97b\udf59 0.37 to 0.55). These findings suggested that symptoms of ADHD are elevated in ELBW children relative to their NBW peers. The analyses indicated significant nonrandom variability among the effect sizes for combined ADHD (T 2 \ud97b\udf59 0.02, Q \ud97b\udf59 22.15, p \ud97b\udf59 .04, I 2 \ud97b\udf59 46%) and inattentive ADHD (T 2 \ud97b\udf59 0.03, Q \ud97b\udf59 12.07, p \ud97b\udf59 .03, I 2 \ud97b\udf59 59%), where about half of the variation in effect sizes was attributable to substantive or methodological dif- ferences among studies. For hyperactive ADHD, findings across studies were more consistent (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 7.22, p \ud97b\udf59 .50, I 2 \ud97b\udf59 0%). (See Figure 2A, and Figures 13A and 14A in the online supplemental materials). Standardized mean differences and stan- dard errors from the 13 children's studies of combined ADHD were analyzed for publication bias by funnel plot. The results formed a symmetrical, inverted funnel-shaped pattern about a mean effect size of approximately 0.70. Although one outlier study showed more extreme scores, it was nonetheless represented within the funnel outlined by the other studies (see Figure 3A). Egger's test ( Egger et al., 1997) revealed no significant asymmetry in the funnel plot, indicating a relatively low risk of bias (small- study effects) among these studies (intercept \ud97b\udf59 0.25, 95% CI \ud97b\udf59 \ud97b\udf591.65 to 2.16, p \ud97b\udf59 .75). Summary effects for teacher reports of ELBW children were small-to-moderate with respect to combined ADHD (g \ud97b\udf59 0.54, 95% CI \ud97b\udf59 0.29 to 0.79), hyperactive ADHD (g \ud97b\udf59 0.35, 95% CI \ud97b\udf59 0.19 to 0.50) and inattentive ADHD, (g \ud97b\udf59 0.54, 95% CI \ud97b\udf59 0.27 to 0.82). The analyses for combined ADHD (T 2 \ud97b\udf59 0.07, Q \ud97b\udf59 18.78, p \ud97b\udf59 .01, I 2 \ud97b\udf59 68%) and inattentive ADHD (T 2 \ud97b\udf59 0.04, Q \ud97b\udf59 6.20, p \ud97b\udf59 .10, I 2 \ud97b\udf59 52%) suggested substantial variability across studies, whereas effect sizes were more consistent for the hyperactive type of ADHD (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.89, p \ud97b\udf59 .30, I 2 \ud97b\udf59 0%). (See Figure  2B, and Figures 13B and 14B in the online supplemental materi-als.) The patterns of results for parent-rated and teacher-rated symptoms across ADHD types were generally comparable, in that both informants located greater attention problems in ELBW groups. Childhood internalizing and externalizing problems. Parents also reported significantly higher internalizing and externalizing scores in ELBW children than in NBW controls, with a moderate summary effect for internalizing (g \ud97b\udf59 0.42 95% CI \ud97b\udf59 0.26 to Figure 2. Forest plots of (A) parent-and (B) teacher-reported attention-deficit/hyperactivity disorder (ADHD; combined type) in children. ELBW \ud97b\udf59 extremely low birth weight; NBW \ud97b\udf59 normal birth weight. See the online article for the color version of this figure. Note. Dashes indicate that there were insufficient data for meta-analysis. g \ud97b\udf59 standard mean difference; \ud97b\udf59 2 \ud97b\udf59 between-studies variance; Q \ud97b\udf59 ratio of variation to within-study error; I 2 \ud97b\udf59 the proportion of total observed variation attributed to between-study effects; Prediction interval \ud97b\udf59 the dispersion of true effect sizes; ADHD \ud97b\udf59 attention-deficit/hyperactivity disorder; ODD \ud97b\udf59 oppositional defiant disorder. a p \ud97b\udf59 .05, significant differences between the ELBW and NBW groups or significant Q values. \u2020 p \ud97b\udf59 .06. 0.58), and a small effect for externalizing (g \ud97b\udf59 0.15, 95% CI \ud97b\udf59 0.02 to 0.28). (See Figure 4A, and Figure 15A in the supplemental materials.) The analysis of parent-reported internalizing symptoms indicated significant nonrandom variability across the 11 studies (T 2 \ud97b\udf59 0.05, Q \ud97b\udf59 36.62, p \ud97b\udf59 .0001, I 2 \ud97b\udf59 73%), suggesting meaningful methodological or other differences among them. The standardized mean differences and standard errors for internalizing data from these studies were analyzed by funnel plot, where they formed a symmetrical pattern about a mean effect size of approx- imately 0.4 (see Figure 3B). Egger's test revealed no significant statistical asymmetry in this meta-analysis, consistent with a rel- atively low risk of bias (small-study effects) among these studies (intercept \ud97b\udf59 \ud97b\udf592.09, 95% CI \ud97b\udf59 \ud97b\udf595.18 to 0.99, p \ud97b\udf59 .15). Effect sizes for parent reports of externalizing problems in ELBW chil- dren were consistent (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 2.05, p \ud97b\udf59 .70, I 2 \ud97b\udf59 0%). Effect sizes for teachers were similar, with greater internalizing (g \ud97b\udf59 0.32, 95% CI \ud97b\udf59 0.12 to 0.52) and externalizing (g \ud97b\udf59 0.14, 95% CI \ud97b\udf59 0.00 to 0.29) difficulties in ELBW than NBW children. (See Figure 4B, and Figure 15B in the online supplemental mate- rials.) The analysis suggested some variation in the effect sizes for internalizing (T 2 \ud97b\udf59 0.03, Q \ud97b\udf59 12.50, p \ud97b\udf59 .03, I 2 \ud97b\udf59 60%). Although effect sizes from individual studies of teacher-reported externaliz- ing were consistent, (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.03, p \ud97b\udf59 .95, I 2 \ud97b\udf59 0%), only three studies contributed to the analysis, suggesting that teacher- rated externalizing in ELBW children reported should be inter- preted with caution. Childhood conduct and oppositional behaviors. Parents re- ported small but significant effects of birth weight status for conduct disorder (g \ud97b\udf59 0.23, 95% CI \ud97b\udf59 0.09 to 0.37), whereby children born at ELBW showed more symptoms of conduct dis- order relative to NBW controls (see Figure 5A). The analysis indicated significant variation in the effect sizes (T 2 \ud97b\udf59 0.02, Q \ud97b\udf59 18.95, p \ud97b\udf59 .02, I 2 \ud97b\udf59 58%). In contrast, groups did not differ in parent ratings of oppositional defiant disorder (g \ud97b\udf59 0.14, 95% CI \ud97b\udf59 \ud97b\udf590.01 to 0.28). (See Figure 16A in the online supplemental materials.) The effect sizes for oppositional defiant disorder were consistent across four studies (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.54, p \ud97b\udf59 .90, I 2 \ud97b\udf59 0%). However, we note that the confidence interval for this sum- mary effect included zero, suggesting inconsistency in the group findings across studies. Teachers also reported small, marginally significant effects of birth weight status for conduct problems (g \ud97b\udf59 0.19, 95% CI \ud97b\udf59 \ud97b\udf590.01 to 0.38), although the confidence interval again included zero. The effect size for one study indicated more conduct problems in NBW controls (g \ud97b\udf59 \ud97b\udf590.78, 95% CI \ud97b\udf59 \ud97b\udf591.68 to 0.12; Szatmari et al., 1993), in contrast to four others that found conduct problems primarily in ELBW children (gs \ud97b\udf59 0.16), (T 2 \ud97b\udf59 0.01, Q \ud97b\udf59 5.31, p \ud97b\udf59 .25, I 2 \ud97b\udf59 25%). (See Figure 5B.) Teachers also reported significant levels of oppositional defiant disorder (g \ud97b\udf59 0.79, 95% CI \ud97b\udf59 0.40 to 1.17) in ELBW children relative to their NBW peers (T 2 \ud97b\udf59 0.02, Q \ud97b\udf59 1.11, p \ud97b\udf59 .29, I 2 \ud97b\udf59 10%), but the variance in one study (95% CI \ud97b\udf59 \ud97b\udf590.86 to 1.35) was greater than in the other (95% CI \ud97b\udf59 0.55 to 1.17), even though both studies produced positive effects (gs \ud97b\udf59 0.25). Interpretation of this anal- ysis should be cautious, as only two studies contributed to the analysis. (See Figure 16B in the online supplemental materials.) Childhood social problems and autistic symptoms. Parents reported significant, moderate effects of social problems (g \ud97b\udf59 0.46, 95% CI \ud97b\udf59 0.31 to 0.61) in ELBW children relative to their NBW peers. (See Figure 6A.) The analysis indicated some varia- tion in the effect sizes (T 2 \ud97b\udf59 0.02, Q \ud97b\udf59 9.01, p \ud97b\udf59 .06, I 2 \ud97b\udf59 56%) related to methodological or substantive differences among the studies. Parents also reported significantly more symptoms of autism spectrum disorders (ASD) in ELBW than NBW children (g \ud97b\udf59 0.56, 95% CI \ud97b\udf59 0.29 to 0.83). (See Figure 6B.) There was significant variation in the effect sizes among the three studies that contributed to the analysis (T 2 \ud97b\udf59 0.04, Q \ud97b\udf59 5.83, p \ud97b\udf59 .05, I 2 \ud97b\udf59 Figure 3. Funnel plots for parent ratings of (A) attention-deficit/hyperactivity disorder (ADHD; combined type) and (B) internalizing in children. See the online article for the color version of this figure. 66%). Too few teacher reports of social problems and symptoms of ASD were available to meta-analyze studies of these problems. Childhood prediction intervals. We calculated 95% predic- tion intervals to indicate the range in which the effect sizes for each childhood disorder were found, with narrower intervals indi- cating greater consistency among findings. (See Table 2.) In parent ratings of children, the smallest prediction interval-for symptoms of ASD-was still relatively wide (0.31 to 0.80), suggesting con- siderable variation across studies in parent reports of these symp- toms. The widest prediction intervals for parent ratings of children were for externalizing (\ud97b\udf590.57 to 0.86) and oppositional defiant disorder (\ud97b\udf590.73 to 1.01). More importantly, five of the prediction intervals from parent ratings crossed the zero mark (hyperactive ADHD, internalizing, externalizing, conduct disorder, and oppo- sitional defiant disorder), indicating that some of the studies in each of these meta-analyses identified negative effects (where NBW children have more problems), whereas others identified positive effects (where ELBW children have more problems). In teacher ratings, analyses of combined ADHD (0.39 to 0.69) and internalizing disorders (0.17 to 0.47) generated the most consistent findings, with the smallest prediction intervals. The widest predic- tion interval for teacher ratings was for childhood externalizing problems (\ud97b\udf591.03 to 1.31). Like parent ratings, some of the findings from teacher reports appeared to be unstable, as five of the prediction intervals from teacher ratings crossed the zero mark (hyperactive and inattentive ADHD, externalizing, conduct disorder, and oppositional defiant disorder), suggesting teachers in some studies identified more of these problems in NBW children, whereas others found more problems in ELBW children. Adolescence. Results from random effects meta-analyses of mental health outcomes in adolescents are presented in Table 3. As teacher reports were provided in only two studies (20%)  of this age group, we elected to analyze mental health ratings provided only by parents and teens. Adolescent attention problems. Meta-analyses revealed sig- nificant, small to moderate effects of birth weight status for pa- rental assessments of combined ADHD (g \ud97b\udf59 0.52, 95% CI \ud97b\udf59 0.19 to 0.85), hyperactive ADHD (g \ud97b\udf59 0.26, 95% CI \ud97b\udf59 0.10 to 0.43), and inattentive ADHD (g \ud97b\udf59 0.40, 95% CI \ud97b\udf59 0.24 to 0.56). (See Figures 17A to 19A in the supplemental materials.) Although these results suggested that ELBW teens were more likely than NBW controls to exhibit symptoms specific to the combined type of ADHD, there was considerable variation among the study effect sizes (T 2 \ud97b\udf59 0.12; Q \ud97b\udf59 30.23, p \ud97b\udf59 .00001, I 2 \ud97b\udf59 87%). There was little variation among studies of parent-reported hyperactive ADHD (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.50, p \ud97b\udf59 .45, I 2 \ud97b\udf59 0%) and inattentive ADHD (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.05, p \ud97b\udf59 .80, I 2 \ud97b\udf59 0%), but each of these analyses was based on two studies. As such, these findings should be interpreted with caution. In contrast, adolescent self-reports revealed no consistent group differences in problems related to combined ADHD (g \ud97b\udf59 \ud97b\udf590.03, 95% CI \ud97b\udf59 \ud97b\udf590.28 to 0.23). Notably, the confidence interval for this summary effect crossed the zero mark, indicating that some studies suggested that NBW teens had greater problems related to com- bined ADHD, whereas others suggested that ELBW teens had more difficulties. (See Figures 17B to 19B in the online supple- mental materials.) This analysis revealed considerable method- ological or other variability among the individual effects related to adolescent reports (T 2 \ud97b\udf59 0.07, Q \ud97b\udf59 19.95, p \ud97b\udf59 .0005, I 2 \ud97b\udf59 80%). In addition, adolescent self-reports revealed significant effects of birth weight status for both hyperactive (g \ud97b\udf59 \ud97b\udf590.23, 95% CI \ud97b\udf59 \ud97b\udf590.39 to \ud97b\udf590.07), and inattentive ADHD problems (g \ud97b\udf59 \ud97b\udf590.23, 95% CI \ud97b\udf59 \ud97b\udf590.38 to \ud97b\udf590.07), that pointed to greater attentional difficulties in NBW teens. Within these analyses, the effect sizes from the indi- vidual studies were very consistent (hyperactive ADHD; T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.25, p \ud97b\udf59 .60, I 2 \ud97b\udf59 0%; inattentive ADHD; T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.00, p \ud97b\udf59 .90, I 2 \ud97b\udf59 0%), although in each case, summary effects were based on only two studies, suggesting these findings should be interpreted with caution. Adolescent internalizing and externalizing problems. Parents reported that ELBW teens had significantly more prob- lems with internalizing than did NBW teens (g \ud97b\udf59 0.51, 95% CI \ud97b\udf59 0.26 to 0.76). However, they indicated no group differ- ence for externalizing, generating only a small, nonsignificant effect of birth weight status (g \ud97b\udf59 0.29, 95% CI \ud97b\udf59 \ud97b\udf590.26 to 0.84). (See Figures 20A and 21 in the online supplemental materials.) There was little variation among the individual effects of the three studies of the internalizing (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 1.24, p \ud97b\udf59 .50, I 2 \ud97b\udf59 0%), whereas the individual effects of the two studies of externalizing tended to diverge (T 2 \ud97b\udf59 0.11, Q \ud97b\udf59 3.43, p \ud97b\udf59 .06, I 2 \ud97b\udf59 71%). The nonsignificant effect for parent- reported externalizing in ELBW adolescents should be inter- preted with caution due to the small number of studies contrib- uting to the analysis, and because the confidence interval for this effect crossed the zero line, indicating that the two studies reported greater externalizing in different groups. Adolescents themselves reported no significant group differ- ences in internalizing behavior (g \ud97b\udf59 0.31, 95% CI \ud97b\udf59 \ud97b\udf590.44 to 1.06). The confidence interval for this summary effect also crossed the zero mark, reflecting inconsistency in the effect sizes of the two studies, indicated by a similar trend in the meta-analysis (T 2 \ud97b\udf59 0.20, Q \ud97b\udf59 2.77, p \ud97b\udf59 .10, I 2 \ud97b\udf59 64%). (See Figure 20B in the supplemental materials.) Only one study provided teens' self- reported estimates of externalizing, precluding meta-analysis. More data are needed before firm conclusions can be drawn about internalizing and externalizing tendencies in ELBW adolescents. Adolescent conduct and oppositional behaviors. Meta- analyses indicated no significant effects of birth status for either parent-reported conduct disorder (g \ud97b\udf59 \ud97b\udf590.30, 95% CI \ud97b\udf59 \ud97b\udf591.58 to 0.98) or oppositional defiant disorder (g \ud97b\udf59 \ud97b\udf590.03, 95% CI \ud97b\udf59 \ud97b\udf590.21 to \ud97b\udf590.14) among adolescents. (See Figures 22A and  23A in the online supplemental materials). Similar to parent- reported externalizing, the confidence interval for the summary effect for conduct problems crossed the zero mark, reflecting disparity among the studies (particularly between the Gardner (g \ud97b\udf59 0.43, 95% CI \ud97b\udf59 \ud97b\udf590.14 to 1.00) and Taylor studies (g \ud97b\udf59 \ud97b\udf591.69, 95% CI \ud97b\udf59 \ud97b\udf591.96 to \ud97b\udf591.42), and wide within-study variation in the Saigal study (g \ud97b\udf59 \ud97b\udf590.02, 95% CI \ud97b\udf59 \ud97b\udf591.41 to 1.37). The wide disparity among effect sizes was confirmed by analysis (T 2 \ud97b\udf59 1.56, Q \ud97b\udf59 77.58, p \ud97b\udf59 .0001, I 2 \ud97b\udf59 96%). In contrast, the analysis of parent-reported oppositional defiant disorder suggested no signif- icant difference between studies, (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.52, p \ud97b\udf59 .45, I 2 \ud97b\udf59 0%), although this null finding rests on only two studies, and should be interpreted with caution. Adolescents themselves reported no significant group difference between ELBW and NBW teens with respect to conduct disorder (g \ud97b\udf59 \ud97b\udf590.17, 95% CI \ud97b\udf59 \ud97b\udf590.38 to 0.05), but the confidence interval for the conduct disorder effect crossed the zero mark, where three studies reported greater risk of conduct disorder in NBW teens (gs \ud97b\udf59 \ud97b\udf590.05), while one study reported no mean difference but showed wide within-study variation (Gardner: g \ud97b\udf59 0.03, 85% CI \ud97b\udf59 \ud97b\udf590.61 to 0.67). (See Figure 22B in the supplemental mate- rials.) There was some variation among the study effect sizes (T 2 \ud97b\udf59 0.02, Q \ud97b\udf59 6.58, p \ud97b\udf59 0.09, I 2 \ud97b\udf59 54%). With respect to oppositional defiant disorder, adolescents reported higher levels among NBW teens (g \ud97b\udf59 \ud97b\udf590.34, 95% CI \ud97b\udf59 \ud97b\udf590.54 to \ud97b\udf590.13). (See Figure 23B in the online supplemental materials.) Effect sizes for oppositional defiant disorder were consistent (g \ud97b\udf59 \ud97b\udf590.44 vs. \ud97b\udf590.23; T 2 \ud97b\udf59 0.01, Q \ud97b\udf59 1.45, p \ud97b\udf59 .20, I 2 \ud97b\udf59 31%), although again, this finding was based on two studies and merits cautious interpretation. Adolescent social problems. Parents reported significantly more social difficulties in ELBW than NBW teens (g \ud97b\udf59 0.52, 95% CI \ud97b\udf59 0.00 to 1.03), with a marginal difference between the effects from these two studies (T 2 \ud97b\udf59 0.08, Q \ud97b\udf59 2.18, p \ud97b\udf59 .10, I 2 \ud97b\udf59 54%). (See Figure 24A in the online supplemental materials.) Adoles- cents themselves indicated no significant differences between ELBW and NBW teens with respect to social problems (g \ud97b\udf59 0.21, 95% CI \ud97b\udf59 \ud97b\udf590.16 to 0.57). (See Figure 24B in the online supple- mental materials.) Effect sizes for adolescent-reported social prob- lems were small and consistent (T 2 \ud97b\udf59 0.00, Q \ud97b\udf59 0.15, p \ud97b\udf59 .65, I 2 \ud97b\udf59 0%), but we note wide variation within the Gardner study (g \ud97b\udf59 0.44, 95% CI \ud97b\udf59 \ud97b\udf590.81 to 1.69). Adolescent prediction intervals. All results from the parent and self-reports of adolescents should be interpreted with caution, however, as they are based on small numbers of studies. Moreover, the 95% prediction intervals for the summary effects are very wide, suggesting considerable variation among the effect sizes from different studies. Importantly, for every meta-analyses for this age group, the prediction intervals for parent-ratings of ado- lescents and adolescent self-ratings crossed the zero mark. (See Table 3.) The smallest prediction interval-for parent ratings of social problems (\ud97b\udf590.21 to 1.24)-was still substantial, and the largest prediction intervals-for parent-rated conduct disorder (\ud97b\udf594.79 to 4.19) and externalizing (\ud97b\udf595.26 to 5.85)-were very wide. In adolescent self-ratings, the smallest prediction interval- for conduct disorder (\ud97b\udf590.77 to 0.44)-was substantial, yet con- trasted with much larger prediction intervals for social problems (\ud97b\udf595.30 to 5.71) and internalizing (\ud97b\udf597.12 to 7.75). Thus, associa- tions between birth weight status and either parent-or self-rated mental health problems in adolescence appeared to be unstable and contradictory. Narrative findings. The majority (77%) of eligible studies contributed data to the meta-analyses. Only five studies of children ( Hille et al., 2001;Horwood et al., 1998;Johnson et al., 2010b;Szatmari, Saigal, Rosenbaum, Campbell, & King, 1990; Whitfield, Grunau, & Holsti, 1997) and three studies of adolescents, (Burnett et al., 2014;Hallin & Stjernqvist, 2011b;Meth\u00fasalemsd\u00f3ttir, Egilson, Gu\u00f0mundsd\u00f3ttir, Valdimarsd\u00f3ttir, & Georgsd\u00f3ttir, 2013) were not analyzed. Reasons for exclusion included data that were redundant with those from another related study which were ana- lyzed ( Burnett et al., 2014;Hallin et al., 2011b;Johnson et al., 2010b;Szatmari et al., 1990), insufficient information to calculate effect sizes ( Hille et al., 2001), averaged parent/teacher responses ( Horwood et al., 1998), assessment by a clinician only ( Whitfield et al., 1997), and in one case, measurement instruments that were significantly different from those selected for meta-analysis ( Meth\u00fasalemsd\u00f3ttir et al., 2013). Effect sizes for these omitted studies are available in Table 1 for comparison with effect sizes of those that were meta-analyzed. In line with the findings from the meta-analyses, Horwood et al. (1998)    group differences in quality of life between in 17-year-old Icelan- dic teens born at ELBW and their NBW peers, which favored the normal group: psychological well-being (g \ud97b\udf59 \ud97b\udf591.06, SE \ud97b\udf59 0.64) and overall mood (g \ud97b\udf59 \ud97b\udf590.92, SE \ud97b\udf59 0.61) were notably lower in ELBW teens. Adulthood. Six studies of adults (aged 22 to 36 years) met inclusion criteria for the review. Data were collected from self- reports or assessments by trained interviewers (see Table 1). These were not meta-analyzed because five of the studies collectively represented the same unique cohort, and the data provided for the remaining cohort did not permit calculation of standardized mean differences. A narrative description of these findings follows to summarize the findings for the reader. Adult attention problems. No group differences in the risks for ADHD problems reached significance in studies of adult ELBW survivors (e.g., Boyle  Adult internalizing, externalizing, and social problems. The risk for self-reported internalizing problems was significantly greater in young adults born at ELBW than in their NBW peers (d \ud97b\udf59 0.42, p \ud97b\udf59 .01;Boyle et al., 2011). Similarly, shyness persisted in the ELBW group in adulthood (d \ud97b\udf59 0.38, p \ud97b\udf59 .03;Schmidt, Miskovic, Boyle, & Saigal, 2008), and adult survivors were at higher risk for self-reported anxiety problems (d \ud97b\udf59 0.34, p \ud97b\udf59 .02;Boyle et al., 2011). If they had been exposed to cortico- steroids prenatally, they were also at higher risk for clinical levels of anxiety, for example, generalized anxiety disorder (OR \ud97b\udf59 3.42, 95% CI \ud97b\udf59 1.06 to 11.06) and social anxiety disorder (OR \ud97b\udf59 5.80, 95% CI \ud97b\udf59 1.20 to 27.99, ps \ud97b\udf59 .05), as assessed by structured psychiatric interview (Van Lieshout et al., 2015a). In a European cohort ( Natalucci et al., 2013), self-reported mental health (d \ud97b\udf59 \ud97b\udf590. 43, p \ud97b\udf59 .01) and social functioning (d \ud97b\udf59 \ud97b\udf590.40, p \ud97b\udf59 .05) were poorer in young adults born at ELBW, in comparison to community norms. In addition, distress related to interpersonal sensitivity (d \ud97b\udf59 0. 21, p \ud97b\udf59 .05) was higher. However, Figure 6. Forest plots of parent-reported (A) social problems and (B) autism spectrum disorder (ASD) symptoms in children. ELBW \ud97b\udf59 extremely low birth weight; NBW \ud97b\udf59 normal birth weight. See the online article for the color version of this figure. adults from this cohort reported no group differences in anxiety problems (d \ud97b\udf59 0.24, p \ud97b\udf59 .10;Natalucci et al., 2013). As in adolescence, externalizing problems at age 22 to 26 did not differ between groups (Boyle et al., 2011: d \ud97b\udf59 0.01, p \ud97b\udf59 .90). The adult risk for substance use disorders was significantly lower in ELBW survivors than in NBW controls (Van Lieshout et al., 2015a: OR \ud97b\udf59 0.38, 95% CI \ud97b\udf59 0.17 to 0.86, p \ud97b\udf59 .05). In general, mental health problems related to internalizing, anxiety, and/or interpersonal sensitivity appeared to persist in adults born at ELBW, particularly those exposed to antenatal corticosteroids, whereas the risk for substance-related disorders was lower than in NBW controls. Indices of mental well-being were also lower in adult survivors.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Subgroup Analyses", "text": "To explore the possibility that nonrandom variability may ac- count for some of the disparities among study effects, regional variations, differences in birth era (pre-1990 vs. post-1990), and the effects of including or excluding individuals with NSI were assessed in subgroup analyses. Parental reports were numerous enough for analysis of most of the childhood problems reported. Parental and self-reports for combined ADHD in adolescents also permitted meaningful analysis. Influence of regional differences. Tests for significant re- gional differences in mental health outcomes were conducted in RevMan 5.3. If the overall regional test for a mental health outcome was significant, we then conducted separate pairwise Z tests of the effect sizes from Australia versus Europe, Europe versus North America, and North America versus Australia for that outcome. Results are presented in Table 4, Figures 7 to 9, and Figures 25 to 32 in the online supplemental materials. There were significant overall regional differences only for parent-rated childhood inattentive ADHD, (Q \ud97b\udf59 6.32, p \ud97b\udf59 .04, I 2 \ud97b\udf59 68%; see Figure 26 in the online supplemental materials), internalizing, (Q \ud97b\udf59 7.58, p \ud97b\udf59 .02, I 2 \ud97b\udf59 74%; see Figure 8), and ASD symptoms (Q \ud97b\udf59 4.96, p \ud97b\udf59 .03, I 2 \ud97b\udf59 80%; see Figure 31 in the online supplemental materials), and also for parent-rated combined ADHD in adolescence (Q \ud97b\udf59 30.19, p \ud97b\udf59 .00001, I 2 \ud97b\udf59 93%; Figure 9). None of the pairwise Z tests was significant for parent-rated inattentive ADHD (all ps \ud97b\udf59 .24). In contrast, the effect sizes for both parent-rated childhood internalizing (Z \ud97b\udf59 2.69, p \ud97b\udf59 .01) and ASD symptoms (Z \ud97b\udf59 2.19, p \ud97b\udf59 .03), were significantly larger in Europe than in North America. Effect sizes for parent-rated ado- lescent combined ADHD were also significantly larger in Europe than North America (Z \ud97b\udf59 5.29, p \ud97b\udf59 .001), or Australia (Z \ud97b\udf59 \ud97b\udf594.62, p \ud97b\udf59 .001). No other regional differences reached significance. Results from the adolescent analyses should be interpreted cautiously due to the small numbers of studies available for each region (especially Australia, which is represented by one or two studies in these subgroup analyses). Influence of birth era. We also conducted pairwise Z tests to ascertain whether the effect sizes for birth cohorts born before 1990 differed from those born in 1990 or later. Results of these subgroup analyses are presented in Table 5, Figure 10, and Figures 33 to 38 in the online supplemental materials. Only one significant difference emerged (Q \ud97b\udf59 5.91, p \ud97b\udf59 .02, I 2 \ud97b\udf59 83%; see Figure 10). In adolescent self-reports, teens born prior to 1990 were more likely to identify high levels of ADHD (combined) in ELBW groups, whereas teens born in 1990 or later were more likely to identify high levels of ADHD (combined) in NBW groups, a contrast that was statistically significant (Z \ud97b\udf59 \ud97b\udf592.43, p \ud97b\udf59 .02). However, the overall pattern of tests related to birth era suggested that most mental health outcomes did not differ between children or adolescents who were born before versus after 1990. Influence of NSI. Pairwise Z tests were conducted to test whether mental health outcomes differed depending on the inclu- sion of individuals with NSI. Results from these subgroup analyses are presented in Table 6, Figure 11, and Figures 39 to 48 in the online supplemental materials. Only one significant difference emerged (Q \ud97b\udf59 14.62, p \ud97b\udf59 .0001, I 2 \ud97b\udf59 93%; see Figure 11). In adolescent self-reports, the exclusion of teens with NSI from the ELBW group was likely to result in higher rates of self-reported ADHD (combined) in the ELBW group, whereas the inclusion of teens with NSI in the ELBW group was likely to result in slightly lower rates of combined ADHD, a contrast that was statistically significant (Z \ud97b\udf59 3.90, p \ud97b\udf59 .001). However, only one study con- tributed data that included teens with NSI, suggesting cautious interpretation, and the overall pattern of findings suggested that the risks for childhood and adolescent mental health outcomes were not increased by the inclusion of ELBW survivors with significant NSI for any of the outcomes examined. Informant comparisons. Ratings of mental health outcomes were compared between informants in subgroup analyses, using the same Z test strategy. Results are presented in Table 7, Figure  12 and Figures 49 to 51 in the online supplemental materials. Parents' and teachers' ratings of mental health outcomes in ELBW and NBW children did not differ for any of the outcomes for which data were available: ADHD (all types), internalizing, externaliz- ing, and conduct disorder (all ps \ud97b\udf59 .20). In contrast, ratings by parents and adolescents differed for several adolescent mental health outcomes. Parents were more likely to report ADHD prob- lems in ELBW teens, (combined ADHD: Q \ud97b\udf59 6.49, p \ud97b\udf59 .01, I 2 \ud97b\udf59 85%; see Figure 12; hyperactive ADHD: Q \ud97b\udf59 18.40, p \ud97b\udf59 .0001, I 2 \ud97b\udf59 95%, see Figure 49 the online supplemental materials; inat- tentive ADHD: Q \ud97b\udf59 29.24, p \ud97b\udf59 .0001, I 2 \ud97b\udf59 97%; see Figure 50 in the online supplemental materials). In contrast, adolescents were more likely to identify attention problems in NBW controls (com- bined ADHD: Z \ud97b\udf59 \ud97b\udf592.60, p \ud97b\udf59 .01; hyperactive ADHD: Z \ud97b\udf59 \ud97b\udf594.24, p \ud97b\udf59 .001; inattentive ADHD, Z \ud97b\udf59 \ud97b\udf59.5.63, p \ud97b\udf59 .001). In addition, although parents reported no group difference in oppositional defiant disorder between ELBW and NBW adoles- cents, (Q \ud97b\udf59 5.01, p \ud97b\udf59 .03, I 2 \ud97b\udf59 80%), adolescents themselves were more likely to identify oppositional defiant disorder among NBW control teens, (Z \ud97b\udf59 \ud97b\udf592.26, p \ud97b\udf59 .03). (See Figure 51 in the online supplemental materials.) Figure 7. Regional effects on parent ratings of childhood attention-deficit/hyperactivity disorder (ADHD; combined type). NBW \ud97b\udf59 normal birth weight; ELBW \ud97b\udf59 extremely low birth weight. See the online article for the color version of this figure.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Sensitivity Analyses", "text": "A series of sensitivity analyses was carried out to assess whether findings differed between studies that were deemed to be at greater versus lesser risk of bias, according to their NOS ratings. Results are presented in Table 8 and in Figures 52 to 59 in the online supplemental materials. None of these analyses revealed signifi- cant differences in the findings between studies at risk of greater or lesser bias (meta-analyses: all ps \ud97b\udf59 .13; Z scores: all ps \ud97b\udf59 .06). The differences in the effect sizes were small (\ud97b\udf590.24) except for internalizing (0.29) and externalizing (0.56). However, es- timates of mental health from studies at high risk of bias had larger standard errors and appeared to be more unstable than those from studies at low risk of bias. These included parent- rated childhood hyperactive ADHD (Z score \ud97b\udf59 0.37; M \ud97b\udf59 0.38, 95% CI \ud97b\udf59 \ud97b\udf590.03 to 0.78), internalizing (Z score \ud97b\udf59 0.20; M \ud97b\udf59 0.34, 95% CI \ud97b\udf59 \ud97b\udf590.21 to 0.89), externalizing (Z score \ud97b\udf59 0.42; M \ud97b\udf59 0.07, 95% CI \ud97b\udf59 \ud97b\udf590.33 to 0.46), and conduct disorder (Z score \ud97b\udf59 0.81; M \ud97b\udf59 0.12, 95% CI \ud97b\udf59 \ud97b\udf590.15 to 0.39). Similar analyses of parent-rated adolescent internalizing (Z score \ud97b\udf59 1.11; M \ud97b\udf59 0.34, 95% CI \ud97b\udf59 \ud97b\udf590.05 to 0.74) and externalizing (Z score \ud97b\udf59 1.84, M \ud97b\udf59 0.02, 95% CI \ud97b\udf59 \ud97b\udf590.37 to 0.41) suggested unstable estimates from studies at higher risk of bias. In these analyses, the 95% CIs for the summary effects from all of the groups of studies at higher risk of bias happened to cross the zero mark. In contrast, confidence intervals for the summary effects from groups of studies at lower risk of bias did not include zero.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Discussion", "text": "This systematic review summarizes all available literature on the mental health of individuals born at ELBW or EP up until mid-2016. Evidence from a large number of participants suggested that these individuals are at increased risk for particular mental health problems, beginning in childhood and extending into the fourth decade of life. Analyses of ADHD studies revealed that children born at ELBW were more likely to have attention difficulties than their NBW peers, with moderate to large effects in parent ratings and small to moderate effects in teacher ratings. Parents reported significantly more ADHD problems in these children in almost every study included in this review. Children born extremely preterm were also significantly more prone to internalizing, externalizing, conduct problems, social problems, and symp- toms of ASD, relative to age-matched controls. Effect sizes for parent-rated internalizing, social problems, and ASD symptoms were moderate, and small but significant for externalizing and conduct disorder. Teachers, as well as parents, identified small but significant group differences in internalizing and external- izing problems. Prediction intervals of the effect sizes for some parent and teacher ratings crossed the zero line, (e.g., external- izing, conduct, oppositional defiance, and ADHD), indicating that findings were sometimes contradictory across studies. While the majority of studies identified greater difficulties in ELBW children, a few found more problems in NBW children. (See Table 2.) Meta-analyses of parent reports of adolescents also suggested significantly greater risk for ADHD (all three types), internalizing (especially anxiety), and social difficulties in the ELBW group, with moderate effect sizes, excepting a small effect for hyperactive ADHD. Parent-reported group differences in externalizing, con- duct, and oppositional problems did not reach statistical signifi- cance at this life stage. In contrast to parental reports, adolescents reported significantly lower risks of combined, hyperactive, and inattentive ADHD and oppositional behavior in ELBW teens than in NBW teens, although the effects were small. Levels of internaliz- ing, externalizing, and social problems were similar across groups. The sparse numbers of adolescent studies in our analyses and the wide dispersion of their effects, (seen in the large prediction intervals for both types of informant) require a cautious approach to interpretation, Note. ADHD \ud97b\udf59 attention-deficit/hyperactivity disorder. \ud97b\udf59 p \ud97b\udf59 .05. (Outcome differs by birth era.) Figure 9. Regional effects on parent ratings of adolescent attention-deficit/hyperactivity disorder (ADHD; combined type). NBW \ud97b\udf59 normal birth weight; ELBW \ud97b\udf59 extremely low birth weight. See the online article for the color version of this figure. but they provide a general outline of the current status of mental health research in this age group. (See Table 3.) Higher levels of anxiety, depression, and shyness, and lower self-rated social functioning were self-identified by adults born at ELBW. In adulthood, any risks for combined or inattentive ADHD were higher in ELBW groups only under special cir- cumstances, namely, exposure to antenatal corticosteroids, or having lower levels of fluid intelligence. Prenatal exposure to corticosteroids among ELBW survivors appeared to amplify group differences in social anxiety disorder and generalized anxiety disorder (Van Lieshout et al., 2015a). Although levels of antisocial behavior were similar in both groups, adults born  Figure 10. Effects of birth era on self-ratings of adolescent attention-deficit/hyperactivity disorder (ADHD; combined type). NBW \ud97b\udf59 normal birth weight; ELBW \ud97b\udf59 extremely low birth weight. See the online article for the color version of this figure. at ELBW actually manifested a lower risk of alcohol and substance use than those born at NBW. These findings concurred with extant reviews of children born at VLBW (Aarnoudse- Moens et al., 2009;Bhutta et al., 2002;Breslau, 1995). On the basis of a relatively small set of studies, Bhutta and colleagues (2002) reported a higher preva- lence of ADHD, internalizing, and externalizing in children born preterm (\ud97b\udf592,500 g). Here, we report moderate to large effect sizes for risks of ADHD, internalizing, social problems, and ASD symptoms in children born at ELBW.  Figure 11. Effects of neurosensory impairment (NSI) on self-ratings of adolescent attention-deficit/ hyperactivity disorder (ADHD; combined type). NBW \ud97b\udf59 normal birth weight; ELBW \ud97b\udf59 extremely low birth weight. See the online article for the color version of this figure. Our results were also broadly consistent with recent narrative reviews of children (Johnson, 2007), adolescents (Johnson & Wolke, 2013) adults (Johnson & Marlow, 2014). These authors noted a significantly higher prevalence of attention and social problems in children and adolescents born preterm, but found little consensus in the literature with regard to internalizing and externalizing difficulties. For individuals born extremely pre- term, the higher risk of psychiatric symptoms was likely to endure into adulthood, based on higher prescription usage and hospital admissions for psychiatric and addiction problems in this population (Johnson & Marlow, 2014). Findings from our systematic review of prospective studies confirmed this general trend in nonhospitalized adults born at ELBW. Some researchers have worried that disparities between chil- dren born at ELBW versus NBW would persist over time or even widen with age (e.g., Taylor et al., 2000; see also Whit- field et al., 1997), while others argued that postnatal experi- ences may be able to compensate for behavioral and develop- mental effects related to prematurity (Aylward, 2005;Murphy, Barkley, & Bush, 2002;Wilson-Ching et al., 2013). Between childhood and adulthood, increases in attention span and de- clining attention problems are reported both for adults born at full-term ( Murphy et al., 2002) and those born very preterm (Breeman, Kaekel, Baumann, Bartmann, & Wolke, 2015). De- spite putative maturational changes, however, vulnerability to attention problems may not entirely disappear. Clinically sig- nificant attention problems may be outgrown by adulthood in term-born individuals, but ADHD diagnoses have shown greater stability between childhood and adulthood in adults born very preterm (Breeman et al., 2015;Halm\u00f8y, Klungs\u00f8yr, Skjaerven, & Haavik, 2012). In contrast to attention problems, the risk of internalizing did not appear to diminish in adolescence or adulthood, as ELBW participants continued to note difficulties with depression, anx- iety, and avoidant personality problems at these maturing life stages. Parents reported higher levels of internalizing and social difficulties in ELBW adolescents than in their NBW peers, and unremitted as well as remitted anxiety symptoms between early and late adolescence (Taylor, Margevicius, Schluchter, Andreias, & Hack, 2015). Adolescents themselves endorsed a preoc- cupied attachment style ( Hallin et al., 2011b); higher levels of emotional problems ( Gardner et al., 2004); and reduced athletic, vocational, and romantic confidence, in comparison to NBW controls ( Grunau, Whitfield, & Fay, 2004). Research on adults born at ELBW suggests that internalizing problems may worsen in the transitions between childhood and adolescence, or ado- lescence and adulthood, (Schmidt, Miskovic, Boyle, & Saigal, 2010). Internalizing (Boyle et al., 2011) and non-substance- related psychiatric problems may remain higher over time in ELBW cohorts (Van Lieshout et al., 2015a), and social func- tioning is impaired ( Natalucci et al., 2013). Although these kinds of problems do not appear to resolve with age, given the paucity of adolescent and adult assessments of mental health in individuals born extremely preterm, our present understanding of the developmental trajectories of mental health issues in this population remains necessarily tentative. The consequences of mental health and social problems that do not diminish with increased age may be far-reaching. Poor social skills may lead to reduced social acceptance in childhood and adolescence (e.g., bullying; Meth\u00fasalemsd\u00f3ttir et al., 2013;Yau et al., 2013). Adequate social skills continue to underpin academic and vocational success in adulthood, even in the general population. Not only psychological well-being, but educational attainment and even income level may be adversely affected by social difficulties related to extremely preterm birth ( Allin et al., 2006;Mathiasen, Hansen, Nybo Anderson, & Greisen, 2009;Moster, Lie, & Markestad, 2008;Saigal et al., 2016), suggesting that the costs of enduring mental health problems and social deficits may well increase with age, as young adults assume greater personal and financial responsi- bilities than were required in their youth.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Subgroup Analyses", "text": "In addition to issues of developmental change, we attempted to evaluate the effects of regional and cohort differences on mental health outcomes in ELBW across studies and to ascer- tain whether group differences in the risk for psychiatric prob- lems related to the presence of NSI in ELBW samples. Surpris- ingly, group differences for most syndromes were relatively robust to all three factors: birth region, birth era (i.e., significant developments in neonatal intensive care), and the presence of NSI, with a few exceptions. Regional distributions. Regional disparities were relatively rare. In children, a pattern of widely distributed, moderate effect sizes were found for most disorders, other than inatten- tion, internalizing, and ASD symptoms. The general consis- tency of the findings across regions suggests that attentional, behavioral and social outcomes in individuals born at ELBW may have a developmentally programmed, biological basis. However, regional disparities in corticosteroid use may ac- count, at least in part, for regional differences in attention difficulties. Repeated antenatal exposure to corticosteroids may result in more distractible and hyperkinetic behavior during childhood (French, Hagan, Evans, Mullan, & Newnham, 2004;Rieger et al., 2004; for a review see Van den Bergh, Mulder, Mennes, & Glover, 2005). Unfortunately, few of the studies provided information on maternal treatment with corticoste- roids, except for two Norwegian studies of the same cohort ( Elgen et al., 2012;Fevang, Hysing, Markestad, & Sommerfelt, 2016), where 70% to 71% of ELBW neonates had received prenatal steroids, and one Australian study (Hutchinson et al., 2013), where 88% of ELBW had received prenatal steroids. Birth era. One drawback to examining studies of infants born extremely preterm over a 29-year period (1974( [Whitfield et al., 1997] to 2003 [Scott et al., 2012]) was that prenatal and neonatal care did not remain uniform during that time ( Bhutta et al., 2002). We attempted to account for changes in neonatal care by comparing pre-and post-1990 cohorts because surfac- tant and steroid therapies were widely adopted in the late 1980s and early 1990s. The majority of mental health outcomes ap- peared to be independent of birth era, with the exception of combined ADHD in adolescent self-reports, where ADHD was likely to be slightly higher in the ELBW group among teens born before 1990 and slightly higher in the NBW group among those born after 1990. (See Table 5 and Figure 10.) Given that risks for other kinds of impairments (e.g., NSI) have remained strong in the postsurfactant era (Anderson, & Doyle, & the Victorian Infant Collaborative Study Group, 2003;Hack et al., 2009;Hanke et al., 2003), further research is needed to deter- mine whether changes to modern neonatal care have led to mental health benefits, apart from any secular improvements. Neurosensory impairment. Recently, it has been suggested that the higher risk for mental health problems in individuals born at ELBW might be driven by a subgroup of the population with severe handicaps (e.g., Johnson & Wolke, 2013). Indeed,  most early studies excluded children with NSI, in a bid to identify the relatively subtle consequences of low birth weight in survivors apart from any effects of major neurological dam- age (Breslau, 1995). Here, the inclusion of children with NSI did not widen the group differences in psychiatric outcomes, suggesting that any detrimental effects of birth weight status on childhood mental health were not related to the presence of significant handicaps in ELBW cohorts, but to prematurity itself. (See Table 6.) The only exception was a significantly higher risk of self-reported combined ADHD in studies that excluded teens with NSI. (See Figure 11). This finding requires corroboration, as only one study in which NSI was excluded was available for analysis. Like the higher rates of NSI that accompanied improved survival (Fanaroff et al., 2003), higher rates of mental health difficulties in ELBW samples may simply reflect the fact that more extremely preterm babies with high biological risk now survive.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Informant Discrepancies", "text": "One of the advantages of collecting reports from multiple in- formants is that congruent evidence from independent sources may support the stability of theorized effects. On the other hand, informant discrepancies may allude to real differences in infor- mants' perspectives of particular behaviors (Achenbach, 2011). Informant discrepancies may also reflect legitimate contextual differences, for example, between the attention and behavioral control required for the classroom setting versus expectations at home. Accordingly, researchers have begun to analyze such dis- crepancies for latent information that is not contained in the individual ratings of either group (De Los Reyes, 2011). In the childhood studies of mental health problems in ELBW survivors reported here, group differences between children born at ELBW and NBW were similar in parent and teacher-ratings of ADHD (all types), internalizing, externalizing, and conduct disor- der (all Z scores \ud97b\udf591.77, ns; cf. Kohen, Brooks-Gunn, McCormick, & Graber, 1997;Zeiner, 1997). In contrast to the similarity be- tween parent and teacher ratings, adolescent self-ratings indicated lower levels of hyperactive ADHD, inattentive ADHD, and oppo- sitional defiant problems in ELBW teens than their NBW peers, whereas parents reported the opposite. Parent-teen reporting discrepancies were epitomized in a study where parents rated ELBW teens as having higher attention problem scores, while adolescents gave ELBW teens lower scores (Wilson-Ching et al., 2013). However, effects of birth weight status in parent and youth reports were similar for internalizing, conduct disorder, and social problems. How are such parent-adolescent discrepancies to be interpreted? Assuming they contain real information, possible explanations for adult-teen discordance in the reporting of mental health problems include teenage optimism ( Gardner et al., 2004;Hallin & Stjernqvist, 2011a), lack of insight ( Burnett et al., 2014), downward revision of expectations (Saigal, Pinelli, Hoult, Kim, & Boyle, 2003), and the fact that teens are likely to have access to infor- mation about themselves that parents do not have ( Gardner et al., 2004). Parents may have significant concerns about behavior that their adolescent offspring do not regard as problematic (De Los Reyes, 2011). Alternatively, underreporting of problems may be related to social acceptability in ELBW adolescents, who tend to be more cautious (Hack et al., 2002), shy ( Schmidt et al., 2008;Waxman, Van Lieshout, Saigal, Boyle, & Schmidt, 2013), and conforming ( Allin et al., 2006;Pesonen et al., 2008) than their NBW counterparts. This may be particularly true for sensitive parents (Bilgin & Wolke, 2015), who see their ELBW children as good, fragile, or in need of assistance. The issue of parent- adolescent discrepancy is not trivial, as unresolved differences in perspective may lead to negative outcomes: for instance, differing interpretations of conflict by teens and parents within the family may predict later adjustment difficulties for the teens ( Greenley et al., 2007). Sorting out the relevance and underlying causes of parent-adolescent discrepancies in mental health reports merits further study.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Risk of Bias", "text": "The strength of a meta-analytic review depends heavily on the studies available for assessment. Therefore, it is critical to assess potential reporting biases in the published findings. One potential problem is the tendency of smaller studies to produce effects that differ from (and are often larger than) those of larger studies- known as small-study effects (Sterne, Gavaghan, & Egger, 2000). Indeed, significant findings from meta-analyses that are based primarily on small studies have occasionally been contradicted by larger, better-controlled studies at a later date ( Egger et al., 1997). Accordingly, where appropriate, we created funnel plots to deter- mine whether the meta-analytic results relied on small-study ef- fects. Because funnel plots are recommended when meta-analyses include at least 10 studies, we performed these analyses for only two syndromes, namely, childhood combined ADHD and internal- izing. Funnel plots and Egger tests of the findings from studies of parent-rated combined ADHD and internalizing yielded no evi- dence of small-study effects, suggesting that parents of children born at ELBW are more likely than parents of children born at NBW to report significant attentional and internalizing problems in their offspring. Second, sensitivity analyses were performed to determine whether studies produced discrepant findings, depending on whether they carried greater or lesser risk of bias according to the NOS ratings. None of these analyses revealed significant differ- ences in the findings. Notably, the summary effects from high-bias studies were numerically smaller than those from studies at lower risk of bias, suggesting that high bias studies tended to provide more conservative estimates of mental health problems for youth born at ELBW than low-risk studies. However, the findings from studies at higher risk of bias tended to be inconsistent, as indicated by the 95% CIs for summary effects that included zero. The inconsistency may be partly attributable to the very small numbers of high-bias studies and wide within-study variability in some studies.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Implications for Theories of Development", "text": "Developmental origins of health and disease. In addition to describing the current state of knowledge regarding mental health in ELBW survivors, the present findings also have implications for theories of development. The DOHaD hypothesis posits that fetal adaptation to environmental influences encountered during gesta- tion may increase susceptibility to chronic health problems later in life. Findings that individuals born extremely preterm are more likely than their NBW peers to experience mental health problems appear to be consistent with this general framework. Within the DOHaD hypothesis, different explanations have emerged, including the mismatch hypothesis and the cumulative stress hypothesis. The mismatch hypothesis is illustrated by fetal adaptation to aversive conditions that confer a relatively immedi- ate advantage in utero, but do not fit the demands of the environ- ment encountered after birth, with negative long-term conse- quences for the individual (Gluckman & Hanson, 2004). The cumulative stress model is similar to models of allostatic load. This hypothesis suggests that individuals who encounter aversive con- ditions during gestation may be sensitized by the exposure and become more vulnerable to aversive challenges that arise later in life (e.g., Davis, Waffarn, & Sandman, 2011). Nederhof and Schmidt (2012) proposed an integration of these models, whereby both interpretations relate to individual differ- ences in fetal sensitivity to programming effects-that is, fetal propensity to phenotypic plasticity. In this integrated model, fe- tuses whose endophenotypes are relatively more sensitive to pro- gramming will manifest higher levels of prenatal adaptation. If the environment to which they adapted becomes substantially different from the one anticipated, these fetuses are likely to experience an environmental mismatch. Those low-reactive fetuses whose endo- phenotypes are relatively less sensitive to programming are likely to show additive effects of stress when adversity is encountered again later in life. Developmental programming effects may thus represent specific manifestations of a more general biological mechanism, namely, a continuum of developmental plasticity (e.g., Barker, 2004;Gluckman et al., 2010). If a fetus were to adapt extensively to adverse prenatal condi- tions, that could well result in an individual who thrives in envi- ronments where attention-switching and fast reactions are adaptive and ensure survival (match), but fares poorly in environments where sustained concentration is required for success, and impul- sive behavior is detrimental (mismatch). In this case, the \"adapted\" fetus would be at increased risk of disease only if the environment failed to match the adaptation. On the other hand, if the fetus were less sensitive to programming, it might show limited adaptation, but still \"take a hit\" from prenatal adversity. In this scenario, weak adaptation may increase the risk of disease only slightly, depend- ing on the number of additional \"hits\" experienced over time. As demonstrated in animal models, the effects of early adversity and exposure to stressors later in life may be additive (e.g., Choy, de Visser, & van den Buuse, 2009). For individuals with greater physiological or cognitive vulnerabilities than their peers, a mod- icum of cautiousness may be generally adaptive, but when addi- tional life stressors arrive (such as the challenges of adolescence and young adulthood), dispositional cautiousness might intensify to the point where it becomes pathological, resulting in differen- tially higher levels of internalizing like those seen in individuals born at ELBW. One of the predictions of the Nederhof and Schmidt (2012) model is high variability in the outcomes when studies fail to account for individual differences in sensitivity to programming. In the present study, the prediction intervals for both childhood and adolescent mental health problems varied considerably by syn- drome. We suggest that the wide prediction intervals among ELBW mental health outcomes are more consistent with differen- tial sensitivity to programming among individuals and do not preferentially support accounts of either cumulative stress or mis- match. Mechanisms for programming sensitivity: Epigenetic modifications. In current thinking, one of the most plausible mechanisms for plasticity is epigenetic modification (Gluckman et al., 2010). Although genotypes are critical determinants of phys- iology, phenotypic variations may be brought about by environ- mental factors that can influence gene expression without altering the genotype itself. Processes such as DNA methylation (the addition of methyl groups to genetic material) often dampen gene expression, making characteristics that are related to \"silenced\" genes less likely to develop. On the other hand, changes in histone structure that facilitate transcription factor access to DNA tend to promote gene expression, increasing the likelihood of characteris- tics that are controlled by those genes (Moore, 2015). Importantly, nutritional and hormonal conditions experienced by the fetus dur- ing gestation may initiate epigenetic changes that are central to developmental plasticity (Gluckman, Hanson, & Beedle, 2007;Gluckman et al., 2010). Epigenetic changes related to hormonal regulation may consti- tute a proximal physiological mechanism by which the fetus adapts to maternal stress (Meaney, 2010). In a recent human study, Monk et al. (2016) demonstrated an association between mothers' per- ceived stress and an important index of neurobehavioral develop- ment in the fetus, namely, coupling between fetal heart rate and movement ( DiPietro et al., 2010). The association was mediated by epigenetic effects: increased methylation of placental genes in- volved in the glucocorticoid pathway predicted reduced coupling in fetuses of stressed mothers. One of the placental genes exam- ined by these researchers (HSD11\ud97b\udf592) is associated with an enzyme that metabolizes maternal cortisol to produce inactive cortisone. Methylation of that gene may impede synthesis of the protective enzyme, thereby exposing the fetus to high levels of maternal cortisol. As described by Ellison (2005), cortisol is heavily involved in mechanisms related to energy mobilization and allocation, and in particular, energy allocation trade-offs. Therefore, cortisol levels are likely to change in response to environmental conditions that affect energy allocation. Powerful gestational effects of cortisol are demonstrated in the final trimester of pregnancy when fetal corti- sol levels rise (Donaldson, Nicolini, Symes, Rodeck, & Tannirandorn, 1991) and in the subsequent metabolic crisis experienced by the fetus near the end of gestation ( Smith et al., 2005). When the energy demands of the developing fetus begin to exceed the energetic resources supplied through the placenta, hypothalamic- pituitary-adrenocortical (HPA) axis activity in the fetus increases to enhance the energetic supply. However, the ensuing rise in corticosteroids from the fetal side of the placenta may lead to a cascade of events that initiates labor. Importantly, the timing of this metabolic shift involving enhanced HPA activity may be influenced by conditions such as maternal diabetes, undernutrition, multiple pregnancies, or other energetic stressors, such that birth onset may be early or delayed (Ellison, 2005). Thus, the same mechanism-change in cortisol levels-appears to be able to affect the timing of delivery and developmental outcomes (e.g., Glover, O'Connor, & O'Donnell, 2010;Quesada, Trist\u00e3o, Pratesi, & Wolf, 2014;W\u00fcst, Entringer, Federenko, Schlotz, & Hellhammer, 2005). Exposure of the developing brain to conditions that maintain high levels of cortisol during gestation may lead to poor outcomes, including mental health problems (Huang, 2011;Schlotz & Phillips, 2009). Of course, prenatal programming effects do not account for all of the mental health risks associated with being born at ELBW. Equally important exposures occur when the preterm neonate is exposed to the world ex utero while still in a vulnerable state of development ( Quesada et al., 2014). Premature exposure to phys- ical sensations (e.g., light, sound, and handling), mechanical mon- itors, and painful, sometimes tissue-damaging treatments are stressful for preterm infants (Johnston, Barrington, Taddio, Carbajal, & Filion, 2011). However, in infants born too early-before the rise in fetal cortisol levels in late gestation ( Donaldson et al., 1991)-the HPA axis is likely to be still in a suppressed state, resulting in adrenal insufficiency (Fernandez & Watterberg, 2009;Watterberg & Scott, 1995), whereby the amount of cortisol pro- duced in response to a stressor is too little to maintain homeostasis. The ability to mount an effective cortisol response to stressors when it is needed is essential for survival, and adrenal insuffi- ciency exposes preterm neonates to heightened risk of both short- term and long-term consequences of stress (M\u00f6relius, He, & Shorey, 2016). Moreover, endocrine set points may be altered in children born extremely preterm ( Grunau et al., 2007;Quesada et al., 2014). In both EP and full-term infants, basal cortisol levels decline continuously until 18 months of age ( Grunau et al., 2007). Because the decline is noticeably greater in full-term infants, EP infants showing depressed basal levels of cortisol at 3 months show higher levels of baseline cortisol than do term-born children by 8 months. These relatively higher levels are maintained up to at least 18 months in EP infants, suggesting an enduring shift in cortisol regulation. We submit that durable effects of extremely preterm birth on endocrine and other systems, in combination with additional life stressors, may be sufficient to increase the risk for psychiatric problems in individuals born at ELBW.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Strengths and Limitations", "text": "This review provides a qualitative and quantitative overview of the current state of knowledge concerning mental health outcomes in individuals born extremely preterm. The review followed a formal protocol, examining 41 studies from 24 unique cohorts distributed across the world over an extensive period and a wide range of ages (5 to 36 years). Because we limited this review to individuals born at ELBW, individual differences in participants' baseline weight and gestational age did not vary much across the selected studies, ensuring a relatively homogenous study popula- tion. Pooling information from multiple cohorts generated much larger samples in case and control groups within each age category than could feasibly be collected by individual hospitals or labora- tories, thereby reducing the impact of low statistical power. In studies of children and adolescents, information was often avail- able from multiple informants. Our findings also suggest some testable hypotheses for exam- ining the pathogenesis of psychopathology following extremely preterm birth, including possible biological bases for particular mental health problems, and the question of whether prenatal exposure to corticosteroids constitutes a mechanism that compro- mises mental health. With improved understanding of mechanisms that plausibly affect mental health outcomes, more focused thera- peutic interventions may be developed to lessen the impact of psychiatric problems in individuals born at ELBW. It is also important to note several limitations when interpreting these findings. First, while methodological quality was relatively high, there was a substantial range of heterogeneity across studies. As well, mental health outcomes were reported from a wide variety of scales and range of measurement time points, even within developmental categories. Among the 41 reviewed studies, well over 40 different instruments were used to assess mental health between ages 5 and 36 years. Most, but not all of the assessment measures were validated. Further, we assumed that assessment instruments from different studies had comparable sensitivity and specificity. This overlooks existing subtle differences between instruments, as well as variation in the administration of those tests. Although we tried to minimize this concern by analyzing odds ratios from directly comparable questionnaires, standardiza- tion in the instruments used to assess mental health problems and, importantly, in the reporting of the findings was generally lacking (Milfont & Fischer, 2010;Van Lieshout, Boyle, Schmidt, Saigal, & Ferro, 2015b). Second, issues of measurement invariance with respect to the assessment instruments commonly administered to this population should be acknowledged. Although it is generally assumed in psychological testing that a given scale measures the same psy- chological construct in all of the groups to which it is administered, if this is not true, then comparisons across ELBW and control groups will not be valid. In the only ELBW study to address this issue to date, measurement invariance was established more clearly for attention-deficit hyperactivity problems than for emo- tional and conduct problems in ELBW children (e.g., Van Lieshout, et al., 2015b). Third, the number of extant studies of mental health in individuals born at ELBW declined sharply beyond childhood. While reports of mental health sequelae of extremely preterm birth were available in 25 studies of children, eligible adoles- cent studies numbered fewer than half that number (i.e., 10 studies). Most of the cohorts will have now reached adulthood, yet studies of adults were rare, with only two unique cohorts represented among the six adult studies eligible for this review. Part of the problem stems from attrition over long-term follow- up. Attrition itself is unlikely to overestimate any effects of ELBW because those lost to follow-up tend to have poorer outcomes (Breeman et al., 2015;Breslau, 1995). However, sample size reductions in cohorts that were already limited when first established make it more difficult to conduct studies in adolescence and adulthood. Therefore, our conclusions for adolescents and adults were based on less evidence than was available for children, and findings in these groups may be more variable or less stable. Given the limited availability of mental health outcomes data at older ages, we were unable to adequately address questions of whether particular mental health problems declined with the attainment of maturity or the nature of their trajectories. Fourth, the number of studies entered in meta-analyses was typically a subset of all the studies that assessed similar outcomes. Some meta-analyses included only two studies, especially in anal- yses of adolescent data, allowing only cautious interpretations of the findings. However, the narrative findings from reports that were not analyzed were similar to those derived from the meta- analyses, as described earlier. In this review, we were also unable to parse any effects of being born SGA from those associated with being born at ELBW. Although researchers sometimes reported the percent- age of their sample that was born SGA or experienced intra- uterine growth restriction (15 of 41 studies), only two of these studies stratified their analyses by size for gestational age (SGA vs. appropriate for gestational age (AGA; Boyle et al., 2011;Van Lieshout et al., 2015a). Proportions of ELBW cohorts born SGA were relatively low, ranging from 7% (n \ud97b\udf59 6; Stahlmann, Rapp, Herting, & Thyen, 2009) to 47% (n \ud97b\udf59 26; Natalucci et al., 2013), with most studies falling between 12% and 25%. The etiology of mental health problems in individuals born at ELBW and SGA warrants further investigation (e.g., Abel et al., 2010;Lahti et al., 2015), particularly in light of differences within the SGA population between infants that experienced asymmetric prenatal growth (in which the body is dispropor- tionately small at birth while the head is relatively large) and those that experienced symmetric but limited growth, in which constraints affected body and head more proportionately. Of interest, the Boyle et al. (2011) and Van Lieshout et al. (2015a) studies (of the same cohort at different adult ages) reported dose effects of preterm status on mental health risk: Those born at ELBW and SGA were more likely than adults born at ELBW and AGA to have psychiatric problems, who, in turn, were at greater risk than were term-born controls born at NBW.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Recommendations for Further Research", "text": "Limitations in the literature to date are considerable barriers to a scientific understanding of the pathogenesis and treatment of psychological problems in ELBW and EP survivors. They also impede clinical efforts to prevent or treat mental disorders in this population. We therefore make the following recommendations in an attempt to advance the field. First, it is important that individ- uals born at ELBW be followed into adulthood if possible. We know that survival rates have increased without a comparable decrease in morbidity, and that difficulties with certain mental health problems appear to persist in young adulthood. If the effects of extremely preterm birth on mental health are enduring, it will be important to collect data on ELBW survivors as they reach young adulthood, mature, and age. Without follow-up data from older ages, it is impossible to know how and when to treat mental health concerns that change with age, or which concerns remain trouble- some in adulthood. Second, we recommend that individual studies attempt to use similar, valid measures across developmental epochs, if possible. Longitudinal studies not only constrain idiosyncratic variability, allowing a clearer view of symptoms and their trajectories over time, but also allow issues like homotypic or heterotypic continuity to be addressed. These kinds of models may be particularly useful in understanding the nature of mental health symptoms in individ- uals born at ELBW, especially if they are rooted in early occurring biological processes. Third, despite the fact that the issue of wide variation in measurement instruments across different cohorts was noted in a review of low birth weight studies from 2 decades ago (Breslau, 1995), this problem continues to make direct compar- isons across studies problematic, even for recent cohorts. To address it, the measures used to assess mental health should be widely adopted, and the use of gold standard instruments should be more widespread than the use of questionnaires better suited for screening for mental disorders. Given the clinical and re- source implications of psychiatric diagnoses, as well as the stigma attached to mental health difficulties, assessments of mental health should be based on more than a few questions. More comprehensive assessments would allow greater confi- dence in estimated rates of risk, and more detailed descriptions from participants would help establish the phenomenology of mental health problems in ELBW. Fourth, given differences in the brains of ELBW survivors and the unique behavioral phenotypes hypothesized to be pres- ent in preterm survivors, it is not necessarily true that applied measures tap the same constructs in this population and NBW controls. Therefore, it behooves researchers to establish mea- surement invariance for measures of mental health, and then to use the invariant measures to make accurate estimations of the mental health risks in ELBW. Fifth, studies of individuals born at ELBW are frequently underpowered, due to the still limited numbers of survivors of extremely preterm birth. If common data collection protocols were established to allow for appropriate combining of data, data from multiple centers could be pooled to produce larger samples of this relatively rare population, permitting meta- analyses of individual participants' data. The use of population- based, linked registry data will also provide more accurate estimates of risk, although such estimates may be limited by diagnostic coverage or lack data on mediators and moderators of observed links. Sixth, the time has come for studies in the field to progress from descriptive to explanatory accounts, another issue raised previously (e.g., McCormick, 1997). If some subgroups of the ELBW population drive the risk of adverse mental health out- comes more than others, then careful examination of potential social and biological moderators and mediators of these out- comes is indicated to facilitate discovery of factors that promote mental health (or not) following extremely preterm birth. More attention needs to be paid to identifying factors precipitating or mitigating risks of mental health problems in ELBW cohorts, including underlying pregnancy conditions and heterogeneity of preterm birth, parenting style (e.g., Huhtala et al., 2014;Hoff, Hansen, Munck, & Mortensen, 2004;Pyh\u00e4l\u00e4 et al., 2011), socioeconomic status (SES; e.g., Hack et al., 2009;Taylor et al., 2015), cognitive functioning (e.g., Hoff et al., 2004;Nadeau, Boivin, Tessier, Lefebvre, & Robaey, 2001), neonatal interven- tions (e.g., antenatal corticosteroid exposure, early nutrition), intermediate biological phenotypes (e.g., in EEG or neuroim- aging, e.g., Schmidt et al., 2010), and genetic (e.g., Lahat, Van Lieshout, Mathewson, et al., 2016) and epigenetic (e.g., Cruickshank et al., 2013) markers. All of these factors are potential sources of heterogeneity in samples of individuals born pre- term. A more complete understanding of these risk and resil- ience factors would not only allow us to better understand the pathophysiology of mental disorders in this population, but to predict those at highest risk. They would also aid in the devel- opment and targeting of preventive and treatment efforts. Finally, more comprehensive findings should be translated into optimal care protocols and interventions for particular mental health problems, with the optimal time-points for ad- ministering them. Although the Neonatal Intensive Care Unit stay is an obvious time for some interventions, others may need to be implemented following the appearance of symptoms later during development. Our findings suggest that certain mental health problems in ELBW survivors may persist into adulthood, leading to an accumulation of morbidity over time. Therefore, it is important that families and clinicians be aware of these early problems and that appropriate treatment be made available to optimize development and quality of life. Follow-up of children born at ELBW should expand from physical and med- ical considerations to include psychological and family services programs (Farooqi et al., 2007), to inform planning with respect to mental health care during development.", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Summary and Conclusions", "text": "Findings from this review and meta-analysis provide substan- tial evidence that individuals born at ELBW are at greater overall risk for psychological difficulties than are their NBW peers. This does not mean that, in general, infants born ex- tremely preterm will ultimately develop mental health prob- lems-only that the risk of developing such problems is higher in this group than in those born at full term. The difficulties most frequently involve attentional control, anxiety-related problems, and social interaction problems, including social withdrawal. Subgroup analyses suggested that these findings are relatively robust to region of birth, secular changes in care, and the presence of NSI. Regional differences, effects of birth era, and NSI effects on mental health problems principally involved attention problems, internalizing, and ASD. Parent and teacher ratings yielded comparable group differences in mental health symptoms between ELBW and NBW children. Although parents were also likely to report significant atten- tional and social difficulties in teens born at ELBW, teens themselves identified greater attention problems and opposi- tional behavior in typically developing adolescents born at NBW. The wide prediction intervals for most disorders remain to be explained, suggesting additional moderator variables be examined in future studies. The complex needs faced by children born extremely preterm continue throughout development, and include mental health prob- lems that are likely to have long-term consequences for personal relationships, vocational success, and psychological well-being. Innovation will be needed to develop interventions that are neu- roprotective (Hutchinson et al., 2013;Stahlmann et al., 2009), and socially supportive ( Greenley et al., 2007).", "title": "Mental Health of Extremely Low Birth Weight Survivors: A Systematic Review and Meta-Analysis", "file_name": "Mathewson et al. - 2017 - Mental health of extremely low birth weight surviv.pdf"}
{"section": "Method", "text": "We report how we determined our sample size, manip- ulations, and measures in the study. For all experiments, the experimental code, stimuli, and analysis code are publicly available via the Open Science Framework (OSF). The experimental code, analysis code, stimuli, and sample size were all preregistered, with the excep- tion of those for Experiments 8 and 12, and can also be found at the OSF (osf.io/zcbp7).", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Participants", "text": "Fifty participants were recruited on Amazon Mechanical Turk for each of our 12 experiments (N = 600) and paid 40 to 50 cents each for their participation. Across all 12 experiments, 13% of participants completed more than one experiment. We report data from all participants in the main text, but the pattern of reported findings held when these participants were excluded (see the Supple- mental Material available online). We determined our sample size on the basis of a preregistered power calculation using a meta-analytic estimate of the effect size from the studies conducted by Xu and Tenenbaum and Spencer et al. The chosen sample size was approximately twice the estimated sample size necessary to obtain a power of .99 (for details, see the Supplemental Material).", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Stimuli", "text": "Our picture stimuli were gathered on the Internet and closely resembled that of Xu and Tenenbaum and Spencer et al. The referent objects were three sets of 15 pictures from different basic-level categories (veg- etables, vehicles, and animals). Within each category, 5 pictures were subordinate exemplars (e.g., green pep- pers), 4 were basic-level exemplars (e.g., peppers), and 6 were superordinate exemplars (e.g., vegetables; see Fig. 1). The exemplars were divided into learning and generalization sets. For each category, the learning set consisted of 3 subordinate, 2 basic, and 2 superordinate pictures presented in different combinations on differ- ent trials (see the Procedure section). The generaliza- tion set for each category consisted of the remaining 8 pictures. The learning and generalization sets were the same for all participants. The linguistic stimuli were 12 one-syllable novel labels (e.g., \"wug\").", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Procedure", "text": "Participants were first introduced to a picture of a character (\"Mr. Frog\") and instructions describing the task. They were told that the character speaks a dif- ferent language, and their job was to help the charac- ter find the toys he wants. Participants then advanced to the main task, which consisted of a series of 12 trials on separate screens. On each trial, one or three learning exemplars from one of the three stimulus categories appeared at the top of the screen, along with the following instructions: \"Here [is a wug/are three wugs]. Can you give Mr. Frog all of the other wugs?\" Below the learning exemplars, 24 generaliza- tion exemplars (8 from each of the three categories) were displayed in a 4 \u00d7 6 grid. The generalization pictures were displayed in random order across trials. Participants were instructed to select the target cate- gory members (\"To give a wug, click on it below. When you have given all the wugs, click the Next button.\"). When an exemplar was selected, a red box appeared around the picture, and participants were allowed to change their selections by clicking on the picture a second time. The learning exemplars remained visible at the top of the screen during the generalization task. After they had made their selec- tions, participants advanced to the next trial by click- ing the \"Next\" button. There were four trial types, distinguished by the number and conceptual level of the learning exemplars: one subordinate exemplar, three subordinate exem- plars, three basic exemplars, and three superordinate exemplars. Each participant completed each trial type for each of the three stimulus categories (vegetables, vehicles, and animals). Across 12 experiments, we manipulated four aspects of the trial design that differed between the designs of Xu and Tenenbaum and Spencer et al. (summarized in Table 1; all experiments can be viewed in the Sup- plemental Material): presentation timing (simultaneous vs. sequential), trial order (one-three vs. three-one), label (same vs. different), and blocking (blocked vs. pseudorandom). Our set of experiments does not include all possible combinations of these design fac- tors, but all levels were tested in at least one experi- ment. We describe each of these factors in more detail below. Presentation timing. Presentation timing was the key, theoretically motivated experimental design difference between experiments by Xu and Tenenbaum (Experi- ments 1 and 2) 1 and Spencer et al. (Experiments 2 and 3). In the Xu and Tenenbaum study, the learning exemplars were presented statically and simultaneously, whereas in the key conditions from Spencer et al., participants saw a sequence of individual exemplars, with each exemplar visible for only 1 s at a time. In the sequential design, three-exemplar learning trials displayed pictures at three different locations (left, middle, and right) in a sequence that repeated twice, for a total of 6 s. We reproduced these design aspects in the simulta- neous and sequential versions of our experiments. In the one-exemplar sequential trials, the exemplar appeared (1 s) and disappeared (1 s) for three repeti- tions. 2 The generalization pictures did not appear in the sequential condition until after the training pictures had appeared for 6 s, but they remained visible as participants selected generalization exemplars. Trial order. In Xu and Tenenbaum's Experiment 1, the three one-subordinate trials occurred first, followed by all other trial types (one-three; Xu and Tenenbaum's Experi- ment 2 used a between-subjects design). In contrast, in the main experiments in Spencer et al. (Experiments 2 and 3), the three-subordinate trials occurred first (three- one). Spencer et al.'s replication of Xu and Tenenbaum's simultaneous design (Spencer et al.'s Experiment 1) showed a single block of either one-subordinate or three- subordinate trials first (in random order). In Supplemen- tal Experiments 1 and 2, Spencer et al. directly explored whether trial order influenced the effect size by replicat- ing their Experiment 1 with three-subordinate trials fol- lowed by one-subordinate trials. Labels. Xu and Tenenbaum used the same label for each category for the three-subordinate and one-subordinate trials (e.g., both the one-pepper and the three-pepper tri- als would be called wugs; same). In contrast, Spencer et al. used a different novel label on each of the 12 trials, such that the three-subordinate and one-subordinate trials were referred to with distinct labels (different). We repro- duced these two design choices and also randomly mapped labels to categories across trials. Blocking. The studies also differed in whether the trials were blocked by trial type: In the Xu and Tenenbaum study, the first three trials were a block of one-subordinate trials and the remaining nine were at random (pseudo- random), whereas Spencer et al. blocked all four trial types in all experiments (blocked). We also reproduced these two design variants, randomizing the order of the trials within each block for the blocked design.", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Learning Exemplars", "text": "One Subordinate Three Subordinate Basic Three Superordinate Generalization Exemplars Fig. 1. Learning and generalization exemplars from the subordinate, basic, and superordinate conceptual levels of the vegetable category. On a given trial, participants saw one or three exemplars of the same level from the learning set, followed by all exem- plars from the generalization set (along with the generalization sets from the other categories).", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Data analysis", "text": "The key prediction of the suspicious-coincidence effect is that participants should generalize to the basic level more often in one-subordinate trials relative to three- subordinate trials. To measure this effect, for each trial, we calculated the proportion of generalizations to basic exemplars within the same category (out of two) and averaged across categories for each participant. We esti- mated the difference between the one-subordinate and three-subordinate conditions by calculating an effect size for each experiment (Cohen's d; for details, see the Supplemental Material). We then estimated the influ- ence of each of our design manipulations on the overall effect size by fitting a random-effects meta-analytic model with each of our four manipulations as fixed effects. The model included both the present set of experiments and prior experiments by Xu and Tenen- baum and Spencer et al. We used the metafor package (Viechtbauer, 2010) in the R programming environment (R Core Team, 2008) to fit our meta-analytic models. Critically, however, the meta-analytic model across all 12 experiments revealed that only trial order was a reliable predictor of effect size (\u03b2 = \u22121.44, z = \u22129.27, p < .0001), whereas timing (\u03b2 = \u22120.16, z = \u22121.45, p = .15), blocking (\u03b2 = \u22120.1, z = \u22120.56, p = .58), and label (\u03b2 = 0.06, z = 0.51, p = .61; see Table 2) were not. These data thus suggest that the suspicious coincidence is robust to spatiotemporal aspects of the presentation Note: Order refers to the relative ordering of one-and three-subordinate trials. Blocking refers to whether trials were blocked by category or pseudorandomly. Label indicates whether the labels in one-and three-subordinate trials were the same or different. For Cohen's ds, 95% confidence intervals are given in brackets.", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Results", "text": "learning exemplars, in contrast to the conclusion drawn by Spencer et al. Our data also suggest that the three-one ordering interacts with presentation timing: In experiments with the three-one ordering and sequential presentation (Experiments 10-12), we saw a reversal of the suspi- cious-coincidence effect, as observed by Spencer et al. To examine this pattern, we fit a second meta-analytic model that included presentation timing and trial order as additive effects and a third term for their interaction. As in the initial model, there was a main effect of trial order (\u03b2 = \u22121.18, z = \u22128.04, p < .0001) but not presenta- tion timing (\u03b2 = 0.1, z = 0.6, p = .55). However, there was also a significant interaction between the effects of two design parameters (\u03b2 = \u22120.47, z = \u22122.12, p = .03). This interaction effect was due to increased generaliza- tions to the basic level when the three-subordinate trials were presented sequentially (Experiments 10-12) com- pared with simultaneously (Experiments 4-6). In the General Discussion section, we consider why trial order might influence the suspicious-coincidence effect as well as possible reasons for the interaction with pre- sentation timing.", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "General Discussion", "text": "The suspicious-coincidence effect (Xu & Tenenbaum, 2007a) suggests a powerful mechanism by which learn- ers might overcome the inherent ambiguity associated with learning subordinate word meanings. Other evi- dence (Spencer et al., 2011), however, suggests that the effect may occur only under particular learning conditions, namely, when the training exemplars are presented simultaneously to the learner. Across 12 stud- ies, we explored the experimental parameter space of the suspicious-coincidence paradigm and successfully replicated the findings from both sets of authors. Taken together, our studies led us to a different conclusion from that reached by Spencer et al.: The suspicious- coincidence effect is robust to the presentation timing of exemplars but is sensitive to order effects. These order effects (where three-exemplar trials are presented before one-exemplar trials) were not predicted by Xu and Tenenbaum. Below, we offer an account of these results based on recent generalizations of strong sam- pling models to describe pragmatic inferences. The critical difference between the one-three and three-one ordering was the rate of generalization to the basic level in the one-exemplar trial: When the one-exemplar trial occurred second, participants were less likely to generalize to the basic level compared with when the one-exemplar trial was presented first. Why might this ordering matter? Consider a scenario in which first the learner observes a trial with three sub- ordinate peppers followed by a second trial with only a single pepper. Although the two trials were intended to be interpreted as independent from each other, their co-occurrence in the task may have suggested to par- ticipants that they are pragmatically related, leading participants to track their frequency across trials. If true, when the learner observes the single pepper on the second trial, it is effectively the fourth subordinate exemplar from the same category (identical to an exem- plar from the three subordinate trials  Fig. 2. Mean proportion of generalizations to basic-level exemplars in the one-subordinate-exemplar and three-subordinate-exem- plar conditions for all 12 of our experiments. Results are shown separately for each pairing of presentation timing (simultaneous vs. sequential) and trial order (one-three vs. three-one). Error bars indicate bootstrapped 95% confidence intervals. predicts that learners should be less likely to generalize to the basic level when the single exemplar is presented second, consistent with our findings. It also makes a second prediction: In the case of the three-one order- ing, learners should be slightly more likely to generalize to the basic level on the first trial (three exemplars) compared with the second trial (single exemplar, fourth observed exemplar), because seeing four exemplars is a bigger suspicious coincidence than three. We found some evidence consistent with this prediction from the meta-analytic model indicating a reversal of the effect under the sequential-timing, three-one ordering conditions. Although Xu and Tenenbaum's model does not directly predict participants' behavior in the three-one ordering, there is a broader class of Bayesian models, of which Xu and Tenenbaum's model is an instance, that does. These models account for pragmatic reason- ing by assuming that speakers reason about the inten- tion of other people when making linguistic inferences (e.g.,    assume discourse continuity across trials. Indeed, there is experimental evidence that children reason about the intention of the speaker to assume discourse continuity when inferring the meaning of a novel word (Horowitz & Frank, 2016). In future work, the pragmatic influence of discourse continuity in this task can be eliminated by using a between-subjects design, as in Xu and Tenenbaum's experiment with children (Experiment 2). If indeed participants interpret the one-exemplar trial in three-one orders as a fourth exemplar, then it is somewhat surprising that the identity of the label between the two trials does not matter: We saw the same pattern when the labels were different (Experi- ment 10) as when they were the same (Experiments 11 and 12). Given evidence that children and adults tend to assume that different words have different meanings (Clark, 1987), we might expect that a different label on the one-exemplar trial would lead participants to treat the new exemplar as referring to a new category. How- ever, there are a number of reasons that participants may not have carefully attended to labels across trials.", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Lewis & Frank", "text": "First, participants were never tested on the meaning of labels, and the labels were not directly relevant to com- pleting the generalization task. Second, the three-and one-exemplar trials for the same category rarely occurred adjacent to each other (because the one- exemplar trials were always blocked), and this delay might have made it more difficult for participants to remember the labels across the critical trials. Consistent with this pattern, we found that label identity did not mediate the suspicious-coincidence effect across experiments. We also found that trial order interacted with pre- sentation timing: When we replicated Spencer et al.'s experiments, sequential presentation in the three-one ordering led to a reversal of the suspicious-coincidence effect. Spencer et al.'s theory predicts the reversal under sequential-presentation conditions, but it does not pre- dict the observed interaction with trial order. There is also not a straightforward explanation from Xu and Tenenbaum's model. We offer one highly speculative account: Sequential-presentation conditions may have appeared relatively more complex to participants com- pared with simultaneous-presentation conditions, result- ing in higher overall uncertainty in the generalization judgment. This increased uncertainty may have led par- ticipants to be more likely to generalize conservatively- at the basic level-on the first trial when exemplars were presented sequentially as opposed to simultaneously. Future research could test this cognitive load explana- tion more directly. Broadly, our findings highlight the influence of seemingly minor experimental design parameters on the observed pattern of data. In the present studies, experiments with the one-three versus three-one ordering differed by an effect size of 1.42-a sizable difference that is likely to invite an unwarranted theo- retical explanation. Experimental-design parameters are especially important in the context of replication. When conducting a replication of an existing finding, small design parameters may influence the magnitude of the effect (Lewis & Frank, 2016) and even its presence ( Phillips et al., 2015). This sensitivity requires that rep- licators reproduce the original design with as much fidelity as possible before concluding that an effect fails to replicate. Only then can the effect be explored and possible confounds and moderators identified. The work by both Xu and Tenenbaum and Spencer et al. addresses an important puzzle in the psychologi- cal sciences: How do learners learn concepts at mul- tiple levels of abstraction? Their work focuses on a simplified version of this puzzle in which the learner must determine the corresponding labels to known concepts. Our findings here support the idea that learn- ers solved this puzzle via probabilistic inferences about the level of abstraction that is most likely given the observed data (the original suspicious-coincidence effect). Importantly, given the assumption that trials are nonindependent, our interpretation is consistent not only with Xu and Tenenbaum's original set of findings but also with the observed trial-order effects. Our data add to the growing body of work suggesting that suspicious- coincidence effects may arise during pragmatic reasoning in language comprehension (Frank & Goodman, 2014;Goodman & Frank, 2016) as well as through nonlinguistic reasoning ( Shafto et al., 2012). Such probabilistic reasoning is likely to play a critical role in learners' ability to make efficient inferences on the basis of sparse linguistic data.", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Action Editor", "text": "D. Stephen Lindsay served as action editor for this article.", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Author Contributions", "text": "Both authors designed the experiments. M. L. Lewis coded the experimental paradigm and collected and analyzed the data. Both authors interpreted the findings. M. L. Lewis drafted the manuscript, and M. C. Frank provided critical revisions. Both authors approved the final manuscript for submission.", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Declaration of Conflicting Interests", "text": "The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Supplemental Material", "text": "Additional supporting information can be found at http:// journals.sagepub.com/doi/suppl/10.1177/0956797618794931", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Open Practices", "text": "All data and materials have been made publicly available via the Open Science Framework and can be accessed at osf .io/zcbp7/. The design and analysis plans for all experiments except 8 and 12 were preregistered and can also be found at osf.io/zcbp7/. The complete Open Practices Disclosure for this article can be found at http://journals.sagepub.com/doi/ suppl/10.1177/0956797618794931. This article has received the badges for Open Data, Open Materials, and Preregistration. More information about the Open Practices badges can be found at http://www.psychologicalscience.org/publications/badges. ", "title": NaN, "file_name": "Lewis and Frank - Still Suspicious The Suspicious-Coincidence Effec.pdf"}
{"section": "Abstract", "text": "Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect-imagined contact reducing prejudice-showed weak support for replicability. And two effects-flag priming influencing conservatism and currency priming influencing system justification-did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Overview of the Present Research", "text": "Little attempt has been made to assess the variation in rep- licability of findings across samples and research contexts. This project examines the variation in replicability of 13 classic and contemporary psychological effects across 36 samples and settings. Some of the selected effects are known to be highly replicable; for others, replicability is unknown. Some may depend on social context or partici- pant sample, others may not. We bundled the selected stud- ies together into a brief, easy-to-administer experiment that was delivered to each participating sample through a single infrastructure (http://projectimplicit.net/). There are many factors that can influence the replicabil- ity of an effect such as sample, setting, statistical power, and procedural variations. The present design standardizes procedural characteristics and ensures appropriate statistical power in order to examine the effects of sample and setting on replicability. At one extreme, sample and situational characteristics might have little effect on the tested effects -variation in effect magnitudes may not exceed expected random error. At the other extreme, effects might be highly contextualized -for example, replicating only with sample and situational characteristics that are highly consistent with the original circumstances. The primary contribution of this investigation is to establish a paradigm for testing replicability across samples and settings and provide a rich data set that allows the determinants of replicability to be explored. A secondary purpose is to demonstrate support for replicability for the 13 chosen effects. Ideally, the results will stimulate theoretical developments about the conditions under which replication will be robust to the inevitable variation in circumstances of data collection.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Researcher Recruitment and Data Collection Sites", "text": "Project leads posted a call for collaborators to the online forum of the Open Science Collaboration on February 21, 2013 and to the SPSP Discussion List on July 13, 2013. Other colleagues were contacted personally. For inclusion, each replication team had to: (1) follow local ethical proce- dures, (2) administer the protocol as specified, (3) collect data from at least 80 participants, 1 (4) post a video simula- tion of the setting and administration procedure, and (5) document key features of recruiting, sample, and any changes to the standard protocol. In total, there were 36 samples and settings that collected data from a total of 6,344 participants (27 data collections in a laboratory and 9 conducted online; 25 from the US, 11 from other coun- tries; see Table 1 for a brief description of sites and for a full descriptions of sites, site characteristics, and participant characteristics by site).", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Selection of Replication Studies", "text": "Twelve studies producing 13 effects were chosen based on the following criteria: 1. Suitability for online presentation. Our primary con- cern was to give each study a ''fair'' replication that was true to the original design. By administering the study through a web browser, we were able to ensure procedural consistency across sites. 2. Length of study. We selected studies that could be administered quickly so that we could examine many of them in a single study session. 3. Simple design. With the exception of one correlational study, we selected studies that featured a simple, two-condition design.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "1", "text": "One sample fell short of this requirement (N = 79) but was still included in the analysis. All sites were encouraged to collect as many participants as possible beyond the required 80, but the decision to end data collection was determined independently by each site. Researchers had no access to the data prior to completing data collection. 4. Diversity of effects. We sought to diversify the sample of effects by topic, time period of original investiga- tion, and differing levels of certainty and existing impact. Justification for study inclusion is described in the registered proposal (http://osf.io/project/ aBEsQ/).", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "The Replication Studies", "text": "All replication studies were translated into the dominant language of the country of data collection (N = 7 languages total; 3/6 translations from English were back-translated). Next, we provide a brief description of each experiment, original finding, and known differences between original and replication studies. Most original studies were con- ducted with paper and pencil, all replications were con- ducted via computer. Exact wording for each study, including a link to the study, can be found in the supple- mentary materials. The relevant findings from the original studies can be found in the original proposal. 1. Sunk costs (Oppenheimer, Meyvis, & Davidenko, 2009). Sunk costs are those that have already been incurred and cannot be recovered (Knox & Inkster, 1968). adapted from Thaler, 1985) asked participants to imagine that they have tickets to see their favorite football team play an important game, but that it is freezing cold on the day of the game. Participants rated their likelihood of attending the game on a 9-point scale (1 = definitely stay at home, 9 = definitely go to the game). Partici- pants were marginally more likely to go to the game if they had paid for the ticket than if the ticket had been free.  (Tversky & Kahneman, 1981). The original research showed that changing the focus from losses to gains decreases participants' willingness to take risks -that is, gamble to get a bet- ter outcome rather than take a guaranteed result. Par- ticipants imagined that the US was preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Participants were then asked to select a course of action to combat the disease from logically identical sets of alternatives framed in terms of gains as follows: Program A will save 200 people (400 people will die), or Program B which has a 1/3 probability that 600 people will be saved (nobody will die) and 2/3 probability that no people will be saved (600 people will die). In the ''gain'' framing condition, participants are more likely to adopt Program A, while this effect reverses in the loss framing condition. The replication replaced the phrase ''the United States'' with the country of data collec- tion, and the word ''Asian'' was omitted from ''an unu- sual Asian disease.'' 3. Anchoring (Jacowitz & Kahneman, 1995). Jacowitz and Kahneman (1995) presented a number of scenarios in which participants estimated size or distance after first receiving a number that was clearly too large or too small. In the original study, participants answered 3 questions about each of 15 topics for which they esti- mated a quantity. First, they indicated if the quantity was greater or less than an anchor value. Second, they estimated the quantity. Third, they indicated their con- fidence in their estimate. The original number served as an anchor, biasing estimates to be closer to it. For the purposes of the replication we provided anchoring information before asking just for the estimated quantity for four of the topics from the original study - distance from San Francisco to New York City, popu- lation of Chicago, height of Mt. Everest, and babies born per day in the US for countries that use the metric system, we converted anchors to metric units and rounded them. 4. Retrospective gambler's fallacy (Oppenheimer & Monin, 2009). Oppenheimer and Monin (2009) inves- tigated whether the rarity of an independent, chance observation influenced beliefs about what occurred before that event. Participants imagined that they saw a man rolling dice in a casino. In one condition, participants imagined witnessing three dice being rolled and all came up 6's. In a second condition two came up 6's and one came up 3. In a third condition, two dice were rolled and both came up 6's. All partic- ipants then estimated, in an open-ended format, how many times the man had rolled the dice before they entered the room to watch him. Participants estimated that the man rolled dice more times when they had seen him roll three 6's than when they had seen him roll two 6's or two 6's and a 3. For the replication, the condition in which the man rolls two 6's was removed leaving two conditions. 5. Low-versus-high category scales (Schwarz, Hippler, Deutsch, & Strack, 1985). Schwarz and colleagues (1985) demonstrated that people infer from response options what are low and high frequencies of a behav- ior, and self-assess accordingly. In the original demon- stration, participants were asked how much TV they watch daily on a low-frequency scale ranging from ''up to half an hour'' to ''more than two and a half hours,'' or a high-frequency scale ranging from ''up to two and a half hours'' to ''more than four and a half hours.'' In the low-frequency condition, fewer partici- pants reported watching TV for more than two and a half hours than in the high-frequency condition. 6. Norm of reciprocity (Hyman & Sheatsley, 1950). When confronted with a decision about allowing or denying the same behavior to an ingroup and outgroup, people may feel an obligation to reciprocity, or consis- tency in their evaluation of the behaviors (Hyman & Sheatsley, 1950). In the original study, American par- ticipants answered two questions: whether communist countries should allow American reporters in and allow them to report the news back to American papers and whether America should allow communist report- ers into the United States and allow them to report back to their papers. Participants reported more sup- port for allowing communist reporters into America when that question was asked after the question about allowing American reporters into the communist coun- tries. In the replication, we changed the question slightly to ensure the ''other country'' was a suitable, modern target (North Korea). For international replica- tion, the target country was determined by the researcher heading that replication to ensure suitability (see supplementary materials). 7. Allowed/Forbidden (Rugg, 1941). Question phrasing can influence responses. Rugg (1941) found that respondents were less likely to endorse forbidding speeches against democracy than they were to not endorse allowing speeches against democracy. Respondents in the United States were asked, in one condition, if the US should allow speeches against democracy or, in another condition, whether the US should forbid speeches against democracy. Sixty-two percent of participants indicated ''No'' when asked if speeches against democracy should be allowed, but only 46% indicated ''Yes'' when asked if these speeches should be forbidden. In the replication, the words ''The United States'' were replaced with the name of the country the study was administered in. 8. Quote Attribution (Lorge & Curtiss, 1936). The source of information has a great impact on how that informa- tion is perceived and evaluated. Lorge and Curtiss (1936) examined how an identical quote would be per- ceived if it was attributed to a liked or disliked individ- ual. Participants were asked to rate their agreement with a list of quotations. The quotation of interest was, ''I hold it that a little rebellion, now and then, is a good thing, and as necessary in the political world as storms are in the physical world.'' In one condition the quote was attributed to Thomas Jefferson, a liked individual, and in the other it was attributed to Vladi- mir Lenin, a disliked individual. More agreement was observed when the quote was attributed to Jefferson than Lenin (reported in Moskowitz, 2004). In the rep- lication, we used a quote attributed to either George Washington (liked individual) or Osama Bin Laden (disliked individual). with four photos and asked to estimate the time of day at which they were taken. In the flag-prime condi- tion, the American flag appeared in two of these pho- tos. In the control condition, the same photos were presented without flags. Following the manipulation, participants completed an 8-item questionnaire assess- ing views toward various political issues (e.g., abor- tion, gun control, affirmative action). Participants in the flag-primed condition indicated significantly more conservative positions than those in the control condi- tion. The priming stimuli used to replicate this finding were obtained from the authors and identical to those used in the original study. Because it was impractical to edit the images with unique national flags, the American flag was always used as a prime. As a con- sequence, the replications in the United States were the only ones considered as direct replications. For inter- national replications, the survey questions were adapted slightly to ensure they were appropriate for the political climate of the country, as judged by the researcher heading that particular replication (see sup- plementary materials). Further, the original authors suggested possible moderators that they have consid- ered since publication of the original study. We included three items at the very end of the replication study to test these moderators: (1) How much do you identify with being American? (1 = not at all; 11 = very much), (2) To what extent do you think the typical American is a Republican or Democrat? (1 = Democrat; 7 = Republican), (3) To what extent do you think the typical American is conservative or liberal? (1 = Liberal; 7 = Conservative). 10. Currency priming (Caruso, Vohs, Baxter, & Waytz, 2013). Money is a powerful symbol. Caruso et al. (2013) provide evidence that merely exposing partic- ipants to money increases their endorsement of the current social system. Participants were first pre- sented with demographic questions, with the back- ground of the page manipulated between subjects. In one condition the background showed a faint pic- ture of US$100 bills; in the other condition the back- ground was a blurred, unidentifiable version of the same picture. Next, participants completed an 8-ques- tion ''system justification scale'' (Kay & Jost, 2003). Participants in the money-prime condition scored higher on the system justification scale than those in the control condition. The authors provided the ori- ginal materials allowing us to construct a near identi- cal replication for US participants. However, the stimuli were modified for international replications in two ways: First, the US dollar was usually replaced with the relevant country's currency (see supplemen- tary materials); Second, the system justification ques- tions were adapted to reflect the name of the relevant country.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Imagined contact (Husnu & Crisp, 2010; Study 1).", "text": "Recent evidence suggests that merely imagining con- tact with members of ethnic outgroups is sufficient to reduce prejudice toward those groups (Turner, Crisp, & Lambert, 2007). In Husnu and Crisp (2010), British non-Muslim participants were assigned to either imagine interacting with a British Muslim stranger or to imagine that they were walking outdoors (con- trol condition). Participants imagined the scene for one minute, and then described their thoughts for an additional minute before indicating their interest and willingness to interact with British Muslims on a four-item scale. Participants in the ''imagined con- tact'' group had significantly higher contact inten- tions than participants in the control group. In the replication, the word ''British'' was removed from all references to ''British Muslims.'' Additionally, for the predominately Muslim sample from Turkey the items were adapted so Christians were the out- group target. 12. Sex differences in implicit math attitudes (Nosek, Banaji, & Greenwald, 2002). As a possible account for the sex gap in participation in science and math, Nosek and colleagues (2002) found that women had more negative implicit attitudes toward math com- pared to arts than men did in two studies of Yale undergraduates. Participants completed four Implicit Association Tests (IATs) in random order, one of which measured associations of math and arts with positivity and negativity. The replication simplified the design for length to be just a single IAT. 13. Implicit math attitudes relations with self-reported attitudes ( Nosek et al., 2002). In the same study as Effect 12, self-reported math attitudes were measured with a composite of feeling thermometers and seman- tic differential ratings, and the composite was posi- tively related with the implicit measure. The replication used a subset of the explicit items (see supplementary materials).", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Procedure", "text": "The experiments were implemented on the Project Implicit infrastructure and all data were automatically recorded in a central database with a code identifying the sample source. After a paragraph of introduction, the studies were pre- sented in a randomized order, except that the math IAT and associated explicit measures were always the final study. After the studies, participants completed an instruc- tional manipulation check (IMC; , a short demographic questionnaire, and then the moderator measures for flag priming. See Table S1 2 for IMC and summary demographic information by site. The IMC was not analyzed further for this report. Each replica- tion team had a private link for their participants, and they coordinated their own data collection. Experimenters in lab- oratory studies were not aware of participant condition for each task, and did not interact with participants during data collection unless participants had questions. Investigators who led replications at specific sites completed a question- naire about the experimental setting (responses summarized in Table S1), and details and videos of each setting along with the actual materials, links to run the study, supplemen- tal tables, datasets, and original proposal are available at https://osf.io/ydpbf/.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Confirmatory Analysis Plan", "text": "Prior to data collection we specified a confirmatory analysis plan. All confirmatory analyses are reported either in text or in supplementary materials. A few of the tasks produced highly erratic distributions (particularly anchoring) requir- ing revisions to those analysis plans. A summary of differ- ences between the original plans and actual analysis is reported in the supplementary materials.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Summary Results", "text": "Figure 1 presents an aggregate summary of replications of the 13 effects, presenting each of the four anchoring effects separately. Table 2 presents the original effect size, median effect size, weighted and unweighted effect size and 99% confidence intervals, and proportion of samples that rejected the null hypothesis in the expected and unexpected direction. In the aggregate, 10 of the 13 studies replicated the original results with varying distance from the original Figure 1. Replication results organized by effect. ''X'' indicates the effect size obtained in the original study. Large circles represent the aggregate effect size obtained across all participants. Error bars represent 99% noncentral confidence intervals around the effects. Small circles represent the effect sizes obtained within each site (black and white circles for US and international replications, respectively).", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "2", "text": "Table names that begin with the prefix ''S'' (e.g., Table S1) refer to tables that can be found in the supplementary materials. Tables with no prefix are in this paper. original anchoring article did not provide sufficient information to calculate effect sizes for individual scenarios, therefore an overall effect size is reported. The Anchoring original effect size is a mean point-biserial correlation computed across 15 different questions in a test-retest design, whereas the present replication adopted a between-subjects design with random assignments. One sample was removed from sex difference and relations between implicit and explicit math attitudes because of a systemic error in that laboratory's recording of reaction times. Flag priming includes only US samples. Confidence intervals around the unweighted mean are based on the central normal distribution. Confidence intervals around the weighted effect size are based on noncentral distributions. effect size. One study, imagined contact, showed a signifi- cant effect in the expected direction in just 4 of the 36 sam- ples (and once in the wrong direction), but the confidence intervals for the aggregate effect size suggest that it is slightly different than zero. Two studies -flag priming and currency priming -did not replicate the original ef- fects. Each of these had just one p-value < .05 and it was in the wrong direction for flag priming. The aggregate ef- fect size was near zero whether using the median, weighted mean, or unweighted mean. All confidence intervals in- cluded zero. Figure 1 presents all 36 samples for flag prim- ing, but only US data collections were counted for the confirmatory analysis (see Table 2). International samples also did not show a flag priming effect (weighted mean d = .03, 99% CI [\u00c0. 04, .10]). To rule out the possibility that the priming effects were contaminated by the contents of other experimental materials, we reexamined only those participants who completed these tasks first. Again, there was no effect ( When an effect size for the original study could be calculated, it is presented as an ''X'' in Figure 1. For three effects (contact, flag priming, and currency priming), the original effect is larger than for any sample in the present study, with the observed median or mean effect at or below the lower bound of the 95% confidence interval for the ori- ginal effect. 4 Though the sex difference in implicit math attitudes effect was within the 95% confidence interval of the original result, the replication estimate combined with another large-scale replication (Nosek & Smyth, 2011) sug- gests that the original effect was an overestimate. suggests that very little in the variability of effect sizes can be attrib- uted to the samples, and substantial variability is attribut- able to the effect under investigation. To illustrate, Figure 2 shows the same data as Figure 1 organized by sample rather than by effect. There is almost no variation in the average effect size across samples.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Variation Across Samples and Settings", "text": "However, it is possible that particular samples would elicit larger magnitudes for some effects and smaller mag- nitudes for others. That might be missed by the aggregate analyses. Table 3 presents tests of whether the heterogene- ity of effect sizes for each effect exceeds what is expected by measurement error. Cochran's Q and I 2 statistics  None of the effects was moderated by which position in the study procedure it was administered. 4 The original anchoring report did not distinguish between topics so the aggregate effect size is reported. revealed that heterogeneity of effect sizes was largely ob- served among the very large effects -anchoring, al- lowed-forbidden, and relations between implicit and explicit attitudes. Only one other effect -quote attribution -showed substantial heterogeneity. This appears to be partly attributable to this effect occurring more strongly in US samples and to a lesser degree in international samples. To test for moderation by key characteristics of the setting, we conducted a Condition \u00b7 Country (US or other) \u00b7 Location (lab or online) ANOVA for each effect. Table 3 presents the essential Condition \u00b7 Country and Condition \u00b7 Location effects. Full model results are avail- able in supplementary materials. A total of 10 of the 32 moderation tests were significant, and seven of those were among the largest effects -anchoring and allowed- forbidden. Even including those, none of the moderation ef- fect sizes exceeded a g p 2 of .022. The heterogeneity in anchoring effects may be attributable to differences in knowledge of the height of Mt Everest, distance to NYC, or population of Chicago between the samples. Overall, whether the sample was collected in the US or elsewhere, or whether data collection occurred online or in the labora- tory, had little systematic effect on the observed results. Additional possible moderators of the flag priming effect were suggested by the original authors. On the US participants only (N $ 4,670), with five hierarchical regres- sion models, we tested whether the items moderated the effect of the manipulation. They did not (p's = .48, .80, .62, .07, .05, all DR 2 < .001). Details are available in the online supplement.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Discussion", "text": "A large-scale replication with 36 samples successfully rep- licated eleven of 13 classic and contemporary effects in psychological science, some of which are well-known to be robust, and others that have been replicated infrequently or not at all. The original studies produced underestimates of some effects (e.g., anchoring-and-adjustment and allowed versus forbidden message framing), and overesti- mates of other effects (e.g., imagined contact producing willingness to interact with outgroups in the future). Two effects -flag priming influencing conservatism and cur- rency priming influencing system justification -did not replicate. A primary goal of this investigation was to examine the heterogeneity of effect sizes by the wide variety of samples and settings, and to provide an example of a paradigm for testing such variation. Some studies were conducted online, others in the laboratory. Some studies were conducted in the United States, others elsewhere. And, a wide variety of educational institutions took part. Surprisingly, these fac- tors did not produce highly heterogeneous effect sizes. Notes. Tasks ordered from largest to smallest observed effect size (see Table 2). Heterogeneity tests conducted with R-package metafor. REML was used for estimation for all tests. One sample was removed from sex difference and relations between implicit and explicit math attitudes because of a systemic error in that laboratory's recording of reaction times. *Moderator statistics are F value of the interaction of condition and the moderator from an ANOVA with condition, country, and location as independent variables with the exception of relations between impl. and expl. math attitudes for is reported the F value associated with the change in R squared after the product term between the independent variable and the moderator is added in a hierarchical linear regression model. Details of all analyses are available in the supplement. Intraclass correlations suggested that most of the variation in effects was due to the effect under investigation and almost none to the particular sample used. Focused tests of moderating influences elicited sporadic and small effects of the setting, while tests of heterogeneity suggested that most of the variation in effects is attributable to measure- ment error. Further, heterogeneity was mostly restricted to the largest effects in the sample -counter to an intuition that small effects would be the most likely to be variable across sample and setting. Further, the lack of heterogeneity is particularly interesting considering that there is substan- tial interest and commentary about the contingency of effects on our two moderators, lab versus online (Gosling, Vazire, Srivastava, & John, 2004;Paolacci, Chandler, & Ipeirotis, 2010), and cultural variation across nations ( Henrich et al., 2010). All told, the main conclusion from this small sample of studies is that, to predict effect size, it is much more impor- tant to know what effect is being studied than to know the sample or setting in which it is being studied. The key vir- tue of the present investigation is that the study procedure was highly standardized across data collection settings. This minimized the likelihood that factors other than sam- ple and setting contributed to systematic variation in effects. At the same time, this conclusion is surely con- strained by the small, nonrandom sample of studies repre- sented here. Additionally, the replication sites included in this project cannot capture all possible cultural variation, and most societies sampled were relatively Western, Educated, Industrialized, Rich, and Democratic (WEIRD; Henrich et al., 2010). Nonetheless, the present investigation suggests that we should not necessarily assume that there are differences between samples; indeed, even when mod- eration was observed in this sample, the effects were still quite robust in each setting. The present investigation provides a summary analysis of a very large, rich dataset. This dataset will be useful for additional exploratory analysis about replicability in general, and these effects in particular. The data are avail- able for download at the Open Science Framework (https:// osf.io/ydpbf/).", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Conclusion", "text": "This investigation offered novel insights into variation in the replicability of psychological effects, and specific infor- mation about the replicability of 13 effects. This methodol- ogy -crowdsourcing dozens of laboratories running an identical procedure -can be adapted for a variety of inves- tigations. It allows for increased confidence in the existence of an effect and for the investigation of an effect's depen- dence on the particular circumstances of data collection (Open Science Collaboration, 2014). Further, a consortium of laboratories could provide mutual support for each other by conducting similar large-scale investigations on original research questions, not just replications. Thus, collective ef- fort could accelerate the identification and verification of extant and novel psychological effects.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Note From the Editors", "text": "Commentaries and a rejoinder on this paper are available (Crisp, Miles, & Husnu, 2014;Ferguson, Carter, & Hassin, 2014;Kahneman, 2014;Klein et al., 2014;Monin & Oppenheimer, 2014;Schwarz & Strack, 2014;doi: 10.1027/1864).", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Acknowledgments", "text": "We thank Eugene Caruso, Melissa Ferguson, Daniel Oppenheimer, and Norbert Schwarz for their feedback on the design of the materials. This project was supported by grants to the second and fifty-first authors from Project Implicit. Ratliff and Nosek are consultants of Project Impli-cit, Inc., a nonprofit organization that includes in its mission ''to develop and deliver methods for investigating and applying phenomena of implicit social cognition, including especially phenomena of implicit bias based on age, race, gender or other factors.'' Author contributions: Designed research: R. K., B. N., K. R.; Translated materials: S. B., K. B., M. Brandt, B. B., Z. C., N. F., E. G., F. H., H. I., R. K., R. P., A. V., M. Vianello, M. Vranka; Performed research: R. A., S. B., M. Bernstein, K. B., M. Brandt, C. B., Z. C., J. C., W. C., W. D., T. D., M. E., N. F., D. F., E. G., J. A. H., J. F. H., S. J. H., J. H., H. I., M. J., J. J., H. K., R. K., L. K., J. K., C. L., R. M., W. M., A. N., J. N., G. P., R. P., K. R., A. R., K. S., J. L. S., R. S., T. S., J. S., L. V., D. T., A. V., L. V., M. Vranka, A. L. W., J. W.; Analyzed data: M. Vianello, F. H., R. K.; Wrote paper: B. N., K. R., R. K., M. Vianello, J. C. We report all data exclusions, manipulations, measures, and how we deter-mined our sample sizes either in text or the online supple-ment. All materials, data, videos of the procedure, and the original preregistered design are available at the project page https://osf.io/ydpbf/.", "title": "Investigating Variation in Replicability A ''Many Labs'' Replication Project", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Abstract", "text": "This paper considers some of the interdisciplinary scholarship on radio and sound more generally for the purposes of considering how geopolitical scholarship might reconsider its predominantly visual focus. The fi rst part considers radio and its relationship to studies of propaganda, international diplomacy and even everyday life. Thereafter, attention is given to new themes such as researching radio cultures, broadcasting infrastructure and technology and, fi nally, the affective impacts of radio on audiences. The conclusion of this paper urges further critical consideration of radio, sound and broadcasting/listener engagement with the well-established geographical literature on music.", "title": "Radio geopolitics: broadcasting, listening and the struggle for acoustic spaces", "file_name": "Pinkerton and Dodds - 2009 - Radio geopolitics broadcasting, listening and the.pdf"}
{"section": "I Introduction", "text": "We have some places. Just stay quiet and you'll be OK. We are returning to the airport. We open this article, as does the 9/11 Com- mission Report (2004), with the words attributed to Mohammad Atta, the alleged ringleader of the 19 hijackers responsible for the attacks on the Twin Towers of the World Trade Center, the Pentagon and the crash of United 93 in a Pennsylvanian field. This radio transmission made by the hijackers on board American Airlines Flight 11 was the first aural clue for federal air traffic controllers seeking to comprehend the unusual movements (and lack of radio communication) of a number of planes fl ying over the eastern part of the United States. Given the subsequent developments, these words, as some journalists noted at the time, have an unquestionably 'chilling quality' not least because the hijackers perished with nearly 3,000 other victims. Unable to see the hijackers or the other occupants of American Airlines Flight 11, those on the ground were reliant on listening to the occasional com- munications by the hijackers. And then radio silence as Flight 11 crashed into the Twin Towers at 0847 am. Shortly after the impact of the second plane into the Twin Towers, terrified on- lookers were shown, via television cover- age, exhibiting a range of responses from stunned silence to crying, wailing and hold- ing highly animated conversations with fellow onlookers. To describe these attacks *Author for correspondence: Email: a.d.pinkerton@rhul.ac.uk as a visual assault on America is to miss a fundamental point; this was an audiovisual spectacle of horrifying proportions. As the buildings began to smoulder, the sight and sound of people jumping from the Twin Towers to their certain death was captured on film. Produced by two French brothers (Jules and Gedeon Naudet) who accom- panied members of the New York Fire Department to the Twin Towers, the docu- mentary film 9/11 (2002) is particularly poignant in that regard. It remains the only footage taken inside the Twin Towers on 11 September 2001. Finally, the thundering noise accompanying the collapse of the Twin Towers and the diffusion of suffocat- ing clouds of dust completed, one might say, the first deadly phase of this audiovisual spectacle (Debrix, 2007). The recorded statements of the hijackers and other utterances such as 'let's roll' issued by one of the passengers on board United 93 has been much analysed and debated by official investigators, political leaders and relatives of the dead. The bipartisan 9/11 Commission, created by the Bush admin- istration to investigate the 11 September at- tacks, considered all forms of sonic and visual information germane to their extensive brief. Mobile phone conversations between the airborne victims of 9/11 and their friends and relatives have provided further poignant sonic testimony of those last minutes before impact and certain death. 1 Those conver- sations form part of what National Public Radio described as their Sonic Memorial project. As Cohen and Willis (2004) record: How could public radio -a medium focussed solely on aural experiences -contribute to storytelling these tragedies? After September 11th, National Public Radio (NPR) took up the task of aural remembering for the nation through an unprecedented public radio col- laboration, consisting of at least 100 NPR stations and their affiliates that contributed and aired up to 30 hours of NPR-produced programming during the week before the anniversary [ie, September 2002]. (Cohen and Willis, 2004: 592) Moreover, NPR also produced a Sonic Memorial on the internet, hosted by the September 11th Digital Archive. 2 Radio broadcasting (and noise for that matter) is playing a major part in the American-led War on Terror. Following James Sidaway's interventions on the sub- ject matter of 'banal geopolitics', we should draw attention to the manner in which radio is now routinely used by military personnel in their pursuit of those who masterminded the 11 September assault on New York and Washington, and associated counter-terror strategies (Sidaway, 2001;2003;Graham, 2007). As part of developing new urban- based fi ghting strategies, the American mili- tary has created a series of training environ- ments, which include 'bombarding' US troops with radio broadcasting in Arabic for the pur- pose of developing a sense of geographical and cultural 'authenticity', while simultan- eously desensitizing the troops to foreign cultural practices. If we wanted to understand more fully some of the popular cultural dimensions to post-9/11 America, then talk radio would be an excellent place to begin such an investi- gation (see Croft, 2006). Inaugurated in the 1930s and 1940s, talk radio fundamentally changed in the 1980s when the Federal Com- munications Commission removed the so- called 'fairness doctrine', thereby absolving radio stations from having to provide a 'right to reply' for those criticized. The consequ- ences of such a decision were substantial and radio hosts such as Rush Limbaugh have been able to fl ourish and enjoy large listening audiences. Talk radio has proven to be an important place for politically right-wing radio commentators and listeners to air their grievances about the world, a broadcasting phenomenon that was illustrated in Oliver Stone's timely, and appropriately named, movie Talk Radio, released in 1988. In other areas relating to the War on Terror, loud music and sound more generally have been deployed on suspected terrorists around the world in the belief that these sonic activities will lead to vital intelligence being divulged. In Iraq and Afghanistan, companies such as Homeland Securities Strategies Inc are developing new radio jamming technologies in the hope that they will be able to interfere with improvised explosive devices placed by the sides of high- ways and other strategic places. Finally, the Department of State is funding, via the Voice Of America (VOA), a plethora of new radio stations and local language broadcasting in the hope that America's 'soft power' will be able to persuade millions in the Global South not to support violence directed against American personnel and material interests. While we have opened with some ob- servations about 9/11 and its aftermath, this article's more general purpose is to shift the centre of gravity towards radio for the pur- pose of broadening popular geographical/ geopolitical horizons and encouraging further work, which has tended to be dominated by social and cultural geographical interest in music rather than radio broadcasting, or listening more generally (see, for example, Smith, 1994;Leyshon et al., 1998;Jazeel, 2005;Jones, 2005;Matless, 2005). If the twentieth century was in part characterized by the invention of the car and the moving image, it was also the century of mechanic- ally reproduced sounds (Bull, 2004: 248;Whittington, 2007: 1; and other scholars such as Mattellart, 1996;Hugill, 1999). This paper is composed of three substantial parts and initially it considers radio within the popular geopolitics literature for the purpose of explaining its importance especially in the Global South. Radio's role in public diplomacy deserves particular mention here and the role this media technology has played in postwar international crises such as the 1956 Suez Crisis and the 1982 Falklands campaign. There is of course a large literature on radio propaganda during the interwar period and the second world war, which will also be noted. Second, we consider some research themes that deserve further elaboration by geographers such as 'researching radio' and the sound archive, radio and broadcasting infrastructure, and radio and its audiences. Finally, the paper connects up with the established literature on music to consider the geographical implications and consequences of sound. While we specifi cally address radio and its geopolitical implications, we are mind- ful of a long-established body of literature more generally concerned with communica- tions, technology and the role of states and empires (for example, Innis, 1950;Mattellart, 1996;Hugill, 1999;Meinig, 2004).", "title": "Radio geopolitics: broadcasting, listening and the struggle for acoustic spaces", "file_name": "Pinkerton and Dodds - 2009 - Radio geopolitics broadcasting, listening and the.pdf"}
{"section": "II Popular geopolitics of radio: identity, propaganda and soft power", "text": "Shortly after President Bush's decision to declare a War on Terror, the US State Depart- ment-funded Voice of America (VOA) was given extra funding so that it could expand radio broadcasting in Arabic, Dari, Farsi and Pashto. Created in February 1942, Voice of America is the one of the largest state- funded broadcasters and rivals BBC World Service and Radio Moscow in terms of its broadcasting coverage, language provision and weekly audiences. In 2002 a Middle East Radio Network was launched and one of the most signifi cant developments was the launch of Radio Farda, which is designed to broadcast news and music in English and Farsi to Iran and Central Asia. New invest- ment in radio broadcasting and public diplo- macy was judged to be critical in establishing a broader legitimacy for more violent forms of intervention. In his weekly radio address, President Bush announced on 15 September 2001 that: This is a conflict without battlefields or beachheads, a conflict with opponents who believe they are invisible. Yet, they are mistaken. They will be exposed, and they will discover what others in the past have learned: those who make war against the United States have chosen their own destruction. Victory against terrorism will not take place in a single battle, but in a series of decisive actions against terrorist organizations and those who harbor and support them. We are planning a broad and sustained cam- paign to secure our country and eradicate the evil of terrorism. And we are determined to see this confl ict through. Americans of every faith and background are committed to this goal. 3 Radio broadcasting remains part of that 'broad and sustained campaign' and this is the case not only with regard to overseas broadcasting but also domestically, notwith- standing the highly televised culture of the United States. It is important to recall that the weekly radio address by the President has been a regular feature of American pub- lic life since the Franklin D. Roosevelt ad- ministration , and there are many other examples we could draw upon to high- light the iconic role of radio broadcasting involving European political leaders and dic- tators, such as Charles de Gaulle, Winston Churchill, Joseph Stalin and Benito Mussolini as well as postcolonial leaders such as Nehru and his 1947 independence broadcast. America's commitment to direct funding towards overseas radio broadcasting in the post-9/11 era was motivated by a simple but important development. Since the 1960s, the widespread availability of the transistor radio has meant that many communities, without access to television and uninter- rupted power supplies, continue to depend on the radio for their news and other output such as music. Major broadcasting organiza- tions such as VOA and BBC World Service, as a consequence, have recognized that local language broadcasting offers opportunities to reach and potentially infl uence audiences in countries such as Afghanistan, Pakistan and Iran. Local language broadcasting has often, as a consequence, been highly sen- sitized to regional geopolitical change. One example from an earlier era involves VOA radio broadcasting in Spanish and Portuguese to Latin America. In 1956, VOA broadcasting to Latin America consisted of one hour of daily programming and was in English. Foreign language broadcasting had been downgraded as VOA invested its energies in eastern European and southeast Asian language programming. By March 1960, and in the aftermath of the 1959 Cuban revolution, VOA Spanish language broadcasting was restored, and a year later Portuguese language broadcasting resumed. 17% of total VOA foreign programming was directed towards Latin America and a new radio station was constructed in Central America (Fejes, 1986: 170-71). While national governments in the Global South have sought in the past to 'block' trans- missions from foreign broadcasters, radio transmission -especially if broadcast from powerful transmitters located around the world -is extremely effective in transcending national boundaries. This does not mean, however, we should assume that such fl ows of sound are automatically tied into influ- ence and the subsequent manipulation of political behaviour and collective identities. The record here, as we shall note, is mixed. In the case of recent endeavours by the American government, any transmission associated with VOA is, for some listeners, treated with contempt and suspicion, be- cause of the contemporary military occupa- tion of Afghanistan and Iraq (Gregory, 2004). Colonial radio stations in the English, French and Dutch speaking worlds were frequently linked to expressions of anticolonial agi- tation and national liberation movements. Moreover, as Katz and Weddell (1977: 8) noted, 'many were prepared to accept train- ing by expatriate broadcasters in order to prepare themselves for the day when lib- eration would come'. Such training would have disastrous consequences in Rwanda in April 1994 when so-called 'hate radio' was instrumental in persuading thousands to slaughter their fellow citizens. The ensu- ing genocide was later to encapsulate no fewer than 800,000 lives and the Inter- national Criminal Tribunal in Arusha indi- cted broadcasters associated with Radio Television Libres des Mille Collines for their role in transmitting 'hate propaganda' (see Prunier, 1995;Thompson, 2007). In other disciplines such as history and media studies, there has been a great deal of interest in radio propaganda, foreign lan- guage broadcasting and public diplomacy, which would complement a growing interest in popular geopolitics (see, for example, Douglas, 1999;Horten, 2002). In the case of the latter, it is now entirely orthodox to claim that popular representations of global geo- politics need to be investigated alongside elite understandings. Indeed terms such as 'geopolitical culture' have been deployed in order to consider how fi lm, cartoons, comic books and television contribute to expressions of popular geopolitics and how and with what consequences audiences react to those media (Dodds, 2007). Recent research has demon- strated how certain kinds of emotional re- actions and investments may be provoked and mobilized by particular productions such as a fi lm like Black Hawk Down (2001) or a comic character like Captain America (Dittmer, 2005;Carter and McCormack, 2006). Geographical scholars such as Chris Gibson and his co-workers have helped to ensure that popular geopolitical scholars do consider how music contributes to debates about contemporary communications, popu- lar culture and national identity (Gibson, 1996;Dunbar-Hall and Gibson, 2000;Gibson and Connell, 2003;but critically see Power, 2001). Gibson's researches into the music in Australia has been particularly effective in highlighting how indigenous communities can express social identities and contest the hegemonic representations of Anglophone and white Australia. There is surely much more work to be done on the popular geo- politics of music and Anglophone performers and bands such as Bob Dylan, Midnight Oil and Neil Young would feature, as would par- ticular musical genres, such as gangster rap and rock, which have critically refl ected on confl icts from Vietnam to Iraq. Until recently only one geopolitical scholar, Marcus Power, had explored in any detail the geopolitical consequences of radio broadcasting, in this case with reference to colonial Mozambique (Power, 2001;cf. Pinkerton, 2007;2008a). The absence, therefore, of detailed con- sideration of radio is a major lacuna and de- serves to be remedied as the critical geopolitics project enters its third decade of existence. The task is not daunting in the sense that other disciplines have been interested in radio broadcasting and listening for quite some considerable time. With regards to making connections with popular geopolitics, we would highlight several strands of this re- search as relevant. First, there has been exten- sive interest in radio propaganda especially since the 1920s with many research papers and monographs devoted to Nazi Germany, Soviet Russia and other European states such as Italy, Spain and the United Kingdom (for example, Miller, 1941;Isola, 1995;Shirer, 1999;Menduni, 2004). In his book Backing Hitler, Robert Gellately not only shows the importance of radio to fascist German propa- ganda but also traces the penalties imposed on those citizens caught or exposed listening to overseas broadcasting by the BBC and others. Listening was a matter of life and death and this remains the case in many parts of the world (Gellately, 2002: 186-87). If radio has been used for propaganda purposes, it has also been a medium that gov- ernments have sought to control, 'to jam' and to limit access to other forms of broad- casting. The apparent ubiquity of radio should not be overestimated as national governments have and continue to attempt to engage in political suppression, prohibit foreign broadcasting, limit ownership of radio transmitters and create conditions where listening is far from mundane. Instead, radio listening in contemporary Iran and Zimbabwe and the former Soviet Union could be a dangerous, frustrating and deadly experience. China continues to jam radio broadcasting alongside the internet. Cuba remains particularly active in 'blocking' radio broadcasting from exile groups in Miami and the US government funded Radio Marti. Alternatively, others have pointed to the ability of radio broadcasting, often in dire circumstances, to provide some comfort and solace, as the British captive Terry Waite acknowledged. Held for several years in Beirut, Waite and his fellow captives recorded their gratitude for BBC World Service broad- casting because it, in his words, 'helped to keep us alive both spiritually through the work of the religious departments and mentally through the varied cultural and news programmes' (Walker, 1992: 165). In different ways, therefore, the geographies of broadcasting and listening continue to be highly contingent. Second, the radio studies literature has long been preoccupied with domestic radio environments in the western world and keen to decipher codes, conventions, formats and practices of radio programming and recep- tion. Well-known works such as Radio broad- casting: an introduction to the sound medium have been important in shaping future scholar- ship with its focus on the American domestic radio scene and international broadcasting (Hillard, 1985). As a commercial medium, radio has survived and even prospered in 'tabloid culture' America ( Glynn, 2000). Its enduring success owes in part to the fact that radio can be listened (and engaged with more generally) in the car, at home, in the work- place and on the street (Hendy, 2000: 2). Moreover, the diversifi cation of radio broad- casting (with the obvious exception of US National Public Radio) has allowed for a series of overlapping listening environments to exist. These specialized listening popula- tions are in some cases further served and encouraged by the development of high- profi le 'shock-jock' radio presenters such as Howard Stern who enjoy a highly defined listening segment as defi ned by race, gender, educational attainment, political persuasion and age. Securing such segments is vitally important for commercial radio stations be- cause of the strong connection with advert- ising and revenue generation. As historians of American radio have noted, the place of this medium in national life has long been contested and as such pre- dates some of the contemporary contro- versies associated with outspoken right-wing radio presenters ( Barfi eld, 1986). In the 1920s, for instance, American jazz was banned on most radio stations because it was judged by regulatory authorities to be both scandalous and subversive. Some radio stations resisted the ban and the music of Louis Armstrong and Bessie Smith was played in Chicago and New York. According to Susan Douglas (1999), jazz was seen as threatening because it was equated with 'jungle music' and thus would promote an erosion of the sonic-social order in the sense of promoting sexual aban- donment. While jazz was later aired more generally in the 1930s and 1940s, African- American musicians attracted large numbers of white listeners and indeed jazz was cre- dited with playing a part in building cross- community solidarities during the struggle for civil rights in the 1955-65 period. Such solidarities need to be set alongside the production and circulation of racist re- presentations of African-Americans via the radio. Shows such as 'Amos'n' Andy' were hugely important, if controversial, in devel- oping the radio serial drama and in this case concentrated on the changing social and eco- nomic fortunes of two African-American workers (Douglas, 1999). While resisted and resented by many African-American listen- ers, radio was later to provide an important public forum for the discussion of racial pol- itics in contemporary American life. By 1941, nearly every family in the United States owned a receiver and radio created an aural public sphere for acknowledging, in pro- grammes such as federally funded 'Freedom's People', the contribution African-American communities have made to labour, music, sports, literature, military service and edu- cation (Savage, 1999: 70). The year 1943 was a landmark in American radio history, as Barbara Savage noted, because funding and broadcasting restrictions tightened in response to concerns from African-American communities about segregation and discrim- ination in American life. In postwar America, progressive dissent continued to appear on the airwaves and radio's place in the social and cultural transformation of the United States is beginning to attract ever more cri- tical attention (see, for example, Hilmes and Loviglio, 2002). Scholars such Paddy Scannell (1991), Andrew Crissell (1996), Martin Shingler and Cindy Wierniga (1998) have considered radio's role in the social and cultural lives of many citizens, including Americans. As Scannell's landmark edited volume showed, radio's 'expressive dimensions of communi- cation, how things are said, why and for what possible effects' was considered in great detail (Scannell, 1991: 11). There are two aspects to this apparent ubiquity. On the one hand, scholars have examined the role of radio in the making of 'extraordinary' rituals and ceremonies in modern society such as the 1953 coronation of Elizabeth II, the trial of Nazi war criminals and the role of radio during the second world war. Radio audiences were often signifi cant, especially in countries where access to television was either just becoming widespread or had yet to emerge. However, in a salutary study of the role of radio in the 1961 trial of Adolf Eichmann in Jerusalem, a team of scholars found that the coverage by Israeli state radio was consistently overestimated in frequency and duration by listeners. In fact, radio broad- casting was more sporadic than people re- called and many believed that the medium must have featured more strongly in daily life because of the sheer magnitude of this public event (Pinchevski et al., 2007: 18). On the other hand, the radio has played (and continues to play) a major role in the more mundane moments of everyday life. As Shingler and Wierniga (1998: ix) contend, 'For many of us, it is the fi rst thing we hear in the morning and the last thing we hear be- fore we fall asleep'. Radio listening whether in bed, while taking a shower or eating our breakfast is part of the daily fabric of many people around the world. The capacity of radio to be so widespread and intrinsic to our daily lives has contributed to it being taken for granted at the expense of other media such as television. In Britain, for example, few people inter- ested in the daily machination of domestic politics would fail to tune in and listen to the morning news programme 'Today' on Radio 4. The so-called Andrew Gilligan affair of May 2003, in the midst of concerns over Iraq's Weapons of Mass Destruction capabilities, highlighted quite how important radio broadcasting and listening reactions could be. In this case a well-known Radio 4 defence journalist (Gilligan) used his short early morning report to claim that a highly placed security source had told him that the Labour government had 'sexed up' its dossier on Iraq's military capabilities for the sole purpose of persuading parliamentari- ans and the public that the March 2003 invasion of Iraq was necessary. The reaction to the broadcast was extraordinary and led to the BBC being forced to apologize for the report after the so-called Hutton Inquiry found in favour of the Labour government (see also the Neil Report, 2004). It also, tragically, led to the suicide by Gilligan's source -Dr David Kelly. Alternatively, as Fraser MacDonald has noted with regard to the nightly weather broadcast on Radio 4, 'the cadences and rhythms of the UK shipping forecast have a soporific effect \u2026 a familiar and comforting register of sea areas and coastal stations that have come to define the symbolic boundary of the nation' (MacDonald, 2006: 628). What both cases share in common is that listening to Radio 4 is an important daily ritual for many citizens in the United Kingdom. David Hendy's (2000) Radio in the global age is arguably the most impressive inter- vention by a radio studies scholar because of his willingness to explore how radio broad- casting and listening is now a global phe- nomenon, even if there are signifi cant local and regional variations depending inter alia on the integration with internet technologies, commercialization, listener availability, pol- itical control and transnational connections with other broadcasters. As Hendy notes: While being a local medium par excellence, radio is able to reach across large spaces, potentially threatening place-specifi c cultures with its homogenized content, potentially forging new delocalized communities of interest; it has a history in which nation-states often led the way in establishing services, but its oral code of communication allows it to tie itself to communities of language, which ignore offi cial borders; it betrays a commercial imperative to reach large, high-spending audi- ences, but it also has a cost structure, which creates at least the possibility of a community station surviving on the tiniest of audiences. It is, in short, the most adaptable of 'media' in fi nding its audience. (Hendy, 2000: 215) In this way, radio is often imbued with an aura of accessibility and democratizing potential (listeners can be contributors and thus not just passive receivers) even if there is a long history of radio broadcasting being either tightly regulated by states or restricted by national authorities throughout the Global North and South. Finally, within International Relations and Political Science, there has been interest in radio as a form of 'soft power' and its role in public diplomacy during and after the second world war. In his well-known work, Joseph Nye (2004a) has claimed that the Bush administration has lost interest in cul- tural diplomacy and what he has called 'soft power'. As he noted in an article in Foreign Affairs: Skeptics of soft power (Secretary of State Donald Rumsfeld professes not even to understand the term) claim that popularity is ephemeral and should not guide foreign policy. The United States, they assert, is strong enough to do as it wishes with or without the world's approval and should simply not accept that others will envy and resent it. The world's only superpower does not need permanent allies; the issues should deter- mine the coalitions, not vice versa, according to Rumsfeld. (Nye, 2004b: 16) Given that Secretary of State Rumsfeld no longer holds offi ce, this sceptical view may no longer be quite so prevalent in the face of rising anti-Americanism and the mount- ing losses of US service personnel in Iraq. However, Nye detected an apparent dis- regard of the power of media including radio to 'persuade' others about America's interventions in southwest and central Asia. As he noted, 'During the Cold War, radio broadcasts funded by Washington reached half the Soviet population and 70 to 80% of the population in Eastern Europe every week; on the eve of the September 11th attacks, a mere 2% of Arabs listened to the Voice of America' (Nye, 2004b: 18). The problem, however, with this kind of assertion is that it is implicitly assumed that listening is some- how correlated with potential infl uence. The other key difference between Cold War east- ern Europe and the contemporary Muslim world is that the United States had not invaded and occupied an Eastern European state and supported other countries such as Israel, which many Arabic speaking communities believe disadvantages Palestinians. In other words, the question of legitimacy is likely to be critical in determining potential infl uence. Other studies on radio diplomacy and 'soft power' have in the past been better at examining radio broadcasting and state controls rather than listening audiences and potential effectiveness. Hale (1975) and Rawnsley (1996) both consider British broad- cast policies and particular international crises and confl icts in the post-1945 era. In the case of Rawnsley, for example, his work considers radio broadcasting with regard to the 1956 Suez Crisis, the 1956 Hungarian Uprising, the 1962 Cuban Missile Crisis and the American engagement with Vietnam. In each case, British and American broadcasting was examined and judged with regard to how it reported these particular crises and the kinds of political and broadcasting confl icts reports from the BBC and VOA engendered. The earlier work by Hale considers three models of international radio broadcasting - the Nazi, the Communist and the BBC. In conclusion, Hale contends that the BBC remains somewhat different to Nazi and Communist broadcasters because it sought since the creation of the Empire Service in 1927 to provide 'reliable' news and informa- tion even if that meant clashing with the British government during the second world war. Hale (1975: ix-x) claimed that by the end of the confl ict, a clear distinction existed between the overt and offensive propaganda of Nazi broadcasting compared to the BBC's reputation for 'reliable' news broadcasting. The American and Soviet broadcasters were judged to be single-minded in their deter- mination to pursue their national priorities as reflected by state-controlled radio stations such as Voice of America and Radio Moscow respectively. More recent research, however, would call into question the claim regarding the BBC's reputation for reliable and impartial broadcasting. Although Hale (1975: 48) rec- ognized that this claim might be a 'carefully cultivated myth', he maintained that many listeners continued to use the BBC World Service in particular as a means to 'check up on the news that they had heard from other sources ' (1975: 48). While there is some validity to this claim that other news organizations do indeed use BBC reporting as a benchmark to judge particular news items, it completely underestimates how the 'reputation' of the BBC itself has been historically and geographically contested (Pinkerton, 2007). In Iran, for example, many Iranian listeners still complain to this day that the BBC is unreliable and an agent of British imperialism because of its radio reports concerning the 1953 CIA-backed coup against the Mosaddeq government. Elsewhere in south Asia, for instance, the BBC has enjoyed a chequered history of listening as it has been accused also of being either an agent of British imperialism and/or pro-Pakistani in its reporting of particular crises. Alternatively, there are still many listeners who tune into the BBC World Service in both countries and value it for its news and current affairs reporting. Audience figures alongside listener letters and email comments sent to local BBC offi ces provide some invaluable insights into these particular listening communities. Voice of America and the BBC World Service are funded by the State Department and the Foreign Offi ce respectively. Likewise Radio Moscow continues to be funded by the Russian government. While editorial control and content varies between these three global broadcasters, former Foreign Secretary Robin Cook described the BBC's as Britain's 'Voice around the World' de- spite its claims to impartiality and editorial independence. There is still a great deal of research to be done on the impact on listeners and listener communities, especially in the Global South. The reputations and impact of global broadcasters has been varied even if each has the capacity to broadcast over large areas that far exceed their country's respective national boundaries. Moreover, as earlier studies did not readily acknow- ledge, listeners to the BBC, VOA and Radio Moscow also listen, read and watch other media such as television and newspapers. So, in the case of the 1956 Suez Crisis, readers of the Observer and the Guardian (which were critical of the Anglo-French intervention) might have also listened to BBC World Ser- vice broadcasts because they believed them to be willing to be critical of offi cial govern- ment statements about the nature and long- term purpose of the intervention (Shaw, 1996). As Roger Silverstone (1994) had noted, people have their own 'media signatures' and researchers need to understand better how and with what consequences people access different forms of media.", "title": "Radio geopolitics: broadcasting, listening and the struggle for acoustic spaces", "file_name": "Pinkerton and Dodds - 2009 - Radio geopolitics broadcasting, listening and the.pdf"}
{"section": "III Media and popular geopolitics: new directions", "text": "The neglect of radio by existing studies of popular geopolitics is unfortunate but not unsurprising given the preoccupation with visual media and the visual tradition of geopolitics more generally. As \u00d3 Tuathail argued in Critical geopolitics (1996), the intel- lectual tradition of geopolitics has long been preoccupied with the visual whether it be in the form of the Olympian gaze or maps and other representations of global geopolitical space. Critical geopolitical scholars have tended to focus their energies on visual media such as film, television and comics and textual productions such as magazines like the Reader's Digest, which combine text and images (Sharp, 2000). This apparently visual preoccupation needs to be treated with caution, however. In one key respect, terms such as 'visual media', as Mitchell (2005: 258) has warned us, are misleading. As he notes, there is no such thing as 'visual media' as all media including radio are 'mixed media' that involve other senses such as touching, looking and listening. One only has to consider how radio listening often involves looking at the radio transmitter to appreciate that it may well be a case of simply acknowledging a visual predominance. As a form of 'mixed media', radio and fi lm (for instance) involve a braiding and nesting of these senses. As noted earlier, we believe that this neglect of radio is, if you forgive a visual pun, short-sighted. Three areas deserve further attention -researching radio, radio and broad- casting infrastructure, and listening. First, with regard to researching radio, we need to consider how this intellectual fi eld might be further developed. While radio would appear to have an ethereal quality, which might make research appear at fi rst glance problematic, it is not impossible to reconstruct the varied geographies and geopolitics of broadcast- ing and listening. Radio, as Tamar Liebes has noted, is capable of creating a series of 'acoustic spaces' through which listeners and communities can express their collective identities. In her research, Liebes (2006) examined Israeli broadcasting and sought to reconstruct how listeners were joined to- gether through ethereal listening networks. Using personal accounts, novels and news- paper reports and articles within the Israeli media, alongside interviews and oral testi- monies, she focused on how those radio reports had contributed to a sense of public memory and collective geohistory on the eve of Israeli independence. This task was made all the harder in the sense that there were no offi cial recordings of the broadcasts and many of the interviewees were being asked to remember associated events from over 60 years ago. It is precisely because radio is often not stored in the same manner as news- papers and other printed texts in archives that led researchers to underestimate the sig- nifi cance of radio broadcasting and listening. As Liebes noted, 'The ephemeral quality of the medium, and in Israel's case, the gradual disappearance of records and recordings, and the (partly resultant) reliance of historians on print, contribute to an incorrect recollection of the salience of radio on moulding uniform public outlooks' (Liebes, 2006: 70). Her research raises some interesting questions about researching radio and, for instance, the relationship between acoustic and textual sources in radio broadcasting and listening research. As Scannell and Cardiff (1991) refl ected: There is an inescapable paradox at the heart of this project of which we have been acutely aware of all along -our object of study no longer exists. The early pioneers of radio as an art form lamented the 'ghastly impermanence' of their medium. Radio, and later television, developed fi rst as live systems of transmission and recording technologies came later. Thus, although there are some recordings of the more signifi cant programmes broadcast from the mid thirties onwards, the vast bulk of output perished in the moment of transmission. The fleeting, unrecorded character of early radio seems obstinately to resist the possibility of historical reclamation. ( Scannell and Cardiff, 1991: xiii) In their research on radio, Scannell and Cardiff draw upon radio's textual presence in the form of minutes of BBC Management Boards and departmental meetings held at the BBC Written Archive Centre in Reading. There are also listener reports, press cuttings and policy fi les, which can help reconstruct the broadcasting-listening context so vital for understanding the geopolitical and cul- tural signifi cance of radio reporting especially during moments of international crisis such as the 1965 Indo-Pakistani war. However, as Dolan (2003) has noted, there is an inherent tension between the aural and textual sources used by many radio scholars and the value that is placed on written documentation, which is often seen as a 'poor substitute for a voice that cannot be heard ' (2003: 67). Second, broadcasting infrastructure and technology needs to be better understood by popular geopolitical writers whether it be associated with radio, television, the internet or television (see Hugill, 1999). A great deal of existing discussion within popular geopolitics shows little awareness either of the media technologies and infrastructure necessary to broadcast or of the values attached to the technologies themselves. Take radio listen- ing as an example. In the UK, if you want to listen to BBC World Service then you are guaranteed FM quality listening throughout the day and night. Both of us can and do listen to the radio via our digital televisions because we have access to it via the Freeview digital platform. Other listeners can access World Service via digital radio broadcasting tech- nologies. The improved audibility and re- liability has brought new listeners to the BBC's international services. However, other listeners in other parts of the world are less fortunate when it comes to availability and accessibility. Just as there is a digital divide, there is a radio listening schism. Listeners in India, for instance, are prevented from accessing BBC World Service on local FM because the Indian government prevents the BBC from rebroadcasting on these channels. As a consequence, the listening experience is quite different and listeners in south Asia often comment that one has to listen to the BBC more 'intently' and 'carefully' than if you were in the UK. Seasonal differences such as the monsoon can also interfere with broadcasting quality of international radio services. The inherent challenges of short- wave audibility appear to demand a more committed and careful form of listening and this can generate different kind of audi- ence reactions and, as we shall note below, emotional investments with the medium, especially if listeners gather collectively to listen to particular reports and news items. Infrastructural developments such as short-wave and FM broadcasting availability need to be better understood. The physical infrastructure of radio broadcasting is also signifi cant. The location of a radio transmitter is critically important in determining radio broadcasting range and frequency. Given radio's capacity to transcend international borders, radio transmitters have become frequent objects of geopolitical discord. In south Asia, for example, the proposed loca- tion of a radio transmitter by the BBC and subsequently VOA in the early 1960s gen- erated a substantial and highly emotive re- sponse in India as local political leaders and newspaper editors complained that America was trying to increase political and cultural infl uence. At a time when India was a leading member of the Non-Aligned Movement, this was judged to be an unacceptable breach of this foreign policy disposition. The reaction in Pakistan was also highly charged -both radio broadcasters were accused of trying to influence domestic political sensibilities. Cartoonists for newspapers such as Dawn (Karachi) recorded the controversy visually with at least one image (12 July 1963) de- picting two large loudspeakers belonging to Voice of America, located in north India, broadcasting anti-Pakistani and anti-Chinese propaganda (Figure 1). The geographies of broadcasting and transmitting deserve greater attention especially in the Global South. In part, this is symptomatic of a broader malaise which con- cerns a tendency of popular geopolitics to concentrate on the experiences of the Euro- American world and 'western' outliers such as Australia and New Zealand. This is not just an issue for Anglophone geopolitical studies but also for other intellectual fields such as media studies. As Curran and Park (2000: 3) have warned, there is a real need to 'de-westernize' media studies because it leads to generalizations about the availabil- ity of media including radio, the nature of listening and commercial relationships that have no purchase in the Global South. While BBC short-wave services in Mandarin are routinely jammed by the Chinese authorities, other places such as Ascension Island, Cyprus, Oman and a new transmitter in southern Thailand (replacing the former transmitter in Hong Kong) play a critical role in facilitating global broadcasting. These transmitter stations have a considerable geopolitical im- portance as, for instance, Ascension is the key nodal point for reaching Latin American and African audiences. It also hosted the British government's propaganda station Radio Atlantico del Sur, during the Falklands conflict of 1982 ( Figure 2). The island was also used in the 1970s by American forces to carry out anti-Cuban operations in Angola. Finally, as other scholars including geo- graphers have recently noted, our attention should now increasingly turn to understanding better the role of audiences and listening practices. As Stephen Barnard (2000) noted: Thinking about one's own use of a mass com- munication medium such as radio is a good point of departure for a study of how the media operate and wield the power they have. Until you appreciate your own viewing, listening or reading habits, trying to understand the habits and predilections of audiences -and the way the media perceives their audiences, respond to, adapt and cater for them -is almost im- possible. ( Barnard, 2000: 1) This means in part shifting the direction from further analyses of broadcasting stra- tegies and radio propaganda per se to better In wartime, as at other moments of crisis, there is a need to create unity or a unified community, to make the individual feel part of something greater. Radio played a key role in bringing people out of isolation and inte- grating them with the mass \u2026 Radio offered its listeners a 'bridge' between the individual and the mass, and played an essential role in community building. (Fox, 2004: 95, emphasis added) Although she does not dwell on how British wartime radio broadcasting performed as a 'bridge', other sources (textual and visual) have played a signifi cant role in creating an understanding of how individuals and fam- ilies listened to the radio. 4 If they did 'feel part of something greater' then it was per- haps due to certain forms of behaviour such as gathering in the living room of a house to listen to reports about the second world war. Likewise the radio occupied a central position in American life as President Roosevelt com- municated to over 60% of radio listeners on the eve of the country's entry into war. On 29 December 1940, Roosevelt delivered his famous 'arsenal of democracy' speech to around 65 million listeners (Douglas, 1999). Unsurprisingly, radio's role in mobilizing the nation for war has been much considered by scholars, not least because of the inherent tensions of using a democratizing medium such as radio for the purpose of centralizing control over a populace. What is clear from this research is that the reactions of listeners were incred-ibly varied and depended on a raft of factors including ethnicity, gender, social class and geographical location. As with any medium, radio listening varied among listening communities in the sense of emotional investments and audience dis- positions. In the case of post-1945 south Asia, for instance, listeners to BBC World Service reacted in very different ways to reports about international events such as the 1965 Figure 2 Radio Atlantico del Sur 'QSL' card (1982) at UNIV OF WISCONSIN-MADISON on February 1, 2015 phg.sagepub.com Downloaded from Alasdair Pinkerton and Klaus Dodds: Radio geopolitics 23 Indo-Pakistani War, the independence of Bangladesh and the Indian Emergency in the 1970s. On the one hand, there were those who viewed the BBC reporting as 'trust- worthy' and 'reliable' compared to local radio stations such as All India Radio and Radio Pakistan, which were condemned as 'propa- ganda mouth-pieces' for the Indian and Pakistani governments respectively. On the other hand, the BBC could also be condemned for being an unwelcome 'colonial' presence in south Asia and intent on secretly pursuing a British government agenda designed to re- tain control over its former colonies. Evidence for these mixed reactions comes in part from listener letters sent to the BBC World Service offi ce in New Delhi and through interviews with listeners, who often recalled vividly where they were and who they were with when they first heard BBC reports on a particular crisis (Pinkerton, 2008a). Audience share does not provide suffi- cient nuance in terms of judging listening habits and emotional investment. Take a recent example involving the BBC Pashto Service, which is broadcast primarily to Afghan audiences. Before the October 2001 assaults, 3.5 million were tuning in to hear a soap opera entitled 'New Home, New Life'. It was the most popular programme in the country and such was its popularity that the Taliban regime was reluctant to impose a ban on radio entertainment for fear of stimu- lating an insurrection. The BBC's reputation was enhanced and many listeners accord- ingly tuned into other programmes including news and current affairs. However, that did not mean that the British military's involve- ment in the country was any less contro- versial and that listeners were not perfectly capable of enjoying some programmes while deriding others for being 'propaganda' and 'imperialist' in nature. As listeners have demonstrated in many parts of the world, especially with regards to international broad- casters, listening communities can alter in tone and substance. As radio scholars have noted, listening can always be considered in a more social and cultural context. As Susan Douglas (1999) has claimed: the way people listened to radio was pro- foundly shaped by the era they began to listen \u2026 In other words different generations learned to listen and use the radio differently. So it's not only what people listened to \u2026 that defi ned generations. Its how they listened as well that shaped people's memories, associations with others, their sense of who they were and their place in history. (Douglas, 1999: 6) A focus on listening (and how we learn to listen and different modes of listening de- pending on whether one is listening to news, the weather, music, sport and so on) is essen- tial in developing a critical appreciation of radio. We might, to paraphrase Douglas, also note that the 'place of geography' mat- ters because, as Keith Jones has noted with reference to music and factory work in the midst of the second world war, acts of listening and singing were also shaped by geographical factors such as access to radio, proximity to loudspeakers, the segregated nature of the workplace and the availability of colleagues and friends (Jones, 2005). If this was true of factory work in Britain during the second world war then it is certainly pertinent to acknowledge that listening in places such as India, Pakistan and Afghanistan was as much infl uenced by prevailing weather patterns as it was by accessibility to the radio and the segregated nature of communal listening.", "title": "Radio geopolitics: broadcasting, listening and the struggle for acoustic spaces", "file_name": "Pinkerton and Dodds - 2009 - Radio geopolitics broadcasting, listening and the.pdf"}
{"section": "IV Conclusions: towards a radio geopolitics", "text": "This review of some of the literature asso- ciated with radio needs to be contextualized with regard to geography's longer-standing engagement with music and sound. Over 10 years ago, Susan Smith (1994) was one of the fi rst to alert geographers to acoustic spaces and the connections between music and sonic geographies. Over the following decade and a half, others have followed and considered in more detail the role of sound in creating spaces, borders and power; sound as a form of memory and belonging; sound and everyday life; sound's capacity to stimulate movement, dance and performance; the institutions and industries that market and sell aural culture and the machines and technologies that produce, store and disseminate sound in the form of music and other forms of noise (see, for example, Bull, 2004). In short, a great deal of literature now exists on how and with what consequences sound is embedded in history, cultures, institutions, technologies and of course geographies. From our point of view, the special issue on sonic geographies in the journal Social and Cultural Geography (2005) is richly sug- gestive in terms of how we might further pursue a sonic geopolitics. First, following David Matless's (2005) paper on 'Sonic geo- graphy in a nature region', we might consider the acoustic ecologies and how a sonic envir- onment is defi ned, valued, experienced and communicated. One example that illustrates that potential well is the impact on the Falkland Islands community in April 1982, fol- lowing the landing of the Argentine invasion task force. After a short bout of resistance, the Islands were taken over and the radio station was ordered to broadcast instructions of Islanders about the situation confronting them. In the period between the landing and the takeover, the station manager Patrick Watts received countless phone calls from Islanders passing on details about the military task force and using their phones and two- way radios to communicate with the radio station (Pinkerton, 2008b). In a community scattered over a number of islands and with- out access to television, the radio was the key medium in people's lives. It enabled news to be broadcast, music to be played and gossip to be exchanged. Many Islanders later recorded that the takeover of the radio station by the Argentine military authorities was the most traumatic event of the occupation, in a conflict which was later to be mercifully light on civilians compared to later confl icts in the 1990s and beyond. When the local broadcasting station became 'Argentine', listeners relied on BBC World Service for further news about the military situation affecting the Falklands (Pinkerton, 2007). Second, using the work of Nick Megoran (2006) on the politics of remembrance following the 11 September 2001 attacks, popular geopolitics should consider how prayers, readings, songs and sermons co-join in complex ways to produce commemor- ative encounters in, for instance, St Paul's Cathedral in London. Listening, as Susan Smith (2000: 634) has noted, is much more than simply overhearing sounds, it involves a performance, which helps listeners decipher, classify and interpret sound. How do certain sonic experiences then become connected to geopolitical cultures and affect? If the com- bination of music, silence, praying and crying have the power to move embodied subjects, then this deserves more detailed consider- ation because expressions of American geo- political power in Afghanistan and Iraq owe as much to strategic planning as they do to an affective response seeking not only to punish those who planned the attacks but also to seek a form of uplifting denouement for a country and its populace hurt, humiliated and traumatized by 11 September 2001. What is lacking from important interventions by scholars such as \u00d3 Tuathail (2003), for ex- ample, is a clearly stated commitment to de- velop audio-ethnographies so necessary to better our understanding of listening prac- tices. The embodied habits that Michael Billig (1995) mentions, such as saluting the fl ag, need to be co-joined with other prac- tices such as singing and listening to national anthems. Likewise, it should be noted that religious chanting and exhortation in the form of 'God is great' is also central to the embodied habits of Islamic radicals. Third, we might further contemplate the unequal geopolitics of sound and consider how some countries such as the United States, Britain and Russia have a greater capacity than others to intrude on sonic environments. For instance, the BBC World Service, as noted earlier, while often valued by many listeners around the world also has been a deeply resented presence in the sonic lives of listeners in Iran, India and Pakistan. During the post-cold-war era, these global broadcasters continue to use their network of transmitters to produce and disseminate news, music and commentary to listening communities around the world. Listeners and other broadcasters are not simply sonic sponges as, in many parts of the world, these overseas sources are just one radio source and popular geopolitical scholarship needs to understand better how listeners combine different radio sources and other analogue and digital media to produce individual and collective media signatures. Finally, other scholars have pointed to the need to consider the place of 'affect' and the manner in which radio alongside other mediated communication can be registered at the level of the physical body. Affect, as Brian Massumi (1987) has noted, is not the same as emotion and feeling. Emotions are social, feelings are personal and affects are pre-personal. In the case of the latter, it is the most abstract in the sense that it refers to a moment of unformed and unstructured potential. Affect adds intensity and helps determine the relationship between the body, the environment and others. New re- search in media studies is, as a consequence, calling into question the notion of 'media effects' and instead considers affective resonance. Radio scholars such as John Tebbutt (2006: 859) have analysed the role of commercial radio in Australia, and 'the audience body is a site of a multiplicity of potential responses and the media the site of a range of possible provocations. Their encounters give rise to \"affective states\". Here we are not dealing with \"messages\" but rather moods, perceptions'. So if music has a capacity to 'move' then the cultural effects of radio (the tone of a voice, background music and so on) deserve further consideration especially with regard to the micro-bodily geographies of listening.", "title": "Radio geopolitics: broadcasting, listening and the struggle for acoustic spaces", "file_name": "Pinkerton and Dodds - 2009 - Radio geopolitics broadcasting, listening and the.pdf"}
{"section": "V Coda", "text": "On 31 October 1938, the New York Times reported how listeners to a radio production of 'The War of the Worlds' feared that New York and its environs were being subjected to a Martian attack. 'Shaken' and 'agitated' residents left their houses and fl ed to parks and other open spaces in an attempt to es- cape a suspected and imminent gas attack. Local police and emergency support ser- vices were overwhelmed. In the aftermath, it was apparent that few listeners had dis- tinguished the fictional nature of the pro- gramme and were persuaded by its veracity in part because of the manner in which it was delivered -as a news bulletin format. 4. 'London Bridge' was the name given to the BBC's direct programming to Britain's South Atlantic colonies (including the Falkland Islands) during 1944. The programme evolved into the 'Calling the Falklands' programme, which rose to prominence during the 1982 confl ict with Argentina.", "title": "Radio geopolitics: broadcasting, listening and the struggle for acoustic spaces", "file_name": "Pinkerton and Dodds - 2009 - Radio geopolitics broadcasting, listening and the.pdf"}
{"section": "Acknowledgements", "text": "We would like to thank Alan Ingram, Alec Murphy, Marcus Power, Francis Robinson and James Sidaway for their helpful com-ments on an earlier draft. The referees offered many helpful comments. Alasdair Pinkerton gratefully acknowledges the support of the Economic and Social Research Council and the British Academy in the form of a doctoral studentship and a Postdoctoral Fellowship respectively, and Klaus Dodds acknowledges the support of the Leverhulme Trust in the form of the Philip Leverhulme Prize (2005).", "title": "Radio geopolitics: broadcasting, listening and the struggle for acoustic spaces", "file_name": "Pinkerton and Dodds - 2009 - Radio geopolitics broadcasting, listening and the.pdf"}
{"section": "Abstract", "text": "The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N = 2,696) and with an online sample (N = 737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences-conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects. Word count = 151 Many Labs 3: Evaluating participant pool quality across the academic semester via replication University participant pools provide access to participants for a great deal of published behavioral research. The typical participant pool consists of undergraduates enrolled in introductory psychology courses that require students to complete some number of experiments over the course of the academic semester. Common variations might include using other courses to recruit participants or making study participation an option for extra credit rather than a pedagogical requirement. Research-intensive universities often have a highly organized participant pool with a participant management system for signing up for studies and assigning credit. Smaller or teaching-oriented institutions often have more informal participant pools that are organized ad hoc each semester or for an individual class. To avoid selection bias based on study content, most participant pools have procedures to avoid disclosing the content or purpose of individual studies during the sign-up process. However, students are usually free to choose the time during the semester that they sign up to complete the studies. This may introduce a selection bias in which data collection on different dates occurs with different kinds of participants, or in different situational circumstances (e.g., the carefree semester beginning versus the exam-stressed semester end). If participant characteristics differ across time during the academic semester, then the results of studies may be moderated by the time at which data collection occurs. Indeed, among behavioral researchers there are widespread intuitions, superstitions, and anecdotes about the \"best\" time to collect data in order to minimize error and maximize power. It is common, for example, to hear stories of an effect being obtained in the first part of the semester that then \"disappears\" in a follow-up study collected at the end of the semester. Beliefs about this variation can be so strong that some laboratories adopt policies to avoid data collection during 5 particular time periods. Are these concerns warranted? There is some evidence that individual differences among participants vary slightly across the academic semester (Table 1), but there is almost no evidence to indicate whether that variation on average has any impact on the detectability and effect magnitudes of correlational or experimental results. We investigated variation in detectability of 10 previously reported effects across 20 participant pools (N = 2,696) and an online resource (N = 737). Time of Semester Effects: Legitimate Concern or Superstition? Concerns about time-of-semester effects are not new. The existing evidence supports the belief that participants at the beginning of the semester are different on average than participants at the end of the semester. However, the differences are modest. For example, later participation in the semester is related to lower levels of conscientiousness (Witt, Donnellan, & Orlando, 2011) and higher levels of openness to experience (Aviv, Zelenski, Rallo, & Larsen, 2002; see Table 1). In addition, individuals who participate late in the semester show lower intrinsic motivation when compared to those who participated earlier (Hom, 1987; Nicholls, Loveless,", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "text": "University participant pools provide access to participants for a great deal of published behavioral research. The typical participant pool consists of undergraduates enrolled in introductory psychology courses that require students to complete some number of experiments over the course of the academic semester. Common variations might include using other courses to recruit participants or making study participation an option for extra credit rather than a pedagogical requirement. Research-intensive universities often have a highly organized participant pool with a participant management system for signing up for studies and assigning credit. Smaller or teaching-oriented institutions often have more informal participant pools that are organized ad hoc each semester or for an individual class. To avoid selection bias based on study content, most participant pools have procedures to avoid disclosing the content or purpose of individual studies during the sign-up process. However, students are usually free to choose the time during the semester that they sign up to complete the studies. This may introduce a selection bias in which data collection on different dates occurs with different kinds of participants, or in different situational circumstances (e.g., the carefree semester beginning versus the exam-stressed semester end). If participant characteristics differ across time during the academic semester, then the results of studies may be moderated by the time at which data collection occurs. Indeed, among behavioral researchers there are widespread intuitions, superstitions, and anecdotes about the \"best\" time to collect data in order to minimize error and maximize power. It is common, for example, to hear stories of an effect being obtained in the first part of the semester that then \"disappears\" in a follow-up study collected at the end of the semester. Beliefs about this variation can be so strong that some laboratories adopt policies to avoid data collection during particular time periods. Are these concerns warranted? There is some evidence that individual differences among participants vary slightly across the academic semester (Table 1), but there is almost no evidence to indicate whether that variation on average has any impact on the detectability and effect magnitudes of correlational or experimental results. We investigated variation in detectability of 10 previously reported effects across 20 participant pools (N = 2,696) and an online resource (N = 737).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Time of Semester Effects: Legitimate Concern or Superstition?", "text": "Concerns about time-of-semester effects are not new. The existing evidence supports the belief that participants at the beginning of the semester are different on average than participants at the end of the semester. However, the differences are modest. For example, later participation in the semester is related to lower levels of conscientiousness (Witt, Donnellan, & Orlando, 2011) and higher levels of openness to experience (Aviv, Zelenski, Rallo, & Larsen, 2002; see Table 1). In addition, individuals who participate late in the semester show lower intrinsic motivation when compared to those who participated earlier (Hom, 1987;Nicholls, Loveless, Thomas, Loetscher, & Churches, 2014). Research on variation in actual task performance, however, has produced mixed results. For instance, Wang and Jentsch (1998;N = 49) asked participants to complete a cued recall task, testing their memory for the English meanings of 24 learned foreign words after a 30-minute period. They found no significant difference in cued recall between the earliest and latest participants over the course of four semesters. Openness .14 -.01 Note: Values represent Pearson's r between personality trait and week of participation ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Research Questions", "text": "The present project is informally called \"Many Labs 3\" as it follows the model established in two prior investigations for conducting the identical procedure in many different laboratories ( Klein et al., 2014;Klein et al., 2015). In Many Labs 3, we investigated the extent to which 10 psychological effects and multiple individual difference variables varied across the academic semester. The same experimental procedure was administered in 20 participant pools at institutions in the United States and Canada. This allowed us to investigate the extent to which participant characteristics and the magnitudes of different effects vary across the academic semester. If time of semester effects were observed, we also obtained a Mechanical Turk sample (MTurk; N = 737) to help distinguish between time of semester effects (unique to students) versus time of year effects. A secondary interest was to provide additional evidence about the included effects using large scale replication: their overall effect size, variation by site and sample, and moderation by time of semester. Some of the effects we included are heavily studied, but others are relatively new or have not been replicated frequently enough to clarify boundary conditions or moderating influences. The final materials and dataset will be of substantial use beyond this initial report, particularly to explore moderating influences not examined for this report. All data and materials are available for additional investigation by others (https://osf.io/ct89g/).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Participants", "text": "An open invitation for researchers to participate as a data collection site was issued in early 2014 for data collection to occur from August through December. To be eligible for inclusion, participating labs agreed to administer the study procedure to at least 80 participants total with at least 40 from the first half of the semester and at least 40 from the second half of the semester. To ensure that teams were operating on similar academic calendars, participation was limited to institutions in the United States and Canada. Twenty teams completed the data collection with the average sample size being 135.40 (SD = 63.00), ranging from 45 to 321 (see Table S1 for details of each team and Table S2 for characteristics of each participant pool). One team was unable to meet the minimum participant cutoff (N = 45), but earned authorship through other contributions. Their data are included in the aggregate set and all subsequent analyses. Overall, 69.8% of the sample was female, the average age was 19.3 years (SD = 3.7), and 53.7% were White, 9.4% Black, 16.0% Asian, 10.6% Hispanic, and 10.3% other. These participants came from a wide range of institutions, producing a relatively diverse undergraduate sample. Although all of the directly replicated effects collected data from undergraduate participants, the current sample differs in a few ways. None of the original study collection sites are represented in the current sample. Two original studies recruited undergraduates independent of a participant pool, and two other original studies were conducted at European institutions. Finally, the current sample has a heavier representation of females compared to original studies that reported this demographic (55.5%). Sample differences that seem particularly relevant are noted in the descriptions of each effect. We simultaneously collected participants from MTurk over the same time period (N = 737) as a comparison sample for time of year effects and sample diversity. In the MTurk sample, 48.6% of the sample was female, the average age was 35.1 years (SD = 10.9), and 66.4% were White, 15.4% Asian, 7% Black, 4.7% Hispanic, and 6.5% other. This sample was drawn from the United States and there were no requirements for previous MTurk experience (e.g., minimum number of previous HITs completed). MTurk participants received $1.25 as compensation for their time.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Selection of Effects", "text": "The primary aim of the project was to detect possible variability in effect magnitudes across the academic semester when using university participant pools. To obtain a candidate list of effects and individual difference measures, we held a round of open nominations and invited submissions for any effect that fit the defined criteria. Those nominations were supplemented by ideas from the project team and from direct queries to independent experts in psychological science. Given the areas of interest of the project coordinators and most collaborators, nominations came largely from the fields of social and personality psychology. The coordinating team sought effects and individual difference measures that fit the following criteria: (1) highly feasible implementation through a web browser or in the lab, (2) brevity of study procedures, and (3) high interest value of the theoretical domain or phenomenon. In addition, for the collected set of effects and measures we sought: (1) diversity of represented research domains, (2) diversity of known or presumed likelihood of variation across the semester, and (3) diversity of \"classic\" well-established effects and contemporary effects that have untested replicability. The project coordinating team collectively evaluated the nominated studies (see Table S3 for a list of considered effects). No specific researcher was \"targeted\" for replication because of concerns or skepticism about an effect. In fact, any included effect that was not reproducible at all would produce little insight about variation across the semester, which was the central research question for this project. Given this, one strategy would have been to only select classic, well-established effects for replication. However, it is possible that these effects are well established because they are resistant to contextual variation. Had we selected only well- established effects, we could have undermined the possibility of observing context effects. Our presumption was that time-of-semester effects are most likely to occur for so-called \"fragile\" effects that might be particularly sensitive to context. As such, we included high-profile, contemporary effects with less certain replicability, particularly from domains in which popular debate suggests fragility or sensitivity to context. This project was most concerned with detecting whether or not time of semester variation happens in regular research practices. Therefore, if we had limited our effects to one or two research domains (e.g., effects moderated by attention, Nicholls et al., 2014), we might have maximized testing \"can semester variation alter effects?\", but sacrificed testing \"does time of semester variation alter effects?\" in ordinary research practice. Furthermore, reduced attention can be reasonably hypothesized as moderators for many effects, even if they have not been previously demonstrated as influential. In other words, we aimed to examine time-of-semester as the highly available explanation when two behavioral lab studies show different results, whatever the topic of study. Once selected for inclusion, a member of the research team contacted the corresponding author (if alive) to obtain the original study materials and get advice about adapting the procedure for use in this study. 1 In particular, we asked the original authors if there were moderators or other limitations to obtaining the result that would be useful for the team to understand in advance or to anticipate during data collection. The team implemented a draft of the proposed study procedure and solicited feedback from the original authors to further improve the design. This process was undertaken to minimize reasons to expect different outcomes between the original outcomes and the replications. Sometimes this led to adaptations of the procedure in order to maximize its relevance in the present context, or changes to fit the constraints of the present procedure (see Table S4 for a summary of procedure adaptations). Also, some initially selected effects were eliminated during review if we could not address a priori design concerns effectively. We implemented a draft study procedure to pre-test for length. Data collection constraints required completion of all study materials within 30 minutes. A pilot sample of 30 volunteers completed the on-line portion of the study procedure. We calculated the time required for 85% of participants to complete each study procedure. Following this piloting, we needed to remove three individual difference measures, shorten one procedure (Stroop task), and eliminate two effects to meet the time constraints. After this intensive review, 10 effects, 10 individual difference measures, 3 data quality indicators, and a selection of demographics items were confirmed for inclusion in Many Labs 3. In administration of the actual procedure, we did not impose a 30 minute time constraint, but individual data collection sites could let participants go before data collection completion if circumstances demanded it. 97.2% of non-MTurk participants completed the entire study.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Procedure", "text": "The study procedures and materials were reviewed and approved by the University of Virginia Institutional Review Board for the Social and Behavioral Sciences as well as IRBs from all other participating institutions. Eight of the effects were administered in a single computerized experiment script that began with informed consent, then presented the procedures for each target effect in a random order, then presented the ten individual difference measures and three data quality indicators, and closed with demographics items and debriefing. Two of the effects could not be administered via computer, one because the participants were required to hold the measures in their hands (Weight Embodiment) and another because the original author suggested that it required a paper- pencil administration format (Metaphoric Restructuring). As such, the participant was instructed to go to the experimenter for instructions at a random point during presentation of the eight computerized tasks. At this point, the two \"in-person\" tasks were administered in a counterbalanced order. The script for the experiment and video simulations of experiment administration are available publicly (https://osf.io/ct89g/). The procedure for the MTurk sample was the same except that we removed the two \"in- person\" tasks and one of the computer-administered tasks that involved deception and concerned an issue at the participant's university (Elaboration Likelihood).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Demographics Measures", "text": "Age. Participants noted their age in years in an open-response box. Sex. Participants selected \"male\" or \"female\" to indicate their biological sex. Data Quality Indicators. Several items at the end of the study, just prior to the demographics items, assessed carelessness or lack of effort. Participation questions. To assess the quality of the participant's engagement in the study, we asked: \"How much effort did you put into the tasks during this experiment?\" (1 = no effort to 5 = I tried my hardest) and \"How closely did you pay attention to the instructions and tasks during the experiment?\" (1 = none to 5 = I gave the tasks my undivided attention). Participants also responded to items assessing: (1) whether they were participating as part of a class requirement, extra credit, payment, or other; (2) the type of class that required/incentivized this participation (i.e., introductory course in psychology, secondary/upper- division course in psychology, any class above secondary, research methods/statistics course, or other); and, (3) if required, how close they were to completing their subject pool requirements (this is my first study, about 25% done, about 50% done, about 75% done, this is my last study, I am not participating for a class requirement). Instructional attention check. The instructional attention check presented a paragraph of instructions in which the last sentence read: \"So, in order to demonstrate that you have read the instructions, please ignore the preferences form below, and simply write 'I read the instructions' in the box below.\" Immediately below this paragraph is an item saying \"In my free time I prefer:\" with response options of (1) engaging in hobbies, (2) watching TV, reading, music, (3) being in nature, (4) exercising, (5) cooking or eating, and (6) other (with an open response area for writing in the correct answer).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Individual Difference Measures", "text": "Brief individual difference measures were selected as possible moderators of psychological effects based on prior evidence that participant characteristics vary across the semester or because of their widespread use in psychological science. Table 2 shows the descriptive statistics for all of the individual differences measures (see Table S5 for correlations among these measures). When comparisons were available, reliabilities for measures were similar to or better than prior uses.  was assessed with two items on 7-point response scales from 1 (disagree strongly) to 7 (agree strongly). Reliabilities are somewhat lower than other, longer scales, but the five scales show satisfactory retest reliabilities (cf. Gnambs, 2014) and substantial convergent validities with longer Big Five instruments (e.g., Ehrhart et al., 2009;Gosling et al., 2003;Rojas & Widiger, 2014). Daily Mood (adapted from Schwarz & Clore, 1983). We measured daily mood using two items that assess the extent to which the participant is in a good or bad mood. Items begin with the same statement, \"Today I generally feel...\" Each set of response options are on a 7-point Likert scale, ranging from 1 (very unhappy) to 7 (very happy), and 1 (very bad) to 7 (very good). Perceived Stress Scale -short form (Cohen, Kamarck, & Mermelstein, 1983). We measured perceived stress over the last week using a 4-item short-form scale that is an alternative to the original, 14-item Perceived Stress Scale (Cohen, et al., 1983). Participants respond on a 5-point Likert scale, ranging from 0 (never) to 4 (very often). The original study suggested that the shortened scale was relatively reliable (\u03b1 = .72) and the factor structure was consistent with the long form.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Need for Cognition Scale (adapted from Cacioppo & Petty, 1982; Skulborstad,", "text": "unpublished data). We measured need for cognition with six items that ask about the degree to which the participant enjoys engaging in complex, deliberative, and abstract thinking. Each of the items are on a 5-point Likert scale, ranging from 1 (extremely uncharacteristic) to 5 (extremely characteristic). Following past research we selected the top six factor loading items of the original scale (e.g., Verplanken, 1991;Verplanken, Hazenberg, & Palenewen, 1992; Skulborstad, unpublished data). We used this shortened version instead of the 34 item (Cacioppo & Petty, 1982) or 18 item versions (Petty, Cacioppo, & Kao, 1984) because of time constraints.  .67 1-5", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "The Effects", "text": "Next, we describe the 10 selected effects with an abstract reporting the main idea of the original research with the sample size, inferential test, and effect size. Details on the methodology and analysis plan that was defined in the pre-registered protocol for each effect can be found presented in the supplementary material (https://osf.io/ct89g/). We report the aggregate result of the replications at the end of each subsection; these results are summarized in Figures   1a, 1b, and Table 3. The focus of this replication project is to estimate the variability in effect magnitude by time of semester. As such, we aimed to identify or simplify original study designs that could be tested as two-condition experiments or as correlations when possible. Some original studies had additional conditions that were relevant for the theoretical purposes of the investigation. In those cases, the replication designs identified the key conditions relevant for estimating the effect. Also, in some cases, multiple dependent variables were included in the original design. If the dependent variables could be administered quickly, they were usually retained in the replication. When multiple outcomes were included, because they are likely to be correlated, just one or an aggregate was identified as the primary object for replication and examining variation across the semester; the others were considered secondary. Secondary outcome measures are reported in footnotes or the supplemental material. Finally, correspondence with original authors during the design process identified some potential moderating influences that could be examined with additional analyses.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Stroop task (Stroop, 1935)", "text": "In the Stroop task (Stroop, 1935), participants view words one at a time in different colors. Participants categorize the color of the font and do not need to do anything with the meaning of the word. This task is more difficult when there is a discrepancy between the color of the font and the word. For example, it is easier to categorize the font as \"blue\" when it is presented on the word \"tree\" or the word \"blue\" compared to being presented on the word \"red.\" The meaning of the word \"red\" interferes with categorization of the font color as \"blue.\" This task is very robust and has been used in thousands of research applications (MacLeod, 1991). Effects on the Stroop task can be larger when participants are tired, or otherwise cognitively or emotionally depleted, because they have fewer available resources to overcome the response competition. In the present study, we incorporated a simple version of the Stroop task to test whether similar variation would be observed across the semester cycle. The Stroop task is a within-person experiment with two response conditions -font color congruent with color word and font color incongruent with color word -and response latency as a dependent variable. We used the D scoring algorithm for analysis of these data (Greenwald, Nosek, & Banaji, 2003), an analysis technique that has general application to response latency contrasts (Nosek & Sriram, 2007) and avoids confounding influences in response latency comparisons that influence other analytic techniques (Sriram, Greenwald, & Nosek, 2010 scenarios describing the location of an object in reference to a stick figure (referred to as \"you\"). Participants in the object-moving condition saw scenarios in which two objects were described in relation to one another. Participants indicated whether the statement about the picture was true or false. On the second page, participants read an ambiguous temporal statement (e.g., \"Next Wednesday's meeting has been moved forward two days\") and indicated to which day the meeting had been rescheduled (e.g., \"Monday\" or \"Friday\") and how confident they felt about their choice from 1 (not at all confident) to 5 (very confident Based on the original author's recommendations, this task was completed on paper-and- pencil in the face-to-face portion of the study to ensure comparability to the original procedure, and three conditions were included: ego-prime, object-prime, and control. We excluded trials compared to congruent trials (M = .016, SD = .049), t (3337 participants from the analyses if, in the priming condition, they failed to answer all four priming questions (see materials) correctly, or if, in any condition, they failed to select one of the two possible correct options for the day of the meeting (Monday or Friday). In the in-lab replication studies (N = 2,191), ego-priming was more likely to induce the answer of Friday (67.8%) than Monday (32.2%), whereas object-priming showed a bias in the same direction but to a lesser extent with Friday (59.5%) being more popular than Monday (40.5% ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "1973, Study 3)", "text": "Tversky and Kahneman (1973) examined whether undergraduates recruited separately from a participant pool would overestimate the frequency of easier-to-imagine words relative to harder-to-imagine words. People find it easier to think of English words that begin with a certain letter (k, l, n, r, or v) than to think of words with this letter in the third position. However, these letters actually show up about twice as often in the third position compared to the first position. Participants judged whether each of these letters was more likely to show up in the first or the third position and estimated the ratio of the frequency with which they appear in each position. Tversky and Kahneman (1973) found that 105/152 participants judged the first position to be more frequent for the majority of letters and that 47/152 participants judged the third position to be more frequent for the majority of letters. The authors reported that a sign test (Grissom & Kim, 2012 , and the effect size was much stronger than with the original estimation strategy (note that Figure 1a shows data for the original estimation strategy). 4 (Table 3 here, located at end of document for reference) The current study examined variation in persistence across the semester. To conceptually replicate the relation between persistence and conscientiousness, we used the unsolvable anagram task, which has been used as a measure of persistence (e.g., Aspinwall & Richter, 1999;Sommer & Baumeister, 2002). In this task, participants are presented with a number of anagrams to unscramble. Some anagrams are solvable, others are not. Participants choose to stop working on the task whenever they would like. Persistence is the amount of time spent on the task before moving on to the next task.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "The relation between persistence and conscientiousness (De", "text": "Unlike the others, this is not a direct replication. The original work examined the correlation between self-perception of persistence and a long-form personality measure using a clinical sample. We added this effect as a conceptual replication because persistence and In an aggregate analysis of the replication studies (N = 2,969), participants in the high-power (M = 3.75, SD = 1.55) and low-power (M = 3.80, SD = 1.57) conditions thought that the sincerity of the message would be interpreted similarly, t (2967 In the replication studies, we excluded participants from the analyses if the experimenter noted any behavior that would have diffused the weight of the clipboard (e.g., sitting down, resting the clipboard on a table). Across all in-lab replications (N = 2,285), participants in the heavy (M = 6.16, SD = 1.02) and light (M = 6.14, SD = 1.03) clipboard conditions believed that it 5 We tested whether the length of participants' responses to the power prime (measured as the number of characters in their response) moderated the effect of high versus low-power conditions on sincerity ratings. However, we did not find a reliable Condition \u00d7 Response Length interaction, F(1, 2961) = 0.39, p = .53, r = .01. 6 Additional analyses controlling for participants' mood and for task difficulty did not change the direction of the effects, though controlling for mood did weaken the effect (p = .095). was similarly important for the university committee to listen to the students' opinions, t (2283 Cacioppo and colleagues (1983) investigated the impact of argument strength on persuasion, inviting participants who scored in the upper or lower third on the Need for Cognition Scale (Cacioppo & Petty, 1982) to participate. Participants either read a set of strong or weak arguments concerning the institution of comprehensive exams for undergraduates at their university. Afterwards, participants rated the quality of the arguments and how persuaded they were by them. They found that participants found stronger arguments to be more compelling than weaker arguments overall, F(1, 110) = 160.86, p < .001, \u03b7 p \u00b2 = .59, 95% CI = 8 We constructed a hierarchical multivariate model testing the effect of the manipulation (reading about a communal or agentic individual) with the additional predictors of gender of the individual in the prompt, participant gender, actual room temperature (step 1), and the interaction between target and participant gender (step 2) predicting the participant's temperature estimate of the room. Only the actual room temperature reliably predicted the participants' temperature estimation, F(1, 1,824) = 160.74, p < .001, r = .28. All other predictors were not significant (ps > .41). [.47, .67]. However, participants who were high in need for cognition showed this effect more strongly than those low in need for cognition, F(1, 110) = 22.45, p < .001, \u03b7 p \u00b2 = .17, 95% CI = [.06, .29]. This study demonstrated that the quality of a persuasive message impacts people differently depending on the extent to which they process the message. We conducted a similar test using linear regression to predict ratings of argument quality The reliability of the need for cognition scale used (\u03b1 = .67), was lower than has been observed for the full 34 item scale (\u03b1 = .87, Cacioppo & Petty, 1982). This reduction in reliability would be expected to attenuate the target effect. However, given the statistical power of the sample, it is unlikely that this attenuation would solely eliminate the effect. It could also be that low need for cognition participants are more against comprehensive exams at baseline. However, need for cognition was not reliably related to ratings of argument quality, F(1, 2,361) = 2.386, p = .123, \u03b7 p \u00b2 = .001, 95% CI = [0, .005].", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "It feels like yesterday: Self-esteem, valence of personal past experiences, and judgments of subjective distance (Ross & Wilson, 2002, Study 2)", "text": "According to the theory of temporal self-appraisal, time is a psychological variable that can vary by \"closeness.\" Closeness refers to an individual's perception of the temporal distance between the past and the present irrespective of the actual temporal distance. For example, a person may have gotten married 15 years ago, but that experience might \"feel like\" it occurred much more recently. Ross and Wilson (2002) examined how subjective temporal distance varies when recalling negative compared to positive events and whether differences in self-esteem may be associated with how distant events subjectively feel, irrespective of how distant they actually are. Overall, participants were expected to feel further from negative events compared to positive events in order to buffer their self-worth against the implications those negative events have for current self-view. Because individuals with high self-esteem are more motivated to preserve their self-worth, the authors hypothesized that individuals with high self-esteem would show this effect more strongly than individuals with low self-esteem. They randomly assigned students (N = 357) to reflect either on a positive or negative academic experience. In the positive condition, participants identified the best grade they received in the previous semester. In the negative condition, participants identified the worst grade they received in the previous semester. Participants then reported how distant the course felt to them and how often they thought about this course since it ended. From a hierarchical regression model (with actual time since the class as step 1, the main effects of self-esteem and condition as step 2, and the interaction of self-esteem and condition as step 3) there was an interaction between self-esteem and condition when predicting ratings of subjective distance, .019]. Also, self-esteem weakly predicted subjective distance, independent of condition, with higher self-esteem predicting closer subjective distance, F ( ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Moral credentials and the expression of prejudice (Monin & Miller, 2001, Study 1)", "text": "Monin and Miller (2001) tested whether participants were more willing to express prejudicial attitudes when their prior behavior provided evidence that they were non-prejudiced. Two hundred two undergraduates (115 men and 87 women) were approached on a university campus by the experimenter to complete an anonymous survey. This survey first asked whether five statements were right or wrong. These statements expressed sexist views, and were either phrased as describing \"most women\" or \"some women,\" with the intent of inducing greater agreement with the \"some\" statements as opposed to the \"most\" statements. The authors For the replication design, we included only the \"some\" and \"most\" conditions. In an aggregate analysis of all replication studies (N = 3,134), there was a main effect of moral ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Results by site, task order, and time of semester", "text": "For each data collection site, we computed the number of days in which the participant pool was available during the semester. For each participant, their participation date was normalized by dividing the day that they participated by the total number of days available such that participation on the first day of the pool was (1/total days) and participation on the last day of the pool was 1 (see Figure S1 for distribution of participation). This accounted for the fact that some participant pools were open for longer periods than others (e.g., sites using semesters compared to quarters). This value was tested as a moderator of the association of effect of condition for each of the outcome measures in the study.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Effects", "text": "The primary aim of the pre-registered design and analysis plan was to evaluate variation in effects across the academic semester. In the first stage of analysis, we examined the aggregate effect sizes without testing whether those effects varied across the semester. Those results, reported above in the introduction to each effect, suggested that some of the primary replication effects had effect sizes near 0. It was possible that this aggregate result would reveal a positive effect at some points in time and a negative effect at other points in time. However, it was also possible that this indicated a uniformly null result. If the latter, then we would have no opportunity to learn about variation across the academic semester from those effects. As a consequence, prior to conducting tests of variation across time, we decided to add three theoretically relevant main effects for studies in which the key test was an interaction effect that did not occur (Elaboration Likelihood, Self-Esteem and Subjective Distance, Credentials and Prejudice). Variation by site. For each effect, we computed an aggregate effect size estimate with 99% confidence intervals. Figures 1a and 1b represent the effect size estimates for each of the data collection sites for each effect. We also computed the variability in effect estimates following standard statistics for meta-analyses-Q and I 2 -to determine if the amount of variability across samples exceeds that expected by random error. With identical study procedures, variability exceeding expectations of sampling error is likely attributable to variation in the effect due to sample or setting. These analyses are presented in Table 4. Overall, two effects, Self-Esteem and Subjective Distance and Credentials and Prejudice, showed signs of inter-site variation. For both effects, the interactions (I 2 = 39.25%, p = .026; I 2 = 28.17%, p = .068, respectively) and main effects (I 2 = 35.81%, p = .063; I 2 = 40.31%, p = .023, respectively) showed small to moderate variation, according to meta-analytic standards ( Higgins et al., 2003). All other effects showed little inter-site variation (Q < 22.40, p > .288).  Table 3). Heterogeneity tests conducted with R-package metafor. REML was used for estimation for all tests. Variation by task order. Across the session, effects may weaken if participants get fatigued or if prior measures interfere with subsequent measures. To investigate this possibility, we conducted moderator analyses on each of the 10+3 effects, testing for linear and quadratic order effects (see Table 5 for summary and Table S6 for other tests of order effects). Overall, we observed very little variation by task order (average \u03b7 p \u00b2 = .0003 for effects with non-binomial outcomes, average d = .04 for effects with binomial outcomes). In addition, we analyzed the data from each effect when it was presented first in the task sequence. Comparing these results to the aggregate results revealed little variation. Metaphoric Restructuring was slightly weaker when presented first (\u0394d = -.18) and Availability was slightly stronger (\u0394d = .17). The Elaboration Likelihood main effect was also slightly stronger (\u0394\u03b7 p \u00b2 = .04) when presented first. All other effects showed similar strength. ( Table 5 here, located at end of document for reference) Variation by time of semester. Our primary interest was in the variation of effects across the academic semester. For each of the 10 replicated effects (and 3 additional main effects) we first constructed an unconditional model, predicting the outcome variable from a fixed intercept and a random intercept of site. This was to determine the amount of variation in outcome variables between sites before examining time of semester variation in effect detection. For all but two of the models, site accounted for 1.1% of the variance or less in the dependent variable. There were non-trivial site effects for the persistence measure (5.0%; Persistence and Conscientiousness) and for temperature (22%; Warmth Perceptions). Students at some sites were more persistent with the anagrams than at other sites, and some lab rooms were perceived as warmer than others. Otherwise, there was little variation in the dependent variables by site. Then, we constructed a mixed effects model, with the Time of Semester \u00d7 Replication Independent Variable(s) as a fixed effect. We included a random intercept of site and random slope of the fixed effect by site. For many models, this random slope overparameterized the model, and was thus simplified or dropped. The final model for each effect was compared to a model without Time of Semester as a fixed interaction to test whether adding Time of Semester provided a better fit for the data. 15 We performed these analyses on participant pool participants first, and we planned to use the MTurk sample as a comparison when time of semester variation was observed. See Table 6 for a summary of variation by semester analyses. (Table 6 here, located at end of document for reference) There was little evidence for variation by time of semester for most effects. Of the 13 tested effects, model fit comparisons provided very weak evidence for three effects with slight With so little evidence for a time of semester effect, we conducted a follow-up exploratory analysis comparing data from the first 80% of the semester to the last 20% of the semester. This was to focus the test on the intuition that the inattentive or unmotivated participants are those that complete studies at the very end of the semester. Results are summarized in supplementary Table S7. Again, we found little evidence of variation in effect magnitudes, observing the largest difference for Metaphoric Restructuring. The large number of comparisons suggests caution in interpreting this effect, however. Overall, the data revealed little evidence for variation in effect magnitudes by time of semester. Even when just considering effects that replicated in aggregate, only two of six effects showed hints of time of semester variation (Stroop, Metaphoric Restructuring). In both cases, the effects were actually larger toward the end of the semester compared to the rest of the semester.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Data Quality Indicators", "text": "Participants reported fairly high levels of effort (M = 3.71, SD = .78; Scale 1 = no effort to 5 = tried my hardest) and attention (M = 3.92, SD = .74; Scale 1 = none to 5 = I gave my undivided attention), and 37.3% failed the instructional attention check, similar to prior demonstrations with this challenging check (Oppenheimer, Meyvis, & Davidenko, 2009;Hauser & Schwarz, in press). Participants demonstrated some awareness of their attention levels. Participants who passed the attention check reported higher attention (M = 4.03, SD = .70) than those who failed the check (M = 3.78, SD = .77), t (2606 Only one effect, Availability Heuristic, was reliably moderated by performance on the attention check, with those who failed the check actually showing a stronger effect (p = .032, d = .08; see Table S8 for a full summary of results). The attention check did not moderate any of the time of semester effects observed either (ps > .512). To analyze time of semester variation in these data quality indicators, we constructed mixed effects models predicting the data quality indicators with Time of Semester as a fixed effect and a random intercept of Site (see Table 6). We compared these models to models without Time of Semester as a fixed effect. Unconditional models revealed that 2.5% of variance in Reported Effort, 1.6% of the variance in Reported Attention, and 4% of the variance in the Attention Check was explained by inter-site variation. This suggests more variation in effort and attention across sites than variation in responses on most of the dependent variables. ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Demographics", "text": "To observe demographic trends over the semester, we constructed mixed effects models predicting participant sex, age, ethnicity, and year in school from Time of Semester as a fixed effect and a random intercept of Site (see Table 6 for a summary). Time of Semester only reliably improved the model for participant sex, \u03c7 2 (1, N = 2598) = 17.57, p < .001. Participants were more likely to be male as the semester progressed, r(2598) = .12, p < .001, 95% CI = [.08, .16].", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Individual Differences", "text": "To evaluate variation across the semester, we constructed linear mixed effects models testing each of the 10 individual difference variables (see Table 6). We compared a model with Time of Semester as a fixed effect and a random intercept of Site with a model lacking the Time of Semester fixed effect. These model comparisons revealed that Time of Semester reliably improved models for conscientiousness (p < .001), mood (p = .005), and stress (p = .001). Follow up analyses showed that as the semester progressed, participants were less conscientious, r(2626) = -.14, p < .001, 95% CI = [-.18, -.10], reported worse mood, r(2634) = -.07, p = .001, 95% CI = [-.10, -.03], and reported being more stressed, r(2621) = .08, p < .001, 95% CI = [.04, .12]. All of these effects were small, and none of the other individual differences were reliably moderated by time of semester. Finally, in exploratory analyses, we investigated whether the data quality indicators or individual differences that varied over the semester moderated the effects that varied over the semester (Stroop, Metaphoric Restructuring). However, none of these data quality indicators or individual differences moderated Stroop or Metaphoric Restructuring (all p's > .253, see supplementary materials for details).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Discussion", "text": "This crowdsourced project evaluated whether variation in effect magnitudes can be partially attributed to the time of semester of data collection. The answer from the 10+3 investigated effects is largely no. Detected effects had similar effect sizes regardless of when data collection occurred and effects that were not detectable during some part of the semester were not detectable at any point during the semester. Consistent with literature showing that Stroop effects are sensitive to the availability of cognitive resources to overcome response competition (Kane & Engle, 2003), the Stroop effect was slightly stronger toward the end of the semester (last 20% d = .92) compared to the beginning (first 80% d = .89), but even that effect was very small. Also, there was a hint of stronger effects for Metaphoric Restructuring at the end of the semester compared to earlier. All told, effects showed little to no moderation by time of semester, site of data collection, and order in which the tasks were administered. Qualifying the generality of the conclusion, of the ten original effects we examined, only three replicated the original result, regardless of the time of semester. After observing this, but prior to testing time of semester effects, we added three successful main effect replications. These provided no additional evidence for time of semester effects. Examining only the reliable effects, two of the six showed any time of semester variation, and those two effects became very slightly stronger, not weaker, in the latter parts of the semester. The conclusions would be more definitive had a greater proportion of the effects shown a reliable result. Moreover, the selection of effects was by no means a random selection or representative sample of all possible effects. As such, the present results provide a provocative, but constrained conclusion. With a very high-powered design, time of semester was largely irrelevant for estimating the magnitude of experimental and correlational effects. The extent to which the present results will generalize across replicable experimental and correlational effects is unknown.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "What does change across the academic semester", "text": "If effects do not change across the semester, what does? The present study replicated and extended prior observations ( Nicholls et al., 2014;Witt et al., 2011). As the semester progressed, participants reported slightly less effort and attention, were slightly more likely to fail an attention check, were slightly less conscientious, had slightly worse mood, had slightly higher stress, and had slightly higher representation of men compared to women. These effects are regularly hypothesized and easily recognized by frequent users of participant pools even though time of semester accounts for only about 1% of the variance in each. As such, participant characteristics did shift slightly across the semester, but these shifts had little impact on the detectability of the tested correlations and experimental results. In fact, these indicators suggest slightly weakening data quality later in the semester, but the two effects that did change actually showed stronger effects toward the end.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Moderation of effects", "text": "A common explanation for the challenges of replicating results across samples and settings is that there are many seen and unseen moderators that qualify the detectability of effects (Cesario, 2014). As such, when differences are observed across study administrations, it is easy to default to the assumption that it must be due to features differing between the samples and settings. Besides time of semester, we tested whether the site of data collection, and the order of administration during the study session moderated the effects. Whether the task was administered first, in the middle, or last had minimal impact on the investigated effects. This is consistent with the first \"Many Labs\" study ( Klein et al., 2014). This suggests against the possibility that there is something about the procedure of combining studies into a single session that disrupts detectability of effects. We did observe some evidence of variation by sample or setting for main effects and interactions of two of the ten studies, Self-Esteem and Subjective Distance and Credentials and Prejudice. These are demonstrations of a truism in social psychology -that effects vary by sample and setting. If anything, it is notable that sample and setting variation was not more prevalent. Investigating variation by sample and setting is the focus of the second \"Many Labs\" study with many samples and societies included in the study ( Klein et al., 2015). Another potential moderator of well-known effects is participant knowledge. For example, Elaboration Likelihood and Availability Heuristic are often taught in introductory psychology classes. If students learned about these effects in their courses, it is possible that this would reduce observed effects (see, however, Lambdin & Shaffer, 2009). However, if that were the case, we would expect to observe time of semester variation on classic effects, as students would presumably learn about these effects sometime during the academic term, making them less detectable near the end of the term. In addition, students would likely vary in their knowledge of these effects from site to site, as different lessons would be taught at different universities. Given the lack of variation from these two sources this seems unlikely to have occurred.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Insights about the Selected Effects", "text": "We were surprised that several effects showed null effects in our large sample. The present study's very large sample size and lack of moderating effects by site, order, and time of semester does provide precision and some definitiveness about these paradigms under these conditions. However, under these conditions, is a critical qualifying phrase. The present results do not definitively suggest that the observed nulls are always null, nor do they definitively suggest that the original positive results are false positives. What can be concluded is that those effects are not distinguishable from zero with the samples, settings, materials, and procedure employed here. Even among effects that did replicate in aggregate, we observed smaller effect sizes compared to the original demonstrations. Although past replication projects have observed similar declines in effect strength (Open Science Collaboration, 2015), there are several possible explanations for the declines observed in this study. For instance, in some cases, the original materials or methods were altered to accommodate the constraints of this investigation. 16 Based 16 Detailed explanations of all known alterations are located in Supplementary Information: Methods for Selected Effects and a summary of alterations can be found in Table S4. on a priori theorizing and review these alterations were not expected to alter the results of the replications substantially. 17 Even so, such changes might have had unexpected influences. For example, the original weight importance study was conducted with a Dutch sample. Our samples were from North America, and we did not anticipate this change to qualify the effect based on the current theoretical understanding. Across our 20 sites, we observed mean scores in both conditions of the weight importance study that were within a point of the ceiling reducing the power to detect an effect. The original study had lower means particularly in the light clipboard condition making it an outlier by comparison. It is possible that the change in samples is responsible for the shift in means. Another possibility is that the original study's lower means, particularly in one condition, were an unusual chance occurrence. Parsing between these possibilities requires conducting a replication that includes the original (Dutch) population. Furthermore, many of the theories invoked by the selected effects have been demonstrated using various methods. For our purposes, we had to select a single instantiation. Thus, our results can only speak to those instantiations, not necessarily the broader theory. A better theoretical understanding of each effect, and the theories they are derived from, will be achieved when the conditions for influencing the effect magnitude are articulated and demonstrated empirically. The present evidence and further explorations of the dataset may provide useful hypothesizing for how to begin that search. A notable procedural difference between this and the original studies is that the effects were investigated in a single experimental protocol. It is reasonable to hypothesize that this procedural difference produced smaller effects in the replications particularly if they occur later in the ~30 minute protocol. The present evidence suggests that this did not occur, particularly because the order of tasks did not moderate the observed effects, including considering only the first task completed. Moreover, the first Many Labs project ( Klein et al., 2014) had a similar procedure and reliably detected 10 (and the 11th weakly) of 13 effects, with some effects producing larger effect size estimates compared to the original. An untested possibility is that the procedure weakened effects of even the first task completed because participants anticipated doing many tasks. However, because there were no order effects in this study, such an influence would need to be equal to the disruptive impact of having just experienced the other tasks. While we do not find this idea to be particularly plausible, it would be straightforward to test in new research. Three of the investigated interaction effects did not replicate: Elaboration Likelihood, Self-Esteem and Subjective Distance, and Credentials and Prejudice. In all three cases, a theoretically relevant main effect was observed. For Credentials and Prejudice, we observed the effect of credentials on prejudice, but that effect was not qualified by gender (Monin & Miller, 2001). Our perception is that the interaction effect is much less theoretically essential than the main effect of credentials. In fact, Monin and Miller (2001) did not anticipate an interaction; it emerged unexpectedly in their first study and did not persist in their second study, which used a different manipulation of credentials. As such, the present replication can be seen as affirming their original theoretical expectations, and disconfirming the unexpected moderation by gender. Likewise, for Self-Esteem and Subjective Distance (Ross & Wilson, 2002), we did observe that positive past events felt closer than negative past events, but we did not replicate the moderation of this effect by self-esteem. In our view, the main effect is most vital theoretically. Finally, in Elaboration Likelihood ( Cacioppo et al., 1983), we observed that stronger arguments were more persuasive than weaker arguments, but this was not qualified by need for cognition. The failure to replicate this interaction is one of the most surprising results from this study. The Elaboration Likelihood Model is among the most developed and empirically investigated theories in psychology (Petty & Bri\u00f1ol, 2012). Our result would seem to be an outlier in the literature, albeit a highly precise one. In light of this, it is important to note that we only tested one instantiation relevant to this theory. Post hoc, we examined possible moderators to account for the difference, but did not find support for any of them. We do not have an explanation for why no effect was observed under these circumstances.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Additional Analysis Opportunities", "text": "The amassed dataset is rich for exploring the individual effects, individual difference variables, interactions between the two, and alternate ways to analyze the aggregate data. Our analysis plan for the main article focused on time of semester variability and not, for example, exploring moderating influences in depth. However, the data set and all materials are available publicly to encourage further investigations by others (visit https://osf.io/ct89g/).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Conclusion", "text": "Conventional wisdom among behavioral scientists suggests that the time of semester for data collection from participant pools is an important factor for obtaining effects. Our powerful design across 20 participant pools found more evidence against this conclusion than for it. We did find evidence that the characteristics of the sample changes across the semester, but those changes did not alter detection of the selected effects. Should researchers now discount conventional wisdom? Therein is the incompleteness of any single investigation. The present results are the only known large-scale investigation of the influence of time of semester on a variety of effects. As such, the present results should give pause to speculative invocations of time of semester as an explanatory factor. At the same time, conventional wisdom often has a basis in experience. It is possible that there are some conditions under which the time of semester impacts observed effects. However, it is unknown whether that impact is ever big enough to be meaningful.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf"}
{"section": "Abstract", "text": "This research investigated whether stereotypes or individuating information take primacy in implicit and explicit person perception. Study 1 investigated whether variation in the diagnosticity of individuating information moderated stereotype bias in implicit and explicit person perception. Increases in diagnosticity produced a linear reduction in explicit and implicit stereotype bias; with more diagnostic individuating information, there was less bias. Studies 2 and 3 examined the effects on person perception of racial stereotypes and of diagnostic in-dividuating information that varied in valence. Study 2 found no substantial implicit or explicit anti-Black stereotype bias in the presence of diagnostic individuating information and large individuating information effects on explicit person perception. Study 3 found no explicit anti-Black stereotype bias in the presence of diagnostic individuating information and that individuating information influenced both implicit and explicit person perception. Together, these studies showed that individuating information can reduce or eliminate stereotype bias in implicit and explicit person perception and that its effect depends on the diagnosticity of the information. In addition, patterns of reliance on stereotypes and individuating information in implicit and explicit person perception generally converged. Results are discussed in the context of current controversies about the processes underlying implicit and explicit social cognition.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Introduction", "text": "What sources of information do people prioritize when consciously and subconsciously forming impressions of others? Do they primarily rely on stereotypes (general beliefs about the characteristics of social groups and their individual members; Ashmore & Del Boca, 1981) or on individuating information (any information about an individual group member other than category information; Kunda & Thagard, 1996;Locksley, Borgida, Brekke, & Hepburn, 1980;cf. Brewer, 1988)? Does this reliance change when the impressions are implicit rather than ex- plicit? The present research investigated the roles of individuating in- formation and stereotypes in implicit and explicit person perception and addressed the convergence versus divergence of these two modes of impression formation. 2. Do stereotypes or individuating information take primacy in explicit person perception?", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Theoretical perspectives", "text": "Early theoretical models of impression formation disagreed about the primacy of stereotypes versus individuating information in explicit person perception. In these perspectives, primacy generally refers to information that dominates person perception with effects that are ty- pically large or difficult to eliminate. Some argued that stereotypes take primacy over individuating information in person perception (Brewer, 1988;Fiske & Neuberg, 1990). Others asserted that neither in- dividuating information nor stereotypes take primacy in person per- ception by default-rather, that reliance on stereotypes versus in- dividuating information in person perception depends in part on the characteristics of the individuating information and the judgment task (Kunda & Thagard, 1996). In addition, some have argued that stereotypes lead people to ignore individual differences (e.g., APA, 1991;Aronson, 2011;Aronson, Wilson, Akert, & Sommers, 2015;Whitley & Kite, 2009). For example, In contrast to explicit social cognition, implicit social cognition has typically been defined as thoughts and feelings about social objects that are at least partially outside of conscious awareness (Bargh & Chartrand, 1999;Devine, 1989;Greenwald & Banaji, 1995). The pro- cesses underlying implicit and explicit social cognition have been the subject of extensive debate (see Cone, Ferguson, Mann, & Wojnowicz, 2014). The classic dual-process per- spective (e.g., Gawronski & Bodenhausen, 2006Sloman, 1996Sloman, , 2014Smith & DeCoster, 2000) dis- tinguishes between two types of cognitive processes: associative pro- cesses and propositional processes. In contrast, more recent perspec- tives reject the argument for two processes, instead espousing multi- process (Amodio, 2014;Amodio & Ratner, 2011) or single-process (DeHouwer, 2014a(DeHouwer, , 2014bHughes, Barnes-Holmes, & DeHouwer, 2011) models of social cognition. The present research did not take an a priori position with respect to this debate. Instead, we derived a series of competing hypotheses from this literature to empirically address the unresolved questions of (a) whether stereotypes or individuating information take primacy in im- plicit person perception, and (b) whether patterns of reliance on in- dividuating information and stereotypes in implicit and explicit person perception converge or diverge.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Do stereotypes or individuating information take primacy in implicit person perception?", "text": "4.1. Theoretical perspectives consistent with the primacy of individuating information in implicit person perception A central tenet of propositional models of implicit evaluations (DeHouwer, 2014a(DeHouwer, , 2014b; see also Hughes et al., 2011) is that the automatic formation or activation of propositions mediates implicit evaluations. Propositional information can be defined as a statement about the world that has an objective truth value (e.g., \"The sky is blue\"; DeHouwer, 2014a; Gawronski & Bodenhausen, 2006). According to this definition, many types of individuating information are propositional in nature because they provide information about targets that generally has objective truth values (e.g., a target did or did not engage in a behavior, received a certain score on a test, etc.). Be- cause individuating information is oftentimes inherently propositional, these propositional models can be interpreted as predicting sensitivity of implicit evaluations to individuating information. In addition, the associative-propositional evaluations (APE) dual process model of implicit and explicit attitude change (Gawronski & Bodenhausen, 2006) posits that implicit evaluations should be sensitive to information that affirms new propositions. According to this model, revision of existing implicit attitudes and beliefs to incorporate counterinformation falls under this umbrella. This idea can easily be extended to reliance on counterstereotypic individuating information (the processing of which presumably constitutes affirmation of new propositions; see Mann & Ferguson, 2015) in person perception. The resulting prediction is that individuating information should take pri- macy over stereotypes in implicit person perception.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Theoretical perspectives consistent with the primacy of stereotypes in implicit person perception", "text": "Other perspectives suggest that, once they are formed, implicit evaluations should be less likely than explicit evaluations to take into account new information (Amodio, 2014;Amodio & Ratner, 2011;Gregg, Seibt, & Banaji, 2006;Sloman, 1996;Smith & DeCoster, 2000;Wilson, Lindsey, & Schooler, 2000). Several of these perspectives draw the distinction between slow-learning, asso- ciative processes, more recently named \"System 1,\" and fast-learning, rule-based, propositional processes known as \"System 2\" Sloman, 1996;Smith & DeCoster, 2000). This view posits that, because System 1 processes are based on associations that accumulate gradually (cf. Gregg et al., 2006), they less readily in- corporate new information than do System 2 processes (Smith & DeCoster, 2000). Some of these theories contend that at least that some types of implicit social cognition are System 1 processes (Amodio, 2014;Amodio & Ratner, 2011;Smith & DeCoster, 2000; see also Wilson et al., 2000). In contrast, explicit social cognition is a System 2 process according to this view Smith & DeCoster, 2000). Taken together, these perspectives are con- sistent with the prediction that implicit evaluations should be relatively resistant to the influence of individuating (new) information and in- stead be based on stereotypes (existing associative information), whereas explicit evaluations should readily incorporate individuating information.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Empirical evidence regarding the responsiveness of implicit person perception to new information is mixed", "text": "Some empirical research shows that implicit evaluations of in- dividuals do not easily change in response to new information Rydell, McConnell, Strain, Claypool, & Hugenberg, 2007). Other research has found that implicit evaluations readily in- corporate new information (Brannon & Gawronski, 2017;Peters & Gawronski, 2011;Whitfield & Jordan, 2009, Study 3), or that they do so to an extent (Cao & Banaji, 2016). Still other evidence suggests that implicit evaluations of individuals can be revised under particular cir- cumstances (Cone & Ferguson, 2015;Gawronski, Rydell, Vervliet, & De Houwer, 2010;Mann & Ferguson, 2015, 2017Rydell, McConnell, Mackie, & Strain, 2006;Wyer, 2010Wyer, , 2016). Thus, no single pattern has emerged regarding the sensitivity of implicit evaluations of individuals to new information (for a review, see Cone, . Two of these previous programs of research are particularly relevant to the present studies. In one, Cone and Ferguson (2015) found that a single instance of highly diagnostic behavior induced revision of im- plicit evaluations of an individual. The present research builds on this by investigating the effect of diagnostic individuating information on stereotype bias in implicit person perception and by examining implicit impressions in a specific domain (intelligence or unintelligence) rather than overall positivity or negativity. In the other, Cao and Banaji (2016) found that counterstereotypic An explicit stereotype bias effect size of r < 0.10 is considered \"trivial\" in this table and in the paper, as is D < | 0.15|, the cutoff for \"meaningful\" implicit preferences ( Nosek et al., 2007).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "b", "text": "An explicit stereotype bias effect size of r \u2265 0. 10 and an implicit stereotype bias effect of | D| \u2265 0.15 are considered \"substantial\" in this table and in the paper. individuating information reduced, but did not eliminate, stereotype bias in implicit person perception. The present research extends that of Cao & Banaji by examining the moderating effects of the diagnosticity of the individuating information on implicit stereotype bias and by using individuating information that is inherently different than that employed by Cao & Banaji. In addition, it attempts to identify condi- tions under which implicit stereotype bias is eliminated rather than reduced. 5. Do patterns of reliance on individuating information and stereotypes in implicit and explicit person perception converge or diverge? The APE model (e.g., Gawronski & Bodenhausen, 2006) ar- gues that learning information that runs counter to existing attitudes or beliefs should induce \"corresponding changes in implicit and explicit attitudes\" (2006, p. 706; emphasis added). Thus, the APE model is consistent with the prediction that patterns of reliance on stereotypes (existing beliefs) and individuating information (new information) in implicit and explicit person perception should converge. Other perspectives explicitly predict the divergence of implicit and explicit evaluations in their responsiveness to updated information ( Gregg et al., 2006;). Because individuating information is \"new\" information relative to previously established stereotypes, these accounts are consistent with the prediction of di- vergence of patterns of reliance on stereotypes and individuating in- formation in implicit versus explicit person perception.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Contributions of the present research", "text": "Previous research has not identified circumstances under which implicit stereotype bias in person perception is eliminated by in- dividuating information. Thus, the present research was the first to address the question of, under what circumstances might perceivers rely exclusively on individuating information in implicit person perception, thereby disregarding their stereotypes? This was an important question to answer because even small implicit biases can have real-world sig- nificance (Greenwald, Banaji, & Nosek, 2015). Moreover, the present research provided further empirical evidence relevant to the debate about the processes underlying implicit and ex- plicit social cognition; although it was not designed specifically to ad- dress this issue, we drew from this literature to derive our hypotheses. If perceivers' implicit evaluations were not influenced by individuating information and were instead based on stereotypes, this would be more consistent with the slow-learning perspective on implicit social cogni- tion (e.g., Gregg et al., 2006;Smith & DeCoster, 2000). On the other hand, if implicit evaluations were based on individuating information, this would be more consistent with propositional models of implicit evaluations (e.g., DeHouwer, 2014aDeHouwer, , 2014bHughes et al., 2011) and the APE model (Gawronski & Bodenhausen, 2006).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Research questions and hypotheses", "text": "The present research tested three sets of competing predictions derived from alternative perspectives that addressed the power of in- dividuating information versus stereotypes in implicit and explicit person perception (Research Question 1). It also compared patterns of reliance on stereotypes and individuating information in implicit person perception with those in explicit person perception (Research Question 2).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Research question 1", "text": "What are the relative effects of stereotypes and individuating information in implicit and explicit stereotype-relevant target evaluations? In this section, Hypotheses labeled \"a\" (e.g., Hypothesis 1a) are consistent with perspectives on explicit person perception that emphasize the importance of individuating information over stereotypes (Jussim, 2012;Kunda & Thagard, 1996), and with propositional models of im- plicit evaluations (e.g., DeHouwer, 2014aDeHouwer, , 2014b) and the APE model of implicit and explicit attitude change (Gawronski & Bodenhausen, 2006). In contrast, hypotheses labeled \"b\" (e.g., Hypothesis 1b) are consistent with the competing view that stereotypes cause percei- vers to ignore individual differences at the explicit level (e.g., Aronson, 2011) and with the slow learning perspective on implicit social cogni- tion (e.g., ). See Table 1 for a summary of all hypotheses relevant to Research Question 1. Hypothesis set 1 comprised alternative predictions about stereotype bias that were derived from the competing perspectives. These hy- potheses addressed the effects of variation in the diagnosticity of in- dividuating information: Stereotype bias Hypothesis 1a. Stereotype bias should decrease as the diagnosticity of available individuating information increases. Stereotype bias Hypothesis 1b. Stereotype bias in person perception should be equal in magnitude regardless of the diagnosticity of available individuating information. Hypothesis set 2 consisted of alternative predictions regarding the presence or absence of stereotype bias after exposure to highly diag- nostic individuating information. Stereotype bias Hypothesis 2a. There should be no stereotype bias after exposure to highly diagnostic individuating information. 2 Stereotype bias Hypothesis 2b. Stereotype bias in person perception should persist despite highly diagnostic individuating information. The third set of hypotheses addressed individuating information effects from the standpoint of the competing perspectives. Individuating information Hypothesis 3a. All individuating information effects should be substantial. Individuating information effect Hypothesis 3b. All individuating information effects should be trivial or nonexistent.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Research Question 2", "text": "When the results of all three studies are considered as a whole, the present research also addresses the question of, do perceivers show si- milar patterns of reliance on individuating information and stereotypes in implicit versus explicit person perception, or do these patterns diverge? The former outcome would be consistent with the APE model ( Gawronski & Bodenhausen, 2006, while the latter result would be congruent with the slow-learning perspective on implicit evaluations (e.g., Gregg et al., 2006;Smith & DeCoster, 2000).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Overview of the present research", "text": "A series of preliminary studies was conducted to create the stimulus materials that were used in the main studies. In the three main studies, Study 1 examined the effect of variation in the diagnosticity of available individuating information on stereotype bias in implicit and explicit person perception and the effects of this variation on explicit person perception. Study 2 varied the valence of diagnostic individuating in- formation and investigated its effect on stereotype bias in implicit and 2 Theories of impression formation promoting the power of explicit stereotypes make a similar prediction, but only when the information is counterstereotypic (Fiske & Neuberg, 1990), under certain motivational circumstances (Brewer, 1988;Fiske & Neuberg, 1990), and provided adequate attentional capacities (Fiske & Neuberg, 1990). These theories are not cited here because the present research does not address all of these conditions.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "R.S. Rubinstein et al.", "text": "Journal of Experimental Social Psychology 75 (2018) 54-70 explicit person perception and its effect on explicit person perception. Studies 1 and 2 assessed effects of individuating information on implicit and explicit racial biases and on explicit person perception. They did not assess effects of individuating information on implicit person perception (i.e., they did not assess differences in perceivers' judgments about individuals with differing types or levels of in- dividuating information). To do so, implicit measures-in this case, Implicit Association Tests (IATs; Greenwald, McGhee, & Schwartz, 1998)-must compare implicit evaluations of two individuals who differ with respect to individuating information (e.g., an intelligent vs. unintelligent target) rather than individuals who differ with respect to their racial group memberships. Because the IATs in Studies 1 and 2 compared evaluations of two individuals of different racial back- grounds about whom equivalent individuating information was pro- vided, these IATs only assessed whether individuating information re- duced the application of racial stereotype bias to these pairs of individuals. Like Study 2, Study 3 varied the valence of diagnostic individuating information. However, it differed because it was the only study to assess implicit individuating information effects. This study also examined the effects of diagnostic individuating information on explicit person per- ception and the effects of diagnostic individuating information on ste- reotype bias in explicit person perception.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Preliminary studies", "text": "The pilot studies ensured that: (a) names were perceived as proto- typically Black or White, (b) application information conveyed high or low intelligence, and (c) counterbalanced application information in- dicated an equal level of intelligence (see Supplementary Materials  Tables S1-S4 for full statistical details of all pilot results). In one study, participants rated how prototypically White or Black each name was on a scale ranging from 1 (very White) to 7 (very Black). We chose the most stereotypical (extreme) Black and White names. In another, par- ticipants evaluated how competent and intelligent college applicants with the GPAs and SAT scores used on the applications in our studies were on scales ranging from 1 (e.g., very incompetent) to 7 (e.g., very competent). Results revealed that these GPAs and SAT scores conveyed either high or low competence and intelligence and that the counter- balanced information did not differ in potency. We also compared the relative strength of the stereotype and in- dividuating information manipulations. To permit this comparison, stereotypicality and intelligence/competence ratings needed to be put on the same scale. To do so, we first reverse scored the stereotypicality ratings of the White names so that higher scores now indicated more stereotypicality for both Black and White names. We then created a new variable: stereotypicality scores for all Black and White names. Next, we created a new variable capturing the extremity of the combined in- telligence and competence ratings. To do so, we put the evaluations of competence and intelligence on the same scale of extremity for the strong and weak application information by reverse scoring the ratings of the weak application information. Higher scores now indicated more extreme ratings. We then averaged ratings for intelligence and com- petence to create a new overall extremity rating. To compare the strength of the stereotype and individuating in- formation manipulations, we compared the mean stereotypicality rat- ings of the names to the mean extremity ratings for competence and intelligence. The mean stereotypicality rating was more extreme than was the mean combined intelligence and competence rating [5.64 vs 5.17, t(65) = 3.26, p = .002, 95% CI difference (\u2212 0.75, \u2212 0.18), d = 0.83]. Because the stereotype manipulations were more extreme than were the individuating information manipulations, our studies provided a conservative test of the effects of the power of individuating information and a liberal test of the power of stereotypes.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Overview", "text": "Participants evaluated either: (a) racial groups, (b) one Black and one White individual given somewhat diagnostic individuating in- formation about them, or (c) one Black and one White individual given highly diagnostic individuating information about them. In the first condition, participants completed a questionnaire that assessed per- ceptions of the intelligence of \"the average Black person\" and \"the average White person.\" This questionnaire assessed explicit racial ste- reotypes about intelligence. Participants also completed an IAT that assessed implicit racial stereotypes about intelligence. In the other two conditions, participants completed a questionnaire and IAT that as- sessed evaluations of the individuals' intelligence. The questionnaire assessed application of explicit racial stereotypes to the pairs of in- dividuals and explicit individuating information effects. The IAT as- sessed application of implicit racial stereotypes to the pairs of in- dividuals. Study 1 assessed Hypothesis sets 1 (changes or no changes in ste- reotype bias given variation in the diagnosticity of individuating in- formation), 2 (the absence or presence of stereotype bias given highly diagnostic individuating information) and 3 (the presence or absence of individuating information effects; see Table 1 for a summary of all Hypotheses).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Method", "text": "In accordance with the 21-word solution (Simmons, Nelson, & Simonsohn, 2012), we report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures for all studies reported in this paper. All three studies in this program of re- search averaged at least 80% power to detect all possible effects (see Supplement 2.1 for details of power analyses). Data collection for all three studies stopped before analysis began. All raw data from all three studies will be retained indefinitely and is publicly posted at https:// osf.io/hymkc.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Participants", "text": "The sample size in Study 1 was based on a goal of having at least 30 participants per cell in the design. However, we collected extra data to ensure adequate power given planned data exclusions and the potential for incomplete or missing data. Thus, we collected data from 183 par- ticipants, who were General Psychology students at a large northeastern state university. Their participation partially fulfilled a course re- quirement. In all studies in this research, data from all Black and mixed- race participants were excluded from analysis because Black perceivers' implicit and explicit racial stereotypes and attitudes differ from those of other racial groups (e.g., Nosek et al., 2007;Nosek, Banaji, & Greenwald, 2002). Consequently, data from the 14 Black and 10 mixed race participants were discarded. Also discarded were 13 participants who failed manipulation checks. The final sample included 146 stu- dents (see Supplement 2.1 for power analysis), 79 of whom were male. There were 76 White, 50 Asian, and 15 Latino students, and 5 students who identified with another racial group.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Experimental design", "text": "The implicit experimental design was a one-way between-subjects design (individuating information: no information vs. somewhat diag- nostic information vs. highly diagnostic information). Because IAT scores are difference scores, they inherently incorporate the race of target factor. The explicit experimental design was a 3 (individuating informa- tion: no information vs. somewhat diagnostic information vs. highly diagnostic information) \u00d7 2 (race of target: Black vs. White) mixed- model design. Individuating information was the between-subjects factor. 10.2.3.2. Somewhat diagnostic information condition. These participants viewed the college applications of one Black and one White applicant to the university at which the study was run. (See Supplement 3 for representative college applications from all three studies). The names on the applications (Jamal DeShawn Robinson and Luke Connor Reed) were selected on the basis of the pilot data that measured the racial prototypicality the names (Supplementary Materials Tables S1 and S2). Only demographic information (e.g., fictitious hometown) was included in these applications; academic information was omitted. However, these applications still constituted individuating information because not every person applies to college, and even among college applicants, not all apply to universities of this caliber. 10.2.3.3. Highly diagnostic information condition. This condition provided additional individuating information on the applications that was highly diagnostic of intelligence. Academic information included SAT scores, high school GPAs, and the fact that the applicants were National Merit Scholars. The pilot studies showed that evaluations of the intelligence and competence of applicants with the following credentials were similarly high: (a) SAT score of 1400 and GPA of 3.9, and (b) SAT score of 1420 and GPA of 3.8 (Supplementary  Materials Tables S3 and S4). This information was counterbalanced with race of target. Extracurricular activity information was also included (see Supplement 3).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Measures", "text": "All measures were administered using Inquisit software. A ques- tionnaire assessed explicit evaluations of targets' competence. Some items were open-ended (e.g., \"What is your estimate of Jamal's IQ?\"), and others were trait ratings that employed Likert-type scales (e.g., \"How intelligent is Jamal?\"; 1 = Not intelligent at all, 5 = Very in- telligent; see Supplement 2.2 for complete questionnaire 3 ). In the highly diagnostic information and somewhat diagnostic in- formation conditions, participants evaluated the applicants' compe- tence. In the no information condition, participants evaluated the competence of \"the average Black [White] person.\" The specification of \"the average Black person\" and \"the average White person\" approxi- mated a group stereotype condition while still remaining in the realm of person perception. The IAT measured implicit stereotype-relevant as- sociations regarding intelligence as they related to the individuals (highly diagnostic and somewhat diagnostic individuating information conditions) or racial groups (no information condition; see Supplement 2.5 for details of the format and administration of the IAT and Supplement 2.6 for IAT stimuli).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Procedure", "text": "In the somewhat and highly diagnostic individuating information conditions, participants had seven minutes to memorize the applica- tions (ostensibly for a memory test) and to answer eight manipulation check questions. These questions ensured that participants were re- viewing the applications (e.g., \"What is Jamal's GPA?\"). Next, experi- menters collected the applications and participants completed the ex- plicit measures and the IAT, answered demographic questions, were probed for suspicion, and were debriefed. The procedure in the no in- formation condition was identical to that in the individuals with highly diagnostic and somewhat diagnostic information conditions except that participants did not review college applications.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Preliminary analyses", "text": "The scoring procedures recommended by Greenwald, Nosek, and Banaji (2003) were followed to calculate D scores (IAT effects; see Supplement 2.5). Larger positive D scores indicated stronger implicit anti-Black stereotype bias, and greater negative D scores indicated stronger implicit anti-White stereotype bias. The eight trait rating items that measured participants' evaluations of the targets' competence demonstrated strong internal consistency, \u03b1 Black = 0.94, \u03b1 White = 0.93 and were combined to form a competence scale. The competence scale was computed separately for each target. The potential range of values for the competence scale was from 6 to 30. The dependent measures in this experiment were estimated IQ, the competence scale, and D scores (see Supplementary Materials Table S5 for Study 1 explicit variable descriptive statistics and for correlations among all Study 1 dependent measures).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Standards of comparison", "text": "10.3.2.1. Benchmark for determining the presence or absence of implicit preferences. Traditionally, nonsignificant IAT scores are interpreted as indicating a lack of implicit preference (i.e., the \"true zero\" interpretation; e.g., Stout, Dasgupta, Hunsinger, & McManus, 2011). Another standard that has been put forth is that | D| \u2265 0.15 indicates substantial implicit preferences (Nosek et al., 2007, p. 10) regardless of statistical significance. We adopted this standard because statistical significance is an arbitrary function of sample size rather than exclusively measuring the amount of bias, and because the IAT is \"right biased\"-egalitarian responding on other measures often corresponds to IAT scores greater than zero (Blanton, Jaccard, Strauts, Mitchell, & Tetlock, 2015).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Standard of comparison for determining reduction or elimination", "text": "of stereotype bias. Although the primary purpose of this research was to examine reliance on stereotypes and individuating information in person perception, we intentionally included a condition in Study 1 (the no information condition) that was designed to assess group stereotypes. Responses to explicit and implicit measures in this constituted a benchmark for subsequent comparisons evaluating whether individuating information of varying levels of diagnosticity reduced or eliminated bias (see Locksley et al., 1980, for a similar approach). 10.3.3. Stereotype bias in implicit person perception 10.3.3.1. Implicit stereotype bias in the presence of individuating information that varied in diagnosticity. A single-sample t-test on D scores in the no information condition established the presence of implicit group stereotype bias in the absence of individuating information. According to standard IAT score interpretation conventions (e.g., Rudman, 2011) this analysis showed slight to moderate implicit anti-Black stereotype bias, 4 D = 0.29 (> 0.15), SD = 0.32, 95% CI (0.20, 0.38), t(51) = 6.49, p < .001; people implicitly believed that White people were smarter than Black people. Stereotype bias Hypothesis 1a predicted that stereotype bias would decrease with increasingly diagnostic individuating information. In contrast, stereotype bias Hypothesis 1b predicted that stereotype bias would be equal in magnitude regardless of the diagnosticity of the in- dividuating information. A one-way between-subjects ANOVA on the D scores tested these hypotheses and revealed a significant main effect for individuating information, F(2, 140) = 4.09, p = .02, \u03b7 = 0.24. The amount of implicit anti-Black stereotype bias was reduced by the diagnosticity of available individuating information (Ds = 0.29, 0.20, 0.11, respectively, for the no information, somewhat diagnostic in- dividuating information, and highly diagnostic individuating informa- tion conditions). Although the ANOVA supported Hypothesis 1a, a stronger test of this hypothesis was provided by an a priori linear contrast testing the prediction that implicit stereotype bias would decline as diagnosticity increased. The no information cell was coded as 1, the somewhat di- agnostic information cell as 0, and the highly diagnostic information cell as \u22121. The contrast was significant, F(1, 140) = 8.17, p = .005, \u03b7 = 0.23, and the cell means were perfectly correlated with the contrast coefficients, r(142) = 1.00. Thus, the contrast explained 100% of the systematic between-groups variance. No systematic residual between- groups variance remained, F(2, 140) = 0.01, p = .99, \u03b7 = 0.009. These results showed an exactly linear pattern of decreased implicit stereo- type bias in the presence of individuating information that was in- creasingly diagnostic. These findings further supported Hypothesis 1a, which predicted that stereotype bias would be moderated by the di- agnosticity of individuating information. 10.3.3.2. Implicit stereotype bias with highly diagnostic individuating information. Hypothesis 2a predicted no substantial stereotype bias in the presence of highly diagnostic individuating information, whereas Hypothesis 2b predicted substantial stereotype bias in the presence of highly diagnostic individuating information. Single-sample t-tests performed on D scores from the highly diagnostic information condition tested these hypotheses at the implicit level. Results supported Hypothesis 2a: there was no substantial implicit stereotype bias in this condition, D = 0.11 (< 0.15), SD = 0.28, 95% CI (0.02, 0.20), t(43) = 2.60, p = .013. In contrast, there was substantial bias both in the somewhat diagnostic information condition, D = 0.20, (> 0.15), SD = 0.31, 95% CI (0.12, 0.29), t(46) = 4.59, p < .001, and in the no information condition (D = 0.29, reported above).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Stereotype bias in explicit person perception", "text": "Analyses next assessed the competing hypotheses as they related to explicit evaluations. Two 3 (individuating information: no information vs. somewhat diagnostic information vs. highly diagnostic information) \u00d7 2 (race of target: Black vs. White) mixed-model ANOVAs were per- formed to test these hypotheses: one on IQ estimates, and the other on the competence scale. Individuating information was the between- subjects factor. 10.3.4.1. Explicit stereotype bias in the presence of individuating information that varied in diagnosticity. A significant main effect for race of target on both explicit dependent variables indicated that overall, Black targets were evaluated as less competent than White targets, IQ estimate F(1, 142) = 23.77, p < .001, \u03b7 = 0.11, competence scale F(1, 143) = 8.69, p = .004, \u03b7 = 0.06 (see Table 2 for means, standard deviations, and 95% confidence intervals). In addition, both race of target X individuating information interactions were sig- nificant, IQ estimate F(2, 142) = 4.55, p = .01, \u03b7 = 0.07, competence scale F(2, 143) = 11.83, p < .001, \u03b7 = 0.09 (see Table 3 for full de- sign cell means, SDs, and 95% CIs). These analyses disconfirmed Hy- pothesis 1b's prediction that stereotype biases would be similar re- gardless of the diagnosticity of the individuating information. Additional analyses identified how individuating information moderated stereotype biases, further addressing Hypothesis 1a's pre- diction of decreased stereotype bias with greater diagnosticity of in- dividuating information. First we established the presence of stereotype bias in the absence of individuating information, IQ estimate t(50) = 5.18, p < .001, d = 1.46, competence scale t(50) = 5.55, p < .001, d = 1.54 (see Table 3 for full design cell means, SDs, and 95% CIs). This subsequently served as a standard of comparison for bias reduction. Next, we performed a one-way ANOVA on perceived racial differ- ence scores for each dependent measure and then performed a priori linear contrasts on these difference scores. Racial difference scores were computed by subtracting evaluations of the White target from evalua- tions of the Black target (see Table 4 for cell means, SDs, and 95% CIs). Both ANOVAs showed significant individuating information effects on racial biases, IQ estimate difference F(2, 142) = 4.55, p = .01, \u03b7 = 0.25, competence difference score F(2, 143) = 11.83, p < .001, \u03b7 = 0.38. For the linear contrasts performed on the difference scores, the no information cell was coded as 1, the somewhat diagnostic in- formation cell as 0, and the highly diagnostic information cell as \u2212 1. The contrasts were significant both for IQ estimate difference scores, F (1, 142) = 9.09, p = .003, \u03b7 = 0.25, and competence scale difference scores, F(1, 143) = 23.18, p < .001, \u03b7 = 0.37. For these analyses, the top two rows of Table 5 report the correlation of the contrast coeffi- cients with the cell means (both rs \u2265 0.99), the proportion of sys- tematic (between condition) variance explained by the contrast (> 96%), and the significance tests for the variance left unexplained by the contrast (both ps > 0.61). Consistent with stereotype bias Hypothesis 1a, there was a strong pattern of less explicit racial ste- reotype bias with individuating information that was increasingly di- agnostic. 10.3.4.2. Explicit stereotype bias in the presence of highly diagnostic individuating information. Hypothesis 2a predicted that there would be no stereotype bias in the presence of highly diagnostic individuating information, whereas Hypothesis 2b posited that stereotype bias would persist. Contrast t-tests comparing evaluations of Jamal to evaluations Note. Means within race of target or individuating information conditions that do not share superscripts statistically differ (p < .01). Race of applicant was within-subjects; individuating information was between-subjects. Higher scores indicate greater perceived IQ and competence. a n = 146. b n = 146. c n = 52. d n = 49. e n = 44.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "R.S. Rubinstein et al. Journal of Experimental Social Psychology 75 (2018) 54-70", "text": "of Luke indicated no stereotype bias in the presence of highly diagnostic individuating information, IQ estimate t(43) = 0.69, p = .49, d = 0.10, competence scale t(41) = \u2212 1.31, p = .19, d = 0.18 (see Table 3 for means). These results supported Hypothesis 2a. In comparison, in the somewhat diagnostic information condition, there was anti-Black bias in IQ estimates, t(48) = 2.77, p = .006, d = 0.41, but not on the competence scale, t(48) = 1.12, p = .26, d = 0.16 (see Table 3 for means).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Individuating information effects in explicit person perception", "text": "The next set of competing predictions related to individuating in- formation effects. Hypothesis 3a predicted individuating information effects: Explicit evaluations should follow a pattern of increasingly more favorable evaluations with increasingly diagnostic, positive in- dividuating information, regardless of target race. Hypothesis 3b pre- dicted trivial or no individuating information effects: Evaluations of all White targets should be equally favorable, whereas evaluations of all Black targets should be equally unfavorable. Hypothesis 3b was dis- confirmed by the significant individuating information X race of target interactions (discussed above). It was hypothetically possible that Hypothesis 3a would be sup- ported for one racial group but not the other. For example, perceivers might have relied on individuating information for Black targets but not for White targets. Thus, one-way ANOVAs performed separately for Black and White targets provided appropriate tests of Hypothesis 3a. These ANOVAs assessed the effects of variation in the diagnosticity of available individuating information on evaluations of competence. The evaluations were more favorable (see Table 3 for cell means, SDs, and 95% CIs). A priori linear contrast tests were performed to investigate the lin- earity of these individuating information effects. The no information cell was coded as 1, the somewhat diagnostic information cell as 0, and the highly diagnostic information cell as \u2212 1. The contrasts were sig- nificant for estimated IQ of Black targets,  Table 5 report the correlation of the contrast coefficients with the cell means (rs > 0.83), the proportion of systematic (between condition) variance explained by the contrast (> 69%) and the significance tests for the variance left unexplained by the contrast (ps < 0.001). Overall, individuating information effects were mostly-but not entirely-linear. A series of Tukey's HSD tests assessed differences among experi- mental conditions. Black targets in the highly diagnostic information were evaluated as significantly more intelligent and competent than Black targets in the no information condition on IQ estimates, t(94) = 13.68, p < .001, d = 2.73, and on competence evaluations, t(95) = 15.84, p < .001, d = 3.40 (see Table 3 for cell means). The same was true for White targets, IQ estimate t(95) = 10.40, p < .001, d = 2.07, competence scale t(95) = 12.04, p < .001, d = 2.51. Tu- key's HSD tests also showed that Black targets in the highly diagnostic information condition were evaluated as more intelligent and compe- tent than Black targets in the somewhat diagnostic information condi- tion, IQ estimates, t(91) = 11.45, p < .001, d = 1.99, competence Note. Means within individuating information conditions that do not share a superscript differ at p < .01. Race of applicant was within-subjects; individuating information was between- subjects. Higher scores indicate greater perceived IQ and competence. a n = 52. b n = 49. c n = 44.  Table S6 for full results). Overall, the results confirmed the hypothesis of substantial in- dividuating information effects (Hypothesis 3a), but this was primarily due to the difference in judgments regarding targets in the highly di- agnostic information condition compared to the other targets. Weak support for Hypothesis 3b was found in the nonsignificant differences between evaluations of targets in the no information versus somewhat diagnostic information condition. We characterize this as \"weak\" sup- port because, given that the individuating information was only somewhat diagnostic, this difference does not provide a strong test of the effects of individuating information.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Summary", "text": "There were three main findings in Study 1: (a) decreased implicit and explicit stereotype bias with increasingly diagnostic individuating information; (b) no substantial implicit or explicit stereotype bias in the presence of highly diagnostic individuating information; and (c) in- creasingly favorable explicit target evaluations with increasingly diag- nostic, positive individuating information. Overall, these findings were consistent with hypotheses derived from propositional models of im- plicit evaluations (e.g., DeHouwer, 2014aDeHouwer, , 2014b), the APE model ( Gawronski & Bodenhausen, 2006, and with the perspective that individuating information takes primacy in person perception (e.g., Jussim, 2012). They were inconsistent with hypotheses drawn from the slow-learning perspective on implicit social cognition (e.g., Gregg et al., 2006;Smith & DeCoster, 2000), and with the view that stereotypes cause perceivers to ignore individual differ- ences (e.g., Aronson et al., 2015).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Overview", "text": "One limitation of Study 1 was that the valence of individuating information did not vary; the diagnostic individuating information was only positive in nature. Thus, it did not test the effects of a diverse range of individuating information, and it was possible that positive and ne- gative individuating information would affect person perception dif- ferently (e.g., Cone & Ferguson, 2015). Study 2 addressed this limita- tion by having some participants view high school records of one Black and one White college applicant who had excellent high school records, and others view records of one Black and one White college applicant who had weak high school records. Participants then completed a questionnaire and an IAT in which they explicitly and implicitly eval- uated the applicants' competence. Thus, Study 2 addressed competing hypothesis sets 2 (the presence or absence of stereotype bias given highly diagnostic individuating information) and 3 (the presence of absence of individuating information effects; see Table 1 for a summary of all hypotheses) under different conditions than Study 1.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Experimental design", "text": "The implicit experimental design was a one-way between-subjects design (individuating information: excellent high school record vs. weak high school record). The explicit experimental design was a 2 (individuating information: excellent high school record vs. weak high school record) \u00d7 2 (race of applicant: Black vs. White) mixed-model design. Individuating information was the between-subjects factor.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Participants", "text": "The sample size for Study 2 was based on a goal of having at least 45 participants per cell in the design. However, we collected extra data to ensure that we obtained this goal given planned data exclusions and the potential for incomplete or missing data. Participants were 195 General Psychology students. Participation partially fulfilled a course require- ment. As in Study 1, data from the 17 Black and 13 mixed-race parti- cipants were excluded from the analyses. Also excluded were data from 13 participants whose experimental condition was not randomized properly, 12 participants who did not answer manipulation check questions correctly, and 11 participants who did not follow instruc- tions. Of the 130 participants included in the analyses (see Supplement 2.1 for power analysis), 68 were female. There were 78 White, 36 Asian, and 11 Latino participants, and 5 identified with another racial group.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Stimuli", "text": "The college applications in Study 2 were identical to those used in the highly diagnostic individuating information condition of Study 1 with the following exceptions. First, the White name was changed to Ryan Eric Reed and the Black name to Jamal DeShawn Jackson (these names were determined by the pilot test to be prototypically White and Black, respectively; see Supplemental Materials Tables S1 and S2.) Also, SAT scores were on a 1600-point scale instead of the newer 2400-point scale because Study 2 was conducted before the SATs changed to the 2400-point scoring system. The scores in the excellent high school re- cord condition were 1400 and 1420 with GPAs of 3.9 and 3.8, ", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "R.S. Rubinstein et al. Journal of Experimental Social Psychology 75 (2018) 54-70", "text": "respectively, and in the weak high school record condition, the scores were 1000 and 1020 with GPAs of 2.1 and 2.0, respectively. The aca- demic information in each condition was pretested to be equally ex- cellent or equally weak (Supplementary Materials Tables S3 and S4).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Measures", "text": "The explicit measures used in Study 2 (Supplement 2.3) were si- milar to those from Study 1 with the following exceptions. Several questions relevant to academic capabilities were added [e.g., \"What do you predict Jamal's GPA will be at the end of freshman year (on a 4.0 scale)?\"] because, unlike Study 1, all targets in Study 2 were students. In addition, the response scales for several questions were changed from 5-point to 7-point scales. The IAT was the same as that from Study 1 except that some categories and stimulus words were revised based on changes to the applicants' names (Supplement 2.6).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Procedure", "text": "The procedure for Study 2 was identical to that in the highly di- agnostic information and somewhat diagnostic information conditions in Study 1 aside from the differences in stimulus materials.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Preliminary analysis", "text": "The two items that related to predictions of the applicants' college GPAs showed strong internal consistency, \u03b1 Jamal = 0.93, \u03b1 Ryan = 0.93 and were combined to form a GPA scale, computed separately for each target. Potential scores on this scale could range from 0 to 8, with a higher score representing higher GPA predictions. The eight trait rating items that measured evaluations of the applicants' competence de- monstrated excellent internal consistency, \u03b1 Jamal = 0.96, \u03b1 Ryan = 0.97, and were combined to form a competence scale, computed separately for each target. Potential values for the competence scale ranged from 8 to 56, with a higher score indicating higher evaluations of the target's competence. The explicit dependent measures were estimated IQ, the GPA scale, the competence scale, estimated hours spent studying per week, likelihood of the target needing remedial help, and academic comparison with students from university to which the applicants were applying (see Supplementary Materials Table S7 for descriptive statis- tics). The implicit dependent measure was D, the IAT effect ( Greenwald et al., 2003; see Supplementary Materials Table S8 for correlations among explicit measures and D scores).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Stereotype bias in implicit person perception", "text": "The first set of analyses assessed support for two competing hy- potheses regarding IAT scores: Hypothesis 2a, which posited that highly diagnostic individuating information would eliminate racial stereotype bias, versus Hypothesis 2b, which predicted that racial stereotype bias would persist even in the presence of highly diagnostic individuating information. Overall, across both experimental conditions, perceivers showed no substantial implicit stereotype bias, D = 0.08 (< 0.15), SD = 0.26, 95% CI difference (0.03, 0.12), t(128) = 3.36, p = .001. In the excellent high school record condition, perceivers showed no significant implicit stereotype bias D = 0.06 (< 0.15), SD = 0.27, 95% CI difference (\u22120.01, 0.12), t(65) = 1.66, p = .10. In the weak high school record condition perceivers showed no substantial stereotype bias, D = 0.10 (< 0.15), SD = 0.25, 95% CI difference (0.04, 0.16), t(63) = 3.14, p = .003. The difference in IAT effects in the presence of positive versus negative individuating information were nonsignificant, t(128) = \u22120.95, p = .35, d = 0.15. These results supported Hypothesis 2a, and did not support Hypothesis 2b; implicit racial stereotype bias in person per- ception was effectively eliminated by highly diagnostic individuating information regardless of the valence of the information.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Explicit person perception analyses", "text": "Competing hypotheses about explicit evaluations were assessed next. To do so, explicit measures were subjected to 2 (individuating information: excellent high school record vs. weak high school record) \u00d7 2 (race of applicant: Black vs. White) mixed-model ANOVAs. Individuating information was the between-subjects factor. 11.3.3.1. Stereotype bias in explicit person perception. Hypothesis 2a predicted no stereotype bias in the presence of highly diagnostic individuating information. Hypothesis 2b predicted substantial stereotype bias in the presence of highly diagnostic individuating information. The ANOVAs showed significant race of applicant main effects on five out of the six dependent measures, Fs(1, 128) > 7.13, ps < 0.01, \u03b7s = 0.03-0.06 (see Table 6 for explicit main effect cell means, standard deviations, and 95% CIs; see Table 7 for Study 2 ANOVA results and effect sizes for explicit dependent variables). However, the means revealed that participants evaluated Jamal slightly more favorably than Ryan. This pattern of evaluations indicated that the effect of individuating information was so strong that it reversed stereotype bias (i.e., contrast effects; Jussim, Coleman, & Lerch, 1987). Because the race of target main effects were contrast Note. a n = 130; b n = 130; c n = 66; d n = 64. HS Record = high school record. Means within individuating information or race of target conditions that do not share superscripts e through ab differ statistically (p < .01). Race of target was within-subjects; individuating information was between-subjects. Higher scores indicate greater perceived competence and academic capability. \u2020 Variable reverse coded; reverse coded values reported.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "R.S. Rubinstein et al. Journal of Experimental Social Psychology 75 (2018) 54-70", "text": "effects, they supported Hypothesis 2a, which predicted no stereotype bias in the presence of highly diagnostic individuating information. Nonsignificant high school record X target race interactions on five of six dependent variables, Fs(1, 128) < 2.34, ps > 0.12, \u03b7s < 0.02 ( Table 7), indicated that this effect was not qualified by the valence of the individuating information.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "11.3.3.2.", "text": "Individuating information effects in explicit person perception. Next, we tested competing hypotheses related to individuating information effects. Hypothesis 3a predicted substantial individuating information effects: Excellent applicants should be evaluated as more competent than weak applicants. Hypothesis 3b predicted that individuating information effects would be trivial or nonexistent-that evaluations of excellent and weak applicants of the same race would be equal. There were large main effects for individuating information on all explicit dependent measures, Fs(1, 128) > 26.24, ps < 0.001, \u03b7s = 0.39-0.90 (Table 7), and the means showed that Black and White applicants with excellent high school records were evaluated as more competent than Black and White applicants with weak high school records (Table 6). The lack of significant interactions showed that the pattern of individuating information effects was the same for Black and White targets. Thus, the data were consistent with Hypothesis 3a, which predicted substantial individuating information effects. Moreover, within each target race, explicit evaluations of the excellent applicant were more favorable than the weak applicant, thereby disconfirming Hypothesis 3b.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Summary", "text": "This study provided additional evidence in support of the primacy of individuating information over stereotypes in implicit and explicit person perception: Highly diagnostic individuating information elimi- nated stereotype bias in both modes of person perception. In addition, there were large individuating information effects in explicit person perception. Overall, as in Study 1, the data were consistent with hy- potheses derived from propositional models of implicit evaluations (e.g., DeHouwer, 2014aDeHouwer, , 2014b), the APE model ( Gawronski & Bodenhausen, 2006) and with the perspective that individuating information takes primacy in person perception (e.g., Jussim, 2012). They were inconsistent with hypotheses drawn from the slow-learning perspective on implicit social cognition (e.g., Smith & DeCoster, 2000), and with the claim that stereotypes cause perceivers to ignore individual differences (e.g., Aronson et al., 2015).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Overview", "text": "One limitation of Studies 1 and 2 is that they did not provide direct tests of the effects of individuating information on implicit person perception. Study 3 provided such a test. To do so, as in the high and low information conditions in Study 1 and as in Study 2, participants viewed the two college applications and completed a questionnaire and an IAT. However, in Study 3 participants evaluated either two Black or two White individuals, one of whom had an excellent high school re- cord and one of whom had a weak record. Study 3 tested competing hypothesis sets 2 (the presence or absence of stereotype bias in the presence of highly diagnostic individuating information) and 3 (the presence or absence of individuating information effects; see Table 1 for a summary of all hypotheses).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Experimental design", "text": "The implicit experimental design was a one-way (individuating in- formation: excellent high school record vs. weak high school record) between-subjects design. The explicit experimental design was a 2 (race of applicants: Black vs. White) \u00d7 2 (individuating information: ex- cellent high school record vs. weak high school record) mixed-model design. Unlike Studies 1 and 2, race of applicants was the between- subjects factor and individuating information was the within-subjects factor.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Participants", "text": "The sample size for Study 3 was based on a target of having at least 45 participants per cell in the experimental design. Again, however, we collected data on more participants than needed to obtain this goal after data loss. Participants were 278 General Psychology students. Data from all 29 Black and mixed-race participants were excluded from analysis. Also excluded were data from 33 participants who did not answer manipulation check questions correctly. Of the 216 participants included in the analyses (see Supplement 2.1 for power analysis), 143 were female. The final sample comprised 141 White, 57 Asian, and 12 Latino students, and 6 students who identified with another racial group. The mean age was 18.70 years.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Stimuli", "text": "All participants viewed college applications from two Black or two White applicants (Jamal DeShawn Robinson and Tyrone Darnell Jackson, or Ryan Bradley Winkler and Bruce Eric Reed, respectively). Because more names were needed for this study than for the prior studies, we pretested lists of additional first and last names for racial prototypicality (Supplementary Materials Tables S1 and S2). The Race of applicant x individuating information interaction (df), F, \u03b7 applications were adapted from Study 2. The only changes were that the SAT scores were revised to reflect the new scoring system (1500 or 1520 in the weak information condition and 2100 or 2120 in the ex- cellent information condition) and in the excellent high school record condition, the extracurricular clubs were different (see Supplement 3).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Measures", "text": "The explicit measures (Supplement 2.4) included all questions from Study 1 but also used some items from the Study 2 questionnaire. In the IAT in this study, applicant category names and stimulus words were changed to match the additional names on the applications (Supplement 2.6). In addition, since participants viewed information about applicants of only one racial background, the applicant names in the IAT were either only Black or only White for each participant. Thus, unlike the prior studies, this IAT did not measure racial stereotype bias. Instead, it measured implicit individuating information effects-differences in implicit evaluations of excellent and weak applicants. Positive D scores indicated that the excellent applicant was evaluated more favorably, whereas negative D scores showed that the weak applicant was eval- uated more favorably.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Procedure", "text": "The procedure in Study 3 was identical to that of Study 2 except for the differences in stimulus materials described above.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Preliminary analyses", "text": "The two items that measured participants' predictions of the appli- cants' college GPAs [e.g., \"What do you predict Jamal's GPA will be at the end of freshman year (on a 4.0 scale)?\"] showed adequate internal consistency, \u03b1 excellent = 0.81, \u03b1 weak = 0.61, and were combined to form a GPA scale, which was computed separately for each target. The range of potential scores on this scale was 0 to 8. The eight trait rating items assessing participants' evaluations of the applicants' competence (e.g., \"How intelligent is Jamal?\") on a 1 (e.g., Unintelligent) to 7 (e.g., Very intelligent) scale showed good internal consistency, \u03b1 excellent = 0.83, \u03b1 weak = 0.84, and were combined to form a competence scale. This scale was computed separately for each target. The range of potential scores on this scale was from 8 to 56. The dependent measures in this experiment were estimated IQ, the GPA scale, the competence scale, and D (see Supplementary Materials Tables S9 and S10 for descriptives for Study 3 explicit dependent measures and inter-variable correlations for all dependent measures; see Table 8 for cell means, standard de- viations, and 95% CIs).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Implicit individuating information effect", "text": "We first tested individuating information effect Hypotheses 3a and 3b at the implicit level. Hypothesis 3a predicted substantial in- dividuating information effects. Hypothesis 3b predicted trivial or no substantial individuating information effects. A single-sample t-test in- dicated that excellent applicants were implicitly evaluated more fa- The finding that excellent applicants were implicitly eval- uated more favorably than weak applicants regardless of target race supported the prediction of substantial individuating information ef- fects (Hypothesis 3a).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Explicit person perception analyses", "text": "A 2 (race of target: Black vs. White) \u00d7 2 (individuating information: excellent high school record vs. weak high school record) mixed-model ANOVA was performed on each explicit dependent measure. Race of target was the between-subjects factor. ", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Explicit individuating information effects.", "text": "The next set of analyses tested Hypotheses 3a and 3b at the explicit level. Hypothesis 3a predicted substantial individuating information effects-that excellent college applicants would be perceived more favorably than weak applicants. Hypothesis 3b predicted trivial or nonexistent individuating information effects-that there would be no differences between evaluations of excellent and weak applicants within each target race. Large main effects for individuating information on all explicit de- pendent measures, IQ estimate F(1, 200) = 519.10, p < .001, Note. a n = 216; b n = 216. c n = 107; d n = 109. Excellent HS record = Excellent High School Record; Weak HS record = Weak High School Record. Race of target was between-subjects; individuating information was within-subjects. Cells that do not share superscripts e through m differ at p < .001. Higher scores indicate greater perceived competence and academic capability.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "R.S. Rubinstein et al.", "text": "Journal of Experimental Social Psychology 75 (2018) 54-70 \u03b7 = 0.76, GPA scale F(1, 213) = 1122.53, p < .001, \u03b7 = 0.87, com- petence scale F(1, 213) = 1180.47, p < .001, \u03b7 = 0.88, indicated that excellent applicants were evaluated far more favorably than weak ap- plicants. The nonsignificant interactions reported above revealed that this was true regardless of target race. These results supported Hypothesis 3a, which predicted substantial individuating information effects.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Summary", "text": "Study 3 was important because it provided an implicit individuating information effect size (D = 0.30), which Studies 1 and 2 could not do. In addition, it demonstrated that highly diagnostic individuating in- formation eliminated stereotype bias in explicit person perception. The results were especially noteworthy in terms of the relative power of individuating information over stereotypes in explicit person percep- tion because in this study, race of target was a between-subjects factor. The fact that target race was between-subjects reduced the potential for social desirability bias to influence the results, providing a purer test of racial stereotype bias in explicit person perception. Overall, as in Studies 1 and 2, the data were congruent with pre- dictions derived from propositional models of implicit evaluations (e.g., DeHouwer, 2014aDeHouwer, , 2014b), the APE model ( Gawronski & Bodenhausen, 2006) and with the view that individuating information takes primacy in person perception (e.g., Jussim, 2012). They were incon- gruent with hypotheses that draw from the slow-learning perspective on implicit social cognition (e.g., Smith & DeCoster, 2000), and with the claim that stereotypes cause perceivers to ignore individual differences (e.g., Aronson et al., 2015).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Summary of all findings", "text": "This research tested a series of hypotheses that addressed the question of, what are the relative effects of stereotypes and individuating information in implicit and explicit person perception? We found that the amount of stereotype bias present depended heavily upon the diag- nosticity of the individuating information. For both implicit and explicit evaluations, there was a decline in stereotype bias as the diagnosticity of individuating information increased. This was demonstrated directly by the confirmation of Hypothesis 1a, which predicted decreases in implicit and explicit stereotype reliance with individuating information that was more diagnostic (Study 1). The pattern can also be seen by considering the results across all three studies as an aggregate: (a) in the absence of individuating information there was significant implicit and explicit anti-Black stereotype bias (Study 1); (b) when given somewhat diagnostic individuating information, perceivers demonstrated a modest amount of implicit and explicit stereotype bias (Study 1); and (c) in the presence of highly diagnostic individuating information, there was no substantial explicit (Studies 1, 2, & 3) or implicit (Studies 1 & 2) stereotype bias in person perception. In addition, we found that individuating information consistently and strongly influenced explicit person perception (Studies 1, 2, & 3). Although its effect on implicit person perception was smaller (D = 0.30; Study 3), it was among the largest implicit effects found in this program of research. Thus, there was little or no evidence that perceivers ignore clear individual differences (see Table 9 for summary of support or lack of support for all Hypotheses).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Did patterns of implicit versus explicit evaluations converge or diverge?", "text": "Taken together, these results address our second research ques- tion-that of the convergence or divergence of reliance on in- dividuating information and stereotypes in implicit versus explicit evaluations. We found that, consistent with predictions of the APE model (e.g., Gawronski & Bodenhausen, 2006) patterns of re- liance on stereotypes and individuating information in implicit and explicit person perception generally converged; all of the findings de- scribed above characterized the data from both the IAT and the explicit measures. Thus, results indicated that individuating information took primacy in both modes of person perception, though to a larger extent in explicit than in implicit person perception. This was inconsistent with slow-learning perspectives, some of which explicitly predict the divergence of implicit and explicit evaluations ( Gregg et al., 2006;). 13.3. Implicit social cognition 13.3.1. Comparison with empirical research on updating implicit attitudinal evaluations in the presence of highly diagnostic information Considered in the context of the empirical literature on revision of implicit impressions, our findings fall squarely into the category of evidence suggesting that implicit impressions can be updated under some conditions (e.g., Cone & Ferguson, 2015;Mann & Ferguson, 2015, 2017). The condition we found is that the in- dividuating information must be highly diagnostic. This is consistent with Cone and Ferguson's (2015) findings, which demonstrated that one instance of extreme, diagnostic individuating information led to revised implicit impressions of individuals. However, unlike Cone & Ferguson (Study 2), we did not find differences between the effects of positive versus negative information. Perhaps the valence asymmetry in changes in evaluations based on moral or immoral behavior (as their participants' evaluations were) does not apply in the same way to be- liefs about competence because immoral behavior may be perceived as rarer (and thus more diagnostic) than incompetence.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Comparison with prior empirical research addressing reliance on individuating information and stereotypes in implicit person perception", "text": "Our results partially converge with those of Cao and Banaji (2016), who also examined reliance on stereotypes and individuating in- formation in implicit and explicit person perception. There were two main similarities in the two sets of findings: individuating information (a) influenced both modes of person perception and (b) at least reduced stereotype bias. These are key points of similarity because they indicate that the studies converge on concluding that individuating information plays an important role in implicit person perception. However, there were also some differences. One was that that their manipulation yielded somewhat weaker implicit individuating in- formation effects than ours (D = 0.16, compared with our D = 0.30), a difference that is probably too small to warrant extended theoretical analysis. In addition, they found that rather than eliminating implicit stereotype bias (as we found in the present studies), individuating in- formation reduced implicit stereotype bias (e.g., from D = 0.43 to D = 0.20 in their Study 1). We next consider several possible ex- planations for this divergence. First, perhaps one or the other pattern will subsequently prove difficult to replicate. That is, it is possible that elimination of bias is the norm or conversely that reduction is the norm. However, since both programs of research consisted of multiple studies, it is more likely that the differences reflect something systematic brought about by metho- dological differences. One such difference involved the type of individuating information that was manipulated. Our research manipulated information about targets' academic achievements (suggesting high or low competence), whereas Cao & Banaji's individuating information manipulation in- volved targets' gender-stereotyped occupations. The importance of this distinction is twofold. First, while personal achievements are clearly individuating, occupation is often classified as category information in the impression formation literature (Brewer, 1988;Fiske & Neuberg, 1990;Kunda & Thagard, 1996). Thus, Cao and Banaji (2016) arguably ", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "R.S. Rubinstein et al.", "text": "Journal of Experimental Social Psychology 75 (2018) 54-70 presented participants with two social categories rather than category and individuating information. When two social categories that have conflicting associations with the judgment at hand are presented (as Cao & Banaji did when they provided counterstereotypic occupation information; for instance, \"female\" has a negative association with \"doctor,\" while \"doctor\" has a positive association with \"doctor\"), both sources of information are expected to influence evaluations, though to a reduced extent (Kunda & Thagard, 1996). On the other hand, if a social category and diagnostic individuating information are presented, the individuating information generally takes primacy (e.g., Jussim, 2012;Kunda & Thagard, 1996). The nature of the differences between Cao and Banaji's (2016) findings and ours is consistent with this ex- planation. Second, occupations are a social role. According to the social role theory of stereotype content (e.g., Eagly, 1987), social roles function as foundational building blocks for group stereotypes; when targets enact behaviors associated with roles that are typical for their social group, this influences perceivers' inferences regarding which traits are typical for members of that group. Indeed, Koenig & Eagly (2014, Study 4) empirically demonstrated a causal effect of occupational roles in group stereotypes. Therefore, Cao and Banaji's (2016) occupational informa- tion may not have eliminated stereotype bias because of the particularly strong-indeed causal-part that occupational and other social roles play in stereotype content. Another methodological difference between the two programs of research was the nature of the IAT stimuli. In our studies, the IAT sti- muli were names that previously had been presented as specifically belonging to the target. In contrast, Cao and Banaji (2016) used nick- names for their targets that were never presented as belonging to the actual target. Thus, our IAT stimuli had greater specificity to the actual targets. This also may help to account for the greater persistence of stereotype biases in their studies.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Explicit social cognition", "text": "The present research provides further evidence that diagnostic in- dividuating information, when available, is the primary basis for ex- plicit person perception (e.g., Jussim, 1990;Jussim, Eccles, & Madon, 1996;Jussim, Cain, Crawford, Harber, & Cohen, 2009;Jussim, 2012;Krueger & Rothbart, 1988;Kunda & Thagard, 1996;Monroe et al., 2017). The average individuating information and stereotype effects found in the present research (r = 0.73 and r = 0.05, respectively) were similar to those found in past meta-analyses (r = 0.71 and r = 0.25, respectively) and reviews of the literature (r = 0.70 and r = 0.10, respectively; Jussim, 2012).", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Implications for the elimination of implicit stereotype bias", "text": "The most novel contribution of this research is its finding that highly diagnostic individuating information effectively eliminated stereotype bias at the implicit level in addition to the explicit level (cf. Cao & Banaji, 2016). When considered in concert with Cao & Banaji's findings, the present research represents a preliminary step toward system- atically identifying the conditions under which individuating informa- tion eliminates implicit stereotype bias versus simply reducing it. From consideration of similarities and differences between our research and Cao & Banaji's, we conclude that stereotype biases are likely to be eliminated when the individuating information meets three criteria. It should (a) be unambiguously individuating information rather than category information, (b) not involve social roles that are strongly linked to the stereotypes, and (c) be highly diagnostic. Our finding that implicit and explicit stereotype bias were effec- tively eliminated by highly diagnostic individuating information is especially compelling because pilot data indicated that the stereotype manipulation was far stronger than the individuating information manipulation. Because past literature indicates that greater racial pro- totypicality results in increased explicit stereotype bias (e.g., Blair, Judd, Sadler, & Jenkins, 2002), one might expect an even greater in- fluence of individuating information than we found when the strength of the two manipulations is equal.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Theoretical implications", "text": "Although the present research was not designed specifically to test the processes underlying implicit social cognition (for instance, by comparing the effects of associative versus propositional information; see Kurdi & Banaji, 2017), it tested predictions derived from several recent models of implicit and explicit social cognition. For instance, this research is capable of testing predictions derived from propositional models of implicit evaluation (e.g., DeHouwer, 2014a, 2014b) because previous literature posits that the critical distinction between associa- tive and propositional information is whether there is an indication that the information is valid (Gawronski & Bodenhausen, 2006). Providing information from college applications arguably suggests that the information is valid and thus according to this perspective con- stitutes propositional information. Our finding that this individuating information took primacy in implicit person perception was consistent with the central tenet of propositional models of implicit evaluations that propositional processes typically underlie implicit evaluations. In addition, the sensitivity of stereotype-relevant evaluations to propositional individuating information suggests that, in contrast with claims of some models that specifically address implicit stereotypes, implicit stereotypes are not exclusively (e.g., Amodio, 2014;Amodio & Ratner, 2011) or even mostly (e.g., Smith & DeCoster, 2000) associative in nature. If they were, then they probably would not have been able to be overcome by propositional information; associative information likely would be the only means by which they could have been over- powered. Finally, our research provides evidence inconsistent with slow- learning perspectives (e.g., Smith & DeCoster, 2000) on implicit social cognition. First, implicit evaluations readily changed in response to individuating information. Second, patterns of reliance on individuating information and stereotypes in implicit and explicit evaluations overwhelmingly converged. The latter finding is consistent either with a single propositional process that underlies implicit (e.g., DeHouwer, 2014a, 2014b) and explicit eva- luations or with the interaction of associative and propositional pro- cesses (e.g., Gawronski & Bodenhausen, 2006) instead of the separate implicit versus explicit processes described in slow-learning perspectives.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Implications for the evolution in understanding of implicit biases", "text": "The present findings might help explain why most studies and re- views show that the extent to which group-based implicit bias measures predict discrimination are often quite modest (Greenwald, Poehlman, Uhlmann, & Banaji, 2009;Oswald, Mitchell, Blanton, Jaccard, & Tetlock, 2013), and that interventions that reduce implicit bias have little or no effect on explicit preferences (Forscher et al., 2017). People often have some and, sometimes, a great deal of individuating in- formation about others with whom they interact. Indeed, physical ap- pearance and even brief exposures to nonverbal cues can provide considerable valid individuating information about wealth, religiosity, personality, health, and more (e.g., Ambady & Rosenthal, 1992;Jussim et al., 1987;Naumann, Vazire, Rentfrow, & Gosling, 2009). If sub- stantial amounts of individuating information are routinely available, and if people judge others primarily on the basis of such information, limited effects of implicit stereotypes and attitudes on person percep- tion and discrimination may be the norm rather than the exception.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Limitations and future directions", "text": "We have presented evidence from three studies indicating that R.S. Rubinstein et al. Journal of Experimental Social Psychology 75 (2018) 54-70 highly diagnostic individuating information takes primacy over ste- reotypes in implicit and explicit person perception. However, we only used one implicit measure: the IAT. One limitation of the IAT is that, although it is currently one of the most popular implicit measures, there is some controversy regarding the interpretation of its effects. Some have suggested shifting the zero point to be larger than zero due to the \"right-bias\" of IAT scores (e.g., Blanton et al., 2015;Blanton & Jaccard, 2006), while others have retained the traditional \"true zero\" inter- pretation (i.e., relying on statistical significance; e.g., Stout et al., 2011). The approach to interpreting IAT scores that we used (| D| \u2265 0.15 indicates substantial implicit preferences; Nosek et al., 2007) constitutes a middle ground between these two competing per- spectives; it reduced the likelihood that bias was overestimated while simultaneously not straying far from the true zero interpretation. Nonetheless, future research should address the present research questions using an alternative measure. Moreover, although our research is relevant to the debate regarding the processes underlying explicit versus implicit social cognition, it was not designed specifically as a test of these processes. To provide a purer test of underlying processes, the research would need to directly ad- dress factors such as the distinction between propositional and asso- ciative information. In addition, we examined our research questions only in the domain of academic individuating information and academic target evalua- tions. Perhaps different patterns would emerge with different types of stereotypes. For example, backlash effects emerge in the context of prescriptive rather than descriptive gender stereotypes (Rudman & Glick, 2001). Future research should investigate whether these findings generalize to other types of individuating information and stereotypes. Finally, this research draws conclusions exclusively based on the evaluations of college students that were made in a laboratory at a liberal northeastern university. It is possible that stereotype biases are more pervasive in other populations or in evaluative situations that are more consequential. However, consistent with the findings of the pre- sent experimental studies, an audit field study of Airbnb found that, much like our Study 1\u2032s racial group condition, hosts were less likely to accept guests with names that sounded Black (Cui, Li, & Zhang, 2016). Yet, consistent with all three of our studies, that bias was eliminated when positive or negative individuating information regarding the quality of the guest was provided.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Conclusion", "text": "Our findings are consistent with a slow-moving shift in the under- standing of how implicit beliefs function. Approximately ten years ago, it was written: \u2026when we see a Black (or a White) person, the attitude and ste- reotypes associated with that racial category automatically activate. Further, these attitudes and stereotypes influence our judgments, as well as inhibit countertypical associations (Kang & Banaji, 2006, p. 1084. The present studies have not \"falsified\" these claims. However, our three studies treated the declarative conclusion presented in the second sentence (\"stereotypes influence our judgments and inhibit counter- typical associations\") as a hypothesis. Indeed, our studies identified conditions under which such claims are confirmed: when people lacked individuating information or when it was only somewhat diagnostic. Although this finding does not show any inhibition in counter- stereotypic associations, they were conditions under which stereotypes influenced judgments. And our studies also identified conditions under which such claims were not confirmed-when people had highly diagnostic individuating information. As such, our results are consistent with a modern and slow-moving shift away from what were once the dominant perspec- tives in social psychology focusing exclusively on errors and biases (e.g., Fiske & Taylor, 1991;Nisbett & Ross, 1980), and a return toward views of social perception and implicit cognition that incorporate ac- curacy, limitations to the power of stereotypes to bias judgments and sensitivity of implicit beliefs to valid information in the environment (e.g., Cone & Ferguson, 2015;Jussim, 2012;Kunda & Thagard, 1996;Monroe et al., 2017;Reber, 1989). Although stereotypes (whether im- plicit or explicit) surely sometimes bias judgment, just as surely, beliefs about groups and individuals from those groups sometimes reflect perceivers making good use of the most relevant information available to them.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Open practices", "text": "Open data. The datasets for the three studies are available at the following URL: https://osf.io/hymkc, DOI: 10.17605/OSF.IO/HYMKC. Using the available information, an independent researcher can re- produce the reported results. Open Materials. The college applications and all measures are available at the following URL: https://osf.io/hymkc, DOI: 10.17605/ OSF.IO/HYMKC. Using the available information, an independent re- searcher can reproduce the reported methodology.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Acknowledgements", "text": "We would like to thank Dr. Calvin Lai for his insightful feedback on a previous draft of this manuscript. We also would like to thank Dr. Thomas Cain and Karin Negele for their work on a pilot study.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Appendix A. Supplementary data", "text": "Supplementary data to this article can be found online at https:// doi.org/10.1016/j.jesp.2017.11.009.", "title": "Reliance on individuating information and stereotypes in implicit and explicit person perception", "file_name": "Rubinstein et al. - 2018 - Reliance on individuating information and stereoty.pdf"}
{"section": "Abstract", "text": "Background. Previous research has shown that adults with intellectual disability (ID) may be more at risk of developing dementia in old age than expected. However, the effect of age and ID severity on dementia prevalence rates has never been reported. We investigated the predictions that older adults with ID should have high prevalence rates of dementia that differ between ID severity groups and that the age-associated risk should be shifted to a younger age relative to the general population. Method. A two-staged epidemiological survey of 281 adults with ID without Down syndrome (DS) aged o60 years ; participants who screened positive with a memory task, informant-reported change in function or with the Dementia Questionnaire for Persons with Mental Retardation (DMR) underwent a detailed assessment. Diagnoses were made by psychiatrists according to international criteria. Prevalence rates were compared with UK prevalence and European consensus rates using standardized morbidity ratios (SMRs). Results. Dementia was more common in this population (prevalence of 18.3 %, SMR 2.77 in those aged o65 years). Prevalence rates did not differ between mild, moderate and severe ID groups. Age was a strong risk factor and was not influenced by sex or ID severity. As predicted, SMRs were higher for younger age groups compared to older age groups, indicating a relative shift in age-associated risk. Conclusions. Criteria-defined dementia is 2-3 times more common in the ID population, with a shift in risk to younger age groups compared to the general population.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Introduction", "text": "It has often been assumed that dementia occurs more commonly in the intellectual disability (ID) population than in the general population (Torr, 2005). Although it is now accepted that those with Down syndrome (DS) have a genetic predisposition for dementia re- lated to the APP gene on chromosome 21, dementia may also be more common in the ID population who do not have DS (Cooper, 1997). Furthermore, it has been proposed that dementia in the ID population should occur at a younger age than is usual. Tredgold, a London physician during the first half of the pre- vious century, asserted that 'as would be expected, in most cases of primary amentia, [the] senile form of dementia sets in at an earlier age than the normal. It often begins to show itself in the fourth decade [\u00e2\u20ac\u00a6], and the majority of aments who live much after this usually show definite and progressive mental deterioration ' (Tredgold, 1952). Thompson (1951) be- lieved the earlier age of decline to be related to arrested brain development. More recently, the cognitive reserve hypothesis has been proposed to explain how adults with similar brain insults may present with differing clinical pic- tures. It proposes that intelligence, education and occupational level can influence the occurrence and course of many central nervous system disorders ( Whalley et al. 2004). Stern (2002) proposed two com- ponents to cognitive reserve. The first comprises passive components such as brain size and synapse count or 'hardware ' of the brain, which differs between individuals. Proxies for it include measure- ments such as brain volume and pre-morbid intelli- gence ( Staff et al. 2004). Active components or 'software ' of the brain are developed through edu- cational, leisure and occupational activities that de- velop the use of different neuronal pathways (Stern, 2003). The hypothesis assumes that there is a critical threshold of reserve capacity that needs to be breached by pathological processes before clinical or functional symptoms will develop. Those with more reserve have been found to be less likely to develop dementia or cognitive decline (Whalley et al. 2000 ;Verghese et al. 2003 ;Valenzuela & Sachdev, 2006). Although these studies are consistent with the theory of cognitive re- serve, none specifically studied participants in the ID (mental retardation) range of ability. Adults with ID have, by definition, brain reserve limitations. In addition, many older adults with ID in developed countries have been excluded from edu- cation (Randall Smith, 2005) and have for long periods resided in large, environmentally impoverished in- stitutions. The cognitive reserve hypothesis predicts that older adults with ID should be particularly at risk for dementia and that the age-associated risk should be shifted to a younger group because, theoretically, those with dementia pathology will quickly reach a functional cut-off with early emergence of symptoms ; it also indicates that dementia risk should differ ac- cording to the severity of disability. Despite the long-held assumption that dementia is more common in older adults with ID without DS, there have been only a few small community surveys of dementia prevalence in this group of adults and we have not been able to find any studies that have investigated their age-associated risk or the potential effect of ID severity on dementia rates. We aimed to examine the following : (1) Prevalence of dementia in older adults with ID compared to general population prevalence. (2) The differences, if any, of dementia prevalence rates between ID severity groups. (3) We also hypothesized that the excess risk for de- mentia [standardized morbidity ratios (SMRs) based on prevalence rates] would be greatest in 'younger ' older adults with ID.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Method", "text": "We undertook a two-stage epidemiological survey of dementia in adults with ID without DS aged o60 years living in five inner-city and suburban London boroughs : Camden, Islington, Enfield, Harrow and Greenwich. Adults with DS were excluded because of their known genetic risk for Alzheimer's disease. The protocol received approval from the Thames Valley Multi-centre Research Ethics Committee and was agreed with the R&D offices of all participating National Health Service organizations.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Definition of participants", "text": "ID was defined according to ICD-10 criteria for mental retardation (WHO, 1993) as global developmental de- lay, IQ <70 and impairment of social functioning. Those in whom the ID status was uncertain at screen- ing underwent an assessment and were excluded if they did not meet these ICD-10 criteria. Each partici- pant's severity of ID was rated to be mild, moderate or more severe, according to their early life abilities (including IQ if available) and current skills. Adults with DS were identified from chromosomal analysis in their records or by their characteristic fea- tures, and were excluded from the study.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Identification of participants", "text": "All adults with ID aged o60 years, who were cur- rently resident in any of the five boroughs, were identified from : (1) Social services electronic databases (current and past recipients of social care who have been re- corded at any time to have ID). (2) Any past or present users of the local ID health- care teams. (3) All local residential and day services providers (voluntary or government sector) for adults with ID. (4) In two of the boroughs we also made contact with all geriatricians, old age psychiatrists, mental health teams for older people, and all non-ID resi- dential and nursing homes. This extension of the sampling frame did not result in significant num- bers of additional participants, and was not used in the other boroughs.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Contact with participants and consent procedures", "text": "An information sheet that used simple words, short sentences, large text and pictures was sent to the potential participants and their carers. Potential par- ticipants decided on their own participation if they were able. For those that did not have capacity to consent, we sought agreement from carers and willingness by participants to engage with procedures. We also gained consent from informants for their own participation in the survey. Informants were family members, social workers or care staff who had regular contact with the participants. They must have known the participant for at least 3 months to com- plete the Dementia Questionnaire for Persons with Mental Retardation (DMR) and at least 2 years to provide information on longitudinal change ; if necessary, further informants or historical records were sought.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Screening stage", "text": "All participants were screened for symptoms of dementia or cognitive decline with : (1) The DMR (Evenhuis, 1996), a validated informant- completed screening tool for dementia in this population. The tool has two scales : a cognitive scale, based on short-and long-term memory and orientation ; and a social scale, based on func- tional and behavioural items. Each scale has dif- ferent threshold scores for different ID severity groups. We used the cognitive scale and its pub- lished thresholds for severe, high moderate or mild ID for the three ID severity groups in our study. (2) Informants also completed a brief activities of daily living schedule (ADLs), based on the Adaptive Behaviour Scale ( Nihira et al. 1992) and Activities for Daily Living Schedule (Lawton & Brody, 1969), and any decline in ADLs over the past 2 years was determined. Information about level of functioning in early life was also collected from informants. (3) Participants with ID with sufficient communi- cation skills completed a three-item object memory task based on a modified object memory task (Shoe Box Test ;Burt & Aylward, 2000). Screen-positive criteria were inclusive so that no dementia cases would be missed. Therefore, screen positives were those who scored at or above the cog- nitive score thresholds on the DMR ; or had un- explained decline in ADLs ; or had a delayed recall of less than two out of three items in the Shoe Box Test task. Participants who screened negative on these cri- teria were deemed not to have dementia.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Assessment of screen positives", "text": "Participants who screened positive completed a full assessment to elicit symptoms of dementia and to identify disorders pertinent to a differential diagnosis. The assessment included : (1) Cognitive functioning and symptoms of dementia : (a) A neuropsychological assessment, consisting of the Test for Severe Impairment (Albert & Cohen, 1992), additional memory items from the Severe Impairment Battery (Saxton & Swihart, 1989), the Tower of London (Shallice, 1982), Supermarket Fluency task (Troyer, 2000), British Picture Vocabulary Scale ( Dunn et al. 1997) and Luria three-stage command. (b) Informants completed an additional question- naire based on a modification of the CAMDEX informant questionnaire ( Ball et al. 2004) to elicit a history of changes in memory, personality, general cognitive function and confusion. (2) Physical health (a) A structured physical examination identified neurological signs associated with dementia and also signs of any other relevant physical condition, such as thyroid disorders, neuro- logical conditions and cardiovascular dis- orders. This was based on the procedures for such assessments used previously ( Hassiotis et al. 2003), and a vision and hearing screen. (b) Informants provided information about current physical health and medications. In addition, we reviewed available medical records to re- cord information on previous health status and recent investigations. (3) Psychiatric disorders other than dementia were determined by : (a) A brief mental state examination with the par- ticipants. (b) Informants completed the mini-PASADD (Psy- chiatric Assessment Schedule for Adults with a Developmental Disability), a specific tool for adults with ID (Moss, 2002).", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Diagnosis", "text": "We collated all information in anonymized summaries for independent diagnostic review by two of three psychiatrists (A.H., G.L. or A.S.), two of whom (A.H. and A.S.) are specialists in the psychiatry of ID and the other (G.L.) a specialist in old age psychiatry. An in- strument developed to produce a hierarchical differ- ential diagnosis of dementia in this population was used to determine whether the participants met any criteria for dementia [ICD-10 (WHO, 1993) or DSM-IV (APA, 2000), dementia with Lewy bodies (DLB ; McKeith et al. 1996) or fronto-temporal dementia (FTD ; McKhann et al. 2001)]. This took account of the person's level of ability, the presence of autistic spectrum dis- orders, physical and mental disorders or sensory def- icits as well as changes in the environment. The diagnostic process and how disagreements were re- solved are described elsewhere (Strydom et al. 2007). The participants were then divided into three groups : those with criteria-defined dementia (if they met any of the above diagnostic criteria), potential cases (if there was insufficient information to decide either way), or those who definitely did not have dementia (no de- mentia).", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Analysis", "text": "Data was entered into SPSS version 11 (SPSS Inc., Chicago, IL, USA). The x 2 statistic was used to analyse categorical variables (e.g. sex and ID level by partici- pation or not) unless any cell had an expected count of <5, in which case Fisher's exact test was undertaken. We used t tests to analyse differences in mean age by screen-positive or dementia status. Prevalence rates are presented as percentages. We calculated 95 % symmetrical exact binomial confidence intervals (CIs) with a calculator available at http://statpages.org/ confint.html. The indirect method was used to make comparisons with general population rates in 5-year bands. The most recent Western European general population consensus prevalence rates (Ferri et al. 2005) were used to calculate expected counts for dementia for this study because it provided the only available estimate of dementia prevalence in adults aged o60 years. Further comparison was made for adults aged o65 years using actual prevalence rates obtained from the MRC Alpha study ( Saunders et al. 1993). This study is the one of the largest and most recent UK dementia prevalence studies in urban populations for which data are readily available, and forms part of the well-known European studies of dementia prevalence (EURODEM). The observed count divided by the expected count provided SMRs for all these comparisons ( Page et al. 1995). CIs for SMRs were calculated with a calculator providing exact 95 % Poisson CIs, available at http://home.clara.net/sisa/ smr.htm. We next examined age as a risk factor for dementia by estimating its unadjusted odds ratio (OR), as well as unadjusted ORs for gender and ID level. A logistic regression analysis was then undertaken to determine the independent effect of age by entering these risk factors and their interactions simultaneously.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Participants", "text": "After removing the names of all adults known to have died, moved away, or who were known to have DS, 281 potential participants were identified. Of these, 24 (8.5 %) were ineligible for the study because of un- recorded DS status, being too young, having died re- cently, not having an ID, or were not contactable at the given address. Of the remaining 257 individuals, 222 (86.4 %) participated in the survey. Participants did not differ significantly from non-participants in terms of age or sex. The age range of participants was 60-94 years, with a mean of 68.8 years (S.D.=7.45). Further demographic details of participants are given in Table 1. Sixty ( ", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Dementia cases and potential cases", "text": "The participants were divided into three groups, depending on their dementia status : (1) Those who definitely did not have dementia : 174 participants (78.4 %) were in this category. (2) Criteria-defined dementia cases : 29 participants (13.1 %) met any dementia criteria. (3) Potential cases : 19 (8.6 %) participants who did not have sufficient information to decide either way. We combined criteria-defined and potential cases in some analyses.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Dementia prevalence", "text": "The overall prevalence for criteria-defined dementia cases was 13.1 % (95 % CI 8.9-18.2) in those aged 60 and older and 18.3 % (95 % CI 12.3-25.7) in those aged o65 years. Five-year prevalence rates for both criteria-defined cases and criteria-defined combined with potential cases are given in Comparison with general population rates The observed prevalence for dementia in adults with ID aged o60 years was compared to the expected rate from a recent consensus study (Ferri et al. 2005), which resulted in an SMR of 2.4 (95 % CI 1.6-3.5). The prevalence rate for adults with ID aged o65 years was compared with the expected rate from a large UK ur- ban population study ( Saunders et al. 1993). The SMR for this comparison was 3.9 (95 % CI 2.5-5.7). Age-associated dementia risk  We combined criteria-defined dementia cases with potential cases to remove the potential bias due to diagnostic uncertainty and compared these in 5-year age bands with expected counts calculated from the consensus rates for Western Europe (Ferri et al. 2005) and the MRC Alpha study ( Saunders et al. 1993). SMRs were calculated and are plotted in Fig. 1. The resulting SMRs increased with decreasing age for both com- parisons, so that the SMRs in the 60-65 and 65-70 years age groups were approximately three times that of the o85 years age group (Fig. 1). This difference remained if the prevalence of criteria-diagnosed de- mentia cases (without possible cases) was compared to actual community rates from the MRC Alpha study ( Saunders et al. 1993) (SMR of 7.7 for those aged 65-69 years, compared to an SMR of 2.7 for those aged o85 years).", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Findings", "text": "We have confirmed that older adults with ID (without DS) have a higher prevalence rate of dementia than other older adults. The dementia prevalence did not differ between those with mild, moderate and severe ID. We also confirmed our hypothesis of a downward shift in age-associated risk when compared with the general population. The association of age with de- mentia was not affected by ID severity or sex.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Strengths and limitations", "text": "To our knowledge, this is the largest epidemiological study of dementia in people with ID. By including participants from age 60, we were able for the first time to investigate the possibility of a downward shift in age-associated risk for dementia in this population. We identified all potential participants with rec- ognized ID within a defined geographical area, in- cluded adults with severe disability, and achieved high participation rates. We have also demonstrated that a more aggressive recruitment strategy would not have resulted in significantly more participants. We collected neuropsychological data, informant his- tories and data from medical records and completed physical examinations with participants to make di- agnoses according to international diagnostic classifi- cations. Our study was powered to estimate the overall prevalence of dementia in this population but may not have sufficient power to make within-group comparisons of prevalence rates. A post-hoc sample size calculation with a power of 80 % and a type 1 error of 0.05 suggests that, to compare the observed dementia prevalence rate in the severe ID group (n=29) with an expected rate in the mild ID group of 14.6 %, the sample size required would have needed to be 34. It is possible that we have missed some older adults with ID who are unknown to social or health services. However, we believe this number to be small because older adults with ID are likely to need assistance with the functional problems associated with ageing, and this is more likely for older than younger adults to be provided by agencies outside of the family because informal support networks decrease as people grow older. Furthermore, the care system for people with ID in the UK promotes formal assistance and appropriate use of the ID label. Consequently, nearly 90 % of those aged o80 years receive some form of out-of-home support (Emerson & Hatton, 2004). It can also be argued that older adults who have managed to live independently of service input throughout their lives are highly unlikely to meet the criteria for mental re- tardation as defined by the ICD-10 or DSM-IV. Cross-sectional assessments are less reliable than sequential assessments. We have overcome this limi- tation by supplementing our assessments with his- torical information from informants or medical records, but for a proportion of participants we were not able to decide whether they had dementia or not. ( Strydom et al. 2007). Our study may therefore have underestimated the true prevalence of dementia. A more definitive estimation will only be possible with a cohort design that includes post-mortem examin- ation. Finally, the sample was drawn from London boroughs in the UK and may not be representative of all older adults with ID because of the tendency to place adults with higher needs outside of cities in areas where suitable housing and care settings are more readily available. This could have further re- duced the prevalence of dementia as the study popu- lation may be healthier and more functionally able than the ID population in other areas of the UK.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Prevalence of dementia in people with ID", "text": "The prevalence of criteria-defined dementia in this survey was 13.1 % in those aged o60 years and 18.3 % in those aged o65 years. The prevalence of dementia was not influenced by ID level but diagnostic uncer- tainty (possible cases) increased with increasing severity of ID, and this may have masked underlying differences. There have been two previous community estimates of the prevalence of dementia in this population in Europe. Both reported rates comparable to ours. Patel et al. (1993) reported a prevalence of 8.3 % in 96 adults with moderate and more severe ID aged o50 years in Oldham, UK and Cooper (1997) found a prevalence of 20.2 % in a sample of 129 adults aged o65 years in Leicester, also in the UK. Both these studies had smaller numbers of participants and were less rep- resentative of those with mild ID than the present study. There has only been one North American study to date, which found no difference in SMR for de- mentia in adults with ID than that of the general population ( Zigman et al. 2004). However, their sample was small (n=126) and the sampling method was potentially biased in that it consisted of a sample drawn from known service users combined with a sample of convenience, and only included Alzheimer's dementia. Because of the methodological variation between previous surveys it is difficult to make com- parisons with the present study, but additional sup- port for our finding of an increased prevalence of dementia in this population is from a study that demonstrated that adults with low IQ (borderline in- telligence) had an elevated incidence of dementia when compared to others with normal intelligence ( Schmand et al. 1997).", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Severity of ID and mortality", "text": "Adults with ID have high levels of health morbidity and consequently often die at younger ages than their peers ; increased mortality is especially pronounced in the groups with severe ID and in those with additional problems such as epilepsy (Patja et al. 2000 ;Gustavson et al. 2005). As age is the strongest aetiological factor associated with dementia and has an exponential ef- fect, this may influence the proportion of adults affec- ted in the oldest old, or those with severe disability. These differential mortality rates may result in a co- hort of healthy survivors, who may be less susceptible to dementia. Indeed, once an adult with ID without DS has reached age 65, their life expectancy is comparable to that of the general population (Haveman, 2004). The healthy cohort effect may be another reason for the relatively low rates of dementia in the severe ID group. Age as risk factor for dementia in ID This is, to our knowledge, the first study that has in- vestigated the theoretical shift in age-associated risk in adults with ID. Dementia in this population of adults with ID appears to begin at an earlier age than expected. This is in keeping with the cognitive reserve theory, which predicted a younger age of onset in this group. It is further supported by the finding that smaller brain size has been associated with earlier onset of symptoms ( Schofield et al. 1995). An ac- celerated decline ( Scarmeas et al. 2006) and higher mortality (Geerlings et al. 1999) have been noted when dementia occurs in adults with high ability or edu- cational attainment, giving support to the idea that, in contrast to the present participants, they can tolerate some degree of pathology before developing the clinical syndrome associated with it, which then pro- gresses faster because the pathology is more ad- vanced. However, this has not been demonstrated in all such studies (Del Ser et al. 1999). Whether adults with ID and dementia will have a faster rate of progression, or higher mortality, needs to be studied further. An alternative hypothesis is that some of the underlying causes of ID might also confer increased vulnerability to dementia in later life. It is also possible that the lower SMRs for dementia in the oldest group compared to the younger groups might be explained by increased mortality in adults with ID and dementia.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Policy implications and future research", "text": "Our findings indicate that adults with non-DS ID are more likely than the general population to develop cognitive decline and dementia. Like adults with DS, those with symptoms suggestive of dementia need to be offered assessment to enable early identification and intervention. Other areas for policy and service provision include the provision of accommodation Intellectual disability and dementia 19 and community facilities that is suitable for frail and vulnerable older people. Our study needs to be confirmed with incidence studies of sufficient sample size, which is especially important because prevalence studies may underesti- mate the underlying incidence due to the elevated mortality rate in this population. Incidence studies can also help to reduce diagnostic uncertainty, which could underestimate the true risk for dementia in prevalence studies. Older adults with ID are a high- risk population for dementia, and further studies may help us to better understand the factors associated with the disorder. This is also an important population in which to test the efficacy of non-drug interventions to reduce the risk of dementia.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Acknowledgements", "text": "This study was funded by the Medical Research Council (UK) with a Training Fellowship Grant to A.S. (G106/1160). Additional support was provided by the Penrose Society in the form of a Jancar Travelling Fellowship to A.S. We thank all the participants and their carers, and local area clinicians and social ser-vices staff involved in this study. Our protocol ben-efited from discussions with Professor Tony Holland, and training to A.S. by Mr Paul Patti, Dr Arthur Dalton and others at the New York State Institute for Basic Research in Developmental Disabilities. The research was supported by the R&D departments of Oxleas NHS Trust, Harrow Primary Care Trust (PCT), Enfield PCT, Islington PCT and Camden PCT, who all receive a proportion of funding from the NHS Executive ; the views expressed in this publication are those of the authors and not necessarily those of the funders or the NHS Executive.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Declaration of Interest", "text": "None.", "title": "The relationship of dementia prevalence in older adults with intellectual disability (ID) to age and severity of ID", "file_name": "Strydom et al. - 2009 - The relationship of dementia prevalence in older a.pdf"}
{"section": "Abstract", "text": "Objective Empirical research that cannot be reproduced using the original dataset and software code (replication files) creates a credibility challenge, as it means those published findings are not verifiable. This study reports the results of a research audit exercise, known as the push button replication project, that tested a sample of studies that use similar empirical methods but span a variety of academic fields. Methods We developed and piloted a detailed protocol for conducting push button replication and determining the level of comparability of these replication findings to original findings. We drew a sample of articles from the ten journals that published the most impact evaluations from low-and middle-income countries from 2010 through 2012. This set includes health, economics, and development journals. We then selected all articles in these journals published in 2014 that meet the same inclusion criteria and implemented the protocol on the sample. Results Of the 109 articles in our sample, only 27 are push button replicable, meaning the provided code run on the provided dataset produces comparable findings for the key results in the published article. The authors of 59 of the articles refused to provide replication files. Thirty of these 59 articles were published in journals that had replication file requirements in 2014, meaning these articles are non-compliant with their journal requirements. For the remaining 23 of the 109 articles, we confirmed that three had proprietary data, we received incomplete replication files for 15, and we found minor differences in the replication results for five. PLOS ONE | https://doi.org/10.1371/journal.pone.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Introduction", "text": "In May 2015, two of us, as part of the Replication Program of the International Initiative for Impact Evaluation, convened a group of critics, supporters and others with an interest in repli- cation research for a one-day consultation event in Washington, DC on replication research for international development. To our surprise, one of the more lively discussions at the event centered on whether it is reasonable to expect that the vast majority of published empirical studies can be exactly reproduced. Simply put, is it fair to expect that original data and pro- gramming code from an article exist and can be used by a third party to easily reproduce the published results? All present agreed that this kind of reproduction is the most basic replication question. Some argued that this expectation should be a given-that of course original authors always have the data and code to reproduce their work. Others expressed strong doubts about how frequently authors really can provide the required materials to reproduce the published find- ings. These doubters argued that replication research should focus, at least initially, on this very first line of verification. Empirical research on this kind of verification supports the views of the doubters. For example, McCullough, McGeary and Harrison found that only 14 of 69 articles with data in the Journal of Money, Credit and Banking archive could be replicated [1], and more recently Chang and Li could only replicate 22 of 67 articles for which they requested data and code [2]. In this article, we provide new empirical evidence on whether journal publi- cations of experimental and quasi-experimental studies of interventions in low-and middle- income countries can be verified in this way.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Definition", "text": "We call this concept of replication 'push button replication', as in, can you push the button and reproduce the published results [3]. McCullough calls it reproducibility [4]. However, the term reproducibility has also been used to mean other things, including what we call external replication, which is the test of whether you can implement the program or experiment on a different sample and reproduce the outcomes achieved with the first sample [5]. Clemens defines a reproduction test as \"resampling the same population but otherwise using identical methods to the original study.\" [6] This definition is also different from push button replica- tion. Clemens' definition of verification, \"ensuring that the exact statistical analysis reported in the original paper gives materially the same results reported in the paper,\" is also not the same as push button replication [6]. That definition, which focuses on testing the analysis as reported in the original paper, is what we and others call pure replication [7,8]. Thus, in the interest of being as specific as possible, we use the term 'push button replication' or PBR for the test of whether the code archived or submitted by the original authors can be applied to the data archived or supplied by the original authors to produce the published results. A necessary condition of push button replicability is data accessibility. Most studies that explore push button replication start with this question, that is, are the data and code used to produce the tables and figures in the published article, often called the replication files, avail- able for replication purposes. In cases where journals host an archive or require that replica- tion files be shared when requested, this is a question of compliance. For example, Chang and Li divide their sample into those published in journals requiring replication file accessibility and those published in journals that do not [2]. Even when not required by the journal, how- ever, the accessibility of replication files is a necessary condition for an article to be considered replicable (or more precisely, third-party replicable) as argued by McCullough [4].", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Research question", "text": "In this paper, we explore whether experimental and quasi-experimental intervention studies, hereafter called impact evaluations, conducted in low-and middle-income countries are push button replicable. As Brown, Cameron and Wood argue, the value of replication should be the highest for research that is likely to have policy impact, even from a single study [7]. Impact evaluations of interventions in developing countries, where the need for evidence is high and the quantity and quality of evidence are still relatively low, can often be highly influential. We apply our research question to a sample of development impact evaluations drawn from ten journals that publish a large number of such studies. The studies in our sample are all similar in terms of testing or evaluating some kind of intervention in a low-or middle-income country and using identification strategies based on experimental or statistical counterfactuals. On the other hand, our sample has more diversity compared to other samples drawn for similar repli- cation exercises, as the studies come from different academic disciplines. Most notably roughly half our sample come from public health journals.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Literature review", "text": "Perhaps the best-known example of PBR is McCullough, McGeary and Harrison, hereafter MMH, who explored the replicability of papers in the Journal of Money, Credit and Banking [1]. They selected 192 papers published between 1996 and 2002 that use data and code to pro- duce their results. MMH defined replication as the reproduction of the results of a paper and they used the data and code provided by the authors explaining, \"We made minor alterations to data and code to try to get the code to run with the data, but we did not attempt major alter- ations.\" This definition is consistent with how we implement PBR. They found that only 69 of 186 articles met the data archive requirement of the journal. Out of those 69, only 14 could be replicated using the information from the archive. A more recent example is Chang and Li's study, which looks at the replicability of 60 empir- ical articles in macroeconomics published from 2008 to 2013 [2]. Chang and Li selected articles both from journals that require data and code be made available for replication purposes and from journals that do not. For those journals that do require replication files, Chang and Li were able to obtain these files from the authors for 29 of the 35 papers, and for those that do not, they obtained replication files from only 11 of 26 papers (six had confidential data). For the papers with data and code, Chang and Li conducted replication studies following the defi- nition of MMH. They labelled a replication as successful when they could qualitatively repro- duce the key results of the paper, a criterion that they admit is loose. Overall, they were able to successfully replicate only 22 of the 67 articles using the provided replication files without con- tacting the authors for assistance. Galiani, Gertler, and Romero, hereafter GGR, examined a sample of 205 studies from nine economics journals [9]. Their concern is whether the replication files are publicly posted and whether the replication files meet their four requirements: raw data used in the study, final estimation data set, data manipulation code used to create the final estimation set from the raw data, and estimation code used to produce the final tables and figures. For their sample, they found that only a third of the articles had the raw data posted, while three quarters had at least one of the four types of replication files posted. Giving themselves a time limit of four hours per study, they tested those with publicly posted files and found that only 14% of the articles in the sample of 203 were fully replicable (from raw data to final tables and figures) and only 37% were partially replicable (from estimation data to final tables and figures). Alsheikh-Ali, et al. focus just on the question of data accessibility [10]. They drew a sample of 500 articles comprising the first ten original research articles published in 2009 in the jour- nals with the top 50 impact factors. Because the impact factor is calculated with a preference to the publication processes for basic science and health, all of the journals selected are of those types. Of the 500 articles, 351 were subject to some kind of data policy. Of the 351, 208 did not fully comply with the data policy of the journal where they were published, and only 47 depos- ited full primary raw data online. Of note, not one of the 149 articles in journals without requirements had full primary data publicly available online. Savage and Vickers look specifically at replication file sharing by authors publishing in PLOS journals [11]. They requested the replication files from 10 papers published in either PLOS Medicine or PLOS Clinical Trials. They only received one data set. Their results docu- mented that the PLOS data sharing requirements were largely unenforced at that time. Naudet, et al. select all 37 randomized controlled trials published by The BMJ and PLOS Medicine in the time since each journal adopted data sharing policies [12]. Naudet, et al. audit each article for whether the data were made available upon request and whether they could reproduce the published results using the provided data and the methods section of the published article. They find that 17 articles met their definition of data availability and of those, 14 were repro- ducible for all primary outcomes. Wicherts, Bakker and Molenaar approach the question from a behavioral perspective rather than an audit perspective [13]. For 49 articles published in two major psychology journals they explore the relationship between the willingness to share the research data and the strength of the evidence and reporting errors in each article. Twenty one of the 49 corresponding authors shared some data. Their findings suggest that \"statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions\". In this paper, our subject of study is published studies, and our question is whether their key results are third party verifiable, meaning a third party can obtain the data and the pro- gram code (replication files) and run that code on the data to produce the same results pub- lished in the article. Our focus is not on a particular discipline, but rather on a particular type of research-development impact evaluations-for which even a single study can be policy influ- ential. Although we look at compliance rates for journals with data sharing policies, our con- cern is the verifiability of evidence that may be used for policy making and programming. As such, we do not limit our research question to studies in journals that have data sharing poli- cies. We also wanted to give each study the greatest possibility of being verifiable, so our pro- cess includes requesting replication files from authors, in addition to seeking public files. This audit design follows the example of Chang and Li [2].", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Methodology", "text": "We embarked upon this project in 2015 shortly after the consultation event mentioned in the introduction. While most authors are careful to support the concept of replication in public, in our experience many attack those who conduct it or fund it, so we knew we needed to tread carefully. Our first task was to establish a protocol for push button replication that would make these replication exercises replicable themselves and therefore make it clear that push button replication is intended to be a neutral test. Our second task was to set up the push but- ton replication project to be transparent, again to reaffirm the intention of neutrality.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "The protocol", "text": "We developed the protocol over several months by writing a draft protocol, commissioning other researchers to pilot the protocol, and then revising the protocol based extensive discus- sions around what was learned in the piloting process. Our initial draft benefited from our experiences working with researchers conducting replication studies in the 3ie grants program. The protocol (see supporting information) includes several key features. It lays out specific sequential steps for the PBR researcher to take; it includes detailed checklists of tasks for pre- paring to run the code, running the code, and making the comparison; it requires a pre-regis- tration of the key results; it provides a clear template for the final report; and it includes specific reporting requirements for each classification category. The ordered steps include the communications with the original authors, and we provide templates for each of these messages. If the data and code for an article are publicly available, the protocol still requires the PBR researcher to notify the original authors that their article is included in our study and to share the PBR protocol with them. For cases where the data and code are not publicly available, the protocol provides a timeline for requests and reminders. Table 1 presents the possible classifications for PBRs as defined in the protocol. While McCullough argues that, given data and code, the classification of a replication out- come should be binary-either the code reproduces the published results exactly or it does not- our experience prior to this project suggested it is not always that straightforward [4]. If the dataset provided does not correspond exactly with the final dataset, e.g. limited observations were cleaned later in the analysis, there can be small differences in the coefficients or p-values. Even McCullough acknowledges that different versions of the same software package can yield different estimates. We have also learned over time that arguments about small differences do not benefit the larger objective of verifying findings for the purpose of informing policy. For those reasons, our protocol includes \"comparable\", \"minor differences\", and \"major differ- ences\" classifications, and we avoid the use of the terms \"successful\" or \"failure\". The protocol provides guidelines for assessing the degree of differences between the results from the PBR and the published results. Our goal was to make this comparison objective. For statistical significance, the guidelines state that a difference in a p-value of 0.1 or greater should be considered a major difference; a difference in a p-value between 0.1 and 0.05 should be con- sidered minor; and p-value estimates within 0.05 of each other should be considered compara- ble. The values of the parameters, or coefficient estimates, matter as well. We considered a decision rule using percentage change, but recognized that the meaningfulness of the size of parameters can be very different across studies precluding us from developing a single rule. Thus the protocol instructs the PBR researcher to use judgment, but to carefully document her decisions using summary statistics, like mean values, when available. When we piloted the protocol on studies with dozens if not hundreds of published statisti- cal results, it became clear that focusing on key results would help to make the process of com- paring results both feasible and meaningful. We spent time considering whether we could use a quantitative decision rule such as a cutoff for the share of reported results that have minor or major differences, but that decision rule was complicated, if not impossible, to apply in prac- tice. Chang and Li also focus on \"key qualitative results\" in making their determinations of successful replications [2]. We decided that the most credible way to focus on key results in determining a PBR classification is to publicly pre-register the key results for a study before a PBR on that study is initiated, and we incorporated this step into the protocol. The protocol allows for the classification of incomplete to be matrixed. That is, the data and code can be incomplete but produce comparable results for the tables they do cover, or the data and code can be incomplete and produce differences in the results for the tables they do cover. If the original authors provided data and/or statistical code but we were unable to repro- duce any of the findings from their paper, we coded the study as no access. If the data and sta- tistical code provided allowed us to reproduce the results partially, even if only for some of the tables in the publication, we gave the study two codes, one for having incomplete files and one for the match between the PBR results and the results in the published article. In our analysis here, however, we use incomplete as the primary classification for an article; that is, the finding on data accessibility trumps the finding on comparability. Once we finalized the protocol and embarked on the PBR project, the main features of the protocol stayed the same. However we made two changes during the course of the project that merit mention. These concern the comparison of statistical significance estimates and the requirement for writing and posting the key results. The change in statistical significance deci- sion rules was driven by the difficulties we encountered when trying to recover p-values from code not originally designed to report them. Because of this problem, we allow for categorizing \"one-star\" level changes as minor differences and \"two-star\" level changes as major changes in statistical significance. For the key results, originally we required PBR researchers to read the article and write a key results document before contacting the original authors to request data. After spending the time to write up these documents numerous times and then not receiving data, we changed the protocol to require the write up of key results after the PBR researcher knows the data are available but before replication exercises begin.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Transparency", "text": "The second task in developing the methodology for the PBR project was to make the processes transparent. We did this by setting up the project on the Open Science Framework (OSF). We began by posting all the relevant project documents, including the protocol, in a public folder on our OSF project page. We then set up separate spaces for each of the articles in the sample. Once a PBR researcher obtained the data and code (or was confident she would) for an article, she wrote the key results document and posted that in a public folder. In that way, the original authors, and anyone else, could see from the beginning what would be considered key results for the purpose of determining the PBR classification. Once the PBR was complete, the final report, which includes the classification, was posted in a folder accessible only to the project team and the original authors, where the key results documents now reside as well. We publicly announced the launch of the PBR project in July of 2016 [3] and posted the PBR protocol on the 3ie website at that time.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Process", "text": "After drawing the sample of studies, we divided them up among a group of PBR researchers including the three authors (with Wood and M\u00fcller each conducting a large number), some staff members at 3ie and several interns. One author, M\u00fcller, selected all the articles from the social science journals as a sub-sample with which to write his master's thesis [14]. Each PBR was conducted by a single researcher, with a second researcher conducting a PBR for each study found by the first researcher to have major differences. We coded metadata from each study, such as sector and country, into an Excel spreadsheet along with the classification results. Figures were created using Excel, Stata, and R. The expectation when one requests replication files is that original authors send ready-to- run code. If the code does not immediately run, the protocol requires researchers to \"attempt to troubleshoot minor complications\" in order to get the code to run but not to write new code. In practice, typical troubleshooting included running the code in an older version of the statistical software, ensuring all relevant user written packages were installed on the computer, and removing variables referenced in the code that were unavailable in the dataset. In a few instances we went beyond minor troubleshooting to change \"use\" commands to \"merge\" com- mands in Stata to allow the code to run, updating commands to the current version of the soft- ware, and even correcting typos in an attempt to reproduce the original results. The protocol does not limit the time researchers are allowed to spend on troubleshooting. When trouble- shooting is not successful, the protocol requires PBR researchers to contact the original authors for assistance before making a PBR classification. In all cases where we received data but the code would not run even after troubleshooting, or the data and code were not sufficient to reproduce all the published tables, or the data and code yielded major differences in the results, the PBR researcher contacted the original authors to give them the opportunity to provide an alternative dataset or code to reproduce the pub- lished results. If the code did not run on the data sent by the original authors, and they were not able to send code that runs on the data after follow up, we classified the PBR as no access. If the data and code only reproduced some of the tables, even after follow up, we classified the PBR as incomplete. The protocol specifies certain periods of time to wait for responses from the original authors, similar to the pre-specification of flowtime more recently advocated by Chang for replication research [15]. In practice these waiting periods were only minimums. PBR researchers often allowed for extra time or sent extra reminders. In a few instances, we classified studies as having proprietary data. We created that classifi- cation to account for situations where the authors are unable to provide the data because they do not own them. To determine whether an article qualifies for the proprietary data classifica- tion, we required the authors to provide the estimation code and the contact information for the data owners. In each of these situations, we then contacted the owners of the data to request access for the purpose of the PBR, which also allowed us to confirm the authors' claims that they could not provide the data. In addition to coding a PBR classification for each study, we coded studies as having pub- licly available replication files or not at the time that the PBR for that study was initiated. We followed a number of steps to identify which studies had publicly available replication files. We first looked for data sharing statements in the articles, and determined whether they pointed to public replication files. We then searched both Harvard's Dataverse and the corre- sponding authors' personal webpages for public statements describing a process for accessing the replication files. In some cases the stated process required us to submit data confidentiality agreements, data use plans, or other documentation to gain access to these files. As long as a stated process exists for accessing the replication file, we coded the study as having a publicly available replication file. We coded all others as not having publicly available replication files, including those for which the data are proprietary. We also coded the research funders acknowledged in each of the publications. We reviewed the funding statements and any other author notes included in the articles to identify the research funders. Many of the studies listed multiple funders, each of which we coded.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Ethics statement", "text": "Ethical approval is not required. This investigation audits the availability of data and the computational accuracy of program code for published articles. All data provided to us were anonymized, and we only analyzed them by running the provided program code. See Naudet, et al. for a similar study not requiring ethical approval [12].", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "The sample", "text": "This project was initiated under 3ie's replication program, so our inclusion criteria for the sample reflect 3ie's mandate. 3ie's mandate is to increase the quantity and to improve the qual- ity of impact evaluations of interventions implemented in low-and middle-income countries, what we call development impact evaluations for short. Our inclusion criterion for an impact evaluations requires a study to measure the net effect of an intervention using a counterfactual method. We include experimental designs (randomized controlled trials), natural experiments (or as-if random designs), fixed effects models, and observational studies using matching tech- niques. By development interventions, we mean experiments, projects or programs designed to improve human lives in low-and middle-income countries. While 3ie's mandate focuses more on program effectiveness than on medical efficacy, our inclusion criteria capture a large number of public health studies of medical treatments tested in low-and middle-income countries. We drew our sample from the top ten journals for development impact evaluations as determined using 3ie's impact evaluation repository (IER). The IER is a database of metadata on development impact evaluations that have been identified using a systematic search and screening process designed to capture all such published studies. See Cameron, Mishra and Brown for a description [16]. We identified the top ten journals by looking for those that pub- lished the greatest number of development impact evaluations during the period 2010 through 2012 as catalogued the IER, which was complete through the end of 2012 at the time we started the project. Table 2 presents the top ten journals. As shown in Table 2, the top ten journals for development impact evaluations are quite diverse. Half are public health journals. Three are economics journals, and the other two are multidisciplinary development journals. Thus, this sample allows us to test push button repli- cability for similar types of research but across different academic disciplines. The total num- ber of development impact evaluations published in these ten journals in 2014 is 109. In 2014 none of the public health journals had an open data requirement, and only one had a replica- tion data requirement, the latter meaning that the journal requires authors to provide data and code for replication purposes upon request. Two of three of the economics journals had repli- cation data requirements, and the two development journals did not.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "PBR classifications", "text": "Over half of the sample (59 out of 109) is classified as no access, 15 are incomplete, and another three articles have proprietary data (Fig 1). Thus, verification through full push button replication is not even possible for 71 percent of the articles in the sample. For those with com- plete data, there is none with major differences, five with minor differences, and 27 with com- parable results.  The replication files for the 15 incomplete studies in our sample produce a range of results. Two thirds of these studies have comparable results for the tables and figures we could produce (Fig 2). Four have minor differences, and one has major differences. Table 3 presents the classification results by journal. The rows shaded in the table are for the journals that had replication data requirements in 2014.", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Data access", "text": "Six teams of authors made a proprietary/restricted data claim that we rejected. Of those, five teams claimed they were unable to share the replication files with us because of institutional review board (IRB) or other data sharing limitations, even after we agreed to sign any data shar- ing confidentiality agreement they required. And one research team claimed a company con- trolled their replication file but were unable to document those restrictions for us or provide us contact information for requesting data access. For the three articles that do have proprietary data, we requested the data from the owners so that we could attempt a PBR using the code pro- vided by the authors. We only received data from one, and only partial data at that. In the fig- ures below, we exclude the three articles coded as having proprietary data from our analysis. Open data and code are rare in our sample. Only 14 of the 109 studies in our sample have publicly available replication files, and only four of the 20 articles published in JDE, the journal with a public replication file requirement in 2014, are compliant. For only four of the ten journals in our sample did we receive complete data and code for a majority of the sample articles (Fig 3). Sixty-two of the 109 articles in our sample-those published in JDE, PLOS ONE, and AEJ: AE-are subject to a replication files requirement, either public or upon request. Only 26 arti- cles of those 62 meet the requirement. Ten of the 20 JDE articles have no access or incomplete files. The authors refused to provide replication files for 24 of the 34 PLOS ONE articles in our sample, and two more provided incomplete files. The eight articles in AEJ:AE are fully compli- ant with the journal's requirements, although two of those have proprietary data.   Table 2. https://doi.org/10.1371/journal.pone.0209416.g003 Push button replication: Is impact evaluation evidence for international development verifiable? Ironically, many of the articles in our sample include statements to the effect that the authors will make the data available upon request, yet the authors refused to so this. We saw this frequently with PLOS ONE articles, revealing that the authors indeed understood the jour- nal's requirement when they published, however, seemingly had no intention of honoring it. We also occasionally saw such statements about making data available upon request in articles published in journals without such a requirement, but some of these authors also refused our request. One article's authors stated in the publication that the data underlying their results are fully available without restriction but then responded to us that their data sharing and use agreement prevents them from sharing the data. On the bright side, our request alerted one group of authors that a replication file they thought was publicly available had actually not been uploaded. They promptly publicly released the replication file and thanked us for our efforts. Research funders also sometimes have open data or replication files requirements. These requirements can differ based on department or bureau within the agency, or according to specific grant or contract agreements, so we are unable to classify funders by specific policies. Nonetheless, it may be interesting for readers to see the data access results by funder. Ninety- five of the 109 studies in our sample report funding sources, including one study that reports \"internal funds.\" For these 95 articles, we coded 129 funders by name. The funders include universities, foundations, and public donors. Three funders appear in the sample ten or more times, with the next most prevalent funder appearing only five times. The U.S. National Insti- tutes of Health (NIH) and its branches is the most frequently named funder in our sample with 17 articles. The World Bank has 13; Bill and Melinda Gates Foundation (BMGF) has 10. We received data and code for a majority of the World Bank's sample articles, roughly half of NIH's sample articles, and less than half of BMGF's sample articles (Fig 4).", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Discussion", "text": "In our experience with push button and other kinds of replication research, we often hear authors complain that the cost of preparing and providing replication files is too high. Never- theless, for several studies in our sample we found neatly written and commented code that correctly reproduced every table stored in easily accessible online databases. The economist David McKenzie co-authored five papers in our sample and all of them have data and code accessible online, and all received the classification \"comparable replication\" [17][18][19][20][21]. Olken, Onishi, and Wong was computationally very complex but the results could be reproduced by essentially pushing one button [22]. AEJ:AE had data and code available on their webpage for six out of six papers with non-proprietary data and all received a comparable classification. Some observers argue that replication research is unnecessary given the movement towards open research. The idea is that open data or replication files requirements will motivate researchers to conduct more careful research from the beginning, so it will not be necessary to conduct replication studies to verify their published results. The findings presented here, how- ever, reveal that many economics, development, and public health researchers are a long way from adopting the norm of open research. Not one of the papers in the five public health jour- nals has publicly available replication files. Authors provided a range of reasons for not providing replication files. A few examples include the inability to determine the final version of the statistical code, data use agreements that prevent any replication file sharing, and an unwillingness (or unresponsiveness) from the corresponding author to share the replication file. One researcher responded that the data and code were lost after a computer hard drive crash. Even when the data and code were provided, we encountered multiple instances when the code would not run. Unfortunately, when we contacted the authors about these problems they often did not reply. As we could not repro- duce any of the results from these articles, we coded them as no access. In the cases where the full replication files are available, the results are encouraging. The PBR classification for 84 percent of this subset is comparable and for the remaining 16 percent is minor differences. However, in addition to those articles for which no data are available, there are 25 articles in the sample with incomplete data, and among these there is one with major differences and four more with minor differences. The results of our PBR project suggest that the biggest constraint to the push button replica- bility of published research, here represented by development impact evaluations, is the acces- sibility of replication files. We set the bar low, in that we made the effort to request the data and code, multiple times if necessary, rather than restricting our data question to one of open data as GGR do. We personally believe, however, that by 2016 or later, any data used to publish a study in 2014 should be publicly available except in extreme proprietary circumstances. Funder requirement is one way of ensuring accessibility to replication files. Looking at the five most prevalent research funders in our sample, we find that replication files are available for the majority of the articles funded by the World Bank, USAID, and SIDA. On the other hand, the NIH did have in place requirements in 2014, including the NIH data sharing policy [23] and the NIH guidance for access to research data [24], and yet the majority of NIH- funded articles in our sample have a no access classification. Journals do not appear to be playing a strong role in ensuring the availability of replication files. For example, there is no replication file access for over 70 percent of the articles published in PLOS ONE, which has one of the stricter data availability policies of the journals in our sam- ple [25]. The AEJ:AE is the exception among our sample of ten journals. Six of six studies with- out proprietary data have publicly accessible files. ", "title": "Push button replication: Is impact evaluation evidence for international development verifiable?", "file_name": "Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf"}
{"section": "Abstract", "text": "Using data from the Toronto Stock Exchange 300 companies for a 7-year period, the authors examine the role that institutional activism types and three salient board monitoring mechanisms-CEO/board chair split, board composition, and compensation committee independence-play in influencing CEO contingent compensation in Canada. The authors find that the effect of institutional activism, especially proxy based, is stronger on contingent CEO compensation and that its effects span a longer time. As opposed to the interactions of cumulative proxy-based activism with any of the three monitoring mechanisms, the interactions of cumulative non-proxy-based activism with both CEO/board chair split and compensation committee independence appear to influence CEO contingent compensation. The study's implications are given.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Background and Context", "text": "Previous research on CEO compensation has overwhelmingly focused on large U.S. corporations. For many countries, including Canada, only limited empirical evidence exists on emerging governing mechanisms, such as institutional ownership, and their potential to constrain CEO compensation. Analyzing data from other countries provides an opportunity to better understand the relationship between these two phenomena and allows \"an interna- tional comparison and evaluation of the robustness of other existing research\" (Conyon & Peck, 1998: 146). Canada provides a unique setting for a study of institutional activism and CEO compensation for a number of reasons. First, although the vast majority of Canadian companies have controlling shareholders (MacIntosh, 1996;Montgomery, 1996), in many cases, aggregate institutional holdings rival those of the controlling shareholders. According to various estimates, institutional investors account for approximately two thirds of the dollar value of trading on the Toronto Stock Exchange (TSE), 2 and the role of these investors in the governance of large, publicly traded corporations is on the rise (Shareholder Association for Research & Education [SHARE], 2005;Yang, Wang, & An, 2007). Moreover, the assets of institutional investors are growing rapidly. A 1998 report of the Standing Senate Committee on Banking, Trade, and Commerce estimated this group's total investments at $180 billion. Given this trend, the few hundred professional money managers who invest the tens of billions of dollars held in savings on behalf of individual Canadians are capable of determining the governance of the country's largest and most important public corporations. When unhappy with corporate management, they no longer vote with their feet by selling their shares; instead, they demand that corpo- rate boards take a more active role in keeping management on its toes. Second, with respect to executive compensation, Canadian institutional investors have been active only behind closed doors. But this type of closed-door activism is quickly turning into direct, public opposition to management-initiated pay packages. Two factors have added to institutions shifting to more overt activism. The first factor is the growing involvement of a powerful intermediary-the Pension Investment Association of Canada (PIAC)-in the governance of corporations. The PIAC includes executive compensation as one of the key areas in its list of corporate governance standards (Montgomery, 1996). The second factor is a new require- ment by a major regulatory body, the Ontario Securities Commission (OSC). Effective October 1993, the OSC requires that traded firms disclose the amount and composition of the compensation provided to their five highest paid executives. 3 Third, institutional investors and business press aside, executive compensation has come under attack from non- profit organizations, such as the Canadian Coalition for Good Governance and the SHARE, which are dedicated to promoting shareholder interests, and from the soft-dollar intermedi- aries, such as Fairvest. Given these reasons, it is not surprising that institutional investors are demanding a voice in determining CEO compensation in large Canadian corporations. Wang, Musila, and Chowdhury (2005), for example, found that more than 72% of the share- holder proposals in Canadian corporations targeted issues related to internal governance and executive compensation.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Theory and Hypotheses", "text": "From a power perspective, corporate governance involves a constant struggle for control between three key players-shareholders, managers, and the board of directors-to deter- mine the relative degree of power each party wields in shaping corporate decisions. The power of any specific party may be coupled with or decoupled from (Ocasio, 1994) the power of the remaining parties, depending on the particular contingencies the parties are up against. Bebchuk and Fried (2004) provided a detailed explanation of how the CEO wields power over the board in the very process of setting his or her own compensation. Through their study of 570 large U.S. corporations, Westphal and Zajac (1994) produced evidence of a unique form of CEO power in setting up the CEO's own compensation package. According to Westphal and Zajac, the CEO exercises influence \"subtly\" by encouraging the adoption of long-term incentive plans, which might be largely symbolic but favorable to the stock mar- kets, while discouraging or limiting their use. In a related article, Zajac and Westphal (1995) found that boards legitimize long-term incentive plans for the CEO with any institutional logic that serves their purpose. Therefore, there is good reason to suspect that constraints instigated by coalitions of influencers (Mintzberg, 1983) will affect the relationship between activism on the part of the institutional stockholders and the effectiveness of a corporation's governance, as measured in terms of CEO compensation. Rooted in the need for constraints that the board of directors imposes on management, with the design of an optimal compensation package for the CEO, the board is viewed as seeking to maximize shareholder value. The board is further viewed as a dominant coali- tion in the tripartite relationship and is able to control the behavior of the CEO. In addition, the corporation's large-scale owners can be a dominant coalition representing a real source of power ( Baysinger et al., 1991;Hansen & Hill, 1991;Mangel & Singh, 1993;Mintzberg, 1983;Pfeffer & Salancik, 1978) and are likely to be the key monitors of managerial behav- ior (Beatty & Zajac, 1994;Clyde, 1997). Empirical research has supported the power per- spective through examining the influence of concentrated ownership on certain key strategic decisions, including CEO compensation. When ownership is not concentrated and involved, the CEO gains power that, in turn, allows him or her to extract rents in the form of higher pay. Firms in which one single owner or organization possesses at least 5% of the shares offer lower levels of CEO pay than those firms in which ownership is diffused (Hambrick & Finkelstein, 1995). In an analysis based on the reports of chief compensation officers in 500 U.S. manufacturing firms, Tosi and Gomez-Mejia (1989) found that the level of monitoring is significantly higher for owner-controlled than for management-controlled firms and that the CEO exercises less influence over his or her own compensation when the company has a 5% external shareholder. A particular equity holder who owns 5% or more of the stocks in a firm has the capacity and interest to closely monitor the firm, resulting in high observed levels of monitoring of the firm's CEO compensation (Tosi & Gomez-Mejia, 1994). Even when a large shareholder does not have a controlling position, its monitoring can reduce the extraction of excessive compensation (Bebchuk & Fried, 2004). A strong negative relation- ship between the equity ownership of the largest shareholder and the size of CEO compen- sation (Cyert, Kang, & Kumar, 2002) affirms Bebchuk and Fried's assertion. Similarly, suggesting that the presence of institutional owners serves to reduce rent extraction by exec- utives through excessive pay, several studies have endorsed the efficacy of the power per- spective in executive compensation. David et al. (1998) found that pressure-resistant institutions with an exclusive investment relationship with the firm are able to reduce the level of CEO pay and increase the proportion of long-term incentives in the pay package. With data on 1,914 Standard & Poor's firms from 1991 through 1997, Hartzell and Starks (2003) revealed that a concentrated institutional ownership is related to a lower level of executive compensation. They found that a larger institutional presence leads to more performance-sensitive compensation. Similarly, Khan et al. (2005) confirmed that the con- centration of institutional ownership was associated with lower levels of CEO compensation. The general conclusion of this body of research, then, is that institutional owners, because of the concentration of their aggregate stock ownership, are able to monitor top managers and influence their compensation. We build on this research by adding activism as a means through which institutional owners try to constrain CEO compensation. Although research has examined the relationship between the concentration of institu- tional ownership and different types of corporate decisions, empirical studies on institutional activism per se with regard to its relationship to corporate governance are sparse. We con- sider four empirical studies with direct bearing on the relationship between institutional activism and corporate governance particularly relevant for our article. David et al. (2001) examined the influence of different types of institutional activism on R&D investments with data from the 100 largest U.S. industrial corporations. Their overall finding was that firms increase investment in R&D following institutional activism, but real improvements in R&D outputs do not often follow. Carleton, Nelson, and Weisbach (1998) explored the importance of private negotiations between institutions and the Teachers Insurance Annuity Association- College Retirement Equities Fund (TIAA-CREF). They revealed that, in more than 70% of cases, the agreement was reached through private negotiations with top management, with- out the issue ever coming to a shareholder vote. Smith (1996) found that, during a 5-year period, 72% of targets either adopted proposed governance structure resolutions or made sufficient changes resulting in a settlement with the California Public Employees' Retirement System (CalPERS). Finally, Gillan and Starks (2000) looked at the effectiveness of institutional activism through an analysis of 2,042 corporate governance proposals sub- mitted at 452 companies. They discovered that, in comparison with the activism of individ- ual investors, institutional activism was associated with significantly more votes for initiating corporate governance reforms. These four studies may be placed in two categories based on type(s) of activism employed. Proxy-based activism refers to actions, such as proxy contests and shareholder proposals, that are formally documented in the proxy materials sent to all shareholders. Non- proxy-based activism, on the other hand, refers to actions that may include issue-specific dis- cussions with management through informal means (such as telephone calls, conference calls, and personal one-to-one meetings) and are not formally documented. Although David et al. (2001) studied both proxy-based and non-proxy-based activism, each of the other three- Carleton et al. (1998), Smith (1996), and Gillan and Starks (2000)-focused only on one type of activism. Carleton et al. and Smith focused on the process leading to a negotiated settlement or submission of a proposal, which are the hall- marks of nonconfrontational or non-proxy-based activism. Gillan and Starks, on the other hand, examined shareholder proposals, one very salient form of proxy-based activism. From this synthesis, we can conclude that, depending on the context in which each activism event takes place, there is support for the efficacy of both proxy-based and non-proxy-based activism. TIAA-CREFF and CalPERS are two of the largest, most highly visible institutional investors in the United States and are widely recognized for activism. Therefore, it is not sur- prising that these two institutions were successful in their negotiations with the management of targeted companies. David et al. and Gillan and Starks studied large firms that were tar- geted by different types of institutional investors (such as public pension funds, union-based pension funds, investments groups, religious organizations, and so forth) and produced sup- port for the efficacy of proxy-based institutional activism. Their results are also intuitive, in that a diverse set of institutional investors might not have the patience to negotiate with man- agement for a long period of time. On the other hand, the combined forces of different insti- tutional investors are likely to increase the salience of their collective demand to which top management may be more receptive (Neubaum & Zahra, 2006). These studies endorse the efficacy of both forms of activism, but it appears that the setting in which each works is dif- ferent. Covering multiple institutional investors targeting the largest TSE firms, our study overlaps with David et al.'s in relation to the typology of activism employed and the tempo- ral effects of each type of activism. However, our approach differs from Carleton et al.'s and Smith's; they each considered only one type of activism by a single, large institutional owner. We considered both proxy-based and non-proxy-based activism as mutually exclusive forms of political levers, hypothesizing that their magnitude of effect on CEO compensa- tion would be different. In addition, our article also characterizes some striking differ- ences. First, our national setting and dependent variable-Canada and CEO compensation, respectively-are different from those in other four studies. Although both R&D investments and CEO compensation are important corporate decisions, their nuances are different, as are their implications. Second, in addition to the political perspective, we also employed the reputation perspective and explain how it sets limits on the power of the CEO. Reputation perspective-or its common variants, such as corporate reputation (Fombrun, 1996), media reputation (Deephouse, 2000), and employment reputation (Conyon, 2006;Fama & Jensen, 1983)-signals the public about a firm's governance effectiveness in terms of its CEO com- pensation. This prompts directors to insulate institutional shareholder interests more vigi- lantly, offsetting CEO power to extract an inflated salary. Third, we also considered the interaction between institutional activism and three important governance mechanisms of the board. Such treatment is consistent with our position that activism has to interact with salient board control mechanisms to check the spiraling level of CEO compensation. Although research bearing on the relationship between board mechanisms and CEO compensation abounds, examining the interactions of such mechanisms with forms of institutional activism is certainly novel. The outcome of such interactions has important policy implications for board reformers, depending on whether the results support or negate the substitutability of institutional activism for board mechanisms (e.g., Hoskisson et al., 2002;Rediker & Seth, 1995). A study with such differences has the potential to enrich our understanding of the influence of institutional activism on CEO compensation in a different country and add to the extant literature accordingly. Two sets of factors-legal issues as well as structural dynamics of the Canadian stock market-contribute to institutional owners resorting to activism for constraining excessive CEO compensation. With respect to the first set, although excessive CEO pay hurts institu- tional owners in that it diverts profits (Bebchuk & Fried, 2004;Hambrick & Finkelstein, 1995), institutional owners cannot sue the board directly for granting excessive CEO com- pensation. Instead, they can file a derivative suit alleging that the board has hurt the corpo- ration by giving the CEO an inflated compensation package. But because the board, and not the institutions, is responsible for making decisions on behalf of the corporation, the institu- tions' ability to proceed with a derivative suit is severely restricted by the court (for a dis- cussion of derivative suit, see Bebchuk & Fried, 2004). Similarly, in Canada, the courts surround the right to file a derivative suit with so many procedural requirements that the right eventually has little or no value (Smyth, Soberman, & Easson, 2004). In addition, Canadian corporate law does not require companies to provide complete disclosure of information on items put to vote. With respect to the structural dynamics of the Canadian stock market, because of unprecedented growth of institutional owners' stake, exerting pressure on the board may be the most viable option for the institutional investors ( Hoskisson et al., 2002). Because of their high aggregate ownership, it is difficult for such owners to immediately divest stocks of one corporation and buy those of another. Moreover, until recently, federal regulations required Canadian pension funds to invest at least 70% of their total funds in Canadian secu- rities. This requirement restricted the investment opportunities for Canadian pension funds. A deluge of takeovers, mergers, and acquisitions of Canadian corporations-largely because of the North American Free Trade Agreement-has resulted in the depletion of some rep- utable corporate entities, thus making stock shuttling even more difficult in Canada. For these reasons, even if the institutional investors are dissatisfied with their firm's CEO, they cannot afford to divest their shares but must voice displeasure to influence management. Therefore, to constrain excessive CEO compensation and safeguard their interests, institu- tional investors must exert pressure on the board. Because pressure cannot be exerted in a vacuum, they must have certain levers of power, and different forms of institutional activism act as such levers. Levers can be thought of as an \"armament\" for achieving desired results in certain areas of an organization (Hambrick & Cannella, 1989). Although these levers may take many different forms, the most common forms include proxy contests, initiating share- holder proposals, informal jawboning (direct negotiation with management), and public announcements of dissatisfaction with management. Following David et al. (2001), we group these common forms under two headings-proxy-based and non-proxy-based- which correspond, respectively, to confrontational and nonconfrontational activism (Prevost & Rao, 2000). Although the confrontational-or proxy-based-approach is often preceded by attempts at negotiated settlements or jawboning (that is, non-proxy-based activism), these two forms of activism may be mutually exclusive as well (Prevost & Rao, 2000). Accordingly, and con- sistent with David et al. (2001), we treat them as mutually exclusive forms of activism, and this treatment is important for the hypotheses that we posit and test in this article.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Institutional Activism and CEO Compensation", "text": "For various reasons, institutional owners have an obvious interest in opposing pay pack- ages that are independent of performance. First, exorbitant CEO pay hurts these owners directly through a diversion of profits (Bebchuk & Fried, 2004;Hambrick & Finkelstein, 1995). Second, it encourages inflated pay expectations for other senior executives (Crystal, 1991;Hambrick & Finkelstein, 1995). Third, a lack of sensitivity of a CEO's pay to perfor- mance signals to media and potential investors a corresponding lack of effective oversight of the corporation, thus tarnishing the reputation of the corporation and sending negative sig- nals to stock markets. In fact, this is perhaps the most important reason why institutional owners are so concerned with excessive CEO compensation. The CEO, on the other hand, presses for as much pay as possible. The resulting conflict stems from two related but dis- tinct factors. First, most firms operate in a dynamic arena that presents constraints or sys- tematic risks, such as general economic conditions, the labor market, interest rates, inflation, and political turmoil, which may overwhelm the best-or worst-efforts of the CEO. Therefore, when pay is tied to performance, the amount of a CEO's expected pay is likely to vary substantially. Generally, a risk-averse CEO wants to keep his or her pay as independent of risk as possible, unless a CEO has significant ownership in the corporation. As an indirect manifestation of managerial entrenchment for higher pay in Canadian corporations, Park, Nelson, and Huson found that \"in the absence of oversight, managers choose pay packages that do not provide the level of incentives shareholders would choose\" (2001: 364). Second, a link between pay and performance, if measured by stock returns, may affect a CEO's abil- ity to trade stock returns for firm growth without harming his or her personal income (Gray & Cannella, 1997;Hill & Phan, 1991). To balance the opposing preferences of owners and CEO, a CEO's compensation is typically structured as a combination of contingent pay and noncontingent pay. These two different forms of pay are meant to expose the CEO to different levels of risk, and each serves different incentive objectives (Daily, Johnson, Ellstrand, & Dalton, 1998). Contingent compensation induces real uncertainty in a CEO's pay (Gray & Cannella, 1997) and is designed for the long-term value appreciation of the company, whereas noncontingent compensation provides a CEO with a stable stream of income (Tosi & Gomez-Mejia, 1994) and thus may help to avoid high CEO turnover. Institutional investors prefer contingent pay, whereas a CEO prefers noncontingent pay; thus, some tradeoff between the degree of risk sharing of the former and the latter is likely to determine the proportion of contingent and noncontingent pay. Institutional activism constitutes events that eventually become known-first to the media and various professional groups and then to the public in general. It is true that the media cannot really report a firm's action without the firm taking-or not taking-the action (Deephouse, 2000). The human capital of directors depends on their performance as custo- dians of shareholder rights (Fama & Jensen, 1983), so they take public knowledge of such activism seriously. Human capital aside, some directors also join the board merely for pres- tige and connection. Therefore, it is important for the directors to manage-or influence- other stakeholders' perceptions of the firm's reputation, lest they lose their own. Because public knowledge of institutional activism is likely to create outrage, the outside directors will be loath to approve a CEO compensation package that would embarrass them or com- promise their reputation (Bebchuk & Fried, 2004). Moreover, if the outrage is so serious that it is able to contribute-in isolation or in combination with other factors-to a takeover bid, the directors even risk their jobs and the associated benefits. As a result, public knowledge of activism leads to a coupling of board power with that of the institutions, resulting in the directors shifting their attention toward the institutional investors. This outrage, in combina- tion with the OSC requirement for the disclosure of executive compensation, is likely to keep directors under intense pressure to make CEO compensation contingent on firm performance (McFarland, 2002;Park et al., 2001). Therefore: Hypothesis 1: As more institutional activism occurs, the proportion of CEO contingent compensa- tion is likely to increase accordingly.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Compensation Time Horizon and CEO Compensation", "text": "Gray and Cannella defined compensation time horizon as \"the extent to which the total compensation package provides financial rewards to the executive based on long-term per- formance outcomes\" (1997:523). This definition is consistent with the theory that compen- sation contracts can be used to encourage and reward long-term decision making (Walsh & Seward, 1990). The proportion of long-term incentives in a compensation contract may serve to align the interests of managers with those of stockholders by rewarding the CEO only if the shareholders' returns are enhanced (Beatty & Zajac, 1994;Gomez-Mejia, 1994;Zajac & Westphal, 1995). As long-term incentives lessen the need for vigilant monitoring (Beatty & Zajac, 1994;Zajac & Westphal, 1995), institutions are likely to favor these incentives. However, this long-term orientation is risky for the CEO, particularly for one whose discount rate is higher than that of the firm (Iacobucci, 1996) and whose wealth portfolio is not well diversified (Hall & Murphy, 2002). Strategies to maximize shareholder wealth are necessarily time dependent (Sanders & Carpenter, 2003), and benefits may become apparent only after considerable time has elapsed (Dierickx & Cool, 1989). Therefore, focusing on the potential long-term benefits to both the organization and its shareholders should encourage the building of resources that may serve as sources of future competitive advantage. For enhanced growth and capabilities, which most institutions favor, a company needs to make parallel investments in both tangi- ble and intangible assets. To achieve such results, the CEO must receive sufficient incentives to invest in the risky strategic assets and capabilities of the firm. All else being equal, it means that the CEO will require an increased level of compensation for bearing the addi- tional risk (Lambert & Larcker, 1991;Wiseman & Gomez-Mejia, 1998). In Canada, the labor market for CEOs of large public corporations is tightly integrated. Canadian CEOs learn about their respective pay positions from the mandatory executive salary disclosures (Gelinas, Magnan, & St-Onge, 2004;McFarland, 2004;Park et al., 2001). Besides, Canadian CEOs have access to the much larger and more attractive U.S. executive labor market, which provides them with opportunities to assess their options in terms of career moves. With this information, Canadian CEOs now have effective bargaining chips with which to negotiate their employment contracts upward. Such upward ratcheting is pos- sible largely due to bidding up of CEO pay over time (Ezzamel & Watson, 1998;Finkelstein & Hambrick, 1988;Van Clieaf, 2004). The sales elasticity of CEO compensation in Canadian firms indirectly supports this bidding-up hypothesis. 4 Because the bidding up is time dependent (Crystal, 1991), we maintain that a CEO's demonstrated contribution to the long-term profitability of the corporation can further help him or her negotiate a more attrac- tive employment contract. In such a situation, the CEO may be able to reduce or neutralize the effects of activism on contingent pay. Although institutional owners would prefer to insure a proper match between a well-performing CEO and his or her compensation, an increase in the contingent portion may be acceptable to these owners, lest the CEO be lost to the competition. Therefore: Hypothesis 2: The effect of institutional activism on the proportion of CEO contingent compensation is stronger over the long-term than over the short-term.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Activism Types and CEO Compensation", "text": "Both proxy-based (proxy contests and shareholder proposals) and non-proxy-based (informal jawboning and public announcements) activism are not likely to be equally effec- tive in influencing boards to approve CEO compensation packages that are consistent with the interests of the institutions. Proxy-based activism, which is formally documented in the proxy materials sent to all shareholders, is more effective and direct insofar as it communi- cates institutional investors' dissatisfaction ( David et al., 2001). Consistent with this line of reasoning, David et al. found support for the efficacy of proxy-based activism; that is, proxy- based activism increased R&D inputs, whereas non-proxy-based activism had no such effects. Consistent with Mitchell, Agle, and Wood's (1997) saliency theory, David, Bloom, and Hillman (2007) found that managers are more likely to settle proposals filed by salient shareholders. Gillan and Starks (2000) discovered that proposals sponsored by institutional investors appear to act as substitutes, garnering significant voting support in annual general meetings. Non-proxy-based activism, on the other hand, involves informal discussions with man- agement and is not documented. Frequent breakdowns in the process of negotiation are common; hence, an outcome that is satisfactory to both institutional investors and manage- ment becomes time consuming, which in turn leads to the diffusion of its effects. In the small, tightly connected, external executive labor market for directors in Canada, the ramifications of proxy-based activism might prove costly in terms of their professional careers. Documented proxy fights and shareholder proposals, which indicate more firmness and greater commitment on the part of the institutional owners, might make it difficult for the directors to sell their managerial competence to other potential employers (Conyon, 2006;Fama & Jensen, 1983;Weisbach, 1988). Shareholder proposals signal to the market that managers have been reluctant to address shareholders' area(s) of concern (Prevost & Rao, 2000). Accordingly, impression management theorists (e.g., Bolino, 1999;Schlenker, 1980) suggest that directors deal with proxy-based activism far more decisively. Therefore: Hypothesis 3: The effect of institutional activism on the proportion of CEO contingent compensation is stronger for proxy-based than for non-proxy-based activism.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Institutional Activism, Board Characteristics, and CEO Compensation", "text": "The interpretation of previous research findings (e.g., Baysinger et al., 1991;David et al., 1998;Hansen & Hill, 1991;Hartzell & Starks, 2003;Kochhar & David, 1996) that institu- tional ownership stakes form a basis of power adequate for influencing corporate decisions appears somewhat incomplete, in that ownership stake, per se, is not sufficient to influence a key governance decision involving executive compensation ( David et al., 2001). The con- centration of voting power in institutional investors is only a potential indicator of ownership influence (Mangel & Singh, 1993;Tihanyi et al., 2003). Shareholder power is increased by \"the frequency with which voting congeals into decisive changes\" (Alchian & Demsetz, 1972: 788). Concentrated voting power, if not translated into action, is of little value to insti- tutional owners. Therefore, for the institutional investors to realize their investment goals in a corporation, large ownership stakes aside, they must exert pressure on the board ( David et al., 2001;Mangel & Singh, 1993;Mintzberg, 1983). Because the assessment of a CEO's performance and the determination of his or her compensation is one of the important func- tions of the board (Bebchuk & Fried, 2004;Fama & Jensen, 1983;Finkelstein & Hambrick, 1988;Mintzberg, 1983;Walsh & Seward, 1990), it is indeed the board of directors that has formal power over an incumbent CEO, and it is only through a few internal control mecha- nisms that it can constrain its CEO's compensation. It follows, then, that institutional investors must pressure the board into supporting their activism, and this is where the inter- action of the levers of activism and internal control mechanisms come into play. To exert their influence on the manner in which the corporation is run, institutional investors emphasize the separation of the roles of CEO and the board chair (the antonym being CEO duality), the composition of the board, and the independence of compensation committees. Because of their pervasiveness and effectiveness in the trusteeship role of a board, these mechanisms have been used in prior research (e.g., Beatty & Zajac, 1994;Boyd, 1994;Conyon & Peck, 1998;Daily & Dalton, 1994;Main & Johnston, 1993;Westphal & Zajac, 1994;Zajac & Westphal, 1995). CEO duality is an indication of high CEO power. Empirical findings that bear out a pos- itive relationship between CEO duality and higher CEO compensation (e.g., Core, Holthausen, & Larcker, 1999;Cyert et al., 2002) corroborate the influence of CEO duality. When the roles of the CEO and the board chair are separated, an independent board chair can facilitate an objective assessment of top management performance (Boyd, 1994) and can determine CEO salary accordingly. Therefore, institutional activists consider the separation of CEO and board chair an essential structural arrangement for safeguarding their interests. CalPERS has made the elimination of CEO duality a top priority in its negotiations with some major U.S. corporations (Daily & Dalton, 1997). Outside-or nonexecutive directors-have no hierarchical authority relationships within an organization and can therefore provide critical reviews of CEO actions (including termi- nation, if necessary) without fear of retaliation (Daily & Schwenk, 1996). Outside directors are also less conciliatory than the inside directors toward the CEO (Beatty & Zajac, 1994). Because a compensation committee is a forum in which directors determine the appropriate design of compensation structures for the CEO and align CEO and shareholder interests (Main & Johnston, 1993), it is crucial that this committee be independent (Bebchuk & Fried, 2004;Iacobucci, 1996). An independent compensation committee eliminates any opportu- nity for a CEO to award a self-serving compensation package that is incongruent with insti- tutional shareholder interests (Conyon & Peck, 1998;Main & Johnston, 1993). Thus far, our argument as to why institutional investors emphasize three salient board mechanisms-CEO/board chair split, board composition, and compensation committee independence-was predicated on both anecdotal and empirical support for their efficacy in curtailing managerial excess. This support highlights the direct influence of these three board mechanisms. Besides implicitly suggesting direct effects, our theoretical position sug- gests clear interaction effects between activism types and control mechanisms. As institu- tional activism types and control mechanisms are intended to reduce the extraction of excessive compensation by the CEO, the interactions are likely to be reinforcing and hence more effectual on CEO compensation. Accordingly, in addition to suggesting that institu- tional activism will affect CEO compensation, we also contend that the three salient control mechanisms will potentiate the efficacy of both proxy-based and non-proxy-based institu- tional activism. Thus: Hypothesis 4a: The effect of proxy-based institutional activism on the proportion of CEO contin- gent compensation increases as the occurrence of the separation of board chair and CEO role increases. Hypothesis 4b: The effect of non-proxy-based institutional activism on the proportion of CEO con- tingent compensation increases as the occurrence of the separation of board chair and CEO role increases. Hypothesis 5a: The effect of proxy-based institutional activism on the proportion of CEO contin- gent compensation increases as the proportion of outside directors increases. Hypothesis 5b: The effect of non-proxy-based institutional activism on the proportion of CEO con- tingent compensation increases as the proportion of outside directors increases. Hypothesis 6a: The effect of proxy-based institutional activism on the proportion of CEO contin- gent compensation increases as the independence of the executive compensation committee increases. Hypothesis 6b: The effect of non-proxy-based institutional activism on the proportion of CEO con- tingent compensation increases as the independence of the executive compensation committee increases.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Data and Sample", "text": "Sample firms were selected from the TSE Composite Index (TSE 300). Because institu- tional shareholdings tend to be concentrated in larger, more liquid public corporations ( David et al., 2001), the TSE 300 was considered appropriate as the universe of firms for this study. Almost all large public corporations in Canada are listed on the TSE and approxi- mately 90% of all stock transactions in Canada take place through the TSE 300. This study focused on firms that appeared consistently in the TSE 300 from 1995 to 2002, inclusive. There were three reasons for selecting this time frame. First, the 1993 OSC require- ment for executive salary disclosure allowed us to conduct research on CEO compensation in Canadian firms in relation to other performance and governance variables (e.g., Gelinas et al., 2004;Park et al., 2001;Singh & Agarwal, 2002;Zhou, 2000). Second, although one of the first major public instances of institutional activism in Canada occurred in 1982 (Hutchinson, 1996), overt and fairly large-scale institutional activism did not take hold until the early 1990s. Third, proxy-based activism is usually filed a few months before it is reported in a firm's proxy circular and is voted on some time after it is reported. For most firms, this time period is from the ending months of the previous year to the beginning months of the current year. Thus, activism initiated in 1995 would not be reported until early 1996. Based on the criteria listed earlier, we identified 109 firms, 22 of which were dropped from the sample either because data on them were missing for some years from 1996 to 2002 or because some of them had been taken over and, as a result, had ceased to exist as sepa- rate entities. Thus, we had a total of 87 firms and 522 5 usable observations (87 firms per year \u00d7 6 years).", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Measures", "text": "Dependent variable. We included three measures of CEO compensation: noncontingent pay (minimum base salary), contingent pay (bonuses, securities under options [SUO], stock appreciation rights [SAR], value of restricted shares units [RSU], long-term incentive pay [LTIP], and all other compensation as disclosed in the proxies), and total pay (the combina- tion of noncontingent and contingent pay). Although executive pay practices vary substan- tially across firms, most CEO pay packages contain these basic components (Murphy, 1999;Zhou, 2000). Except for the minimum base salary, all other components, such as bonus, SUO, SAR, RSU, and LTIP, are aggregated into the contingent part, as they are not fixed and are considered to be linked to performance. Given the purpose of our article, we used the proportion of contingent compensation to total compensation as the dependent variable throughout all nine hypotheses. Data on CEO compensation for the years 1996 to 2002 were obtained from the proxy circulars of each company as listed in the System of Electronic Document Analysis and Retrieval (SEDAR). SEDAR is a comprehensive, online archive of securities documents filed by publicly traded companies in Canada. We also used CanCorp to fill in any missing data on CEO compensation. The Black- Scholes model 6 was used to standardize the values of stock options. The Black-Scholes model provides an estimated cost of the stock options incurred by the company (Hall & Murphy, 2002), and cost (or expense) is precisely what shareholders are most concerned about. We also considered the effect of dividend on option value. Thus, the total value of a CEO's compensation is equal to the sum of salary, bonuses, SUO, SAR, RSU, LTIP, and all other compensation as disclosed in the proxy circulars. In all nine hypotheses, the dependent variable is the proportion of contingent pay to its corresponding total pay. In all regressions, we regressed the dependent variable against independent variables, lagged dependent vari- able, and control variables. The lagged dependent variable helped account for possible autoregressive dynamics in the dependent variable. Independent variables. Institutional activism was recorded for the years 1996 to 2002. For proxy-based activism from 1997 to 2002, we used proxy circulars stored at SEDAR and CanCorp (Disclosure Select). For proxy-based activism during 1996, we used the Canadian Business and Current Affairs (CBCA) database. This database contains, among other things, newspaper articles from Canada's two major national newspapers: the Financial Post (a sec- tion of the National Post) and the Business Section of the Globe and Mail. To record non- proxy-based activism, we also used the CBCA database. Using keywords such as activism, activist, annual meeting, announcement, opposition, proposal, proxy, shareholder, and stock plan, we examined each company in the CBCA in relation to articles published in the Financial Post and the Globe and Mail. Whether an incident reported under any of these terms constituted an instance of non-proxy-based institutional activism was unanimously agreed upon by both authors and two research assistants. Data on board control mechanisms were gathered for the years 1996 through 2002 from the proxy circulars or annual reports of the sample companies, as listed in SEDAR. This period of time was sufficiently long to reflect changes in the composition of board members. Data on compensation committee composition and CEO/board chair split were also gathered for the years 1996 through 2002. The CEO/board chair split variable equaled 1 if the firm's CEO did not hold the position of board chair, and 0 otherwise (e.g., Conyon & Peck, 1998). To measure board composition, we used the traditional percentage of outside members (e.g., Conyon & Peck, 1998;Klein, 1998;Westphal & Zajac, 1994;Yermack, 1996). The makeup of the compensation committees was also obtained from the proxy statements. In most cases, the members on the compensation committee were from outside the firm. We used the per- centage of outside compensation committee members to gauge the effect of compensation committee independence on CEO pay. The percentages for the board and the compensation committee were used in regression analyses. Control variables. A review of prior research led to the inclusion of 13 control variables in our analysis. Sales represent one measure of size. Magnan, St-Onge, and Thorne (1995), Park et al. (2001), and Zhou (2000) found a positive relationship between sales and execu- tive compensation in Canadian corporations. Another measure of size is a firm's total assets, which David et al. (1998) and Daily et al. (1998) used. Therefore, we controlled for firm size by using the logarithm of sales and assets. Because Tobin's q measures the market assess- ment of a firm's assets to their replacement value, we used it to account for the variation in firm performance and control for the presence of growth opportunities. Hartzell and Starks (2003) and Harvey and Shrieves (2001) documented a strong relationship between growth opportunities and the presence of incentive compensation. To calculate Tobin's q, we used the approximation method of Chung and Pruitt (1994). Consistent with David et al. (1998) and David et al. (2001), we controlled for market performance with Jensen's alpha and firm financial performance with return on assets (ROA). Firm riskiness can be related to the level of executive compensation (Beatty & Zajac, 1994;Tosi & Gomez-Mejia, 1994). The CEO in firms with high risk may demand a higher level of compensation (Hill & Phan, 1991). We captured firm systematic risk with beta. Sales, assets, and data required to calculate Tobin's q, Jensen's alpha, and beta were obtained from the DataStream. CEO tenure also affects CEO compensation (Hill & Phan, 1991;Ocasio, 1994). A CEO with long tenure is likely to have succeeded in establishing credibility and influence with the board (Hill & Phan). Long tenure also gives the CEO the opportunity to build a coalition and power base by populating boards with supporters. We measured CEO tenure by the number of years an executive had been the CEO of a company. We also controlled for CEO owner- ship. A significant ownership stake in a company makes a CEO more powerful in compari- son with other shareholders (Bebchuk & Fried, 2004;Khan et al., 2005), and this power leads to even higher compensation for the same CEO. Cyert et al. (2002) found a positive relationship between the amount of a CEO's compensation and the level of his or her stock ownership. CEO ownership was measured by the percentage of stocks a CEO holds. Because a firm may be controlled either by its CEO/management, by institutional investors, or by other companies, following Daily et al. (1998), we also included ownership by institutional investors as a control variable. Consistent with David et al. (1998), we decomposed institutional ownership into pressure resistant, pressure sensitive, or pressure indeterminate and used these as control variables. Moreover, based on David et al.'s sugges- tion that blockholders provide better governance in setting compensation policy than do smaller investors, we also included noninstitutional blockholder ownership as another con- trol variable and assigned a value of 1 if a blockholder existed and 0 otherwise. CEO pay is likely to be a function of the prevailing industry market trends in CEO compensation (Hambrick & Finkelstein, 1995). Accordingly, and in line with both Daily et al. (1998) and David et al. (2001), we included industry average contingent pay to total pay to control for possible pay variations across industries. Data on CEO tenure, pressure-resistant ownership, pressure-sensitive ownership, pressure-indeterminate ownership, and blockholder owner- ship 7 were obtained from proxy circulars listed in SEDAR. CEO ownership data were obtained from the System Electronic Disclosure by Insiders database. Industry average con- tingent compensation and total compensation were calculated from proxy circulars in SEDAR at the two-digit Standard Industrial Classification code level.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Analysis", "text": "Given the panel data set, we used fixed effects estimation method for regression analysis so that the unobserved firm-specific effect could be accounted for. Following this, we car- ried out time demeaning on the dependent and all explanatory variables and then employed the weighted least square to perform regression analysis (Wooldridge, 2003). As the total assets variable was found to be related to residuals, we subsequently used the square root of the absolute difference between a corporation's logarithmatic transformed total assets and the corporation's average logarithmatic transformed total assets over the years to account for heteroscedasticity in regression residuals. The lagged dependent variable was included to model temporal changes. To test Hypotheses 1, 2, and 3, we ran the following regression equation: where cp it and cp it-1 is the ratio of contingent pay to the total pay for the CEO in firm i in period t and t -1, respectively; ap it equals the cumulative number of shareholder proposals 8 that were passed into the proxy statement of firm i up to period t, and 0 if there was no such proposal up to period t; anp it equals the cumulative number of non-proxy-based institutional activism events reported for firm i up to period t, and 0 if there were no such events up to period t; nap it equals the number of shareholder proposals reported for firm i during period t, and 0 if there were no such proposals during period t; nanp it equals the number of non- proxy-based institutional activism events reported for firm i during period t, and 0 if there were no such events during period t; cd it equals 1 if firm i had a separate CEO and chair for period t, and 0 otherwise; bs it equals the proportion of the board's outside directors for firm i in period t; cs it equals the proportion of the compensation committee's outside directors for firm i in period t; ls it is the logarithm of sales for firm i for period t; la it is the logarithm of the total assets of firm i in period t; tq it is the Tobin's q ratio for firm i in period t; alpha it is the Jensen's alpha of firm i for period t; roa it is the ROAs for firm i for period t; beta it is the beta value of firm i for period t; ct it represents the number of years that the CEO of firm i has been in the CEO position up to period t; cw it is the CEO stock holding percentage of firm i in period t; pr it is the proportion of firm i owned by pressure-resistant institutions during period t; ps it is the proportion of firm i owned by pressure-sensitive institutions during period t; and pi it is the percentage of firm i owned by pressure-indeterminate institutions; nb it equals 1 if firm i during period t has a blockholder and the blockholder is not the firm's CEO, and 0 otherwise. Last, iap it is the industry average contingent pay to total pay for the industry of firm i in period t. We tested Hypothesis 1 without variables nap it and nanp it , as these two are intended for capturing the immediate short-term correlation effect of institutional activism and for testing Hypothesis 2. If we can prove that at least one of the two coefficients for institutional activism (that is, either \u03b1 2 or \u03b1 3 , or both \u03b1 2 and \u03b1 3 ) are statistically significant and positive, then Hypothesis 1 will hold. Therefore, effects of proxy-based and non-proxy-based activism on contingent compensation can be examined separately and jointly. When exam- ined separately, the resulting regression analysis considers only one type of activism, either proxy-based or non-proxy-based, as if only one type of activism were at work. 9 Such con- sideration provided estimates for a basic model. To measure the aggregated long-term effect, we followed David et al.'s (2001) approach. 10 Therefore, the long-term effect of proxy-based activism is equal to the sum of \u03b1 2 , \u03b1 2 /(1 -\u03b1 1 ) and \u03b1 4 /(1 -\u03b1 1 ). If the sum is greater than \u03b1 4 , then it could be said that the long-term effect of proxy-based activism is stronger than the short-term effect, provided that \u03b1 1 , \u03b1 2 , and \u03b1 4 are all statistically significant. A similar argu- ment can be made for non-proxy-based activism. If the combined values of \u03b1 2 and \u03b1 4 are sta- tistically significant and larger than \u03b1 3 and \u03b1 5 , separately or jointly, Hypothesis 3 will be supported. cp it = \u03b1 1 cp it To test Hypotheses 4a to 6b, we used the following regression equation: We tested Hypotheses 4a to 6b by substituting the internal control mechanism in question with CEO/board chair split (4a and 4b), board structure (5a and 5b), and compensation com- mittee independence (6a and 6b) in regression equation (2). If the coefficient associated with the interaction term was statistically significant and showed the expected positive sign, then the related hypothesis is supported. cp it =", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Results", "text": "Before presenting an analysis of our results, a comment on two key variables in the study is in order, namely CEO contingent compensation and the incidence of institutional activism. Table 1 demonstrates that the proportion of CEO contingent pay to total pay increased from 1996 through 2002 (from 48.39% in 1996 to 52.49% in 2000 and then to 50.70% in 2002). Table 2 indicates that the total instances of institutional activism had also an increasing trend over the same period of time. If, in fact, tying CEO compensation to a corporation's performance is what institutional stockholders really want, it is the upward and downward movement of contingent and noncontingent compensation components, respectively, that really reflect their wishes. Table 3 reports the descriptive statistics and correlations for the dependent, independent, and control variables in the analysis before they were transformed. Statistically, when the magnitude of a correlation coefficient is larger than 0.11, 0.08, and 0.07, then the coefficient has, respectively, 99%, 95%, and 90% confidence that it is not equal to 0, given 522 obser- vations. Besides the significant correlation between the contingent compensation proportion and the lagged contingent compensation proportion, high correlations also exist among other variables. Both the contingent compensation proportion and the lagged contingent compen- sation proportion are significantly related to the cumulative number of proxy-based activism and the new number of proxy-based activism. Sales and total assets are also related to insti- tutional activism variables, as are CEO/board chair split, board composition, and indepen- dence of compensation committee. The correlations between the institutional activism variables are also significantly high. All regression analyses for Tables 4 and 5 were conducted in two steps. In the first step, which involved only one regression (Base Model in Table 4), the proportion of contingent compensation was regressed on all control variables and the three board mechanism vari- ables. In the second step, the predictor and the moderator variables were added to further explain the proportion of contingent compensation. Regression results from the first step provided a basis for their comparison with those from the second step. Regression estimates are reported in Table 4 and Table 5. Table 6 shows changes in the R 2 s across different models and the F statistics for such changes. Table 4 documents the results of panel data analyses for the base model, as well as Hypotheses 1, 2, and 3 for the period 1996 to 2002. The observations for the dependent variables    Note: H1 = Hypothesis 1; H2 = Hypothesis 2; H3 = Hypothesis 3. a. The numbers in parentheses are Student's t statistics. For simplicity of presentation, we did not produce \u03b1's and \u03b2's in equations (1) and (2), respectively, in the table. Note that H1 proxy, H1 nonproxy, H3 proxy, and H3 nonproxy models were not described in the Theory and Hypotheses section. We enclosed them here to show the effects of proxy-based activism and of non-proxy-based activism separately. \u2020p < .10. *p < .05. **p < .01. ***p < .001. Note: H4a = Hypothesis 4a; H4b = Hypothesis 4b; H5a = Hypothesis 5a; H5b = Hypothesis 5b; H6a = Hypothesis 6a; H6b = Hypothesis 6b. a. The interaction terms for Hypotheses 4a, 5a, and 6a are products between cumulative proxy-based activism and CEO/chair split, board composition, and compensation committee independence, respectively, whereas the interaction terms for Hypotheses 4b, 5b, and 6b are for cumulative non-proxy-based activism. The numbers in parentheses are Student's t statis- tics. For simplicity of presentation, we did not produce \u03b1's and \u03b2's in equations (1) and (2), respectively, in the table. \u2020p < .10. *p < .05. **p < .01. ***p < .001. were from 1997 to 2002. The base model shows that important variables influencing CEO contingent compensation include CEO/board chair split, sales, total assets, Tobin's q, beta, CEO tenure, pressure-resistant institutional ownership, pressure-sensitive institutional own- ership, and blockholder ownership. The positive coefficient associated with beta implies that the higher a firm's systematic risk, the more likely it will be to reward its CEO in terms of contingent compensation. A positive Tobin's q indicates that the better the performance of a firm, the higher its CEO's contingent compensation. The negative lagged contingent pay coefficient suggests that the proportion of contingent compensation has displayed a mean reversion tendency. The estimated parameter values indicate that the contingent compensation for the CEO is market based. Note: H1 = Hypothesis 1; H2 = Hypothesis 2; H3 = Hypothesis 3; H4a = Hypothesis 4a; H4b = Hypothesis 4b; H5a = Hypothesis 5a; H5b = Hypothesis 5b; H6a = Hypothesis 6a; H6b = Hypothesis 6b. a. The table reports the changes in R 2 s and the F statistics associated with such changes. The F statistic is calculated as follows: (R u 2 \u2212 R r 2 ) / J F = ------ (1-R u 2 ) / (n-k) where R u 2 and R r 2 are the R-squares for the unrestricted and restricted models (see Note 9), respectively; J is the number of restricted parameters, n is the number of total observations, and k is the number of parameters in the unrestricted model. In each cell, the top number is the change in R 2 (to three decimal places to show complete details of each change), and the bottom number is the F statistic. Note that the models listed in the first row serve as comparators to which the models listed in the first column are compared. \u2020p < .10. *p < .05. **p < .01. ***p < .001. We tested Hypothesis 1 by three regressions for only cumulative proxy-based activism, for only cumulative non-proxy-based activism, and for cumulative proxy-based and cumu- lative non-proxy-based activism combined. The coefficient of the cumulative proxy-based activism is positive and statistically significant (p < .01). One additional proxy-based activism seems to have increased the proportion of contingent compensation by 2% when we controlled for firm size. Thus, Hypothesis 1 is supported. However, the coefficient of the cumulative non-proxy-based activism is statistically insignificant, whether tested indepen- dently or in combination with proxy-based activism. To test Hypothesis 2, we used regression equation (1). The new activism variables mea- sured the number of activisms reported in year t, the same year for which the contingent compensation proportion was measured. In Canada, a firm's proxy circular is usually issued to its shareholders a few months after shareholder proposals are filed with its board, which may be a few weeks-or even a few months-before the final voting on a CEO's compen- sation package. In some cases, a CEO's compensation might be realized a full year after the shareholder proposals are filed. The non-proxy-based activism takes place around the annual meeting and also before the realization of a CEO's compensation. Because of the time lag between institutional activism and the approval and realization of a CEO's compensation, it is appropriate to align the occurrence of institutional activism and the realization of CEO compensation to measure the short-term impact of the former on the latter. For the same rea- son, it is also appropriate to use the cumulative activism to measure the long-term effect. The estimates show a positive and statistically significant long-term effect of cumulative proxy- based activism. The total long-term effect is about 10.60% (0.03 + 0.03/[1 -(-0.71)] + 0.10/[1 -(-0.71)]) for proxy-based activism, which is larger than the 10% short-term effect of proxy-based activism. For non-proxy-based activism, however, the short-term effect is only significant at 10% confidence level, whereas the long-term effect is not significant. When the long-term effects are aggregated in terms of magnitude, then the total aggregated long-term effects become 47.83% (0.03 + 0.03/(1 -0.71) + 0.10/(1 -0.71)). This clearly shows that the effect of institutional activism is stronger in the long-term than over the short- term. So Hypothesis 2 is supported. The last three columns of Table 4 show results for Hypothesis 3. Consistent with Hypothesis 3, the effects of proxy-based activism appear to be stronger than for the effects of non-proxy- based activism. The coefficients associated with cumulative proxy-based activism and new proxy-based activism are of expected positive signs and statistically significant, whereas the coefficients associated with non-proxy-based activism are all insignificant. Regression analy- ses (the last two columns of Table 4) for proxy-based and non-proxy-based activism demon- strate that only proxy-based activism is statistically associated with the proportion of contingent pay, which the corresponding F statistic corroborated. The F statistics are the ones in row 6 and column 3 and row 7 and column 4 of Table 6. Although the first F statistic demon- strates that proxy-based activism significantly improves the model, the second F statistic pro- duced no improvement at all. So Hypothesis 3 is supported. Table 5 presents results for Hypotheses 4a, 4b, 5a, 5b, 6a, and 6b. When the CEO and board chair roles are separated, cumulative proxy-based activism and new proxy-based activism exert their expected effect on contingent CEO compensation (p < .001). The coefficient of the interaction term between cumulative proxy-based activism and CEO/board chair split also shows the expected positive sign, which is not statistically significant. So we found no moderating effect of CEO/board chair split on proxy-based activism, and thus, Hypothesis 4a is not supported. The third column of Table 5 shows that cumulative non-proxy-based activism would be negatively associated with CEO contingent compensation when CEO/board chair roles are not split-that is, when CEO duality exists. However, the coefficient of interaction between cumulative non-proxy-based activism and CEO/board chair split is of an expected positive sign and statistically significant (p < .01). Therefore, Hypothesis 4b is supported. Table 5 also demonstrates the effect of cumulative proxy-based activism on CEO contin- gent compensation (p < .001) as well as the effect of its interaction with board composition (p < .05). However, the coefficients of the interaction terms are statistically negative for both Hypotheses 5a and 5b. Therefore, these hypotheses are not supported. Results for Hypotheses 6a and 6b appear in the last two columns of Table 5. These columns show that the interaction between cumulative proxy-based activism and compensa- tion committee independence does not affect CEO contingent compensation. Moreover, because the coefficient of the cumulative proxy-based activism is also not significant, Hypothesis 6a is not supported. However, the interaction effect between non-proxy-based activism and compensation committee independence is much more pronounced and with an expected positive sign. Therefore, Hypothesis 6b is supported. Finally, Table 6 illustrates how the introduction of activism variables and the interaction terms improves results derived from the base model and from the regression models for Hypothesis 1. The models listed in the first row serve as comparators for the models listed in the first column. For example, the R 2 for Hypothesis 4a is .046 higher than that for Hypothesis 1 (proxy-based). This improvement is significant, given its F statistic of 85.53. Table 6 also confirms that proxy-based activism improves R 2 s more significantly than do non-proxy-based activism. Interaction terms for non-proxy-based activism help explain con- tingent pay more often than the corresponding interaction terms for proxy-based activism. More specifically, the last column of Table 6 shows how the models for Hypotheses 4b, 5b, and 6b improve in relation to the model for Hypothesis 3 (non-proxy-based). The Durbin-Watson statistics for all hypotheses show no autocorrelation in the residuals of regression equations.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Discussion and Conclusion", "text": "Our study is particularly important, given the role that institutional ownership plays in the governance of large, publicly traded corporations in Canada (e.g., SHARE, 2005;Yang et al., 2007) and the correspondingly steady rise in the levels of CEO compensation during the last decade. From a power perspective (e.g., Mintzberg, 1983;Ocasio, 1994;Pfeffer & Salancik, 1978), we argued that institutional investors would engage in political actions to exert their influence on CEO compensation. Using panel data econometric techniques, we tested nine hypotheses that resulted from a triangulation of institutional activism, board mechanisms, and CEO compensation. Besides confirming the three main hypotheses, our results also con- firmed two interaction hypotheses. Given the stringent nature of the estimation techniques and the inclusion of an encompassing list of control variables in the regression equations, we consider our results robust. We found that both the level of CEO contingent pay (and also of total pay) and the instances of institutional activism have been increasing in Canada from 1996 to 2002. This finding corroborates the results of other empirical research showing that the contingent com- ponent of CEO compensation has been on the rise in both Canada (e.g., Zhou, 2000) and the United States (e.g., Conyon, 2006). Our findings reinforce the essence of agency theory (Fama & Jensen, 1983;Jensen & Meckling, 1976); that is, boards of directors allow increases in the contingent component of compensation to align a CEO's interests with those of the stockholders. Coupled with the findings of David et al. (2001), showing that institu- tional owners can pressure management to invest in R&D projects, our results also suggest that institutional investors can influence CEO compensation through proxy-based activism. This finding corroborates what Alchian and Demsetz (1972), David et al., Mangel and Singh (1993), and Tihanyi et al. (2003) emphasized: Although ownership is a source of power, it must be translated into some kind of tangible action for influencing strategic decisions involving managerial opportunism. In the case of institutional ownership, this tangible action is activism. The potential for outrage from the public and the negative consequences for the CEO's and directors' employment reputations leads to a coupling of board power and insti- tutional activism, which, in turn, results in an increase in the contingent proportion of CEO compensation. Because contingent compensation is tied to firm performance, directors are able to deflect pressure from institutional investors by inflating the level of this component. We found support for our hypothesis that the effect of institutional activism on the pro- portion of contingent CEO compensation is stronger in the long-term than in the short-term. The effects vary across proxy-based and non-proxy-based activism. The long-term effect of proxy-based activism on contingent compensation is very strong, whereas the effect was nonsignificant for non-proxy-based activism. Overall, this finding is intuitive. A long-term compensation contract requires an increased level of compensation for additional risk on the CEO's part (e.g., Lambert & Larcker, 1991;Wiseman & Gomez-Mejia, 1998), which is mit- igated with an increased level of CEO contingent compensation when management was faced with proxy-based activism. Our finding that the effect of proxy-based activism on CEO contingent compensation is positive and significant supports the findings of David et al. (2001), Gillan and Starks (2000), and Hartzell and Starks (2003). We also found, consistent with Mitchell et al.'s (1997) theory of stakeholder salience and its empirical verification (e.g., David et al., 2007;Neubaum & Zahra, 2006), that proxy-based activism increases the salience of institutional owners' demands to the executives of the firm. Moreover, from the intertemporal nature of relationships between diverse institutional investors and firm management, our results make intuitive sense. When institutions want to interfere with a firm's decision on CEO compen- sation, they resort directly to proxy-based activism. A combination of many different insti- tutional owners seems unlikely to be involved in a time-consuming series of interactions with the board. In addition, if management has a history of ignoring the potential of suitable compromises, institutions turn directly to proxy-based activism. Because public knowledge of proxy-based activism implies a risk for the company's board in terms of outrage costs (Bebchuk & Fried, 2004), employment reputation (Fama & Jensen, 1983), and for the CEO in terms of prestige and human capital devaluation (Conyon, 2006), the mere possibility of a resolution or proxy contest is sufficient to goad management into concrete actions on CEO compensation. Recent reports of withdrawn proposals in proxy circulars indicate that, in many cases, management is serious about satisfying the proposed filer's demand (see Key Proxy Vote Surveys conducted by SHARE, 2005). It should, then, come as no surprise as to why proxy-based activism had such a significant effect on the con- tingent proportion of a CEO's compensation. Building on the power perspective, we also hypothesized that the interaction of the two levers of activism (proxy-based and non-proxy- based) and three commonly used, yet much debated, board control mechanisms (CEO/board chair split, board composition, and compensation committee independence) would affect the proportion of CEO contingent compensation. Our finding that only the interaction of non- proxy-based activism and CEO board chair split affected the proportion of CEO contingent compensation was a bit surprising. Moreover, contrary to our expectations, we did not find support for any interaction effect either of proxy-based or of non-proxy-based activism with board composition. The interaction of proxy-based activism and compensation committee independence did not affect CEO compensation, but the latter's interaction with non-proxy- based activism did. Lack of support for the four interaction hypotheses-one for CEO/board chair split, two for board composition, and the other for compensation committee independence-appears to be intriguing but not surprising. First, all these three board monitoring mechanisms can serve as substitutes. As Udayasankar, Das, and Lim (2006) found, a need for CEO/board chair split is redundant when the board is dominated by outside directors. Intuitively, truly independent, out- side board members are able to offset problems associated with CEO duality, making CEO/board chair split redundant. Arguably, of the three control mechanisms considered, the independence of the compensation committee is the most relevant as far as CEO compensation is concerned. If board members are truly independent and powerful, this committee can serve as a substitute for the other two control mechanisms. Together, the interactions of non-proxy- based activism with both CEO/board chair split and compensation committee independence might have negated the interactions of proxy-based activism with the full set of control mech- anisms. Over the years, institutional activism has become, in and of itself, a less costly but more effective constraint on self-serving top executives (Pound, 1992;Smith, 1996), resulting in the reduction of managerial opportunism (Useem, 1996). It is from this particular view of the role of institutional investors that their activism could be considered a genuine substitute for the tra- ditional monitoring mechanisms (Rediker & Seth, 1995). Our findings partly square with Hoskisson et al.'s (2002) conclusion that institutional investors and directors substitute for each other. Because effective monitoring mechanisms and institutional activism are aimed at guarding against managerial opportunism, proxy- based activism, largely because of its salience, might have neutralized the role of the three selected control mechanisms. This assertion is consistent with our results in Table 6. As non- proxy-based activism was not as salient, the associated risk was not incorporated in the CEO's compensation package, but nonetheless, boards react to their occurrences. Two con- clusions would seem logical from the analysis of our results on interactions. First, the dichotomy between complementary and substitution effects of alternative governance mech- anisms is definitely context bound, as our results suggest. Whereas proxy-based activism substituted all three board control mechanisms, non-proxy-based activism rather moderated them. Therefore, further research is needed to shed additional light on the role of alternative governance mechanisms. Second, there remains the possibility of a mediated, rather than moderated, set of relationships among institutional activism, CEO/board chair split, board structure, and CEO compensation. Methodologically, mediation requires strong causal mod- eling, which can also distinguish the direct and indirect effects of the independent/control variables. With additional board control or alternative governance mechanisms, such a medi- ated set of relationships is certainly worth pursuing. In summary, our findings are important for three reasons. First, they seem at odds with the notion that institutional investors are out there to depress the level of CEO compensation (e.g., David et al., 1998;Hartzell & Starks, 2003;Khan et al., 2005). Our results reinforce the fact that a significant increase in CEO contingent pay, which is now the case in both Canada and the United States, must have resulted in an increase in the total pay. Therefore, our overall finding-showing a positive relationship between CEO contingent pay and insti- tutional activism-is congruent with what agency theory would ideally predict. This predic- tion, which our results corroborate, reflects the political reality involved in the process of setting CEO compensation. Because the proportion of contingent pay is intended to induce risk taking on the part of the CEO by making his or her stakes in the firm's performance high, institutional investors as well as residual stockholders are more forgiving of this com- ponent of a CEO's compensation. A board takes advantage of this weakness with large con- tingent compensation to its CEO. Second, the relationship between CEO contingent compensation and proxy-based activism might be attributed to different reasons, but the most notable appears to be the con- tiguity of the vast and attractive continental executive labor market. Moreover, even within Canada, the supply of highly skilled CEOs is relatively inelastic, leading to an increase in their compensation. Given these explanations, from the positive activism-contingent com- pensation relationship, we might conclude that institutions have achieved some success in linking CEO pay with firm performance. Third, recent evidence suggests that CEO salaries in Canada increased mainly because of the manifold increase in stock option grants and restricted stock options (McFarland, 2006;Zhou, 2000). Because both option grants and restricted stocks are key components of con- tingent pay, it is important to determine the extent to which the CEO has contributed to such gains. What happens when a CEO steers a company flying on its own, because of a momen- tum in the external environment? During the past few years, the fortunes of a large number of Canadian companies have soared because of steeply increasing commodity prices, such as oil and gas. What can institutional investors do to make stock option plans filter out stock price increases that are not attributable to the actions of a CEO? In fact, this is the issue insti- tutional activists should be most concerned about.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Notes", "text": "1. A KPMG (1995) study of 239 companies listed on the TSE found that total CEO pay increased by 18% in 1994, whereas according to Statistics Canada, the average pay of unionized workers rose by only 1.8% during the same year (Hameon, 1995). After about 10 years, according to a survey done by the Globe and Mail in 2003 (McFarland, 2004), Canada's top executives received an average 30% increase in bonuses, whereas their base salaries increased by 6.3%. By contrast, the Canadian workers' annual wage averaged $36,200 in 2003. Moreover, the same survey revealed that CEO compensation averaged about 8.3% more in 2003 than in 2002. 2. Following its takeover by the Standard & Poor's Corporation of New York on May 2002, the TSE became S&P/TSX Composite Index, and tougher criteria for size and liquidity were imposed. As a result, the Index no longer contains a fixed number of companies. Because this change occurred in mid-2002 and our sample period ended in 2002, we refer to the TSE throughout this article. 3. Until October 1993, the OSC required only group disclosure of compensation data for individuals who were identified as executives. In the United States, however, the Securities and Exchange Commission (SEC) initiated changes in the Securities Exchange Act of 1934, and effective October 15, 1992, the SEC now instructs U.S. cor- porations to provide shareholders with more understandable reports on executive compensation. Accordingly, U.S. corporations are required to disclose all compensation awarded to the CEO and to the four most highly compen- sated executive officers. 4. Both Magnan et al. (1995) and Zhou (2000) found a sales elasticity of executive compensation in Canadian firms that was even greater than that for U.S. firms. In Canada, an increase of 10% in firm size results in an increase of about 2.8% in compensation for executives, whereas in the United States, the same 10% increase in firm size will produce a corresponding 2.3% increase in executive compensation. 5. On the surface, we had 609 usable observations, as we had data on 87 firms for 7 years. In our regression analysis, we introduced the lagged compensation variable, thus reducing the usable observations to 522. 6. Jensen, Murphy, and Wruck (2004) recommend the use of the Black-Scholes model with appropriate down- ward adjustments for early exercise and forfeiture to calculate the cost of the stock option to the granting firm. We did not make adjustments to reflect early exercise and forfeiture, as we did not have access to these data. In addi- tion, some researchers theorize that the intrinsic model, which assigns option value to the difference between the fair market value of the stock and the exercise price, is better because it handles the vesting issue more accurately. However, to amortize the stock option expenses over the vesting period, one must have information on the vesting schedule for each issue of stock option for each company, and this is very difficult to obtain. This difficulty explains why studies on executive compensation have consistently used the Black-Scholes model to calculate the cost of stock options. 7. According to the Canadian Business Corporations Act, activist institutions are not required to disclose their ownership data, and we found no evidence that activist institutions have disclosed those data. Because of this nondis- closure, we were unable to provide complete data regarding activist institutions' ownership of target companies. 8. Proxy-based activism was measured in terms of the number of proposals being filed in our regression analysis. The result is not qualitatively different from the case when the number of instances of proposals is used. 9. When only one type of activism is considered, the resulting model is a constrained regression. In compari- son, both proxy-based activism and non-proxy-based activism are examined in an unconstrained regression model. 10. The total long-term effect is calculated by [coefficient of cumulative non-proxy-based activism / (1-coefficient on lagged dependent variable)]. See David et al. (2001) for details.", "title": "Institutional Activism Types and CEO Compensation: A Time-Series Analysis of Large Canadian Corporations \u2020", "file_name": "Chowdhury and Wang - 2009 - Institutional Activism Types and CEO Compensation.pdf"}
{"section": "Abstract", "text": "The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N = 2,696) and with an online sample (N = 737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences-conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects. Word count = 151 Many Labs 3: Evaluating participant pool quality across the academic semester via replication University participant pools provide access to participants for a great deal of published behavioral research. The typical participant pool consists of undergraduates enrolled in introductory psychology courses that require students to complete some number of experiments over the course of the academic semester. Common variations might include using other courses to recruit participants or making study participation an option for extra credit rather than a pedagogical requirement. Research-intensive universities often have a highly organized participant pool with a participant management system for signing up for studies and assigning credit. Smaller or teaching-oriented institutions often have more informal participant pools that are organized ad hoc each semester or for an individual class. To avoid selection bias based on study content, most participant pools have procedures to avoid disclosing the content or purpose of individual studies during the sign-up process. However, students are usually free to choose the time during the semester that they sign up to complete the studies. This may introduce a selection bias in which data collection on different dates occurs with different kinds of participants, or in different situational circumstances (e.g., the carefree semester beginning versus the exam-stressed semester end). If participant characteristics differ across time during the academic semester, then the results of studies may be moderated by the time at which data collection occurs. Indeed, among behavioral researchers there are widespread intuitions, superstitions, and anecdotes about the \"best\" time to collect data in order to minimize error and maximize power. It is common, for example, to hear stories of an effect being obtained in the first part of the semester that then \"disappears\" in a follow-up study collected at the end of the semester. Beliefs about this variation can be so strong that some laboratories adopt policies to avoid data collection during 5 particular time periods. Are these concerns warranted? There is some evidence that individual differences among participants vary slightly across the academic semester (Table 1), but there is almost no evidence to indicate whether that variation on average has any impact on the detectability and effect magnitudes of correlational or experimental results. We investigated variation in detectability of 10 previously reported effects across 20 participant pools (N = 2,696) and an online resource (N = 737). Time of Semester Effects: Legitimate Concern or Superstition? Concerns about time-of-semester effects are not new. The existing evidence supports the belief that participants at the beginning of the semester are different on average than participants at the end of the semester. However, the differences are modest. For example, later participation in the semester is related to lower levels of conscientiousness (Witt, Donnellan, & Orlando, 2011) and higher levels of openness to experience (Aviv, Zelenski, Rallo, & Larsen, 2002; see Table 1). In addition, individuals who participate late in the semester show lower intrinsic motivation when compared to those who participated earlier (Hom, 1987; Nicholls, Loveless,", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "text": "University participant pools provide access to participants for a great deal of published behavioral research. The typical participant pool consists of undergraduates enrolled in introductory psychology courses that require students to complete some number of experiments over the course of the academic semester. Common variations might include using other courses to recruit participants or making study participation an option for extra credit rather than a pedagogical requirement. Research-intensive universities often have a highly organized participant pool with a participant management system for signing up for studies and assigning credit. Smaller or teaching-oriented institutions often have more informal participant pools that are organized ad hoc each semester or for an individual class. To avoid selection bias based on study content, most participant pools have procedures to avoid disclosing the content or purpose of individual studies during the sign-up process. However, students are usually free to choose the time during the semester that they sign up to complete the studies. This may introduce a selection bias in which data collection on different dates occurs with different kinds of participants, or in different situational circumstances (e.g., the carefree semester beginning versus the exam-stressed semester end). If participant characteristics differ across time during the academic semester, then the results of studies may be moderated by the time at which data collection occurs. Indeed, among behavioral researchers there are widespread intuitions, superstitions, and anecdotes about the \"best\" time to collect data in order to minimize error and maximize power. It is common, for example, to hear stories of an effect being obtained in the first part of the semester that then \"disappears\" in a follow-up study collected at the end of the semester. Beliefs about this variation can be so strong that some laboratories adopt policies to avoid data collection during particular time periods. Are these concerns warranted? There is some evidence that individual differences among participants vary slightly across the academic semester (Table 1), but there is almost no evidence to indicate whether that variation on average has any impact on the detectability and effect magnitudes of correlational or experimental results. We investigated variation in detectability of 10 previously reported effects across 20 participant pools (N = 2,696) and an online resource (N = 737).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Time of Semester Effects: Legitimate Concern or Superstition?", "text": "Concerns about time-of-semester effects are not new. The existing evidence supports the belief that participants at the beginning of the semester are different on average than participants at the end of the semester. However, the differences are modest. For example, later participation in the semester is related to lower levels of conscientiousness (Witt, Donnellan, & Orlando, 2011) and higher levels of openness to experience (Aviv, Zelenski, Rallo, & Larsen, 2002; see Table 1). In addition, individuals who participate late in the semester show lower intrinsic motivation when compared to those who participated earlier (Hom, 1987;Nicholls, Loveless, Thomas, Loetscher, & Churches, 2014). Research on variation in actual task performance, however, has produced mixed results. For instance, Wang and Jentsch (1998;N = 49) asked participants to complete a cued recall task, testing their memory for the English meanings of 24 learned foreign words after a 30-minute period. They found no significant difference in cued recall between the earliest and latest participants over the course of four semesters. Openness .14 -.01 Note: Values represent Pearson's r between personality trait and week of participation ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Research Questions", "text": "The present project is informally called \"Many Labs 3\" as it follows the model established in two prior investigations for conducting the identical procedure in many different laboratories ( Klein et al., 2014;Klein et al., 2015). In Many Labs 3, we investigated the extent to which 10 psychological effects and multiple individual difference variables varied across the academic semester. The same experimental procedure was administered in 20 participant pools at institutions in the United States and Canada. This allowed us to investigate the extent to which participant characteristics and the magnitudes of different effects vary across the academic semester. If time of semester effects were observed, we also obtained a Mechanical Turk sample (MTurk; N = 737) to help distinguish between time of semester effects (unique to students) versus time of year effects. A secondary interest was to provide additional evidence about the included effects using large scale replication: their overall effect size, variation by site and sample, and moderation by time of semester. Some of the effects we included are heavily studied, but others are relatively new or have not been replicated frequently enough to clarify boundary conditions or moderating influences. The final materials and dataset will be of substantial use beyond this initial report, particularly to explore moderating influences not examined for this report. All data and materials are available for additional investigation by others (https://osf.io/ct89g/).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Participants", "text": "An open invitation for researchers to participate as a data collection site was issued in early 2014 for data collection to occur from August through December. To be eligible for inclusion, participating labs agreed to administer the study procedure to at least 80 participants total with at least 40 from the first half of the semester and at least 40 from the second half of the semester. To ensure that teams were operating on similar academic calendars, participation was limited to institutions in the United States and Canada. Twenty teams completed the data collection with the average sample size being 135.40 (SD = 63.00), ranging from 45 to 321 (see Table S1 for details of each team and Table S2 for characteristics of each participant pool). One team was unable to meet the minimum participant cutoff (N = 45), but earned authorship through other contributions. Their data are included in the aggregate set and all subsequent analyses. Overall, 69.8% of the sample was female, the average age was 19.3 years (SD = 3.7), and 53.7% were White, 9.4% Black, 16.0% Asian, 10.6% Hispanic, and 10.3% other. These participants came from a wide range of institutions, producing a relatively diverse undergraduate sample. Although all of the directly replicated effects collected data from undergraduate participants, the current sample differs in a few ways. None of the original study collection sites are represented in the current sample. Two original studies recruited undergraduates independent of a participant pool, and two other original studies were conducted at European institutions. Finally, the current sample has a heavier representation of females compared to original studies that reported this demographic (55.5%). Sample differences that seem particularly relevant are noted in the descriptions of each effect. We simultaneously collected participants from MTurk over the same time period (N = 737) as a comparison sample for time of year effects and sample diversity. In the MTurk sample, 48.6% of the sample was female, the average age was 35.1 years (SD = 10.9), and 66.4% were White, 15.4% Asian, 7% Black, 4.7% Hispanic, and 6.5% other. This sample was drawn from the United States and there were no requirements for previous MTurk experience (e.g., minimum number of previous HITs completed). MTurk participants received $1.25 as compensation for their time.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Selection of Effects", "text": "The primary aim of the project was to detect possible variability in effect magnitudes across the academic semester when using university participant pools. To obtain a candidate list of effects and individual difference measures, we held a round of open nominations and invited submissions for any effect that fit the defined criteria. Those nominations were supplemented by ideas from the project team and from direct queries to independent experts in psychological science. Given the areas of interest of the project coordinators and most collaborators, nominations came largely from the fields of social and personality psychology. The coordinating team sought effects and individual difference measures that fit the following criteria: (1) highly feasible implementation through a web browser or in the lab, (2) brevity of study procedures, and (3) high interest value of the theoretical domain or phenomenon. In addition, for the collected set of effects and measures we sought: (1) diversity of represented research domains, (2) diversity of known or presumed likelihood of variation across the semester, and (3) diversity of \"classic\" well-established effects and contemporary effects that have untested replicability. The project coordinating team collectively evaluated the nominated studies (see Table S3 for a list of considered effects). No specific researcher was \"targeted\" for replication because of concerns or skepticism about an effect. In fact, any included effect that was not reproducible at all would produce little insight about variation across the semester, which was the central research question for this project. Given this, one strategy would have been to only select classic, well-established effects for replication. However, it is possible that these effects are well established because they are resistant to contextual variation. Had we selected only well- established effects, we could have undermined the possibility of observing context effects. Our presumption was that time-of-semester effects are most likely to occur for so-called \"fragile\" effects that might be particularly sensitive to context. As such, we included high-profile, contemporary effects with less certain replicability, particularly from domains in which popular debate suggests fragility or sensitivity to context. This project was most concerned with detecting whether or not time of semester variation happens in regular research practices. Therefore, if we had limited our effects to one or two research domains (e.g., effects moderated by attention, Nicholls et al., 2014), we might have maximized testing \"can semester variation alter effects?\", but sacrificed testing \"does time of semester variation alter effects?\" in ordinary research practice. Furthermore, reduced attention can be reasonably hypothesized as moderators for many effects, even if they have not been previously demonstrated as influential. In other words, we aimed to examine time-of-semester as the highly available explanation when two behavioral lab studies show different results, whatever the topic of study. Once selected for inclusion, a member of the research team contacted the corresponding author (if alive) to obtain the original study materials and get advice about adapting the procedure for use in this study. 1 In particular, we asked the original authors if there were moderators or other limitations to obtaining the result that would be useful for the team to understand in advance or to anticipate during data collection. The team implemented a draft of the proposed study procedure and solicited feedback from the original authors to further improve the design. This process was undertaken to minimize reasons to expect different outcomes between the original outcomes and the replications. Sometimes this led to adaptations of the procedure in order to maximize its relevance in the present context, or changes to fit the constraints of the present procedure (see Table S4 for a summary of procedure adaptations). Also, some initially selected effects were eliminated during review if we could not address a priori design concerns effectively. We implemented a draft study procedure to pre-test for length. Data collection constraints required completion of all study materials within 30 minutes. A pilot sample of 30 volunteers completed the on-line portion of the study procedure. We calculated the time required for 85% of participants to complete each study procedure. Following this piloting, we needed to remove three individual difference measures, shorten one procedure (Stroop task), and eliminate two effects to meet the time constraints. After this intensive review, 10 effects, 10 individual difference measures, 3 data quality indicators, and a selection of demographics items were confirmed for inclusion in Many Labs 3. In administration of the actual procedure, we did not impose a 30 minute time constraint, but individual data collection sites could let participants go before data collection completion if circumstances demanded it. 97.2% of non-MTurk participants completed the entire study.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Procedure", "text": "The study procedures and materials were reviewed and approved by the University of Virginia Institutional Review Board for the Social and Behavioral Sciences as well as IRBs from all other participating institutions. Eight of the effects were administered in a single computerized experiment script that began with informed consent, then presented the procedures for each target effect in a random order, then presented the ten individual difference measures and three data quality indicators, and closed with demographics items and debriefing. Two of the effects could not be administered via computer, one because the participants were required to hold the measures in their hands (Weight Embodiment) and another because the original author suggested that it required a paper- pencil administration format (Metaphoric Restructuring). As such, the participant was instructed to go to the experimenter for instructions at a random point during presentation of the eight computerized tasks. At this point, the two \"in-person\" tasks were administered in a counterbalanced order. The script for the experiment and video simulations of experiment administration are available publicly (https://osf.io/ct89g/). The procedure for the MTurk sample was the same except that we removed the two \"in- person\" tasks and one of the computer-administered tasks that involved deception and concerned an issue at the participant's university (Elaboration Likelihood).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Demographics Measures", "text": "Age. Participants noted their age in years in an open-response box. Sex. Participants selected \"male\" or \"female\" to indicate their biological sex. Data Quality Indicators. Several items at the end of the study, just prior to the demographics items, assessed carelessness or lack of effort. Participation questions. To assess the quality of the participant's engagement in the study, we asked: \"How much effort did you put into the tasks during this experiment?\" (1 = no effort to 5 = I tried my hardest) and \"How closely did you pay attention to the instructions and tasks during the experiment?\" (1 = none to 5 = I gave the tasks my undivided attention). Participants also responded to items assessing: (1) whether they were participating as part of a class requirement, extra credit, payment, or other; (2) the type of class that required/incentivized this participation (i.e., introductory course in psychology, secondary/upper- division course in psychology, any class above secondary, research methods/statistics course, or other); and, (3) if required, how close they were to completing their subject pool requirements (this is my first study, about 25% done, about 50% done, about 75% done, this is my last study, I am not participating for a class requirement). Instructional attention check. The instructional attention check presented a paragraph of instructions in which the last sentence read: \"So, in order to demonstrate that you have read the instructions, please ignore the preferences form below, and simply write 'I read the instructions' in the box below.\" Immediately below this paragraph is an item saying \"In my free time I prefer:\" with response options of (1) engaging in hobbies, (2) watching TV, reading, music, (3) being in nature, (4) exercising, (5) cooking or eating, and (6) other (with an open response area for writing in the correct answer).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Individual Difference Measures", "text": "Brief individual difference measures were selected as possible moderators of psychological effects based on prior evidence that participant characteristics vary across the semester or because of their widespread use in psychological science. Table 2 shows the descriptive statistics for all of the individual differences measures (see Table S5 for correlations among these measures). When comparisons were available, reliabilities for measures were similar to or better than prior uses.  was assessed with two items on 7-point response scales from 1 (disagree strongly) to 7 (agree strongly). Reliabilities are somewhat lower than other, longer scales, but the five scales show satisfactory retest reliabilities (cf. Gnambs, 2014) and substantial convergent validities with longer Big Five instruments (e.g., Ehrhart et al., 2009;Gosling et al., 2003;Rojas & Widiger, 2014). Daily Mood (adapted from Schwarz & Clore, 1983). We measured daily mood using two items that assess the extent to which the participant is in a good or bad mood. Items begin with the same statement, \"Today I generally feel...\" Each set of response options are on a 7-point Likert scale, ranging from 1 (very unhappy) to 7 (very happy), and 1 (very bad) to 7 (very good). Perceived Stress Scale -short form (Cohen, Kamarck, & Mermelstein, 1983). We measured perceived stress over the last week using a 4-item short-form scale that is an alternative to the original, 14-item Perceived Stress Scale (Cohen, et al., 1983). Participants respond on a 5-point Likert scale, ranging from 0 (never) to 4 (very often). The original study suggested that the shortened scale was relatively reliable (\u03b1 = .72) and the factor structure was consistent with the long form.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Need for Cognition Scale (adapted from Cacioppo & Petty, 1982; Skulborstad,", "text": "unpublished data). We measured need for cognition with six items that ask about the degree to which the participant enjoys engaging in complex, deliberative, and abstract thinking. Each of the items are on a 5-point Likert scale, ranging from 1 (extremely uncharacteristic) to 5 (extremely characteristic). Following past research we selected the top six factor loading items of the original scale (e.g., Verplanken, 1991;Verplanken, Hazenberg, & Palenewen, 1992; Skulborstad, unpublished data). We used this shortened version instead of the 34 item (Cacioppo & Petty, 1982) or 18 item versions (Petty, Cacioppo, & Kao, 1984) because of time constraints.  .67 1-5", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "The Effects", "text": "Next, we describe the 10 selected effects with an abstract reporting the main idea of the original research with the sample size, inferential test, and effect size. Details on the methodology and analysis plan that was defined in the pre-registered protocol for each effect can be found presented in the supplementary material (https://osf.io/ct89g/). We report the aggregate result of the replications at the end of each subsection; these results are summarized in Figures   1a, 1b, and Table 3. The focus of this replication project is to estimate the variability in effect magnitude by time of semester. As such, we aimed to identify or simplify original study designs that could be tested as two-condition experiments or as correlations when possible. Some original studies had additional conditions that were relevant for the theoretical purposes of the investigation. In those cases, the replication designs identified the key conditions relevant for estimating the effect. Also, in some cases, multiple dependent variables were included in the original design. If the dependent variables could be administered quickly, they were usually retained in the replication. When multiple outcomes were included, because they are likely to be correlated, just one or an aggregate was identified as the primary object for replication and examining variation across the semester; the others were considered secondary. Secondary outcome measures are reported in footnotes or the supplemental material. Finally, correspondence with original authors during the design process identified some potential moderating influences that could be examined with additional analyses.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Stroop task (Stroop, 1935)", "text": "In the Stroop task (Stroop, 1935), participants view words one at a time in different colors. Participants categorize the color of the font and do not need to do anything with the meaning of the word. This task is more difficult when there is a discrepancy between the color of the font and the word. For example, it is easier to categorize the font as \"blue\" when it is presented on the word \"tree\" or the word \"blue\" compared to being presented on the word \"red.\" The meaning of the word \"red\" interferes with categorization of the font color as \"blue.\" This task is very robust and has been used in thousands of research applications (MacLeod, 1991). Effects on the Stroop task can be larger when participants are tired, or otherwise cognitively or emotionally depleted, because they have fewer available resources to overcome the response competition. In the present study, we incorporated a simple version of the Stroop task to test whether similar variation would be observed across the semester cycle. The Stroop task is a within-person experiment with two response conditions -font color congruent with color word and font color incongruent with color word -and response latency as a dependent variable. We used the D scoring algorithm for analysis of these data (Greenwald, Nosek, & Banaji, 2003), an analysis technique that has general application to response latency contrasts (Nosek & Sriram, 2007) and avoids confounding influences in response latency comparisons that influence other analytic techniques (Sriram, Greenwald, & Nosek, 2010 scenarios describing the location of an object in reference to a stick figure (referred to as \"you\"). Participants in the object-moving condition saw scenarios in which two objects were described in relation to one another. Participants indicated whether the statement about the picture was true or false. On the second page, participants read an ambiguous temporal statement (e.g., \"Next Wednesday's meeting has been moved forward two days\") and indicated to which day the meeting had been rescheduled (e.g., \"Monday\" or \"Friday\") and how confident they felt about their choice from 1 (not at all confident) to 5 (very confident Based on the original author's recommendations, this task was completed on paper-and- pencil in the face-to-face portion of the study to ensure comparability to the original procedure, and three conditions were included: ego-prime, object-prime, and control. We excluded trials compared to congruent trials (M = .016, SD = .049), t (3337 participants from the analyses if, in the priming condition, they failed to answer all four priming questions (see materials) correctly, or if, in any condition, they failed to select one of the two possible correct options for the day of the meeting (Monday or Friday). In the in-lab replication studies (N = 2,191), ego-priming was more likely to induce the answer of Friday (67.8%) than Monday (32.2%), whereas object-priming showed a bias in the same direction but to a lesser extent with Friday (59.5%) being more popular than Monday (40.5% ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "1973, Study 3)", "text": "Tversky and Kahneman (1973) examined whether undergraduates recruited separately from a participant pool would overestimate the frequency of easier-to-imagine words relative to harder-to-imagine words. People find it easier to think of English words that begin with a certain letter (k, l, n, r, or v) than to think of words with this letter in the third position. However, these letters actually show up about twice as often in the third position compared to the first position. Participants judged whether each of these letters was more likely to show up in the first or the third position and estimated the ratio of the frequency with which they appear in each position. Tversky and Kahneman (1973) found that 105/152 participants judged the first position to be more frequent for the majority of letters and that 47/152 participants judged the third position to be more frequent for the majority of letters. The authors reported that a sign test (Grissom & Kim, 2012 , and the effect size was much stronger than with the original estimation strategy (note that Figure 1a shows data for the original estimation strategy). 4 (Table 3 here, located at end of document for reference) The current study examined variation in persistence across the semester. To conceptually replicate the relation between persistence and conscientiousness, we used the unsolvable anagram task, which has been used as a measure of persistence (e.g., Aspinwall & Richter, 1999;Sommer & Baumeister, 2002). In this task, participants are presented with a number of anagrams to unscramble. Some anagrams are solvable, others are not. Participants choose to stop working on the task whenever they would like. Persistence is the amount of time spent on the task before moving on to the next task.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "The relation between persistence and conscientiousness (De", "text": "Unlike the others, this is not a direct replication. The original work examined the correlation between self-perception of persistence and a long-form personality measure using a clinical sample. We added this effect as a conceptual replication because persistence and In an aggregate analysis of the replication studies (N = 2,969), participants in the high-power (M = 3.75, SD = 1.55) and low-power (M = 3.80, SD = 1.57) conditions thought that the sincerity of the message would be interpreted similarly, t (2967 In the replication studies, we excluded participants from the analyses if the experimenter noted any behavior that would have diffused the weight of the clipboard (e.g., sitting down, resting the clipboard on a table). Across all in-lab replications (N = 2,285), participants in the heavy (M = 6.16, SD = 1.02) and light (M = 6.14, SD = 1.03) clipboard conditions believed that it 5 We tested whether the length of participants' responses to the power prime (measured as the number of characters in their response) moderated the effect of high versus low-power conditions on sincerity ratings. However, we did not find a reliable Condition \u00d7 Response Length interaction, F(1, 2961) = 0.39, p = .53, r = .01. 6 Additional analyses controlling for participants' mood and for task difficulty did not change the direction of the effects, though controlling for mood did weaken the effect (p = .095). was similarly important for the university committee to listen to the students' opinions, t (2283 Cacioppo and colleagues (1983) investigated the impact of argument strength on persuasion, inviting participants who scored in the upper or lower third on the Need for Cognition Scale (Cacioppo & Petty, 1982) to participate. Participants either read a set of strong or weak arguments concerning the institution of comprehensive exams for undergraduates at their university. Afterwards, participants rated the quality of the arguments and how persuaded they were by them. They found that participants found stronger arguments to be more compelling than weaker arguments overall, F(1, 110) = 160.86, p < .001, \u03b7 p \u00b2 = .59, 95% CI = 8 We constructed a hierarchical multivariate model testing the effect of the manipulation (reading about a communal or agentic individual) with the additional predictors of gender of the individual in the prompt, participant gender, actual room temperature (step 1), and the interaction between target and participant gender (step 2) predicting the participant's temperature estimate of the room. Only the actual room temperature reliably predicted the participants' temperature estimation, F(1, 1,824) = 160.74, p < .001, r = .28. All other predictors were not significant (ps > .41). [.47, .67]. However, participants who were high in need for cognition showed this effect more strongly than those low in need for cognition, F(1, 110) = 22.45, p < .001, \u03b7 p \u00b2 = .17, 95% CI = [.06, .29]. This study demonstrated that the quality of a persuasive message impacts people differently depending on the extent to which they process the message. We conducted a similar test using linear regression to predict ratings of argument quality The reliability of the need for cognition scale used (\u03b1 = .67), was lower than has been observed for the full 34 item scale (\u03b1 = .87, Cacioppo & Petty, 1982). This reduction in reliability would be expected to attenuate the target effect. However, given the statistical power of the sample, it is unlikely that this attenuation would solely eliminate the effect. It could also be that low need for cognition participants are more against comprehensive exams at baseline. However, need for cognition was not reliably related to ratings of argument quality, F(1, 2,361) = 2.386, p = .123, \u03b7 p \u00b2 = .001, 95% CI = [0, .005].", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "It feels like yesterday: Self-esteem, valence of personal past experiences, and judgments of subjective distance (Ross & Wilson, 2002, Study 2)", "text": "According to the theory of temporal self-appraisal, time is a psychological variable that can vary by \"closeness.\" Closeness refers to an individual's perception of the temporal distance between the past and the present irrespective of the actual temporal distance. For example, a person may have gotten married 15 years ago, but that experience might \"feel like\" it occurred much more recently. Ross and Wilson (2002) examined how subjective temporal distance varies when recalling negative compared to positive events and whether differences in self-esteem may be associated with how distant events subjectively feel, irrespective of how distant they actually are. Overall, participants were expected to feel further from negative events compared to positive events in order to buffer their self-worth against the implications those negative events have for current self-view. Because individuals with high self-esteem are more motivated to preserve their self-worth, the authors hypothesized that individuals with high self-esteem would show this effect more strongly than individuals with low self-esteem. They randomly assigned students (N = 357) to reflect either on a positive or negative academic experience. In the positive condition, participants identified the best grade they received in the previous semester. In the negative condition, participants identified the worst grade they received in the previous semester. Participants then reported how distant the course felt to them and how often they thought about this course since it ended. From a hierarchical regression model (with actual time since the class as step 1, the main effects of self-esteem and condition as step 2, and the interaction of self-esteem and condition as step 3) there was an interaction between self-esteem and condition when predicting ratings of subjective distance, .019]. Also, self-esteem weakly predicted subjective distance, independent of condition, with higher self-esteem predicting closer subjective distance, F ( ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Moral credentials and the expression of prejudice (Monin & Miller, 2001, Study 1)", "text": "Monin and Miller (2001) tested whether participants were more willing to express prejudicial attitudes when their prior behavior provided evidence that they were non-prejudiced. Two hundred two undergraduates (115 men and 87 women) were approached on a university campus by the experimenter to complete an anonymous survey. This survey first asked whether five statements were right or wrong. These statements expressed sexist views, and were either phrased as describing \"most women\" or \"some women,\" with the intent of inducing greater agreement with the \"some\" statements as opposed to the \"most\" statements. The authors For the replication design, we included only the \"some\" and \"most\" conditions. In an aggregate analysis of all replication studies (N = 3,134), there was a main effect of moral ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Results by site, task order, and time of semester", "text": "For each data collection site, we computed the number of days in which the participant pool was available during the semester. For each participant, their participation date was normalized by dividing the day that they participated by the total number of days available such that participation on the first day of the pool was (1/total days) and participation on the last day of the pool was 1 (see Figure S1 for distribution of participation). This accounted for the fact that some participant pools were open for longer periods than others (e.g., sites using semesters compared to quarters). This value was tested as a moderator of the association of effect of condition for each of the outcome measures in the study.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Effects", "text": "The primary aim of the pre-registered design and analysis plan was to evaluate variation in effects across the academic semester. In the first stage of analysis, we examined the aggregate effect sizes without testing whether those effects varied across the semester. Those results, reported above in the introduction to each effect, suggested that some of the primary replication effects had effect sizes near 0. It was possible that this aggregate result would reveal a positive effect at some points in time and a negative effect at other points in time. However, it was also possible that this indicated a uniformly null result. If the latter, then we would have no opportunity to learn about variation across the academic semester from those effects. As a consequence, prior to conducting tests of variation across time, we decided to add three theoretically relevant main effects for studies in which the key test was an interaction effect that did not occur (Elaboration Likelihood, Self-Esteem and Subjective Distance, Credentials and Prejudice). Variation by site. For each effect, we computed an aggregate effect size estimate with 99% confidence intervals. Figures 1a and 1b represent the effect size estimates for each of the data collection sites for each effect. We also computed the variability in effect estimates following standard statistics for meta-analyses-Q and I 2 -to determine if the amount of variability across samples exceeds that expected by random error. With identical study procedures, variability exceeding expectations of sampling error is likely attributable to variation in the effect due to sample or setting. These analyses are presented in Table 4. Overall, two effects, Self-Esteem and Subjective Distance and Credentials and Prejudice, showed signs of inter-site variation. For both effects, the interactions (I 2 = 39.25%, p = .026; I 2 = 28.17%, p = .068, respectively) and main effects (I 2 = 35.81%, p = .063; I 2 = 40.31%, p = .023, respectively) showed small to moderate variation, according to meta-analytic standards ( Higgins et al., 2003). All other effects showed little inter-site variation (Q < 22.40, p > .288).  Table 3). Heterogeneity tests conducted with R-package metafor. REML was used for estimation for all tests. Variation by task order. Across the session, effects may weaken if participants get fatigued or if prior measures interfere with subsequent measures. To investigate this possibility, we conducted moderator analyses on each of the 10+3 effects, testing for linear and quadratic order effects (see Table 5 for summary and Table S6 for other tests of order effects). Overall, we observed very little variation by task order (average \u03b7 p \u00b2 = .0003 for effects with non-binomial outcomes, average d = .04 for effects with binomial outcomes). In addition, we analyzed the data from each effect when it was presented first in the task sequence. Comparing these results to the aggregate results revealed little variation. Metaphoric Restructuring was slightly weaker when presented first (\u0394d = -.18) and Availability was slightly stronger (\u0394d = .17). The Elaboration Likelihood main effect was also slightly stronger (\u0394\u03b7 p \u00b2 = .04) when presented first. All other effects showed similar strength. ( Table 5 here, located at end of document for reference) Variation by time of semester. Our primary interest was in the variation of effects across the academic semester. For each of the 10 replicated effects (and 3 additional main effects) we first constructed an unconditional model, predicting the outcome variable from a fixed intercept and a random intercept of site. This was to determine the amount of variation in outcome variables between sites before examining time of semester variation in effect detection. For all but two of the models, site accounted for 1.1% of the variance or less in the dependent variable. There were non-trivial site effects for the persistence measure (5.0%; Persistence and Conscientiousness) and for temperature (22%; Warmth Perceptions). Students at some sites were more persistent with the anagrams than at other sites, and some lab rooms were perceived as warmer than others. Otherwise, there was little variation in the dependent variables by site. Then, we constructed a mixed effects model, with the Time of Semester \u00d7 Replication Independent Variable(s) as a fixed effect. We included a random intercept of site and random slope of the fixed effect by site. For many models, this random slope overparameterized the model, and was thus simplified or dropped. The final model for each effect was compared to a model without Time of Semester as a fixed interaction to test whether adding Time of Semester provided a better fit for the data. 15 We performed these analyses on participant pool participants first, and we planned to use the MTurk sample as a comparison when time of semester variation was observed. See Table 6 for a summary of variation by semester analyses. (Table 6 here, located at end of document for reference) There was little evidence for variation by time of semester for most effects. Of the 13 tested effects, model fit comparisons provided very weak evidence for three effects with slight With so little evidence for a time of semester effect, we conducted a follow-up exploratory analysis comparing data from the first 80% of the semester to the last 20% of the semester. This was to focus the test on the intuition that the inattentive or unmotivated participants are those that complete studies at the very end of the semester. Results are summarized in supplementary Table S7. Again, we found little evidence of variation in effect magnitudes, observing the largest difference for Metaphoric Restructuring. The large number of comparisons suggests caution in interpreting this effect, however. Overall, the data revealed little evidence for variation in effect magnitudes by time of semester. Even when just considering effects that replicated in aggregate, only two of six effects showed hints of time of semester variation (Stroop, Metaphoric Restructuring). In both cases, the effects were actually larger toward the end of the semester compared to the rest of the semester.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Data Quality Indicators", "text": "Participants reported fairly high levels of effort (M = 3.71, SD = .78; Scale 1 = no effort to 5 = tried my hardest) and attention (M = 3.92, SD = .74; Scale 1 = none to 5 = I gave my undivided attention), and 37.3% failed the instructional attention check, similar to prior demonstrations with this challenging check (Oppenheimer, Meyvis, & Davidenko, 2009;Hauser & Schwarz, in press). Participants demonstrated some awareness of their attention levels. Participants who passed the attention check reported higher attention (M = 4.03, SD = .70) than those who failed the check (M = 3.78, SD = .77), t (2606 Only one effect, Availability Heuristic, was reliably moderated by performance on the attention check, with those who failed the check actually showing a stronger effect (p = .032, d = .08; see Table S8 for a full summary of results). The attention check did not moderate any of the time of semester effects observed either (ps > .512). To analyze time of semester variation in these data quality indicators, we constructed mixed effects models predicting the data quality indicators with Time of Semester as a fixed effect and a random intercept of Site (see Table 6). We compared these models to models without Time of Semester as a fixed effect. Unconditional models revealed that 2.5% of variance in Reported Effort, 1.6% of the variance in Reported Attention, and 4% of the variance in the Attention Check was explained by inter-site variation. This suggests more variation in effort and attention across sites than variation in responses on most of the dependent variables. ", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Demographics", "text": "To observe demographic trends over the semester, we constructed mixed effects models predicting participant sex, age, ethnicity, and year in school from Time of Semester as a fixed effect and a random intercept of Site (see Table 6 for a summary). Time of Semester only reliably improved the model for participant sex, \u03c7 2 (1, N = 2598) = 17.57, p < .001. Participants were more likely to be male as the semester progressed, r(2598) = .12, p < .001, 95% CI = [.08, .16].", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Individual Differences", "text": "To evaluate variation across the semester, we constructed linear mixed effects models testing each of the 10 individual difference variables (see Table 6). We compared a model with Time of Semester as a fixed effect and a random intercept of Site with a model lacking the Time of Semester fixed effect. These model comparisons revealed that Time of Semester reliably improved models for conscientiousness (p < .001), mood (p = .005), and stress (p = .001). Follow up analyses showed that as the semester progressed, participants were less conscientious, r(2626) = -.14, p < .001, 95% CI = [-.18, -.10], reported worse mood, r(2634) = -.07, p = .001, 95% CI = [-.10, -.03], and reported being more stressed, r(2621) = .08, p < .001, 95% CI = [.04, .12]. All of these effects were small, and none of the other individual differences were reliably moderated by time of semester. Finally, in exploratory analyses, we investigated whether the data quality indicators or individual differences that varied over the semester moderated the effects that varied over the semester (Stroop, Metaphoric Restructuring). However, none of these data quality indicators or individual differences moderated Stroop or Metaphoric Restructuring (all p's > .253, see supplementary materials for details).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Discussion", "text": "This crowdsourced project evaluated whether variation in effect magnitudes can be partially attributed to the time of semester of data collection. The answer from the 10+3 investigated effects is largely no. Detected effects had similar effect sizes regardless of when data collection occurred and effects that were not detectable during some part of the semester were not detectable at any point during the semester. Consistent with literature showing that Stroop effects are sensitive to the availability of cognitive resources to overcome response competition (Kane & Engle, 2003), the Stroop effect was slightly stronger toward the end of the semester (last 20% d = .92) compared to the beginning (first 80% d = .89), but even that effect was very small. Also, there was a hint of stronger effects for Metaphoric Restructuring at the end of the semester compared to earlier. All told, effects showed little to no moderation by time of semester, site of data collection, and order in which the tasks were administered. Qualifying the generality of the conclusion, of the ten original effects we examined, only three replicated the original result, regardless of the time of semester. After observing this, but prior to testing time of semester effects, we added three successful main effect replications. These provided no additional evidence for time of semester effects. Examining only the reliable effects, two of the six showed any time of semester variation, and those two effects became very slightly stronger, not weaker, in the latter parts of the semester. The conclusions would be more definitive had a greater proportion of the effects shown a reliable result. Moreover, the selection of effects was by no means a random selection or representative sample of all possible effects. As such, the present results provide a provocative, but constrained conclusion. With a very high-powered design, time of semester was largely irrelevant for estimating the magnitude of experimental and correlational effects. The extent to which the present results will generalize across replicable experimental and correlational effects is unknown.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "What does change across the academic semester", "text": "If effects do not change across the semester, what does? The present study replicated and extended prior observations ( Nicholls et al., 2014;Witt et al., 2011). As the semester progressed, participants reported slightly less effort and attention, were slightly more likely to fail an attention check, were slightly less conscientious, had slightly worse mood, had slightly higher stress, and had slightly higher representation of men compared to women. These effects are regularly hypothesized and easily recognized by frequent users of participant pools even though time of semester accounts for only about 1% of the variance in each. As such, participant characteristics did shift slightly across the semester, but these shifts had little impact on the detectability of the tested correlations and experimental results. In fact, these indicators suggest slightly weakening data quality later in the semester, but the two effects that did change actually showed stronger effects toward the end.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Moderation of effects", "text": "A common explanation for the challenges of replicating results across samples and settings is that there are many seen and unseen moderators that qualify the detectability of effects (Cesario, 2014). As such, when differences are observed across study administrations, it is easy to default to the assumption that it must be due to features differing between the samples and settings. Besides time of semester, we tested whether the site of data collection, and the order of administration during the study session moderated the effects. Whether the task was administered first, in the middle, or last had minimal impact on the investigated effects. This is consistent with the first \"Many Labs\" study ( Klein et al., 2014). This suggests against the possibility that there is something about the procedure of combining studies into a single session that disrupts detectability of effects. We did observe some evidence of variation by sample or setting for main effects and interactions of two of the ten studies, Self-Esteem and Subjective Distance and Credentials and Prejudice. These are demonstrations of a truism in social psychology -that effects vary by sample and setting. If anything, it is notable that sample and setting variation was not more prevalent. Investigating variation by sample and setting is the focus of the second \"Many Labs\" study with many samples and societies included in the study ( Klein et al., 2015). Another potential moderator of well-known effects is participant knowledge. For example, Elaboration Likelihood and Availability Heuristic are often taught in introductory psychology classes. If students learned about these effects in their courses, it is possible that this would reduce observed effects (see, however, Lambdin & Shaffer, 2009). However, if that were the case, we would expect to observe time of semester variation on classic effects, as students would presumably learn about these effects sometime during the academic term, making them less detectable near the end of the term. In addition, students would likely vary in their knowledge of these effects from site to site, as different lessons would be taught at different universities. Given the lack of variation from these two sources this seems unlikely to have occurred.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Insights about the Selected Effects", "text": "We were surprised that several effects showed null effects in our large sample. The present study's very large sample size and lack of moderating effects by site, order, and time of semester does provide precision and some definitiveness about these paradigms under these conditions. However, under these conditions, is a critical qualifying phrase. The present results do not definitively suggest that the observed nulls are always null, nor do they definitively suggest that the original positive results are false positives. What can be concluded is that those effects are not distinguishable from zero with the samples, settings, materials, and procedure employed here. Even among effects that did replicate in aggregate, we observed smaller effect sizes compared to the original demonstrations. Although past replication projects have observed similar declines in effect strength (Open Science Collaboration, 2015), there are several possible explanations for the declines observed in this study. For instance, in some cases, the original materials or methods were altered to accommodate the constraints of this investigation. 16 Based 16 Detailed explanations of all known alterations are located in Supplementary Information: Methods for Selected Effects and a summary of alterations can be found in Table S4. on a priori theorizing and review these alterations were not expected to alter the results of the replications substantially. 17 Even so, such changes might have had unexpected influences. For example, the original weight importance study was conducted with a Dutch sample. Our samples were from North America, and we did not anticipate this change to qualify the effect based on the current theoretical understanding. Across our 20 sites, we observed mean scores in both conditions of the weight importance study that were within a point of the ceiling reducing the power to detect an effect. The original study had lower means particularly in the light clipboard condition making it an outlier by comparison. It is possible that the change in samples is responsible for the shift in means. Another possibility is that the original study's lower means, particularly in one condition, were an unusual chance occurrence. Parsing between these possibilities requires conducting a replication that includes the original (Dutch) population. Furthermore, many of the theories invoked by the selected effects have been demonstrated using various methods. For our purposes, we had to select a single instantiation. Thus, our results can only speak to those instantiations, not necessarily the broader theory. A better theoretical understanding of each effect, and the theories they are derived from, will be achieved when the conditions for influencing the effect magnitude are articulated and demonstrated empirically. The present evidence and further explorations of the dataset may provide useful hypothesizing for how to begin that search. A notable procedural difference between this and the original studies is that the effects were investigated in a single experimental protocol. It is reasonable to hypothesize that this procedural difference produced smaller effects in the replications particularly if they occur later in the ~30 minute protocol. The present evidence suggests that this did not occur, particularly because the order of tasks did not moderate the observed effects, including considering only the first task completed. Moreover, the first Many Labs project ( Klein et al., 2014) had a similar procedure and reliably detected 10 (and the 11th weakly) of 13 effects, with some effects producing larger effect size estimates compared to the original. An untested possibility is that the procedure weakened effects of even the first task completed because participants anticipated doing many tasks. However, because there were no order effects in this study, such an influence would need to be equal to the disruptive impact of having just experienced the other tasks. While we do not find this idea to be particularly plausible, it would be straightforward to test in new research. Three of the investigated interaction effects did not replicate: Elaboration Likelihood, Self-Esteem and Subjective Distance, and Credentials and Prejudice. In all three cases, a theoretically relevant main effect was observed. For Credentials and Prejudice, we observed the effect of credentials on prejudice, but that effect was not qualified by gender (Monin & Miller, 2001). Our perception is that the interaction effect is much less theoretically essential than the main effect of credentials. In fact, Monin and Miller (2001) did not anticipate an interaction; it emerged unexpectedly in their first study and did not persist in their second study, which used a different manipulation of credentials. As such, the present replication can be seen as affirming their original theoretical expectations, and disconfirming the unexpected moderation by gender. Likewise, for Self-Esteem and Subjective Distance (Ross & Wilson, 2002), we did observe that positive past events felt closer than negative past events, but we did not replicate the moderation of this effect by self-esteem. In our view, the main effect is most vital theoretically. Finally, in Elaboration Likelihood ( Cacioppo et al., 1983), we observed that stronger arguments were more persuasive than weaker arguments, but this was not qualified by need for cognition. The failure to replicate this interaction is one of the most surprising results from this study. The Elaboration Likelihood Model is among the most developed and empirically investigated theories in psychology (Petty & Bri\u00f1ol, 2012). Our result would seem to be an outlier in the literature, albeit a highly precise one. In light of this, it is important to note that we only tested one instantiation relevant to this theory. Post hoc, we examined possible moderators to account for the difference, but did not find support for any of them. We do not have an explanation for why no effect was observed under these circumstances.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Additional Analysis Opportunities", "text": "The amassed dataset is rich for exploring the individual effects, individual difference variables, interactions between the two, and alternate ways to analyze the aggregate data. Our analysis plan for the main article focused on time of semester variability and not, for example, exploring moderating influences in depth. However, the data set and all materials are available publicly to encourage further investigations by others (visit https://osf.io/ct89g/).", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Conclusion", "text": "Conventional wisdom among behavioral scientists suggests that the time of semester for data collection from participant pools is an important factor for obtaining effects. Our powerful design across 20 participant pools found more evidence against this conclusion than for it. We did find evidence that the characteristics of the sample changes across the semester, but those changes did not alter detection of the selected effects. Should researchers now discount conventional wisdom? Therein is the incompleteness of any single investigation. The present results are the only known large-scale investigation of the influence of time of semester on a variety of effects. As such, the present results should give pause to speculative invocations of time of semester as an explanatory factor. At the same time, conventional wisdom often has a basis in experience. It is possible that there are some conditions under which the time of semester impacts observed effects. However, it is unknown whether that impact is ever big enough to be meaningful.", "title": "Many Labs 3: Evaluating participant pool quality across the academic semester via replication", "file_name": "Preprint - Many Labs 3.pdf"}
{"section": "Current Incentive Structures Discourage Replication", "text": "The ultimate purpose of science is the accumulation of knowledge. The most exciting science takes place on the periphery of knowledge, where researchers suggest novel ideas, consider new possibilities, and delve into the unknown. As a consequence, innovation is a highly prized scientific contribution, and the generation of new theories, new methods, and new evidence is highly rewarded. Direct replication, in contrast, does not attempt to break new ground; it instead assesses whether previous innovations are accurate. As a result, there are currently few incentives for conducting and publishing direct replications of previously published research (Nosek, Spies, & Motyl, 2012). Current journal publication practices discourage replications (Collins, 1985;Mahoney, 1985;Schmidt, 2009). Journal editors hope to maximize the impact of their journals, and are inclined to encourage contributions that are associated with the greatest prestige. As a consequence, all journals encourage innovative research, and few actively solicit replications, whether successful or unsuccessful (Neuliep & Crandall, 1990). An obvious response to these publication practices is to create journals devoted to publishing replications or null results. Of multiple attempts to start such a journal over the last 30 years, success is fleeting. Several versions exist today (e.g., http://www.jasnh.com/; http://www.jnr-eeb.org/; http://www.journalofnullresults.com/), but challenges remain: journals that publish what no other journal will publish ensures their low status ). It is not in a scientist's interest to publish in low status journals. Because prestigious journals do not provide incentives to publish replications, researchers do not have a strong incentive to conduct them (Hartshorne & Schachner, 2012a;Koole & Lakens, 2012). Scientists make reasonable assessments of how they should spend their time. Publication is the central means of career advancement for scientists. Given the choice between replication and pursuing novelty, career researchers can easily conclude that their time should be spent pursuing novel research. This may be especially true for researchers that do not yet have academic tenure. Complicating matters is the presence of additional forces rewarding positive over negative results. A common belief is that it is easier to obtain a negative result erroneously than it is to obtain a positive result erroneously. This is true when using statistical techniques and sample sizes designed to detect differences (Nickerson, 2000), and when designs are underpowered (Cohen, 1962;Sedlmeier & Gigerenzer, 1989;Lipsey & Wilson, 1993). Although both of these features are common, researchers can design studies so that they will be informative no matter the outcome (Greenwald, 1975). There are many reasons why a null result may be observed erroneously such as imprecise measurement, poor experimental design, or other forms of random error (Greenwald, 1975;Nickerson, 2000). There are also many reasons why a positive result may be observed erroneously such as introducing artifacts into the research design (Rosenthal & Rosnow, 1969), experimenter bias, demand characteristics, systematic apparatus malfunction, or other forms of systematic error (Greenwald, 1975). Further, false positives can be inflated through selective reporting and adventurous data analytic strategies (Simmons et al., 2011). There is presently little basis other than power of research designs to systematically prefer positive results compared to negative results. Decisions about whether to take a positive or negative result seriously are based on evaluation of the research design, not the research outcome. Layered on top of legitimate epistemological considerations are cultural forces that favor significant (Fanelli, 2010(Fanelli, , 2012Greenwald, 1975;Sterling, 1959) and consistent (Giner-Sorolla, 2012) results over inconsistent or ambiguous results. These incentives encourage researchers to obtain and publish positive, significant results, and to suppress or ignore inconsistencies that disrupt the aesthetic appeal of the findings. As examples, researchers might decide to stop data collection if preliminary analyses suggest that the findings will be unlikely to reach conventional significance, examine multiple variables or conditions and report only the subset that \"worked,\" accept those studies that confirm the hypothesis as effective designs and dismiss those that do not confirm the hypothesis as pilots or methodologically flawed because they fail to support the hypothesis ( LeBel & Peters, 2011). These practices, and others, can inflate the likelihood that the results are false positives (Giner-Sorolla, 2012;Ioannidis, 2005;John, Prelec, & Lowenstein, 2012;Schimmack, 2012;Simmons et al., 2011). This is not to say that researchers engage in these practices with deliberate intent to deceive or manufacture false effects. Rather, these are natural consequences of motivated reasoning (Kunda, 1990). When a particular outcome is better for the self, then decision-making can be influenced by factors that maximize the likelihood of that outcome. Researchers may tend to carry out novel scientific studies with a confirmatory bias such that they-without conscious intent-guide themselves to find support for their hypotheses (Bauer, 1992;Nickerson, 1998).", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Reproducibility", "text": "The strong incentives to publish novel, positive, and clean results may lead to problems for knowledge accumulation. For one, the presence of these incentives leads to a larger proportion of false positives, which produces a misleading literature and makes it more likely that future research will be based on claims that are actually false. Any individual result is ambiguous; but because the truth value of a claim is based on the aggregate of individual observations, ignoring particular results undermines the accuracy of a field's collective knowledge. This occurs both by inflating the true size of the effect, and by concealing potential limitations to the effect's generalizability. Knowing the rate of false positives in the published literature would clarify the magnitude of the problem, and indicate whether significant intervention is needed. However, there is very little empirical evidence on the rate of false positives. Simulations, surveys, and reasoned arguments provide some evidence that the false positive-rate could be very high (Greenwald, 1975;Hartshorne & Schachner, 2012a;Ioannidis, 2005). For example, asking psychologists about the proportion of research findings that would be reproduced from their journals in a direct replication yielded an estimate of 53% ( Fuchs et al., 2012). The two known empirical estimates of non-random samples of studies in biomedicine provide disturbing reproducibility estimates of 25% or less (Begley & Ellis, 2012;Prinz et al., 2011). There are few other existing attempts to estimate the rate of false positives in any field of science. The theme of this article is reproducibility, and the focus of this section is on the primary concern of irreproducibility: that the original results are false. Note, however, that the reproducibility rate is not necessarily equivalent to the false positive rate. The maximum reproducibility rate is 1 minus the rate of false positives tolerated by a field. The ubiquitous alpha level of .05 implies a false-positive tolerance of 5%, meaning a reproducibility rate of 95%. However, in practice, there are many reasons why a true effect may fail to replicate. A low- powered replication, one with an insufficient number of data points to observe a difference between conditions, can fail for mathematical rather than empirical reasons. The reproducibility rate can be lowered further for other reasons. Imprecise reporting practices can inadvertently omit crucial details necessary to make research designs reproducible. Description of the methodology-a core feature of scientific practice-may become more illustrative than substantive. This could be exacerbated by editorial trends encouraging short- report formats (Ledgerwood & Sherman, 2012). Even when the chance to offer additional online material about methods occurs, it may not be taken. For example, a Google Scholar search on articles published in Psychological Science-a short-report format journal-for the year 2011 revealed that only 16.8% of articles included the phrase \"Supplemental Material\" denoting additional material available online, even without considering whether or not that material gave a full accounting of methods. As a consequence, when replication does occur, the replicating researchers may find reproduction of the original procedure difficult because key elements of the methodology were not published. This makes it difficult both to clarify the conditions under which an effect can be observed and to accumulate knowledge. In sum, both false positives and weak methodological specification are challenges for reproducibility. The current system of incentives in science does not reward researchers for conducting or reporting replications. As a consequence, there is little opportunity to estimate the reproducibility rate, to filter out those initial effects that were false positives, and to improve specification of those initial effects that are true but specified inadequately. The Reproducibility Project examines these issues by generating an empirical estimate of reproducibility and identifying the predictors of reproducibility.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "The Reproducibility Project", "text": "The Reproducibility Project began in November 2011 with the goal of empirically estimating the reproducibility of psychological science. The concept was simple: Take a sample of findings from the published literature in psychology and see how many of them could be replicated. The implementation, however, is more difficult than the conception. Replicating a large number of findings to produce an estimate of reproducibility is a mammoth undertaking, requiring much time and diverse skills. Given the incentive structures for publishing, only a person who does not mind stifling their own career success would take on such an effort on their own even if they valued the goal. Our solution was to minimize the costs for any one researcher by making it a massively collaborative project. Project design. To estimate the rate and predictors of reproducibility in the psychological sciences, we selected a quasi-random sample of studies from three prominent psychological journals (Journal of Personality and Social Psychology, Journal of Experimental Psychology: Learning, Memory, and Cognition, and Psychological Science) from the 2008 publication year -a year far enough in the past that there is evidence for variation in impact of the studies and variability in independent replication attempts, and not so far in the past that original materials would not be available. Studies were selected for replication as follows: Beginning with the first issue of 2008, the first 30 articles that appeared in each journal made up the initial sample. As project members starting attempting to replicate studies, additional articles were added to the eligible pool in groups of ten. This strategy minimized selection biases by having only a small group of articles available for selection at any one time while maintaining a sufficient number of articles so that interested replication teams could find tasks that match their resources and expertise. Each article in the sampling frame was reviewed with a standard coding procedure (http://bit.ly/rpcodearticles 2 ). The coding procedure documented: (1) the essential descriptors of the article such as authors, topic, and main idea; (2) the key finding from one of the studies and key statistics associated with that finding such as sample size and effect size; (3) features of the design requiring specialized samples, procedures, or instrumentation; and (4) any other unusual or notable features of the study. This coding provided the basis for researchers to rapidly review and identify a study that they could potentially replicate. Also, coding all articles from the sampling frame will allow systematic comparison of the articles replicated with those that were available but not replicated. Most articles contain more than one study. Since The Reproducibility Project is concerned with the state of replicability in general, a single key finding was sampled from a single study. By default, the last study reported in a given article was the target of replication. If a replication of that study was not feasible, then the second to last study was considered. If no studies were feasible to replicate, then the article was excluded from the replication sample. A study was considered feasible for replication if its primary result could be evaluated with a single inference test, and if a replication team on the project had sufficient access to the study's population of interest, materials, procedure, and expertise. Although every effort is made to make the sample representative, study designs that are difficult to reproduce for practical reasons are less likely to be included. In psychology, for example, studies with children and clinical samples tend to be more resource intensive than others. Likewise, it is infeasible to replicate some study designs with large samples, many measurements over time, a focus on one-time historical events, or expensive instrumentation. It is not obvious whether studies with significant resource challenges would have more or less reproducible findings as compared to those that have fewer resource challenges. Maximizing replication quality. A central concern for the Reproducibility Project was the quality of replication attempts. Sloppy, non-identical, or under-powered replications would be unlikely to replicate the original finding, even if that original finding was true. While these are potential predictors of reproducibility, they are not particularly interesting ones. As a consequence, the study protocol involved many features to maximize quality of the replications. As a first step, each replication attempt was conducted with a sufficient number of observations so that replications of true findings would be likely. For each eligible study, a power analysis was performed on the effect of interest from the original study. The power analysis determined the samples necessary for 80%, 90%, and 95% power to detect a statistically significant effect the same size as the prior result using the same analytic procedures. Replication teams planned their sample size aiming for the highest feasible power. All studies were designed to achieve at least 80% power, and about three fourths of the studies conducted to date have an anticipated power of 90% or higher. In another step to maximize replication quality, replication teams contacted the original authors of each study to request copies of project materials and clarify any important procedures that did not appear in the original report (http://bit.ly/rpemailauthors). As of this writing, authors of every original article have shared their materials to assist in the replication efforts, with one exception. In the exceptional case, the original authors declined to share all materials that they had created, and declined to disclose the source of materials that they did not own so that the replication team could seek permission for their use. Even so, a replication attempt of that study is underway with the replication team using its own judgment on how to best implement the study. Next, for all studies, the replication team developed a research methodology that reproduced the original design as faithfully as possible. Methodologies were written following a standard template and included measurement instruments, a detailed project procedure, and a data-analysis plan. Prior to finalizing the procedure, one or two Reproducibility Project contributors who were not a part of the replication team reviewed this proposed methodology. The methodology was also sent to the original authors for their review. If the original authors raised concerns about the design quality, the replication teams attempted to address them. If the design concerns could not be addressed, those concerns were documented as a priori concerns raised by the original authors. The evaluations of the original authors were documented as: endorsing the methods of the replication, raising concerns based on informed judgment or speculation (that are not part of the published record as constraints on the design), raising concerns that are based on published empirical evidence of the constraints on the effect, or no response. This review process minimized design deficiencies in advance of conducting the study and also obtained explicit ratings of the design quality in advance. These steps should make it easier to detect post-hoc rationalization if the replication results violate researchers' expectations. Some studies that were originally conducted in a laboratory were amenable to replication via the Internet. Using the web is an excellent method for recruiting additional power for human research, but it could also alter the likelihood of observing the original effects. Thus, we label such studies \"secondary replications.\" These studies remained eligible to be claimed for \"primary replications\"-doing the study in the laboratory following the original demonstration. As of this writing, there were more than 10 secondary web replications underway in addition to the more than 50 primary replications. This provides an opportunity to evaluate systematically whether the change in setting affects reproducibility. Upon finalization, the replication methodology was registered and added to an online repository. At this point, data collection could start. After data collection, the replication teams conducted confirmatory analyses following the registered methodology. The results and interpretation were documented and submitted to a team member (who was not part of the replication team) for review. In most cases, an additional attempt was made to contact the authors of the original study in order to share the results of the replication attempt and to consult with them as to whether any part of the data collection or data analysis process may have deviated from that of the original study. Finally, the results of the replication attempt were written into a final manuscript, which was logged in the central project repository. As additional replication attempts are completed, the repository is updated and a more complete picture of the reproducibility of the sample emerges (http://openscienceframework.org/project/EZcUj/). The project is ongoing. In principle, there need not be an end date. Just as ordinary science accumulates evidence about the truth-value of claims continuously, the Reproducibility Project could accumulate evidence about the reproducibility, and ultimately truth value, of its particular sample of claims continuously. Also, new resources provide opportunities to improve and enlarge the sample of replication studies. For example, in February 2013, the project received a grant of more than $200,000 to support replication projects. The project team formed a committee and grant application process to encourage more researchers to join the project and strengthen the study. Eventually the collaborative team will establish a closing date for replication projects to be included in an initial aggregate report. That aggregate report will provide an estimate of the reproducibility rate of psychological science, and examine predictors of reproducibility such as the publishing journal, the precision of the original estimate, and the existence of other replications in the published literature.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "What Can and Cannot be Learned from the Reproducibility Project", "text": "The Reproducibility Project will produce an estimate of the reproducibility rate of psychological science. In fact, it will produce multiple estimates, as there are multiple ways to conceive of evaluating replication (Open Science Collaboration, 2012). For example, a standard frequentist solution is to test whether the effect reaches statistical significance with the same ordinal pattern of means as the original study. An alternative approach is to evaluate whether the meta-analytic combination of the original observation and replication produces a significant effect. A third possibility is to test whether the replication effect is significantly different from the original effect size estimate. Each of these will reveal distinct reproducibility rates, and each offers a distinct interpretation. Notably, none of the possible interpretations will answer the question that is ultimately of interest: At what rate are the conclusions of published research true?", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Of the Studies Investigated, Which of Their Conclusions are True?", "text": "The relationship between the validity of a study's results and the validity of the conclusions derived from those results is, at best, indirect. Replication only addresses the validity of the results. If the original authors used flawed inferential statistics, then replicating the result may say nothing of the accuracy of the conclusion (e.g., Jaeger, 2008). Similarly, if the study used a confounded manipulation, and that confound explains the reported results rather than the original interpretation, then the interpretation is incorrect regardless of whether the result is reproducible. More generally, replication cannot help with misinterpretation. Piaget's (1952Piaget's ( , 1954 demonstrations of object permanence and other developmental phenomena are among the most replicable findings in psychology. Simultaneously, many of his interpretations of these results appear to have been incorrect (e.g., Baillargeon, Spelke, & Wasserman, 1985). Reinterpretation of old results is the ordinary process of scientific progress. That progress is facilitated by having valid results to reinterpret. Piaget's conclusions may have been overthrown, but his empirical results still provide the foundation for much of developmental psychology. The experimental paradigms he designed were so fruitful, in part, because the results they generate are so easily replicated. In this sense, reproducibility is essential for theoretical generativity. The Reproducibility Project offers the same contribution as other replications toward increasing confidence in the truth of conclusions. Findings that replicate in the Reproducibility Project are ones that are more likely to replicate in the future. The aggregate results will provide greater confidence in the validity of the findings, whether or not the conclusions are correct.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Of all Published Studies, What is the Rate of True Findings?", "text": "It is of great importance to know the rate of valid findings in a given field. Even under the best of circumstances, at least some findings will be false due to random chance or simple human error. While there is a concern that science may be far from the ideal (e.g., Ioannidis, Ntzani, Trikalinos, & Contopoulos-Ioannidis, 2001), there are little systematic data in any field and hardly any in psychology. There are at least two barriers to obtaining empirical data on the rate of true findings. The first is that accumulating such data across a large sample of findings requires a range of expertise and a supply of labor that is difficult to assemble. In that respect, one of the contributions of the Reproducibility Project is to show how this can be accomplished. The second is that as discussed earlier, failure to replicate a result is not synonymous with the result being a false positive. The Reproducibility Project attempts to minimize the other factors that are knowable and undesirable (e.g., low power and poor replication design), and to estimate the influence of others. There are three possible interpretations of a failure to replicate the results of an original study: Interpretation 1: The original effect was false. The original result could have occurred by chance (e.g., setting alpha = .05 anticipates a 5% false-positive rate), by fraud, or unintentionally by exploiting flexible research practices in design, analysis, or reporting (Greenwald, 1975;John et al., 2012;Simmons et al., 2011). Interpretation 2: The replication was not sufficiently powered to detect the true effect (i.e., the replication is false). Just as positive results occur by chance when there is no result to detect (alpha = .05), negative results occur by chance when there is a result to detect (beta or power). Most studies are very underpowered (Lipsey & Wilson, 1993;Sedlmeier & Gigerenzer, 1989;see Cohen, 1962see Cohen, , 1992. Adequate power is a necessary feature of fair replication attempts. The Reproducibility Project sets 80% as the baseline standard power for replication attempts (Cohen, 1988) and encourages higher levels of power whenever possible. The actual power of our replications can be used as a predictor of reproducibility in the analytic models, and as a way to estimate the false-negative rate among replications. For example, an average power of 85% across replications would lead us to expect a false-negative rate of 15% on chance alone.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Interpretation 3: The replication methodology differed from the original methodology on unconsidered features that were critical for obtaining the true effect.", "text": "There is no such thing as an exact replication. A replication necessarily differs somehow, or else it would not be a replication. For example, in behavioral research, even if the same participants are used, their state and experience differs. Likewise, even if the same location, procedures, and apparatus are used, the history and social context have changed. There are infinite dimensions of sample, setting, procedure, materials, and instrumentation that could be conditions for obtaining an effect. Keeping with the principle of Occam's razor, these variables are assumed irrelevant until proven otherwise. Indeed, if an effect is interpreted as existing only for the original circumstances, with no explanatory value outside of that lone occasion, its usefulness for future research and application is severely limited. Consequently, authors almost never exhaustively report procedural details when writing about effects. Part of standard research practice is to understand the conditions necessary to elicit an effect. Does it depend on the color of the walls? The hardness of the pencils used? The characteristics of the sample? The context of measurement? How the materials are administered? There is an infinite number of possible conditions, and a smaller number of plausible conditions, that could be necessary for obtaining an effect. A replication attempt will necessarily differ in many ways from the original demonstration. The key question is whether a failure to replicate could plausibly be attributed to any of these differences. The answer may rest upon what aspect of the original effect each difference violates: 1. Published constraints on the effect. Does the original interpretation of the effect suggest necessary conditions that are not part of the replication attempt? If the original interpretation is that the effect will only occur for women, and the replication attempt includes men, then it is not a fair replication. The existing interpretation (and perhaps empirical evidence) already imposes that constraint. Replication is not expected. Replication teams avoid violating these constraints as much as possible in the Reproducibility Project. Offering original authors an opportunity to review the design provides another opportunity to identify and address these constraints. When the constraints cannot be addressed completely, they are documented as potential predictors of reproducibility. 2. Constraints on the effect, identified a priori. An infinitely precise description requires infinite journal space, and thus every method section is necessarily an abridged summary. Thus, there may be design choices that are known (to the original experimenters, if to no one else) to be crucial to obtaining the reported results, but not described in print. By contacting the original authors prior to conducting the replication attempt, the Reproducibility Project minimizes this flaw in the published record. 3. Constraints on the effect, identified post hoc. Constraints identified beforehand are distinct from the reasoning or speculation that occurs after a failed replication attempt. There are many differences between any replication and its original, and subsequent investigation may determine that one of these differences, in fact, was crucial to obtaining the original results. That is, the original effect is not reproducible as originally interpreted, but is reproducible with the newly discovered constraints. The Reproducibility Project only initiates this process: For studies that do not replicate, interested researchers may search for potential reasons why. This might include additional studies that manipulate the factors identified as possible causes of the replication failure. Such research will produce a better understanding of the phenomenon.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Errors in implementation or analysis for the original study, replication study, or", "text": "both. Errors happen. What researchers think and report that they did might not be what they actually did. Discrepancies in results can occur because of mistakes. There is no obvious difference between \"original\" or \"replication\" studies in the likelihood of errors occurring. The Reproducibility Project cannot control errors in original studies, but it can make every effort to minimize their occurrence in the replication studies. For example, it is conceivable that the Reproducibility Project will fail to replicate studies because some team members are incompetent in the design and execution of the replication projects. While this possibility cannot be ruled out entirely, procedures including carefully detailed experimental protocols minimize its impact and maximize the likelihood of identifying whether competence is playing a role. Moreover, features of the replication team (e.g., relevant experience, degrees, publishing record) can be used as predictors of reproducibility. The key lesson from this section is that failure to replicate does not unambiguously suggest that the original effect is false. The Reproducibility Project examines all of the possibilities described above in its evaluation of reproducibility. Some can be addressed effectively with design. For example, all studies will have at least 80% power to detect the original effect, and the power of the test will be evaluated as a predictor for likelihood of replication. Also, differences between original and replication methods will be minimized by obtaining original materials whenever possible and by collaborating with original authors to identify and resolve all possible published or a priori identifiable design constraints. Finally, original authors and other members of the collaborative team review and evaluate the methodology and analysis to minimize the likelihood of errors in the replications, and the designs, materials, and data are made available publicly in order to improve the likelihood of identifying errors. Notwithstanding the ambiguity surrounding the interpretation of a replication failure, the key value of replication remains: As data accumulate, the precision of the effect estimate increases.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "What Practices Lead to More Replicable Findings?", "text": "Perhaps the most promising possible contribution of the Reproducibility Project will be to provide empirical evidence of the correlates of reproducibility, or to make a more informed assessment of the reproducibility of existing results. Researchers have no shortage of hypotheses as to what research practices would lead to higher replicability rates (e.g., LeBel & Paunonen, 2011;LeBel & Peters, 2011;Nosek & Bar-Anan, 2012;Vul et al., 2009). Without systematic data, there is no way to test these hypotheses (for discussion, see Hartshorne & Schachner, 2012a, 2012b). Note that this is a correlational study, so it is possible that some third factor, such as the authors' conscientiousness, is the joint cause of both the adoption of a particular research practice and high replicability. However, the lack of a correlation between certain practices and higher replicability rates is-assuming sufficient statistical power and variability -more directly interpretable, suggesting that researchers should look elsewhere for methods that will meaningfully increase the validity of published findings.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Summary", "text": "Like any research effort, the most important factor for success of the Reproducibility Project is the quality and execution of its design. The quality of the design, execution of replications, and ultimate interpretations of the findings will define the extent to which the Reproducibility Project can provide information about the reproducibility of psychological science. As with all research, that responsibility rests with the team conducting the research. The last section of this chapter, summarizes the strategies we are pursuing to conduct an open, large-scale, collaborative project with the highest quality standards that we can achieve (Open Science Collaboration, 2012).", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Coordinating the Reproducibility Project", "text": "The success of the Reproducibility Project hinges on effective collaboration among a large number of contributors. In business and science, large-scale efforts are often necessary to provide important contributions. Sending an astronaut to the moon, creating a feature film, and sequencing the human genome are testaments to the power of collaboration and social coordination. However, most large-scale projects are highly resourced with money, staff, and administration in order to assure success. Further, most large-scale efforts are backed by leadership that has direct control over the contributors through employment or other strong incentives, giving contributors compelling reasons to do their part for the project. The Reproducibility Project differs from the modal large-scale project because it started light on resources and light on leadership. Most contributors are donating their time and drawing on whatever resources they have available to conduct replications. Project leaders cannot require action because the contributors are volunteers. How can such a project succeed? Why would any individual contributor choose to participate? The Reproducibility Project team draws its project-design principles from open-source software communities that developed important software such as the Linux operating system and the Firefox web browser. These communities achieved remarkable success under similar conditions. In this section, we describe the strategies used for coordinating the Reproducibility Project so that other groups can draw on the project design to pursue similar scientific projects. An insightful treatment of these project principles and strategies is provided in Michael Nielsen's (2011) book Reinventing Discovery. The challenges to solve are the following: (1) recruiting contributors; (2) defining tasks so that contributors know what they need to do and can do it; (3) ensuring high-quality contributions; (4) coordinating effectively so that contributions can be aggregated; and (5) getting contributors to follow through on their commitments. The next sections describe the variety of strategies the project uses to address these challenges.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Clear articulation of the project goals and approach", "text": "Defining project goals is so obvious that it is easy to overlook. Prospective contributors must know what the project will accomplish (and how) to decide whether they want to contribute. The Reproducibility Project's primary goal is to estimate the reproducibility of psychological science. It aims to accomplish that goal by conducting replications of a sample of published studies from major journals in psychology. The extent to which prospective contributors find the goal and approach compelling will influence the likelihood that they volunteer their time and resources. Further, once the team is assembled, a clear statement of purpose and approach bonds the team and facilitates coordination. This goal and approach is included in every communication about the Reproducibility Project.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Modularity", "text": "Even though potential contributors may find the project goal compelling, they recognize that they could never conduct so many replications by themselves. The Reproducibility Project's goal of replicating dozens of studies is appealing because it has the potential to impact the field, but actually replicating that many studies is daunting. One solution is crowdsourcing (Estell\u00e9sArolas & Gonz\u00e1lez-Ladr\u00f3n-de-Guevera, 2012), in which work is decomposed into smaller, modular tasks that are distributed across volunteers. Modularity is the extent to which a project can be separated into independent components and then recombined later. Also, if contributors are highly dependent on each other, then the time delay is multiplicative (Davis, 1965): delay by one affects all. The Reproducibility Project is highly modularized. Individuals or small teams conduct replications independently. Some replications are completed very rapidly; others over a longer time-scale. Barriers to progress are isolated to the competing schedules and responsibilities of the small replication teams. Besides accelerating progress, modularizing is attractive to volunteer contributors because they have complete control over the extent and nature of their participation. Modularization is useful, but it will provide limited value if there are only a few contributors. One way for crowdsourcing to overcome this problem is to have a low barrier to entry.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Low Barrier to Entry", "text": "Breaking up a large project into pieces reduces the amount of contribution required by any single contributor. For volunteers with busy lives, this is vital. The Reproducibility Project encourages small contributions so that contributors can volunteer their services incrementally without incurring inordinate costs to their other professional responsibilities, or allowing unfulfilled commitments to impede workflow. Even with effective modularization, prospective contributors may have difficulty in estimating the workload required when making the initial commitment to contribute. Uncertainty itself is a formidable barrier to entry. The Reproducibility Project provides specific documentation to reduce this barrier. In particular, prospective contributors can review studies available for replication in a summary spreadsheet, consult with a team member whose role is to connect available studies to new contributors with appropriate skills and resources, and review the replication protocol that provides instruction for every stage of the process. Effective supporting material and personnel simplify the process of joining the project.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Leverage Available Skills", "text": "Collaborations can be particularly effective when they incorporate researchers with distinct skill sets. A problem that is very difficult for a non-expert may be trivial for an expert. Further, there are many potential contributors that do not have resources or skills to do the central task: conducting a replication. In any large-scale project, there are additional administrative, documentation, or consulting tasks that can be defined and modularized. The Reproducibility Project has administrative contributors with specified roles and contributors who assist by documenting and coding the studies available for replication. There are also consultants for common issues such as data analysis.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Collaborative Tools and Documentation", "text": "As a distributed project, the Reproducibility Project coordination must embrace asynchronous schedules. Communication among the entire team occurs via an email listserv (https://groups.google.com/group/openscienceframework?hl=en) that maintains a record of all communications. New ideas, procedural issues, project plans, and task assignments are discussed on the listserv. Decisions resulting from team discussion are codified in project documentation that is managed with Google Docs and the Open Science Framework (OSF; http://openscienceframework.org/). Print documentation is extensive, as it is the primary means of providing individual contributors with knowledge of (1) what is happening in the project; (2) their role in the project; and (3) what they must do to fulfill their role. The project documentation defines the overall objective of the project, tables of sub-goals and actions necessary to achieve them, protocols for conducting a replication project and templates for communicating results. This workflow is designed to maximize the quality of the replication, make explicit the standards and expectations of each replication, and minimize the workload for the individual contributors. With a full specification of the workflow, templates for report writing, and material support for correspondence with original study authors, the replicating teams can smoothly implement the project's standard procedures and focus their energies on the unique elements of the replication study design and data collection to conduct the highest quality replication possible. Unlike modular replications, administrative tasks require frequent and timely upkeep and can impact the workflow of other team members. Thus, although initially run by volunteers, dedicated administrative support was needed as the project increased in scale. Together, documentation and dedicated administrators provide continuity in the projects' objectives and methods across time and individual replication teams. The highly defined workflow also makes it easy to track progress of one's own replication-and those of others. Each stage of the project has explicitly defined milestones, described in the project's researcher guide, and team members denote on the project tracksheet when each stage is completed. At a glance, viewers of the tracksheet can see the status of all projects. Besides its information value, tracking progress provides normative information for the research teams regarding whether they are keeping up with the progress of other teams. Without that information, individual contributors would have little basis for social comparison and also little sense of whether the project as a whole is making progress.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Light Leadership with Strong Communication", "text": "Large-scale, distributed projects flounder without leadership. However, leadership cannot be overly directive when volunteers staff the project. Project leaders are responsible for facilitating communication and discussion and then guiding the team to decisions and action. Without someone taking responsibility for the latter, projects will stall with endless discussion and no resolution. To maximize project investment, individual contributors should have the experience that their opinions about the project design matter and can impact the direction of the project. Simultaneously, there must be sufficient leadership to avoid having each contributor feel like they shoulder inordinate responsibility for decision-making. Contributors vary in the extent to which they desire to shape different aspects of the project. Some have strong opinions about the standard format of the replication report; others would rather step on a nail than spend time on that. To balance this, the Reproducibility Project leadership promotes open discussion without requiring contribution. Simultaneously, leadership defines a timeline for decision-making, takes responsibility for reviewing and integrating opinions, and makes recommendations for action steps.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Open Practices", "text": "The Reproducibility Project is an open project. This means that anyone can join, that expectations of contributors are defined explicitly in advance, and that the project discussion, design, materials, and data are available publicly. Openness promotes accountability among the team. Individuals have made public commitments to project activities. This transparency minimizes free-riding and other common conflicts that emerge in collaborative research. Openness also promotes accountability to the public. Replication teams are trying to reproduce research designs and results published by others. The value of the evidence accumulated by the Reproducibility Project relies on these replications being completed to a high standard. Making all project materials available provides a strong incentive for the replication teams to do an excellent job. Further, openness increases the likelihood that errors will be identified and addressed. In addition to public accessibility, the Reproducibility Project builds in error checking by requiring each replication team to contact original authors to invite critique of their study design prior to data collection, and by having members review and critique each others' project reports.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Participation Incentives", "text": "Why participate in a large-scale project? What is in it for the individual contributor? The best designed and coordinated project will still fail if contributors have no reason to participate voluntarily. The Reproducibility Project has a variety of incentives that may each have differential impact on individual contributors. For one, many contributors have an intrinsic interest in the research questions the project has set out to answer, or more generally view the project as an important service to the field. Another class of incentives is experiential. Some contributors want to belong to a large- scale collaboration, try open science practices, or conduct a direct replication. For some, this may be for the pleasure of working with a group or trying something new. For others, this may be conceived as a training opportunity. Other incentives are the more traditional academic rewards. The most obvious is publication. Publication is the basis of reward, advancement, and reputation building (Collins, 1985). Contributors to the Reproducibility Project earn co- authorship on publication about the project and its findings. The relative impact for each individual contributor is most certainly reduced by the fact that there are many contributors. However, the nature of the research question, the scale of the project, and (in our humble brag opinion) quality of the endeavor mean that the project may have a high impact on psychology and science more generally. While no contributor will establish a research career using publications with the Open Science Collaboration exclusively, authorship on an important, high- profile project provides an added bonus for the more intrinsic factors that motivate contributions to the Reproducibility Project.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Conclusion", "text": "The Reproducibility Project is the first attempt to systematically, empirically estimate the reproducibility of a subdiscipline of science. It draws on the lessons of open-source projects in software development: leveraging individuals' opinions about how things should be done while providing strong coordination to enable progress. What will be learned from the Reproducibility Project is still undetermined. But, if the current progress is any indicator, the high investment of its contributors and the substantial interest and attention by observers suggest that the Reproducibility Project could provide a useful initial estimate of the reproducibility of psychological science, and perhaps inspire other disciplines to pursue similar efforts. Systematic data on replicability does not exist. The Reproducibility Project addresses this shortcoming. If large numbers of findings fail to replicate, that will strengthen the hand of the reform movements and lead to a significant reevaluation of the literature. If most findings replicate satisfactorily-as many as would be expected given our statistical power estimates- then that will suggest a different course of action. More likely, perhaps, is that the results will be somewhere in between and will help generate hypotheses about particular practices that could improve or damage reproducibility. We close by noting that even in the best of circumstances, the results of any study- including the Reproducibility Project-should be approached with a certain amount of skepticism. While we attempt to conduct replication attempts that are as similar as possible to the original study, it is always possible that \"small\" differences in method may turn out to be crucial. Thus, while a failure to replicate should decrease confidence in a finding, one does not want to make too much out of a single failure (Francis, 2012). Rather, the results of the Reproducibility Project should be understood as an opportunity to learn whether current practices require attention or revision. Can we do science better? If so, how? Ultimately, we hope that we will contribute to answering these questions.", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Author Bio", "text": "Open Science Collaboration The Open Science Collaboration (OSC) began in November 2011 associated with the Open Science Framework (http://openscienceframework.org/). The OSC conducts open, collaborative research projects on a variety of topics. Most projects concern meta-scientific issues -the science of science. The Reproducibility Project is the most prominent project of the OSC. Others include developing a replication value statistic to identify which findings are particularly important to replicate because of their impact and imprecision of existing evidence, and archival investigations of power, effect size, and replication in the published literature. The OSC discussion group is publicly available and anyone can join at: https://groups.google.com/group/openscienceframework?hl=en", "title": "The Reproducibility Project: A model of large-scale collaboration for empirical research on reproducibility Open Science Collaboration 1", "file_name": "Open Science Collaboration - 2014 - A description of how RPP was organized and conduct.pdf"}
{"section": "Collaborative data collection in infancy research", "text": "The aims of the ManyBabies project are importantly different from the aims of previous replication projects such as the Reproducibility Project: Psychology (OSC, 2015), which focused A COLLABORATIVE APPROACH TO INFANT RESEARCH 6 on estimating the replicability of an entire scientific field. Instead, our aim is to understand why different developmental labs, studying the same developmental phenomena using the same or highly similar methods, might find differences in their experimental results. To achieve this goal, we plan to conduct a series of pre-registered, collaborative, multi-site attempts to replicate theoretically-central developmental phenomena. Thus, our approach is much more closely aligned with the \"Many Labs\" projects, from which we take our name. The Many Labs effort focuses on understanding variability in replication success and identifying potential moderators (e.g., Klein et al., 2014). But the effort involved in reproducing even one infant result across a large group of labs is substantial. To make the most of this effort and create high-value experimental data sets, we must navigate the tension between standardization across labs (with the goal of eliminating variability) and documentation of variability (with the goal of analyzing it). For example, there is wide variation in experimental paradigms implemented across infant labs, manifest in both the paradigms that are available in a given lab and in how these paradigms are implemented. For practical reasons, it is not possible to use a single identical paradigm across labs, so in the ManyBabies 1 study described below, we will include several standard paradigms for measuring infant preferences (habituation, headturn preference, and eye- tracking). Each lab using a particular paradigm will be provided with a collaboratively- developed protocol to minimize within-paradigm variability. Deviations from these standards within individual labs, where necessary, will be carefully documented. As a second example of the tension between standardization and documentation, it is clearly impossible to standardize all aspects of the sample of infants that we recruit across sites. Instead, we will document participant-level demographics (e.g., native language, mono-vs. A COLLABORATIVE APPROACH TO INFANT RESEARCH 7 bilingual environment, socio-economic status). In general, our approach will be to choose a relatively small set of potential lab-and participant-level moderators of experimental effects in each project and plan analyses that quantify variation on these variables. In addition to those sources of variation that can be straightforwardly documented and analyzed, there will be other systematic variation across labs on dimensions that are more difficult to quantify, like physical lab space, participant pool, and experimenter interaction. One of the goals of the project is to measure the variability in effect size that emerges from such sources, which is typically difficult to separate from truly random variation. Minimally, we will be able to make precise estimates of the proportion of variance that is explained by (structured) lab-to-lab variation. With the hope of potentially exploring ultimate sources of structured between-lab variation, the group is discussing supplemental steps we can take to ensure high data collection standards, including the video recording and sharing of all experimental procedures (e.g., using sharing platforms like Databrary; Adolph et al., 2012) and the training of RAs and other experimenters with standard videos across sites. Because participant exclusion criteria, preprocessing steps, and specific analytic statistics all present opportunities for analytic flexibility (and hence an inflation of false positives), we will fix these decisions ahead of time. We will use both simulated and real pilot data to establish a processing pipeline and set standards for data formatting, participant exclusion, and the myriad other decisions that must be taken in data analysis. Once analytic decisions are finalized, we will pre-register our experimental protocol and analyses, freezing these confirmatory analyses (providing a model \"standard operating procedure\" for future analyses). This pre-registration does not, however, preclude exploratory analyses, and we anticipate that these will be a significant source of new insights going forward. In this spirit, all of our methods, data, and A COLLABORATIVE APPROACH TO INFANT RESEARCH 8 analyses will be completely open by design. We will use new technical tools (e.g., the Open Science Framework) to share the relevant materials with collaborators and other interested parties. We hope this openness provides other unanticipated returns on our invested effort as others use and reuse our stimuli, protocols, data, and analysis code. Having established a set of goals and an approach, our group next converged on a target case study. After an open and lively discussion with interested labs, we elected via majority vote to examine infants' preference for speech directed to them (infant-directed speech, or IDS) in our first ManyBabies replication study (MB1), described below. We decided to begin with an uncontroversial and commonly-replicated finding so as to provide some expectations for variability across labs and to provide guidelines for planning further studies. Indeed, further down the line, we hope to consider replications of a range of developmental phenomena, including both fundamental phenomena whose replicability is not in question as well as more controversial findings. We also recognize that there is no single approach to collaborative replication that will apply in all cases. For example, when attempting to replicate controversial findings, tight standardization will typically be necessary. However, attempts to assess the generalizability of a well-established finding will instead benefit from documenting variability. In sum, across many different possible targets, we believe that the collaborative approach will yield new empirical and theoretical insights.", "title": "A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building", "file_name": "Frank et al. - 2017 - A Collaborative Approach to Infant Research Promo.pdf"}
{"section": "ManyBabies 1: The preference for infant directed speech", "text": "Infants' preference for speech containing the unique characteristics of so-called infant- directed speech (IDS) over adult-directed speech (ADS) has been demonstrated using a range of experimental paradigms and at a variety of ages (e.g., Cooper & Aslin, 1990;Cooper, Abraham, Berman, & Staska, 1997;Fernald & Kuhl, 1987;Hayashi, Tamekawa, & Kiritani, 2001;Newman & Hussain, 2006;Pegg, Werker & McLeod, 1992;Werker & McLeod, 1989). Moreover, infants perform better in language tasks when IDS stimuli are used, such as detecting prosodic characteristics (Kemler Nelson, Hirsh-Pasek, Jusczyk, & Cassidy, 1989) or learning/recognizing words (e.g. Singh, Nestor, Parikh, & Yull, 2009;Ma, Golinkoff, Houston, & Hirsh-Pasek, 2011;Thiessen, Hill, & Saffran 2005). A typical experimental operationalization of a preference for IDS is that infants will attend longer to a static visual target (e.g., a checkerboard) when looking leads to hearing IDS, as opposed to ADS (Cooper & Aslin, 1990). The preference for IDS is observed robustly across studies, but it is also quite variable. for review), there is variation across languages, and North American English appears to provide an especially exaggerated form (Fernald et al. 1989;Floccia et al., 2014;Kitamura, Thanavishuth, Burnham, & Luksaneeyanawin, 2002;Shute & Wheldall, 1995; though c.f. Farran, Lee, Yoo, & Oller, 2016). Although studies have found preferences for IDS in other languages, including Japanese ( Hayashi et al., 2001) and Chinese ( Werker et al., 1994), the tendency for studies on IDS to come from North America may therefore provide an inflated view of its robustness. This description highlights why a meta-analysis is insufficient: A meta-analysis is only as good as the published body of research it rests on, and conclusions based on it are A COLLABORATIVE APPROACH TO INFANT RESEARCH 10 uncertain when there are biases in data collection (e.g., oversampling North American labs) and publication (notice that the IDS meta-analysis revealed significant bias, as low-precision studies tended to yield larger effect sizes than high-precision ones). In contrast, the collaborative approach advocated here can reduce or eliminate these sources of bias. We selected IDS preference for our first replication study because it satisfies a number of key desiderata. Most importantly, it allows us to measure inter-lab and inter-baby variability because the effect itself is large and robust -at least within North America. With a large effect as a baseline, we can assess variation in that effect across methods (e.g., comparing eye-tracking versus human-coded procedures), across linguistic communities (e.g., comparing infants for whom the stimuli are native versus not), and across ages. Distinguishing these moderators would not be possible in the case of a phenomenon with a smaller effect size. In the worst case, if the original effect were truly null, no moderation relationships would be detectable at all. Preference for IDS also allows MB1 to assess several questions that are important for developmental theory. First, some views of language acquisition attribute a key role to IDS preference in scaffolding language learning, due to its attention-driving properties (e.g., Kaplan, Goldstein, Huckeby, Owren, & Cooper, 1995) or specific linguistic characteristics (e.g., Kuhl et al., 1997). Although the preference for IDS is robust with young infants, fewer studies precisely examine how this preference changes across development (although c.f. Hayashi et al., 2001;Newman & Hussain, 2006). Second, our study also offers an opportunity to examine the classic theory of native-language phonological specialization -in which general preferences and perceptual abilities gradually become language-specific over the course of the first year (Kuhl, 2004;Werker & Tees, 1984) -in a new domain. Use of IDS varies across linguistic communities (see above), but there has been relatively little study of this variation. For example, as mentioned above, British English IDS has less prosodic modification than North American English IDS (Fernald et al., 1989;Shute & Wheldall, 1995). Does this imply that UK infants might be particularly interested (or uninterested) in the intonational characteristics of North American IDS? Should this interest decline as they recognize that the dialect of the IDS is distinct (Butler et al., 2010;Nazzi, Jusczyk & Johnson, 2000)? We can ask the same question for infants learning languages other than English: will the IDS preference decline more quickly with age for these infants compared with North American English (NAE) learning infants? In sum, the MB1 study provides both methodological and conceptual opportunities. To address all of these questions, after substantial consideration, we elected to use precisely the same speech stimuli across labs: samples of IDS from NAE-speaking mothers, the most well-studied IDS source. Our study therefore measures preference for NAE IDS, rather than IDS more generally. This choice was a necessary compromise to meet our other goals. For example, if each lab recorded its own stimuli, lab-related variability would have been confounded with stimulus variability. Under this design, the burden of time and expertise for participating labs would have been substantially greater. We recognize that this decision furthers the existing NAE bias in the literature (cf. Henrich, Heine, & Norenzayan, 2010), but since our goal was to replicate an existing phenomenon, we were constrained by the same literature. Our hope is that this initial study will spur additional research using other languages. At the time of writing, we have divided into committees who are working towards making informed key decisions on all subsequent aspects of this project, including stimuli selection, experimenter training, data collection, and data analyses. We expect to begin data collection in 2017 and complete the study roughly a year later, analyzing and writing up the main results shortly thereafter. We also hope many more manuscripts from this project emerge as participants and the community more generally explore the resulting rich dataset.", "title": "A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building", "file_name": "Frank et al. - 2017 - A Collaborative Approach to Infant Research Promo.pdf"}
{"section": "Challenges and benefits of collaborative data collection", "text": "Any project has costs -in time, money, and research effort -and for a project as large as ManyBabies, these will be considerable. Nevertheless, we believe that the benefits of the project outweigh these costs, both for individual researchers and for the field as a whole. We discuss challenges for individuals (including early career researchers), labs, and the field as a whole in turn and lay out benefits for each in subsequent paragraphs.", "title": "A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building", "file_name": "Frank et al. - 2017 - A Collaborative Approach to Infant Research Promo.pdf"}
{"section": "Individual researchers", "text": "One obstacle to collaborative projects like ManyBabies is that the positive incentives for participation are not as obvious as those for independent research. For example, grant panels and tenure committees may be strongly focused on first-and last-author publications, and may not sufficiently recognize collaborative work even when specific contributions are carefully documented. Early career researchers (ECRs) in particular are especially vulnerable to the need to produce original scholarship on a relatively short timeline. But given the relatively modest investments of time and effort necessary to make a contribution to a large project, we believe these potential downsides are outweighed by a number of substantial positive benefits. Improvements in individual scientific practices. Issues of replication and reproducibility are fundamentally not just problems for the community as a whole, but also problems for individual researchers who may both fail to perform replicable research and fail to replicate others' work. Collaborative projects allow individual researchers to gain experience with community-generated best-practices in experimental design, data analysis, and use of collaborative open-science tools. Such opportunities may be especially valuable for ECRs who do not have access to local training in these practices. As a group, the authors of this paper have found the discussions surrounding project planning to be helpful with their own evolving understanding of issues of reproducibility and study design. Being a pioneer. Although there are still significant impediments, attitudes towards the value of collaborative and replication work are changing. In the coming years, contributions to collaborative work and projects that work to resolve the replicability crisis may be important factors in hiring, promotion, and funding decisions. Researchers who can show a pattern of early adoption of these new attitudes and approaches will demonstrate a visible and potentially field- shaping commitment to replicability in psychological science. Opportunities for secondary analysis. Large-scale collaborative projects yield a multitude of data that, in addition to the planned analyses, can be explored for different kinds of research questions, creating additional publication opportunities for the same effort. Being part of a community. A final important component of the collaborative approach for its participants is the opportunity to collaborate with other researchers. Collaborative efforts provide significant opportunities for networking, mentorship, and the sharing and cross- fertilization of ideas, well beyond those afforded by the standard conference and publication paradigm. With the widespread use of videoconferencing, collaborative projects bring together researchers across timezones in relatively intimate, friendly, supportive, and significant interactions. For ECRs, collaborative projects provide a method for connecting with a broad community of interest and raising awareness about their own skills and abilities. In addition, connections made through collaborative projects may blossom into other professional interactions.", "title": "A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building", "file_name": "Frank et al. - 2017 - A Collaborative Approach to Infant Research Promo.pdf"}
{"section": "Groups, labs, and lab heads", "text": "Even if individuals may be interested in a collaborative project, the decision to commit the resources of a research group or lab may be more complex. For example, often labs have funding obligations that require a specific amount of administrative or participant recruitment resources to be devoted to ongoing projects. In the short term, the ManyBabies group has secured modest funding to support lab involvement where it would otherwise not be possible, but longer-term financial support may be important for sustaining the group's efforts. But again, as in the case of individual researchers, there may be a variety of other subsidiary benefits that outweigh the costs of participation. Implementation of emerging open science practices. The ManyBabies project makes use of a number of emerging practices to ensure reproducibility and to facilitate communication and dissemination, as discussed above. These practices -for example, creating shared project repositories, generating analysis or simulation pipelines, and writing pre-registration documents -provide the same kind of benefits to efficiency and reproducibility when used within a single lab. Contributors to the Many Babies study will be able to bring these tools back with them to their home lab.", "title": "A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building", "file_name": "Frank et al. - 2017 - A Collaborative Approach to Infant Research Promo.pdf"}
{"section": "The field as a whole", "text": "Finally, while it is challenging to coordinate and conduct a multi-site replication study, we believe that there are important reasons why collaborative projects benefit the field as a whole. Replication work leads to more robust science, greater confidence in our findings, and better knowledge-sharing about methodological concerns, which in turn contributes to a culture of data sharing that has benefited fields such as computer science, physics, bioinformatics, sociology but is not yet widespread in developmental science. We highlight two important positive consequences here. Funding is tied to community confidence. As any researcher knows, the public controls the purse strings -if a government is voted into office that is less friendly toward research (or even certain kinds of research), this decision will be very quickly felt within the research community by individual researchers who do not get the grant funding they rely on for their work. Setting aside altruistic desires to do high-quality research, our own self-interest in the field of developmental psychology -and infancy research in particular -should drive us to support endeavors like ManyBabies in order to demonstrate to the public our commitment to improving scientific practice and outcomes. Creating \"best practices\" materials and guidelines for experimental procedures and data analysis. The first ManyBabies project will create a push-and-play implementation of a discrimination experiment with a directional prediction. The natural side effect of this study is that it will lead to a set of consensus decisions about experimental procedures using different paradigms for measuring preference, and a set of open-source analysis scripts for the kind of data such experiments generate. These materials will not only lower commitment costs for labs involved in the study, but will also create a well-realized template for any future work (by ManyBabies participating labs or otherwise) wanting to use these popular developmental methods.", "title": "A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building", "file_name": "Frank et al. - 2017 - A Collaborative Approach to Infant Research Promo.pdf"}
{"section": "Advice for similar multi-site replication efforts", "text": "We hope that the ManyBabies project is an initial foray into a new way of doing research in our field. Although our first study is still ongoing, there are already a number of things we have learned that may be beneficial to others embarking on similar endeavors. First, although the tendency when planning a large, costly project will be to use complex experimental designs or to deal with difficult issues first, we have found that there is much to be done using simple methods to study seemingly well-understood phenomena. Even a straightforward investigation will incur substantial associated complexity as a multi-site project, given the variation of methodological practices within developmental research. In MB1, we were conservative in our initial choice of topic: The preference for infant-directed speech is a phenomenon that we have strong reason to believe is robust and will lead to a \"successful\" outcome (in the sense of finding an overall effect in our analysis). Yet making best-practices decisions on method and stimulus was difficult and time-consuming, despite the existence of many similar studies on the topic. Thus, we advise future studies to choose the simplest design that is informative relative to the question of interest. Second, we suggest that researchers carefully consider policies surrounding authorship, responsibility and credit. From the beginning, ManyBabies decision-making has been democratic in nature, and contributors have largely self-selected their contributions (including in the writing of this paper). This collaborative approach has been surprisingly successful, although it has led to some questions around the attribution of authorship. The area of most concern in this regard has been how to encourage and recognize student participation. Specifically, because so many primary investigators were involved in methodological decision-making and because the manuscript was largely complete prior to the start of data collection, it has been challenging to involve students in meaningful ways. However, students directly associated with setting up, testing and analyzing the data may appear as authors on the experimental paper that will result from this project. We leave the onus on individual laboratories to find ways to engage involved students meaningfully in the science. Whether this approach is successful remains to be seen. Similar projects in the future might want to start by addressing these authorship issues at the outset of the project. Third, we strongly recommend that researchers investigate statistical issues prior to beginning recruitment. While we had a priori ideas regarding the overall structure and analysis of the study, collaborative discussions of the developmental sampling scheme and the desire for crosslinguistic comparison led to crucial refinements in the design and analyses to be conducted. In addition, through consultation with statisticians and quantitative researchers, we came to understand early on that our power to draw meaningful inferences about the influence of methodological considerations on outcomes would be strongly influenced by the number of participating laboratories. This statistical fact has had important consequences for decisions regarding laboratory recruitment and commitment. We have worked to minimize the burden on any individual laboratory participating in a number of ways, and prioritized including more laboratories at the expense of smaller samples per laboratory (although we do impose a minimum contribution of N = 16 based on the power established in the literature discussed above). Fourth, we suggest that researchers make use of collaboration tools to share the workload and maximize transparency. From the start, ManyBabies has relied heavily on online collaboration, which has made a democratic and accountable decision process possible. Materials are shared, reviewed, and revised either with the whole group or within dedicated task forces that everyone is free to join. Decision-making meetings are documented exhaustively in notes to allow those that could not participate -due to prior commitments or time differences -to catch up and comment. Sharing manuscripts and analysis code through platforms like Google Docs, github, and the Open Science Framework has made it possible to have multiple editors, effectively dividing the workload and speeding up the design and planning process. As MB1 recruits more labs to participate, interested researchers can get an overview of the project and join the decision-making process easily by reviewing these shared projects. Finally, collaborative projects benefit from having at least one individual researcher who can coordinate the project, including initiating decision making, moving discussions forward, setting deadlines, identifying potential publication and funding sources, and acting as spokesperson for disseminating findings at conferences and elsewhere. Having a project coordinator prevents the diffusion of leadership that would otherwise stall progress and completion of the work.", "title": "A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building", "file_name": "Frank et al. - 2017 - A Collaborative Approach to Infant Research Promo.pdf"}
{"section": "Conclusions", "text": "The foundational purpose of developmental research is to create and disseminate knowledge about organic changes in learning and behavior over age. This goal is best achieved within a culture of careful, methodological research and widespread sharing of data. The ManyBabies project is a new collaborative effort to promote best practices, evaluate and build on influential findings, and understand different dimensions of variability in laboratory-based infancy research. While data collection is currently ongoing for our first project on infant-directed speech preference, other tangible benefits have already emerged from this collaboration. First, the ManyBabies project served as one inspiration for a recent pre-conference at the International Congress on Infant Studies, titled Building Best Practices in Infancy Research. This pre-conference in turn triggered discussions with the Congress leadership, leading to the special issue you are now reading. Such \"knock-on\" effects are one important benefit of so many people's efforts being directed at these issues. Second, participation in the ManyBabies project has already affected how we conduct our own research. For instance, there have been many fruitful discussions during video conferences among members of the ManyBabies 1 project subgroups, including the methods, stimulus, data analysis, and ethics groups. Both macro-level conceptual issues about conducting rigorous research and micro-level methodological issues about a variety of topics have been discussed, such as how to most effectively reduce parent interference during experiments. These discussions have already informed practices within our own labs. Third and finally, the project has served to promote community-building in infancy research outside of the standard framework. This venue for interaction is likely to enhance the rigor and health of our field in the future by promoting reproducibility, improving methods, sharing ideas and data, encouraging reasonable interpretations of data, and building theories. Central to this effort is the idea that science is fundamentally incremental and collaborative. All graduate students, postdoctoral researchers, research staff, and faculty members are welcome to join the ManyBabies project and our community more generally.  Larger and smaller funnel boundaries show 99% and 95% thresholds, respectively. Dotted line shows the mean effect size from a random-effects meta analytic model.", "title": "A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building", "file_name": "Frank et al. - 2017 - A Collaborative Approach to Infant Research Promo.pdf"}
{"section": "Abstract", "text": "Reproducibility is a defining feature of science. However, because of strong incentives for innovation and weak incentives for confirmation, direct replication is rarely practiced or published. The Reproducibility Project is an open, large-scale, collaborative effort to systematically examine the rate and predictors of reproducibility in psychological science. So far, 72 volunteer researchers from 41 institutions have organized to openly and transparently replicate studies published in three prominent psychological journals in 2008. Multiple methods will be used to evaluate the findings, calculate an empirical rate of replication, and investigate factors that predict reproducibility. Whatever the result, a better understanding of reproducibility will ultimately improve confidence in scientific methodology and findings.", "title": NaN, "file_name": "Open Science Collaboration - 2012 - An Open, Large-Scale, Collaborative Effort to Esti.pdf"}
{"section": "The Reproducibility Project", "text": "Obtaining a meaningful estimate of reproducibility requires conducting replications of a sizable number of studies. How- ever, because of existing incentive structures, it is not in an indi- vidual scientist's professional interest to conduct numerous replications. The Reproducibility Project addresses these barri- ers by spreading the workload over a large number of research- ers. As of August 23, 2012, 72 volunteers from 41 institutions had joined the replication effort. Each contributor plays an important but circumscribed role, such as by contributing on a team conducting one replication study. Researchers volunteer to contribute on the basis of their interests, skills, and available resources. Information about the project's coordination, plan- ning, materials, and execution is available publicly on the Open Science Framework's Web site (http://openscienceframework. org/). Open practices increase the accountability of the replica- tion team and, ideally, the quality of the designs and results.", "title": NaN, "file_name": "Open Science Collaboration - 2012 - An Open, Large-Scale, Collaborative Effort to Esti.pdf"}
{"section": "Selecting Studies for Replication", "text": "Studies eligible for replication were selected from 2008 issues of three prominent journals that differ in topical emphasis and publishing format (i.e., short reports vs. long-form articles): Journal of Experimental Psychology: Learning, Memory, and Cognition, Journal of Personality and Social Psychology, and Psychological Science. 4 To minimize selection biases even within this restricted sample, replication teams choose from among the first 30 articles published in an issue. From the selected article, each team selects a key finding from a single study for replication (the last study by default, unless it is unfeasible to replicate). As eligible articles are claimed, addi- tional articles from the sampling frame are made available for selection. Not all studies can be replicated. For example, some used unique samples or specialized equipment that is unavail- able, and others were dependent on a specific historical event. Although feasibility constraints can reduce the generalizabil- ity of the ultimate results, they are inevitably part and parcel of reproducibility itself.", "title": NaN, "file_name": "Open Science Collaboration - 2012 - An Open, Large-Scale, Collaborative Effort to Esti.pdf"}
{"section": "Conducting the Replications", "text": "The project's replication attempts follow a standardized proto- col aimed at minimizing irrelevant variation in data collection and reporting methods, and maximizing the quality of replica- tion efforts. The project attempts direct replications-\"repetition of an experimental procedure\" in order to \"verify a piece of knowledge\" (Schmidt, 2009, p. 92, 93). Replications must have high statistical power (1\u2212\u03b2 \u2265 .80 for the effect size of the origi- nal study) and use the original materials, if they are available. Researchers solicit feedback on their research design from the original authors before collecting data, particularly to identify factors that may interfere with replication. Identified threats are either remedied with revisions or coded as potential predictors of reproducibility and written into the replication report.", "title": NaN, "file_name": "Open Science Collaboration - 2012 - An Open, Large-Scale, Collaborative Effort to Esti.pdf"}
{"section": "Evaluation of Replication-Study Results", "text": "Successful replication can be defined by \"vote-counting,\" either narrowly (i.e., obtaining the same statistically significant effect as original study) or broadly (i.e., obtaining a directionally similar, but not necessarily statistically significant, result), or quantitatively defined-for example, through meta-analytic estimates combining the original and replication study, com- parisons of effect sizes, or updated estimates of Bayesian priors. As yet, there is no single general, standard answer to the ques- tion \"What is replication?\" so we employ multiple criteria (Valentine et al., 2011). Failures to replicate might result from several factors. The first is a simple Type II error with an occurrence rate of 1\u2212\u03b2: Some true findings will fail to replicate purely by chance. However, the overall replication rate can be measured against the average statistical power across studies. For this reason, the project focuses on the overall reproducibility rate. Indi- vidual studies that fail to replicate are not treated as discon- firmed. Failures to replicate can also occur if (a) the original effect is false; (b) the actual size of the effect is lower than originally reported, making it more difficult to detect; (c) the design, implementation, or analysis of either the original or replication study is flawed; or (d) the replication methodology differs from the original methodology in ways that are critical for successful replication. 5 All of these reasons are important to consider in evaluations of reproducibility, but the most interesting may be the last. Identifying specific ways in which replications and original studies differ, especially when repli- cations fail, can advance the theoretical understanding of pre- viously unconsidered conditions necessary to obtain an effect. Thus, replication is theoretically consequential. The most important point is that a failure to replicate an effect does not conclusively indicate that the original effect was false. An effect may also fail to replicate because of insuf- ficient power, problems with the design of the replication study, or limiting conditions, whether known or unknown. For this reason, the Reproducibility Project investigates factors such as replication power, the evaluation of the replication- study design by the original authors, and the original study's sample and effect sizes as predictors of reproducibility. Identi- fying the contribution of these factors to reproducibility is use- ful because each has distinct implications for interventions to improve reproducibility.", "title": NaN, "file_name": "Open Science Collaboration - 2012 - An Open, Large-Scale, Collaborative Effort to Esti.pdf"}
{"section": "Implications of the Reproducibility Project", "text": "An estimate of the reproducibility of current psychological sci- ence will be an important first. A high reproducibility estimate might boost confidence in conventional research and peer- review practices in the face of criticisms about inappropriate flexibility in design, analysis, and reporting that can inflate the rate of false positives (Greenwald, 1975;John, Loewenstein, & Prelec, 2012;Simmons, Nelson, & Simonsohn, 2011). A low estimate might prompt reflection on the quality of standard practice, motivate further investigation of reproducibility, and ultimately lead to changes in practice and publishing standards (Bertamini & Munaf\u00f2, 2012;LeBel & Peters, 2011). Some may worry that the discovery of a low reproducibility rate will damage the image of psychology or of science more generally. It is certainly possible that opponents of science will use such a result to renew their calls to reduce funding for basic research. However, we believe that the alternative is much worse: having a low reproducibility rate, but failing to investigate and discover it. If reproducibility is lower than acceptable, then it is vitally important that we know about it in order to address it. Self-critique, and the promise of self-cor- rection, is what makes science such an important part of humanity's effort to understand nature and ourselves.", "title": NaN, "file_name": "Open Science Collaboration - 2012 - An Open, Large-Scale, Collaborative Effort to Esti.pdf"}
{"section": "Conclusion", "text": "The Reproducibility Project uses an open methodology to test the reproducibility of psychological science. It also models procedures designed to simplify and improve reproducibility. Readers can review the discussion history of the project, examine the project's design and structured protocol, retrieve replication materials from the various teams, obtain reports or raw data from completed replications, and join the project to conduct a replication (start here: http://openscienceframework .org/project/EZcUj/). Increasing the community of volunteers will strengthen the power and impact of the project. With this open, large-scale, collaborative scientific effort, we hope to identify the factors that contribute to the reproducibility and validity of psychological science. Ultimately, such evidence- and steps toward resolution, if the evidence produces a call for action-can improve psychological science's most important asset: confidence in its methodology and findings.", "title": NaN, "file_name": "Open Science Collaboration - 2012 - An Open, Large-Scale, Collaborative Effort to Esti.pdf"}
{"section": "Declaration of Conflicting Interests", "text": "The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.", "title": NaN, "file_name": "Open Science Collaboration - 2012 - An Open, Large-Scale, Collaborative Effort to Esti.pdf"}
{"section": "Abstract", "text": "We measure how accurately replication of experimental results can be predicted by a black-box statistical model. With data from four large-scale replication projects in experimental psychology and economics, and techniques from machine learning, we train a predictive model and study which variables drive predictable replication. The model predicts binary replication with a cross validated accuracy rate of 70% (AUC of 0.79) and relative effect size with a Spearman \u03c1 of 0.38. The accuracy level is similar to the market-aggregated beliefs of peer scientists (Camerer et al., 2016; Dreber et al., 2015). The predictive power is validated in a pre-registered out of sample test of the outcome of Camerer et al. (2018b), where 71% (AUC of 0.73) of replications are predicted correctly and effect size correlations amount to \u03c1 = 0.25. Basic features such as the sample and effect sizes in original papers, and whether reported effects are single-variable main effects or two-variable interactions, are predictive of successful replication. The models presented in this paper are simple tools to produce cheap, prognostic replicability metrics. These models could be useful in institutionalizing", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Introduction", "text": "Replication lies at the heart of the process by which science accumulates knowledge. The ability of other scientists to replicate an experiment or analysis demonstrates robustness, guards against false positives, puts an appropriate burden on scientists to make replication easy for others to do, and can expose the various \"researcher degrees of freedom\" like p-hacking or forking ( Bavel et al., 2016;Begley and Ioannidis, 2015;De Vries et al., 2006;Gelman and Loken, 2013;Ioannidis et al., 2001;Ioannidis, 2005;Ioannidis et al., 2011Ioannidis et al., , 2014Koch and Jones, 2016;Lindsay, 2015;Martinson et al., 2005;Munaf\u00f2 et al., 2017;Nosek et al., 2015;O'Boyle et al., 2017;Open Science Collaboration, 2015;Silberzahn et al., 2018;Simmons et al., 2011;Simonsohn et al., 2014). The most basic type of replication is \"direct\" replication, which strives to reproduce the creation or analysis of data using methods as close to those used in the original science as possible (Simons, 2014). Direct replication is difficult and sometimes thankless. It requires the original scientists to be crystal clear about details of their scientific protocol, often demanding extra effort years later. Conducting a replication of other scientists' work takes time and money, and often has less professional reward than original discovery. Because direct replication requires scarce scientific resources, it is useful to have methods to evaluate which original findings are likely to replicate robustly or not. Moreover, implicit subjective judgments about replicability are made during many types of science evaluations. Replicability beliefs can be influential when giving advice to granting agencies and foundations on what research deserves funding, when reviewing articles which have been submitted to peer-reviewed journals, during hiring and promotion of colleagues, and in a wide range of informal \"post-publication review\" processes, whether at large international conferences or small kaffeeklatches. The process of examining and possibly replicating research is long and complicated. For example, the publication of Rand et al. (2012) resulted in a series of replications and subsequent replies (Bouwmeester et al., 2017;Rand, 2017;Rand et al., 2013;Tingh\u00f6g et al., 2013). The original findings were scrutinized in a thorough and long process that yielded a better understanding of the results and their limitations. Many more published findings would benefit from such examination. The community is in dire need of tools that can make this work more efficient. Statcheck (Nuijten et al., 2016) is one such framework that can automatically identify statistical errors in finished papers. In the same vein, we present here a new tool to automatically evaluate the replicability of laboratory experiments in the social sciences. There are many potential ways to assess whether results will replicate. We propose a simple, black-box, statistical approach, which is deliberately automated in order to require little subjective peer judgment and to minimize costs. This approach leverages the hard work of several recent multi-investigator teams who performed direct replications of experiments in psychology and economics (Camerer et al., 2016;Ebersole et al., 2016;Klein et al., 2014;Open Science Collaboration, 2015). Based on these actual replications, we fit statistical models to predict replication and analyze which objective features of studies are associated with replicability. We have 131 direct replications in our dataset. Each can be judged categor-ically by whether it succeeded or failed, by a pre-announced binary statistical criterion. The degree of replication can also be judged on a continuous numerical scale, by the size of the effect estimated in the replication compared to the size of the effect in the original study. Our method uses machine learning to predict outcomes and identify the characteristics of study-replication pairs that can best explain the observed replication results (Camerer et al., 2018a;Hastie et al., 2009;Nave et al., 2018;Yarkoni and Westfall, 2017). We divide the objective features of the original experiment into two classes. The first contains the statistical design properties and outcomes: among these features we have sample size, the effect size and p-value originally measured, and whether a finding is an effect of one variable or an interaction between multiple variables. The second class is the descriptive aspects of the original study which go beyond statistics: these features include how often a published paper has been cited and the number and past success of authors, but also how subjects were compensated. Furthermore, since our model is designed to predict the outcome of specific replication attempts we also include similar properties about the replication that were known beforehand.We also include variables that characterize the difference between the original and replication experiments -such as whether they were conducted in the same country or used the same pool of subjects. See Table S1 for a complete list of variables,  and Tables S2-S11 for summary statistics. The statistical and descriptive features are objective. In addition, for a sample of 55 of the study-replication pairs we also have measures of subjective beliefs of peer scientists about how likely a replication attempt was to result in a categorical Yes/No replication, on a 0-100% scale, based on survey responses and prediction market prices (Camerer et al., 2016;Dreber et al., 2015). Market participants predicted replication with an accuracy of 65.5% Our proposed model should be seen as a proof-of-concept. It is fitted on an arguably too small data set with an indiscriminately selected feature set. Still, its performance is on par with the predictions of professionals, hinting at a promising future for the use of statistical tools in the evaluation of replicability.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Methods and Data", "text": "The data are combined from four replication projects, The Reproducibility Project in Psychology (RPP; Open Science Collaboration, 2015), the Experi- mental Economics Replication Project (EERP; Camerer et al., 2016) and Many Labs (ML) 1 and 3 ( Ebersole et al., 2016;Klein et al., 2014). In most cases, one specific statistical test from each paper was selected for replication, but four papers had multiple effects replicated. In RPP and EERP, each experiment was replicated once. In the Many Labs projects all participating labs replicated every experiment and the final results were calculated from the pooled data. A total of 144 effects were studied. 1 After dropping observations with missing variables, our final dataset contains 131 study-replication pairs. For 55 of these observations we also have data from prediction markets prices (Camerer et al., 2016;Dreber et al., 2015).  Table S1 for variable definitions and Figure S1 for a full correlation plot.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Dependent variables", "text": "There is no single best method for measuring whether a replication result is a failure or not. An active literature studies different strategies to evaluate replicability (see e.g. Andrews and Kasy, 2017;Leek et al., 2015;Simonsohn, 2015). For this paper, we have chosen to prioritize simplicity and focus on two measures, one binary and one continuous: Replicated = 1 p replication \u2264 0.05 and effect in same direction 0 otherwise Relative Effect Size = replication effect size (r) original effect size (r) The binary model defines replication success as a statistically significant (p \u2264 0.05) effect in the same direction as in the original study. This measure has often been criticized and is indeed overly simplistic. We use it since it can be compared to the prediction market estimates from previous studies (where subjects traded bets using the same measure). 2 The continuous model estimates the relative standardized effect size of the replication when compared to the original effect. It yields a more fine-grained notion of replication that does not depend on the peculiarities of hypothesis testing. 3", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Independent variables", "text": "For each original-replication pair we have collected a large set of variables (see Figure 1B for the variable names or Table S1 for descriptions). The feature set includes objective characteristics of the original experiment, but also information about the replication that was known before it was carried out. We intentionally provide no theoretical justification for the inclusion of any specific feature, but simply gathered as many variables as possible. We leave it to the user of the model to decide which of these variables are relevant for their specific implementation, and provide information about the relative importance of each feature in Figure 4. The heatmap of Spearman rank-order correlation coefficients in Figure 1B shows some correlation between our two outcomes and other features (the two top rows). Most relationships are weak. Ex ante expected correlations are strong (e.g., sample size and p-value) but not many other relationships are evident visually (see Figure S1 for a full correlation plot). Original effect sizes are correlated with binary replication and so are p-values, with Spearman \u03c1 of 0.26 and 0.38 respectively.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Model Training", "text": "We use cross validation to avoid overfitting. To simultaneously evaluate vari- ability of the accuracy metric 4 we nest two cross validation loops, as shown in Figure 2. In the inner loop, we search and validate algorithm-specific hy- perparameters. Each such optimally configured model is then tested on 20% of the data in the outer loop. Our limited sample size forces us to use these validation sets for both reported performance statistics and algorithm selection (Figures 3 and S3). Because we make decisions based on these performance statistics, also our cross validated measures may suffer from some overfitting. We therefore evaluate pre registered predictions of the results of Camerer et al. (2018b) as a supplement. When training the binary classification models, we do so with the goal to maximize the area under the curve (AUC) of a receiver operating characteristics (ROC) curve (Bradley, 1997). The metric accounts for the trade-off between successfully predicting positive and negative results respectively. Maximizing accuracy might result in a model that always predicts failure to replicate, and accurately predicts all unsuccessful replications, but incorrectly classifies all those that do replicate. The model with the highest AUC will instead be the one that minimizes the effects of this trade off, achieving high prediction rates for both positive and negative results simultaneously. The models predicting relative effect size are trained to minimize the mean squared prediction error. We compare a number of popular machine learning algorithms (see Figure S3) and find that a Random Forest (RF) model has the highest performance. The outcome predicted by an RF algorithm is the result of averaging over a \"forest\" of decision trees. Each tree is fitted using a random subset of variables, and employs a hierarchical sequence of cutoffs to predict observations (Breiman, 2001). A simple tree with depth 2 might fit 0-1 replication success by first dividing cases by if sample size is below a cutoff, then, at each of those two branches, by whether the original effect is a main effect or an interaction. Each end node is a prediction of the outcome variable. The algorithm is popular because it performs well without much human supervision. Repeatedly split into 5 folds 3 2 1 4 Training (80%)  In each run the model is trained on 9/10th of the data and tested on the last decile. The best version (with highest AUC) is trained on all of the training data and its accuracy is estimated on the fifth fold of the outer loop. The process is repeated with a different outer fold held out. After five runs, a new set is drawn, and the process is repeated until 100 accuracy metrics have been generated.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Results", "text": "The Random Forest model trained on the full feature set predicts binary replication with a median AUC of 0.79 (median accuracy of 69% at the 50% probability threshold), shown in the top bars of Figure 3. The bar width is the interquartile range of 100 performance resamples produced by the nested cross validation. We do not show confidence intervals since these resamples are not independent. The median is depicted as a dot. 5 5 Note that because of sampling error and other statistical properties, the upper bound for ideal replication forecasting is less than 100%; rather we would expect it to be around 90% (see Section S3). Why? Consider an artificial sample, measuring a, by construction, genuine effect with tests that have 90% power to detect it. A perfect model predicting significance in a second sample will only be right nine out of ten times. This theoretical ceiling is important since we should arguably normalize the distance between random guessing and the best possible level of prediction. For 69% accuracy, the normalized improvement over a random guess (50%) to perfection is 69\u221250 100\u221250 = 0.38. However using a more accurate upper bound of The model predicting Relative Effect Size achieves a median R 2 of 0.19. The predicted and actual effect sizes have a median Spearman correlation of 0.36. In the pre-registered validation of SSRP, 71% of binary replications are predicted correctly (AUC: 0.73). Relative effect size is predicted with a mean absolute prediction error of 0.43 and a Spearman correlation of 0.25. Individual predictions are presented in Figure 5. A qualitative assessment of these results can be made in both relative and absolute terms. First, classifier performance is substantially higher than that of a random model (which by definition has an AUC of 0.5), and is more accurate than a linear model using the same features (the last bar in each subplot of Figure 3, median AUC = 0.72). Furthermore, the continuous Random Forest (RF) model explains 19% of the variation in relative effect size, compared to the OLS R 2 of 0.06. The rank-order correlation coefficient of 0.36 (OLS: 0.27) between predicted and actual values is higher than the 0.21 correlation between original and relative effect size, indicating a fairly substantive performance improvement over a very simple heuristic. When cross validated accuracy is compared to out of sample tests, the binary classifier achieves similar results. The continuous model performs worse, with a Spearman correlation coefficient that is 30% smaller. Second, these machine-learned predictions, based on objective features, can be compared to subjective beliefs of replicability based on prediction market prices in earlier studies where social scientists traded on the probability of replication success. The market predicted 65.5% of the replications in our data correctly 6 (with an AUC of 0.73). While the model fares slightly better in this data set, it is difficult to draw conclusions on the relative performances of the methods based on such a small sample.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Predictive Power", "text": "In Figure 3, we further compare the performance of models in which certain classes of variables have been excluded. The observation of similar patterns for both sets of models is not surprising, given the high correlation of the two outcome measures (Spearman \u03c1 = 0.79). For both replication measures, the second bar shows that removing the dummy variables encoding the discipline of the study (Economics, Social Psy- chology or Cognitive Psychology) has little bearing on the results. The 64 Social Psychology replications have smaller effect sizes (mean of 0.33 compared to 0.47 for cognitive psychology), slightly larger p-values (0.017 compared to 0.01). Inbar (2016) argues that the association between contextual sensitivity (as measured on a scale from 0-5) and replicability found by Bavel et al. (2016) is spuriously identified from the difference in replication rates between fields. We show that many other variables also mediate these differences. For example, 90%, it is 69\u221250 90\u221250 = 0.48. 6 Accuracy is calculated assuming that market prices reflect replication probabilities ( Wolfers and Zitzewitz, 2006) and using a threshold of 0.5. We only have model predictions and market prices for 55 observations, including data from Dreber et al. (2015) with an accuracy rate of 68% and Camerer et al. (2016) where accuracy was 61%. In two follow-up papers, prediction markets perform better (Camerer et al., 2018b;Forsell et al., 2018). Pooling results across all four papers yields a prediction market accuracy rate of 73% (76/104). For the classifier, the optimal model (first from top) has an average AUC of 0.79 and accuracy of 70% at the 50% probability cutoff (accuracy is mainly driven by a high true negative rate; failed replications are predicted with an accuracy of 80%, while successful only with 56%). The optimal regression model has a median R 2 of 0.19 and a Spearman \u03c1 of 0.38. The second bar from the top in each subplot shows unchanged model performance when dummy indicators for discipline (Economics, Social or Cognitive Psychology) are removed. The third has excluded any features unique to the replication effort (e.g. replication team seniority) with no observable loss of performance. The less accurate fourth model is only based on original effect size and p-value. Last, the model at the bottom is a linear model trained on the full feature set, for reference. See Figure S3 for more models. by construction, holding sample size constant, interaction effects will have lower statistical power. Included social psychology experiments test interaction effects almost twice as often (44% vs 27% in cognitive psychology). If studies of interactions do not increase sample size appropriately, replicability will be lower. The third bar shows no reduction in accuracy for a model in which all replication-specific features are excluded. The reason is likely that replica- tion characteristics were standardized between experiments. No replication is conducted with a really small sample size, for example. The fourth bar uses only original effect size and p-value. The decrease in accuracy shown in this bar implies that also other features are informative.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Feature Importance", "text": "The previous section summarized the general accuracy of the models, using different feature subsets. This section explores which objective features of experimental designs and results correlate with replicability, extending the analysis in RPP (Open Science Collaboration, 2015) with more variables and a larger data set. The action-packed Figure 4 reports two metrics of feature importance for both the binary (blue) and continuous (red) models. The length of horizontal bars represents Random Forest variable importance, a measure of the relative frequency of each feature in the many individual decision trees. Features that are included in a large proportion of the individual trees will have a long bar. For example, the top three bars are the power, p-value, and effect size in the original studies. The variables are sorted by their importance in predicting binary classification. Since the RF model is hierarchical and nonlinear, a single variable can be included in many different individual trees with both positive and negative effects on predicted outcome. While we can identify the most important variables in the model, we cannot determine the direction of their influence. We therefor also present the linear effect of each variable in a Logit model. These are shown in small boxes between the variable names (on the left) and the bars on the right. This analysis uses only variables that have been selected as important by a LASSO. 7", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Pre-registered Out of Sample Validation", "text": "The Social Sciences Replication Project (SSRP; Camerer et al., 2018b) replicated 21 systematically selected papers published in Nature and Science published between 2011 and 2015. The authors also collected beliefs through a survey and a prediction market. We registered the predictions of the model before the replications had been conducted. 8 The results from this out of sample test are summarized and compared to market and survey beliefs in Figure 5 The out of sample predictions achieve accuracy similar to the median cross validated level, at 71% (AUC: 0.73). 9 When compared to researcher beliefs, the model has a mean absolute prediction error of 0.43, while the market achieves 0.30 and the survey 0.35. The difference between model and market is significant (Wilcoxon signed-rank test, z = \u22122.52, p = 0.012, n = 21), however more data is needed to verify these differences. The model predicts relative effect size with a Spearman correlation of 0.25 (p = 0.274), lower than the cross validated measure of 0.38. The mean absolute deviation is 0.33. A Wilcoxon sign-rank test cannot reject that the distributions of predicted and actual effect sizes are the same, z = \u22121.00, p = 0.317. Results are summarized in Figure 5. We see that the model produces quite conservative forecasts of effect size, often closer to 0.5 than the actual outcome. This results in large errors whenever the actual effect is substantially different from the original, which is most often true for failed replications. While the market and survey perform better than the model in this sample, the plot shows 7 The LASSO is a regularization algorithm, minimizing squared errors (or deviance) while keeping the absolute value of coefficients constrained by a penalty term. This method tends to shrink estimated coefficients that are unimportant towards zero, removing some variables completely ( Hastie et al., 2009). For the many variables that are common in the RF trees but have zero LASSO weights, there are blank spaces between variable definitions and RF- frequency bars. The features selected with positive weight by LASSO are then re-fitted in a regular Logit model (to \"unshrink\" their weights) and the coefficients of that non-regularized model are presented Figure 4. 8 Available at: https://osf.io/w2y96 9 The replications were conducted in a two-stage procedure, where more data was collected if the results from the first phase were not significant. Here, we use the results from the pooled data. If the model predicted a successful Stage 1 replication these predictions are used. If it predicted a failed first stage, predicted effect size and replication probability from Stage 2 are used instead. In the Supplementary Material we provide the results when only Stage 1 is used. The results are similar. how the measures often yield the same prediction. When they do not, it is often because the model incorrectly predicts that a replication will fail. represented by a triangle when the replication was successful (p < 0.05 and an effect in the same direction). In the left panel, the shape of purple data points are based on the predicted replication probability instead, using a 50% probability cutoff.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Discussion", "text": "We have derived an automated, data-driven method for predicting replicability of experiments. The method uses machine learning to discover which features of studies predict the strength of actual replications. Even with our fairly small data set, the model can forecast replication results with substantial accuracy - around 70%. Predictive accuracy is sensitive to the variables that are used, in interesting ways. The statistical features (p-value and effect size) of the original experiment are the most predictive. However, the accuracy of the model is also increased by variables such as the nature of the finding (an interaction, compared to a main effect), number of authors, paper length and the lack of performance incentives. All those variables are associated with a reduction in the predicted chance of replicability. The third bar in Figure 3 showed unchanged performance for a model with all replication-specific features excluded. There are a couple of possible reasons why removing replication features has no impact on model performance. For one thing, most variables have a small impact, that would be easier to identify in a larger data set. Second, it is obviously the case that a larger planned sample size has a direct impact on replication probability 10 . The reason why we do not find such a relationship is probably because our data mainly consists of well-powered replications and do not include multiple replications of the same experiment with different sample sizes. This makes it hard for the model to capture any variation in replicability caused by changes in planned sample size. It is also possible that the model is unable to separate the increase in power from the fact that weaker effects required larger replication samples. The fourth bar in Figure 3 presents the accuracy of a simple model that is only trained on effect size and p-value of the original experiment. It is not quite as accurate as models with more features, but still on par with the linear model trained on the full feature set. The analysis of correlations in Open Science Collaboration (2015) indicated the opposite, that experience of the experimenters and other such features are unimportant. With the substantial variability in out-of-sample accuracy, it is difficult to say for sure, but our results do indicate that these other features are correlated and indeed contribute to higher accuracy. We now probe a bit further into three results. The first result is that one variable that is predictive of poor replicability is whether central tests describe interactions between variables or (single-variable) main effects. Only eight of 41 interaction effect studies replicated, while 48 of the 90 other studies did. As Figure 4 shows, the interaction/main effect variable is in the top 10 in RF importance and is predictive, for both the binary and continuous replication measures. There is plenty of room for reasoned debate about the validity of apparent interactions. Here is our view: Interactions are often slippery statistically because detecting them is undermined by measurement error in either of two variables. In early discussion of p-hacking it was also noted that studies which hoped to find a main effect often end up concluding that there is a main effect which is only significant in part of the data (i.e., an interaction effect). The lower replication rates for interaction effects might be spurious, however. Tests of interactions often require larger samples, which could mean that the replications of these studies have lower power relative to those studies evaluating non-interacted effects. Nonetheless, the replicability difference is striking and merits further study. It is possible that the higher standard of evidence for establishing interactions needs to be upheld more closely. The second result is that some features that vary across studies are not robustly associated with poor replication: These include measures of language, location and subject type differences between replication and original experi- ments, as well as most of the variation in compensation (except for having no compensation at all, which is correlated with lower replicability). Our third result is that the model performs on par with previously collected peer judgments (subjective beliefs as measured by prediction market prices). In the sample used to estimate the model, it performed somewhat better than the prediction market, although we only had prediction market data on a subset of n = 55 studies. On the other hand the prediction market performed better than the model on the out of sample prediction test, but this was based on a small sample of n = 21. More data is needed to compare statistical approaches with peer judgments in prediction markets and surveys, to test which approach is associated with the most accurate predictions, and to look for potential complementarities. If the goal is replication prediction, the model has logistical advantages compared to running prediction markets, which require both participants and costly monetary incentives. Studying the differences between our algorithmic predictions and expert scientific judgment adds to a long literature comparing machine and man. For at least seventy years, it has been known that in many domains of professional judgment, simple statistical models can predict complex outcomes -PhD success, psychiatric disorders, recidivism, personality -as accurately as humans do subjectively ( Bishop and Trout, 2004;Dawes, 1979;Meehl, 1954;Youyou et al., 2015). Today, with the tremendous increase in data availability and development of more sophisticated predictive models, statistical prediction has become useful in many new areas (e.g., Kleinberg et al., 2018). It is likely that in some form, statistical methods will also increase the quality of human evaluation and prediction of scientific findings. The results presented in this paper suggests that there could be room for statistical methods to aid researchers when reviewing their peers' experiments.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Applications", "text": "Our method could be used in pre-and post-publication assessment, preferably after a lot more replication evidence is available to train the algorithm. In the current mainstream pre-publication review process, the decision about whether to publish a paper is almost entirely guided by the opinions of a small set of peer scientists and an editor. A systematic, fast, and accurate numerical method to estimate replicability could add more information in a transparent and fair way to this process. For example, when a paper is submitted an editorial assistant can code the features of the paper, plug those features into the models, and derive a predicted replication probability. This number could be used as one of many inputs helping editors and reviewers to decide whether a replication should be conducted before the paper is published. Post-publication, the model could be used as an input to decide which previously published experiments should be replicated. The criteria should depend on the goal of replication efforts. If the goal is to quickly locate papers unlikely to replicate, then papers with low predicted replicability should be chosen. Since replication is costly and laborious, using predicted probability can guide scarce resources toward where they are most scientifically useful. An important concern with any predictive algorithm is that its application will likely change incentives, and impact how scientists design their studies, undermining the algorithm's value. Some of these \"corrupting\" (Campbell, 1979) effects will actually be good: For example, since testing interaction effects seem to negatively associated with predicted replicability, scientists may be motivated to avoid searching for such interactions. But that could be an improvement, if such effects are difficult to find robustly with sample sizes used previously. Alternatively, scientists who are keen to find interactions can use higher-powered designs, which will increase predicted replicability. Other changes in practice to \"game\" the algorithm will likely be harmless, and some changes could reduce predictive accuracy. For example, longer papers tend to replicate less well. If scientists all shorten their papers (to increase their predicted replicability), without changing the quality of the science, then the paper length variable will gradually lose diagnostic value. Any implementation will need to anticipate this type of gaming. Some types of \"gaming\" could be truly unwanted. The trade off between algorithmic fairness and accuracy is a highly important question that is currently being studied extensively. In our case, including the gender composition or seniority of the author team potentially increases the risk that the model is discriminatory. If needed, such variables could easily be removed, with only a small penalty to accuracy. However, excluding a variable like gender composition will not necessarily remove the model's tendency to discriminate, as this variation could be captured also in other features ( Kleinberg et al., 2016). Of course, there are limits to how much we can conclude from our results. The data we use is not representative for experimental social science -the accuracy level and variable importance statistics may be specific to our sample, or to psychology and economics. Our sample of studies is also very small; having more actual replications is crucial to ensure that the model functions robustly. Moreover, the correlations we find do not identify causal mechanisms, so changing research practices (as in the \"gaming\" scenarios above) may have unknown consequences. Rather, our model is theory agnostic by design. We aim to predict replicability, not understand its causes. The promising and growing literature taking a theoretical approach to this questions (see e.g. Andrews and Kasy, 2017;Simonsohn, 2015;Simonsohn et al., 2014) should be seen as a complement to our work and could hopefully be used to improve future versions of this predictive model. Simultaneously, our insights will hopefully be useful for future theoretical investigations. The future is bright. There will be rapid accumulation of more replica- tion data, more outlets for publishing replications , new statistical techniques, and -most importantly -enthusiasm for improving replicability among funding agencies, scientists, and journals. An exciting replicability \"upgrade\" in science, while perhaps overdue, is taking place.", "title": "Predicting the Replicability of Social Science Lab Experiments *", "file_name": "Altmejd et al. - Predicting the Replicability of Social Science Lab.pdf"}
{"section": "Abstract", "text": "We conducted preregistered replications of 28 classic and contemporary published findings with protocols that were peer reviewed in advance to examine variation in effect magnitudes across sample and setting. Each protocol was administered to approximately half of 125 samples and 15,305 total participants from 36 countries and territories. Using conventional statistical significance (\u200b p\u200b < .05), fifteen (54%) of the replications provided evidence in the same direction and statistically significant as the original finding. With a strict significance criterion (\u200b p\u200b < .0001), fourteen (50%) provide such evidence reflecting the extremely high powered design. Seven (25%) of the replications had effect sizes larger than the original finding and 21 (75%) had effect sizes smaller than the original finding. The median comparable Cohen's \u200b d\u200b effect sizes for original findings was 0.60 and for replications was 0.15. Sixteen replications (57%) had small effect sizes (< .20) and 9 (32%) were in the opposite direction from the original finding. Across settings, 11 (39%) showed significant heterogeneity using the Q statistic and most of those were among the findings eliciting the largest overall effect sizes; only one effect that was near zero in the aggregate showed significant heterogeneity. Only one effect showed a Tau > 0.20 indicating moderate heterogeneity. Nine others had a Tau near or slightly above 0.10 indicating slight heterogeneity. In moderation tests, very little heterogeneity was attributable to task order, administration in lab versus online, and exploratory WEIRD versus less WEIRD culture comparisons. Cumulatively, variability in observed effect sizes was more attributable to the effect being studied than the sample or setting in which it was studied. Word count = 265", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Origins of Study Design", "text": "To obtain a candidate list of effects, we held a round of open nomination and invited submissions for any effect that fit the defined criteria (see the Coordinating Proposal available on the OSF: \u200b https://osf.io/uazdm/\u200b ). Those nominations were supplemented by ideas from the project team, and from direct queries for suggestions to independent experts in psychological science. The nominated studies were evaluated individually on the following criteria: (1) feasibility of implementation through a web browser, (2) brevity of study procedures (shorter procedures desired), (3) citation impact of the effect (higher impact desired), (4) identifiability of U n e d i t e d M a n u s c r i p t 10 a meaningful two-condition experimental design or simple correlation as the target of replication (with an emphasis on experiments), (5) general interest value of the effect, and (6) applicability to samples of adults. The nominated studies were evaluated collectively to assure diversity on the following criteria: (1) effects known to be observable in multiple samples and settings and others for which reliability of the effect is unknown , (2) effects known to be sensitive to sample or 1 setting and others for which variation is unknown or assumed to be minimal, (3) classic and contemporary effects, (4) breadth of topical areas in social and cognitive psychology, (5) the research groups who conducted the study, and (6) publication outlet. More than 100 effects were nominated as potentially fitting these criteria. A subset of the project team reviewed these effects to maximize the number of included effects and diversity of the total slate on these criteria. No specific researcher was selected for replication because of beliefs or concerns about their research or the effects they have reported, but some areas and authors were included more than once because of producing short, simple, interesting effects that met the selection criteria. Once selected for inclusion, a member of the research team contacted the corresponding author (if alive) to obtain original study materials and get advice about adapting the procedure for this use. In particular, original authors were asked if there were moderators or other limitations to obtaining the result that would be useful for the team to understand in advance and, perhaps, anticipate in data collection. In some cases, correspondence with original authors identified limitations of the selected 1 Because the project goal was to examine variability in effect magnitudes across samples and settings, we were not interested in including studies that were known or suspected to be unreplicable. U n e d i t e d M a n u s c r i p t 11 effect that reduced its applicability for the present design. In those cases, we worked with the original authors to identify alternative studies or decided to remove the effect entirely from the selected set, and replaced it with one of the available alternatives. We split the studies into two slates that would require about 30 minutes each. We included 32 effects in total before peer review and pilot testing. In only one instance did original authors express strong concerns about inclusion in the study. Because we make no claim about the sample of studies being randomly selected or representative, we removed the effect from the project. With the remaining 31 effects, we pilot tested both slates with participation across the authors and members of their labs to ensure that each slate could be completed within 30 minutes. We observed that we underestimated the time required for a few effects. As a consequence, we had to remove three effects (Ashton-James, Maddux, Galinsky, & Chartrand, 2009;Srull & Wyer, 1979;Todd, Hanko, Galinsky, & Mussweiler, 2011), shorten or remove a few individual difference measures, and slightly reorganize the slates to achieve the final 28 included effects. We divided the studies across slate to be balanced on the criteria above and to avoid substantial overlap in topics. Following the Registered Report model (Nosek & Lakens, 2014), prior to data collection the materials and protocols were formally peer reviewed in a process conducted by the journal editor.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Disclosures", "text": "Preregistration\u200b . The accepted design was preregistered at \u200b https://osf.io/ejcfw/\u200b . Data, materials, and online resources\u200b . Comprehensive materials, data, and supplementary information about the project are available at \u200b https://osf.io/8cd4r/\u200b . Any deviations U n e d i t e d M a n u s c r i p t 12 from the preregistered design in study description or implementation are recorded in supplementary materials (\u200b https://osf.io/7mqba/\u200b ). Any changes to analysis plans are noted with justification and comparisons between original and revised analytic approaches, also available in supplementary materials (\u200b https://osf.io/4rbh9/\u200b ), see Table 1 for a summary. A guide to the data analysis code is available at: \u200b https://manylabsopenscience.github.io/\u200b . Measures\u200b . We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study. Subjects\u200b . The research was conducted in accordance with the Declaration of Helsinki and following local requirements for Institutional Review Board approvals for each of the data collection sites.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Conflicts of Interest\u200b . Brian Nosek is Executive Director of the non-profit Center for", "text": "Open Science which has a mission to increase openness, integrity, and reproducibility of research. U n e d i t e d M a n u s c r i p t", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "16", "text": "Belgium (2), Brazil (1), Canada (4), Chile (3), China (5), Colombia (1), Costa Rica (2), Czech Republic (3), France (2), Germany (4), Hong Kong, China (3), Hungary (1), India (5), Italy (1), Japan (1), Malaysia (1), Mexico (1), The Netherlands (9), New Zealand (2), Nigeria (1), Poland (6), Portugal (1), Serbia (3), South Africa (3), Spain (2), Sweden (1), Switzerland (1), Taiwan (1), Tanzania (2), Turkey (3), The United Arab Emirates (2), The United Kingdom (4), and Uruguay (1). Details about each site of data collection are available here: \u200b https://osf.io/uv4qx/\u200b . Of those that responded to demographics questions, in Slate 1 34.5% were men, 64.4% were women, 0.3% selected \"Other\", and 0.8% selected \"Prefer not to answer\". The average age for Slate 1 was 22.37 (SD = 7.09) . For Slate 2, 35.9% were men, 62.9% were women, 0.4% 2 selected \"Other\", and 0.8% selected \"Prefer not to answer\". The average age for Slate 2 was 23.34 (SD = 8.28) . Variation in demographic characteristics across samples is documented at 3 https://osf.io/g3bza/\u200b .", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Procedure", "text": "The study was administered over the Internet for standardization across locations. At some locations, participants completed the survey in a lab or room on computers or tablets, whereas in other locations the participants completed the survey entirely online at their own convenience. Surveys were created in Qualtrics software (qualtrics.com) and unique links to run the studies were sent to each data collection team to track the origin of data. Each site was assigned an identifier. These identifiers can be found under the \"source\" variable in the public dataset. Data were deposited to a central database and analyzed together. Each team created a 2 Excluding age responses > 100 3 Excluding age responses > 100 U n e d i t e d M a n u s c r i p t 17 video simulation of study administration to illustrate the features of the data collection setting. For languages other than English, labs completed a translation and back translation of the study materials to check against original meaning (cf. Brislin, 1970). Labs decided themselves the appropriate language for their sample and adapted materials for content appropriateness for the national sample (e.g., editing monetary units). Assignment of labs to slates maximized national diversity for both slates. If there was one lab for a country, it was randomly assigned to a slate using random.org. If there was more than one lab for a country, then labs were randomly assigned to slate using random.org with the exception that they were evenly distributed across slates as closely as possible (e.g., 2 in each slate if there were 4 countries). Nearing data collection, we recruited some additional Asian sites specifically for Slate 1 to increase sample diversity. The slates were administered by a single experiment script that began with informed consent, then presented the effects in that slate in a fully randomized order at the level of participants, followed by the individual difference measures in randomized order, and then closing with demographics measures and debriefing.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Demographics", "text": "The demographics below were included to characterize each sample and for possible moderator investigations. Participants were free to decline to answer any question.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Age.\u200b Participants noted their age in years in an open-response box.", "text": "Sex.\u200b Participants selected \"male\", \"female\", \"other\", or \"prefer not to answer\" to indicate their biological sex. Race/ethnicity. \u200b Participants indicated race/ethnicity by selecting from a drop-down menu populated with options determined by the replication lead for each site. Participants could also Political ideology.\u200b Participants rated their political ideology on a scale with response options of: strongly left-wing, moderately left-wing, slightly left-wing, moderate, slightly right-wing, moderately right-wing, strongly right-wing. Instructions were adapted for each country of administration to ensure relevance of the ideology dimension to the local context. For example, the U.S. instructions read: \"Please rate your political ideology on the following scale. In the United States, 'liberal' is usually used to refer to left-wing and 'conservative' is usually used to refer to right-wing.\" Education.\u200b Participants reported their educational attainment on a single item \"What is the highest educational level that you have attained?\" using a 6-point response scale: 1 = no formal education, 2 = completed primary/elementary school, 3 = completed secondary U n e d i t e d M a n u s c r i p t 19 school/high school, 4 = some university/college, 5 = completed university/college degree, 6 = completed advanced degree. Socio-economic status.\u200b Socio-economic status was measured with the ladder technique (Adler et al., 1994). Participants indicated their standing in their community relative to other people in the community with which they most identify on a ladder with ten steps where 1 indicates people at the bottom having the lowest standing in the community and 10 referring to people at the top having the highest standing. Previous research demonstrated good convergent validities of this item with objective criteria of individual social status and also construct validity with regard to several psychological and physiological health indicators (e.g., Adler, Epel, Castellazzo, & Ickovics, 2000;Cohen, Alper, Doyle, Adler, Treanor, & Taylor, 2008). This", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "ladder was also used in Effect 12 in Slate 1 (Anderson, Kraus, Galinsky, & Keltner, 2012, Study", "text": "3). Participants in that slate answered the ladder item as part of the Effect 12 materials and did not receive the item a second time. Data quality\u200b . \u200b Recent research in the area of careless or insufficient effort respond\u200b ing has moved toward refining implementation of established scales embedded in data collection to check for aberrant response patterns . We included two items at the end of the study, just prior to demographic items. The first item asked participants \"\u200b In your honest opinion, should we use your data in our analyses in this study?\"\u200b with yes\u200b and \u200b no\u200b as response options ). The second item was an Instructional Manipulation Check (IMC; Oppenheimer, , in which an ostensibly simple demographic question (\"Where are you completing this study?\") is preceded by a long block of text that contains, in part, alternative instructions for the participant to complete to U n e d i t e d M a n u s c r i p t 20 demonstrate they are paying attention (\"Instead, simply check all four boxes and then press \"continue\" to proceed to the next screen\").", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Individual Difference Measures", "text": "The following individual difference measures were included to allow future tests of effect size moderation.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Cognitive reflection (Finucane & Gullion, 2010)\u200b . The cognitive reflection task (CRT;", "text": "Frederick, 2005) assesses individuals' ability to suppress an intuitive (wrong) response in favor of a deliberative (correct) answer. The items on the original CRT are widely known, and the measure is vulnerable to practice effects (Chandler, Mueller, & Paolacci, 2014). As such, we used an updated version that is logically equivalent and correlates highly with the items on the original CRT . The three items are: (1) \"If it takes 2 nurses 2 minutes to measure the blood pressure of 2 patients, how long would it take 200 nurses to measure the blood pressure of 200 patients?\"; (2) \"Soup and salad cost $5.50 in total. The soup costs a dollar more than the salad. How much does the salad cost?\"; and, (3) \"Sally is making tea. Every hour, the concentration of the tea doubles. If it takes 6 hours for the tea to be ready, how long would it take for the tea to reach half of the final concentration?\" Also, we constrained the total time available to answer the three questions to 75 seconds. This likely lowered overall performance on average as it was somewhat less time than some participants took in pretesting. Subjective well-being (Veenhoven, 2009). \u200b Subjective well-being was measured with a single item \"All things considered, how satisfied are you with your life as a whole these days?\" on a response scale from 1 \"dissatisfied\" to 10 \"satisfied\". Similar items are included into numerous large-scale social surveys (cf. ) and have shown satisfactory U n e d i t e d M a n u s c r i p t 21 reliabilities (e.g., Lucas & Donnellan, 2012) and validities (Cheung & Lucas, 2014;Oswald & Wu, 2010;Sandvik, Diener, & Seidlitz, 1993). Also, the scale had similar predictive validity as the Rosenberg Self-Esteem Scale.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "TIPI for Big-Five personality (Gosling, Rentfrow, & Swann, 2003)\u200b . The five basic", "text": "traits of human personality (Goldberg, 1981) --conscientiousness, agreeableness, neuroticism / emotional stability, openness / intellect, and extraversion --were measured with the Ten Item Personality Inventory ( Gosling et al., 2003). Each trait was assessed with two items on seven point response scales from 1 = \u200b disagree strongly\u200b to 7 = \u200b agree strongly\u200b . The five scales show satisfactory retest reliabilities (cf. Gnambs, 2014) and substantial convergent validities with longer Big Five instruments (e.g., Ehrhart et al., 2009;Gosling et al., 2003;Rojas & Widiger, 2014).", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Mood (Cohen, Sherman, Bastardi, Hsu, McGoey, & Ross, 2007)\u200b . There exist many", "text": "assessments of mood. We selected the single-item from Cohen and colleagues (2007). Respondents answer \"How would you describe your mood right now?\" on a 5-point response scale: 1 = extremely bad, 2 = bad, 3 = neutral, 4 = good, 5 = extremely good. Disgust Sensitivity Scale--Contamination Subscale (DS-R; Olatunji et al., 2007).\u200b The U n e d i t e d M a n u s c r i p t", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "22", "text": "DS-R is a 25-item revision of the original Disgust Sensitivity Scale (Haidt, McCauley, & Rozin, 1994). Subscales of the DS-R were determined by factor analysis. The contamination subscale includes the 5 items related to concerns about bodily contamination. For length considerations, only the contamination subscale was included for Effect 8 in Slate 1. No part of this scale appeared in Slate 2.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "The 28 Effects", "text": "Before describing the main results examining heterogeneity across samples and settings, we describe each of the 28 selected effects. We provide a summary of the main idea of the original research and the sample size, inferential test, and effect size that is the key result for replication. Then, we summarize the confirmatory aggregate result of the replication. The aggregate result is tested by pooling the data of all available samples, ignoring sample origin. An aggregate result was labelled consistent with the original finding if they were in the same direction and statistically significant as the original study conducted in a western, educated, industrialized, rich, democratic society ( Henrich et al., 2010). In four cases, the original study focused on cultural differences in the key effect. Our main replication result is the aggregate effect size regardless of cultural context. Whether effects vary by setting (or cultural context more generally) is examined in the heterogeneity analyses in the results section. If there was opportunity to test the original cultural difference with similar samples, they are reported as additional results in reports of the individual effects. For some of the effects, moderating influences were anticipated in advance by the original authors that could affect comparison of the original and replication effect sizes. If any were planned, we report the \u200b a priori\u200b identified additional, moderator, or subset analyses. The original authors suggested we may only replicate the pattern for \"Western\" participants for whom up and North are aligned with the predicted \"good\" and high-SES evaluation. As suggested by the original authors, the focal test for replicating the effect for \"Western\" participants was completed by selecting only participants across all samples who . This result is consistent with the hypothesis that people reporting that wealthier people tend to live in the North in their hometown also guess that wealthier people will tend to live in the North in a fictional city, and is a substantially larger effect compared to examining the sample as a whole. 24.20, \u200b p\u200b = 6.53e-115, \u200b d\u200b = 1.03; 95% CI [0. 94, 1.12]). This result is consistent with the finding from the original study demonstrating cultural differences in perceived location of wealth in a fictional city correlating with location of wealth in one's hometown. For most participants, the study was completed on a vertically oriented monitor display as opposed to completing a paper survey on a desk as in the original study. The original authors suggested \u200b a priori\u200b this may be important because associations between \"up\" and \"good\" or \"down\" and \"bad\" may interfere with any North/South associations. \u200b At ten data collection sites (\u200b N\u200b = 582), we assigned some participants to complete the slate on Microsoft Surface tablets resting on the table for horizontal administration. This addressed the original authors' hypothesis that the vertical orientation of the monitor would interfere with observing the relationship between cardinal direction on the map and perceived location of wealth. With just the participants using the horizontal tablets, those that said wealth tended to be in the north in their In Kay, Laurin, Fitzsimons, and Landau (2014), 67 participants generated what they felt was their most important goal. Participants then read one of two scenarios where a natural event (leaves growing on trees) was described as being a structured or random event. For example, in the structured condition, a sentence read \"The way trees produce leaves is one of the many examples of the orderly patterns created by nature\u2026\", but in the random condition it read \"The way trees produce leaves is one of the many examples of the natural randomness that surrounds us\u2026\". Next, participants answered three questions about their most important goal on a scale from \"1 = \u200b not very\u200b \" to \"7 = \u200b extremely\u200b \". The first measured subjective value of the goal and the other two measured willingness to engage in goal pursuit. Those exposed to a structured event Alter and colleagues (2007) investigated whether a deliberate, analytic processing style can be activated by incidental disfluency cues that suggest task difficulty. Forty-one participants The original study focused on the two moderately difficult items from the six administered. Our confirmatory analysis strategy was sensitive to potential differences across samples in ability on syllogisms. We first determined which syllogisms were moderately difficult to participants by excluding any of the six items, within each sample, that were answered correctly by fewer than 25% of participants or more than 75% of participants across conditions. The remaining syllogisms were the basis of computing mean syllogism performance for each participant.   The original authors also hypothesized that this effect is sensitive to task order. If people are already thinking carefully (or if they're fatigued), the disfluency manipulation might not change how deeply they engage with the task. As such, the effect may be most detectable when it is done first. Considering only participants who did this task first (\u200b N\u200b = 988), participants in the hard People on the political left (liberal) and political right (conservative) have distinct policy preferences and may also have different moral intuitions and principles. \u200b 1,548\u200b participants across the ideological spectrum \u200b rated whether different concepts such as \u200b purity\u200b or \u200b fairness\u200b were relevant \u200b for deciding whether something was right or wrong. Items that emphasized concerns of harm or fairness (individualizing foundations) \u200b were deemed more relevant for moral judgment by the political left than right (\u200b r \u200b = -0.21, \u200b d\u200b = -0.43, 95% CI [-0.55, -0.32]), whereas items that emphasized concerns for the ingroup, authority, or purity (binding foundations) were deemed more relevant for moral judgment by the political right than left (\u200b r \u200b = 0.25, \u200b d\u200b = 0.52, 95% CI [0. 40, 0.63]) . Participants rated the relevance to moral judgment of 15 items (3 for each 6 foundation) in a randomized order on a 6-point scale from \"not at all relevant\" to \"extremely relevant\". The primary target of replication was the relationship of political ideology with the \"binding\" foundations. In the aggregate sample (\u200b N\u200b = 6,966), items that emphasized concerns for the ingroup, authority, or purity were deemed more relevant for moral judgment by the political  ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "AFFECTIVE PSYCHOLOGY OF RISK (Rottenstreich & Hsee, 2001, Study 1)", "text": "Forty participants chose whether they would prefer an affectively attractive option (a kiss from a favorite movie star) or a financially attractive option ($50). In one condition, participants made the choice imagining a low probability (1%) of getting the outcome. In the other condition, participants imagined that the outcome was certain, they just needed to choose which one. When the outcome was unlikely 70% preferred the affectively attractive option, when the outcome was certain 35% preferred the affectively attractive option (\u03c7\u200b 2\u200b   Bauer and colleagues (2012) examined whether being in a consumer mindset would lead to less trust towards others. In Study 4, 77 participants read about a hypothetical water conservation dilemma in which they were involved. Participants were randomly assigned to either a condition that referred to the participant and others in the scenario as \"consumers\" or as  Miyamoto and Kitayama (2002) examined whether Americans would be more likely than Japanese to show a bias toward ascribing to an actor an attitude corresponding to the actor's behavior, referred to as correspondence bias (Jones & Harris, 1967). In their Study 1, 49 Japanese and 58 American undergraduates learned they would read a university student's essay about the death penalty and infer the student's true attitude toward the issue. The essay was either in favor or against the death penalty, and it was designed to be diagnostic or not very diagnostic of a strong attitude. After reading the essay, participants learned that the student was assigned to argue the pro-or anti-position. Then, participants estimated the essay writer's actual attitude toward capital punishment and the extent to which they thought the student's behavior was constrained by the assignment. ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "INTUITIVE DISAPPROVAL OF GAYS (Inbar, Pizarro, Knobe, & Bloom, 2009, Study 1)", "text": "Behaviors that are deemed morally wrong may be judged as more intentional (Knobe, 2006). Thus, people who judge the portrayal of gay sexual activity in the media as an intentional original \u200b q = \u200b 0.70). Disgust sensitivity was very weakly related to intentionality and there was no mean difference in intentionality between gay kissing and kissing conditions. Follow-up analyses. \u200b The original authors avoided administering these studies on computer, rather than with paper and pencil, to avoid the possibility that the numeric keys on the keyboard might serve as primes. We administered this task with paper and pencil at 11 sites. . This result does not support the hypothesis that more siblings is positively related with prosocial orientation. Direct comparison of effect size is not possible because of changes in measures, but the replication effect size was near zero.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "JUSTIFICATIONS (Hauser, Cushman, Young, Jin, & Mikhail, 2007, Scenarios 1+2)", "text": "The principle of the double effect suggests that acts that harm others are judged as more morally permissible if the act is a foreseen side effect rather than the means to the greater good. Hauser and colleagues (2007)  In the aggregate replication sample (\u200b N\u200b = 6,842 after removing participants that responded in less than 4 seconds), 71% of participants judged the action in the foreseen side effect scenario as permissible, but only 17% of participants in the greater good scenario judged it as permissible. The difference between the proportions was significant (\u200b p \u200b = 2.2e-16), OR = 11.54, \u200b d = \u200b Follow-up analyses. \u200b Variations of the trolley problem are well-known. The original authors suggested the effect may be weaker for participants who have previously been exposed to this sort of task. We included an additional item assessing participants' prior knowledge of the task. Among the 3,069 participants reporting that they were not familiar with the task, the effect size was \u200b d\u200b = 1.47, 95% CI [1.38, 1.57]; and among the 4,107 familiar with the task, the effect size was \u200b d\u200b = 1.20, 95% CI [1.12, 1.28]. This suggests moderation by task familiarity, but the effect was very strong regardless of familiarity.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "SUBJECTIVE WELL-BEING (Anderson, Kraus, Galinsky, & Keltner, 2012, Study 3).", "text": "Anderson and colleagues (2012) examined the relationship between sociometric status (SMS), socioeconomic status (SES), and subjective well-being. According to the authors, SMS refers to interpersonal wealth, whereas SES measures fiscal wealth. Study 3 examined whether SMS has stronger ties than SES to well-being. In a 2 \u200b x\u200b 2 between subjects design, 228 Mechanical Turk participants were presented with descriptions of people who were either relatively high or low on either socioeconomic or sociometric status, and then made upward or downward social comparisons (e.g., participants in the high sociometric status condition imagined and compared themselves with a low sociometric status person). Then, participants wrote about what it would be like to interact with such people, and then reported subjective well-being. Results showed a significant 2 \u200b x\u200b 2 interaction (\u200b F\u200b (1, 224) = 4.73, \u200b p \u200b = 0.03) such that participants in the high sociometric status condition had higher subjective well-being than those There were no differences between the two socioeconomic conditions, \u200b t\u200b (109)  for the main effect of experimental condition; meta-analysis (random effects model) of scenario effect sizes: \u200b d\u200b = 0.66). A later meta-analysis suggests that this effect is robust and moderate in size across a variety of paradigms (\u200b r\u200b = 0.31, Mullen et al., 1985). This study was replicated in Slate 1 and Slate 2 using different scenarios. In Slate 1, participants were presented with the \"supermarket\" vignette from the original study ( ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "CHOICE (Tversky & Kahneman, 1981, Study 10)", "text": "In Tversky and Kahneman (1981), 181 participants considered a scenario in which they were buying two items, one relatively cheap ($15) and one relatively costly ($125). Ninety-three participants were assigned to a condition in which the cheap item could be purchased for $5 less by going to a different branch of the store 20 minutes away. Eighty-eight participants saw another condition in which the costly item could be purchased for $5 less at the other branch. Therefore, the total cost for the two items, and the cost savings for traveling to the other branch, was the same across conditions. Participants were more likely to say that they would go to the other branch when the cheap item was on sale (68%) than when the costly item was on sale (29%, Z = 5.14, \u200b p\u200b = 7.4e-7, OR = 4.96, 95% CI [2.55, 9.90]). This suggests that the decision of whether to travel was influenced by the base cost of the discounted item rather than the total cost. For the replication, in consultation with one of the original authors, dollar amounts were adjusted to be more appropriate for 2014 (i.e., when the replication study was conducted). The stimuli were also replaced with consumer items that were relevant in 2014 and plausibly sold by a single salesperson (a ceramic vase and a wall hanging). In the aggregate replication sample (\u200b N = 7,228), participants were more likely to say that they would go to the other branch when the cheap item was on sale (49%) than when the costly item was on sale (32%, \u200b p\u200b = 1.01e-50, \u200b d\u200b = ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Trolley dilemma 2: \u200b A DISSOCIATION BETWEEN MORAL JUDGMENTS AND JUSTIFICATIONS \u200b (Hauser et al., 2007, Study 1, Scenarios 3+4)", "text": "This study was presented in Effect 11 in Slate 1 using different scenarios. In Slate 2, participants were presented with the \"Ned\" and \"Oscar\" scenarios as the \u200b greater good\u200b and foreseen side effect\u200b scenarios. In the original study, when these two effects were compared, 72% of subjects judged the action in the foreseen side effect scenario as permissible (95% CI [0.69, 0.74]), and 56% of subjects judged the action in the means to a greater good scenario as permissible (95% CI [0.53, 0.59]). The difference between the proportions was significant, \u03c7\u200b 2\u200b [1, N\u200b = 2612] = 72.35, \u200b p\u200b < 0.001, \u200b w\u200b = 0.17, \u200b d = \u200b 0.34, 95% CI [0.26, 0.42]. In the aggregate replication sample (\u200b N\u200b = 7,923) after removing participants who responded in less than 4 seconds, 64% of participants judged the action in the foreseen side effect scenario as permissible and 53% of participants in the greater good scenario judged it as permissible (95%). The difference between the proportions was significant (\u200b p \u200b = 4.66e  ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Tempting fate: WHY PEOPLE ARE RELUCTANT TO TEMPT FATE (Risen & Gilovich, 2008, Study 2)", "text": "Risen and Gilovich (2008) explored the belief that tempting fate increases bad outcomes. The authors tested whether people judge the likelihood of a negative outcome to be higher when they imagined themselves or a classmate tempting fate, compared to when they do not tempt fate. One hundred and twenty participants read a scenario in which either they or a classmate (\"Jon\") tempt fate (e.g., by not reading before class), or do not tempt fate (e.g., by coming to class prepared). Participants then estimated how likely it is that the protagonist (themselves or Jon) would be called on by the professor. The predicted main effect of tempting fate emerged, as participants judged the likelihood of being called on to be higher when the protagonist had In the aggregate replication sample (\u200b N\u200b = 8,000), participants judged the likelihood of being called on to be higher when they had tempted fate (\u200b M\u200b = 4.58, \u200b SD \u200b = 2.44) than when they  For the key confirmatory test, the original authors suggested that the sample should include only undergraduate student samples given the nature of the question. In that subsample (\u200b N\u200b = 4,599), participants judged the likelihood of being called on to be higher when they had tempted fate (\u200b M\u200b = 4.61, \u200b SD \u200b =2.42) than when they had not tempted fate (\u200b M \u200b = 4.07, \u200b SD \u200b = 2.36; t\u200b (  controlling for importance, does not support the original hypothesis that Indians are less likely to construe personal actions than interpersonal actions as choices, despite there being such a main effect in the full sample. Like in the full sample, this effect was moderated by ratings of the importance of the choice, such that interpersonal actions were more likely to be construed as choices at lower levels of importance, whereas personal actions were more likely to be construed as choices at higher levels of importance. This moderation result was not reported in the original paper. higher ratings of the importance of the choice.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "VERSUS INTUITIVE REASONING (Norenzayan, \u200b Smith, Kim, & Nisbett, \u200b 2002, Study 2)", "text": "The way people living in the West think may be more rule-based compared to the way people living in East Asia think. Fifty-two European American (27 men, 25 women), 52 Asian American (28 men, 24 women) and 53 East Asian participants (27 men, 26 women) were randomly assigned to either a classification (decide \"which group the target object belongs to\"; two-thirds of sample) or similarity judgment (decide \"which group the target object is most similar to\"; one-third of sample) condition. All participants categorized targets into two alternative groups, each consisting of four exemplars. Both targets and group exemplars were defined according to four binary features (e.g., long-stemmed or short-stemmed flowers). In Group 1, all exemplars had one feature in common with each other and with the target. In Group 2, there was no feature in common among all exemplars and the target, but one exemplar had three features in common with the target and three exemplars had two features in common with the target (see Figure 1). As a consequence, Group 2 looked more similar to the target, but there was no feature that could be used as a rule to categorize the target as a member of the group. But, for Group 1, a single feature common to all could be used as a rule for classification. Each set of targets and groups had a mirror-image target so that one group could be used for rule-based classification for one target, and the other group could be used for rule-based classification for the other target. When asked \"which Group the target object \u200b belongs \u200b to\" participants across all three cultures preferred to classify based on rule (M = 69% European Americans; M = 70% East Asians) rather than on  For replication, we preregistered to compare the percentage of rule-based responses between the \"belong to\" and \"similar to\" conditions for which European Americans showed no difference (\u200b d\u200b = 0.00, 95% CI [-0.15, 0.15]) and East Asians showed a greater likelihood of selecting rule-based responses in the \"belong to\" than the \"similar to\" condition (\u200b d\u200b = 0.67, 95% CI [0.52, 0.81). Note that this preregistered plan was a comparison across the experimental conditions, whereas Norenzayan et al. focused their analysis and theoretical interest on between cultural groups comparisons within experimental conditions. For the replication analysis, we computed for each participant the percentage of rule-based responses and tested whether the means of the two experimental groups (\"belong to\" versus \"similar to\") on this DV were equal with a \u200b t\u200b -test for independent samples. In the aggregate replication sample (\u200b N\u200b = 7,396), participants who were asked \"which Group the target object \u200b belongs \u200b to\" were more likely to classify based on a rule (\u200b M\u200b = 64%, \u200b SD\u200b = 25%) than family resemblance (36%), and participants asked \"which group the target object is more \u200b similar to\" were more likely to classify based on family resemblance (56%) than a rule (\u200b M\u200b = 44%, \u200b SD\u200b = 21%). The likelihood of using a rule was higher in the \"belong to\" condition compared to the \"similar to\" condition (\u200b t\u200b (7,227.59) = 37.05, \u200b p \u200b = 3.04e-275, \u200b d\u200b = 0.86, 95% CI [0.81, 0.91]). This is in the same direction as the original aggregate result with a somewhat larger effect size--People were more likely to categorize based on a rule when considering what the target \"belongs to\" and more likely to categorize based on family resemblance when considering what the target is \"similar to\". 7", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "7", "text": "Norenzayan et al.'s original study had two key predictions: (1) all cultural groups would show more rule-based responding in the \"belong to\" than in the \"similar to\" conditions, and (2) the European American sample would show more rule-based responding across both conditions. They observed evidence for the first prediction, and evidence for the second prediction only in the \"similar to\" condition. Across the replication samples, we also observed greater likelihood of selecting rule-based responses in U n e d i t e d M a n u s c r i p t 53 Follow-up analyses\u200b . For this effect in particular, we identified \u200b a priori\u200b that it and Tversky and Gati (1978) both involved similarity judgments and thus order of these may be particularly relevant. We compared whether this effect was moderated by Tversky and Gati (1978) appearing before or after, and observed very weak moderation by order (\u200b t\u200b (7,392) = 2.34, p \u200b = 0.02, \u200b d \u200b = 0.05, 95% CI [0.01, 0.10]).", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "THAN HIGH-VALUE OPTIONS (Hsee, 1998, Study 1)", "text": "Hsee (1998) demonstrated the less-is-better effect wherein a less expensive gift can be perceived as more generous than a more expensive gift when the less expensive gift is relatively higher priced compared to other items in its category, and the more expensive item is a low-priced item compared to other items in its category. Eighty-three participants imagined that they were about to study abroad and had received a goodbye gift from a friend. In one condition, participants imagined receiving a $45 scarf bought in a store where the prices of scarves ranged from $5 to $50. In the other condition, participants imagined receiving a $55 coat bought in a store where the prices of coats ranged from $50 to $500. Participants in the scarf condition considered their gift giver significantly more generous (\u200b M\u200b = 5.63) than those in the coat condition (\u200b M\u200b = 5.00; \u200b t\u200b (82) = 3.13, \u200b p\u200b = 0.002, \u200b d\u200b = 0.69, 95% CI [0.24, 1.13]), despite the gift being objectively less expensive. In the replication, the dollar values were approximately adjusted for inflation. We converted this amount to local currencies at sites where the U.S. dollars would be less familiar to the \"belong to\" condition compared to the \"similar to\" condition, but the WEIRD samples showed more use of rule-based responses than less WEIRD samples in the \"belong to\" condition (WEIRD M = 65.2%, less WEIRD M = 59.8%), and less use of rule-based responses than less WEIRD samples in the \"similar to\" condition (WEIRD M = 42.8%, less WEIRD M = 48.4%). ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "MORALITY AND PHYSICAL CLEANSING (Zhong & Liljenquist, 2006, Study 2)", "text": "Zhong and Liljenquist (2006) investigated whether moral violations can induce a desire for cleansing. In Study 2, under the guise of a study assessing personality from handwriting, 27 participants hand-copied a first-person account of an ethical act (helping a co-worker) or unethical act (sabotaging a co-worker). Then, participants rated the desirability of five cleaning products and five non-cleaning products. Participants who copied the unethical account (\u200b M\u200b = 4.95, \u200b SD\u200b = 0.84) reported that the cleansing products were more desirable than participants who The effect of interest for replication is whether condition affects ratings of the cleaning products. In the aggregate replication sample (\u200b N\u200b = 7,001), after removing participants that copied less than half of the target article, participants who copied the unethical account (\u200b M\u200b = 3.95, \u200b SD\u200b = 1.43) reported that the cleansing products were similarly desirable as participants who  ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "PART-WHOLE QUESTION SEQUENCES: A CONVERSATIONAL LOGIC ANALYSIS (Schwarz, Strack, & Mai, 1991, Study 1)", "text": "One hundred participants answered a question about life satisfaction in a specific domain \"How satisfied are you with your relationship?\" and a question about life satisfaction in general \"How satisfied are you with your life-as-a-whole?\" Participants were randomly assigned to the order of answering the specific and general questions. When the specific question was asked U n e d i t e d M a n u s c r i p t 57 first, the correlation between the responses to the two questions was strong (\u200b r\u200b = 0.67, \u200b p\u200b < 0.05). When the specific question was asked second, the correlation between them was weaker (\u200b r\u200b = 0.32, \u200b p\u200b < 0.05). The difference between these correlations was significant, \u200b z\u200b = 2.32, \u200b p\u200b < 0.01, \u200b q\u200b = 0.48, 95% CI [0.07, 0.88]. The authors suggest that the specific-first condition makes the relationship more accessible such that participants are more likely to incorporate information about their relationship when evaluating a more general question about their life satisfaction. Because responses to the two items are linked by the accessibility of relationship information, they should be correlated. In contrast, in the specific-second condition, relationship satisfaction is not necessarily accessible and participants may draw on any number of different areas to generate their overall life satisfaction response. Thus, the correlation between the items is weaker than in the specific-first condition. In the aggregate replication sample (\u200b N\u200b = 7,460), \u200b when the specific question was asked first, the correlation between the responses to the two questions was moderate (\u200b r\u200b = 0.38). When the specific question was asked second, the correlation between them was slightly stronger (\u200b r\u200b = 0.44). The difference between these correlations was significant, \u200b z\u200b = -3.03, \u200b p\u200b = 0.002, \u200b q\u200b = -0.07, Follow-up analysis\u200b . In the original procedure, no other measures preceded the questions. The effect is about the influence of question context, so it is reasonable to presume that task order will have an impact on the estimated effect. As such, the most direct comparison with the original is for the conditions in which this task is administered first. In that subsample U n e d i t e d M a n u s c r i p t 58 (\u200b N\u200b = 470), \u200b when the specific question was asked first, the correlation between the responses to the two questions was strong (\u200b r\u200b = 0.41). When the specific question was asked second, the correlation between them was the same (\u200b r\u200b = 0.41). The difference between these correlations was not significant, \u200b z\u200b = 0.01, \u200b p\u200b = 0.99, \u200b q\u200b = 0.00, 95% CI [-0.18, 0.18].", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "WORSE THAN OTHERS (\u200b Shafir, 1993, Study 1)", "text": "One hundred and seventy participants imagined that they were on the jury of a custody case and had to choose between two parents. One of the parents had both more strongly positive and more strongly negative characteristics (extreme) than the other parent (average). Participants were randomly assigned to either decide to \u200b award\u200b custody to one parent or to \u200b deny\u200b custody to one parent. Participants were more likely to both award (64%) and deny (55%) custody to the extreme parent than the average parent, the sum of probabilities being significantly greater than 100% (\u200b z\u200b = 2.48, \u200b p\u200b = 0.013, \u200b d\u200b = 0.35, 95% CI [-0.04, 0.68]). This finding was consistent with the hypothesis that negative features are weighted more strongly when people are rejecting options, and positive features are weighted more strongly when people are selecting options (Shafir, 1993). In the aggregate replication sample (\u200b N\u200b = 7,901), participants were less likely to both award (45.5%) and deny (47.6%) custody to the extreme parent than the average parent, and the sum of probabilities (93%) was significantly smaller than the 100% we would expect if choosing and rejecting were complementary (\u200b z\u200b = -6.10 \u200b p\u200b = 1.1e-9, \u200b d\u200b = -0.13, 95% CI [-0.18, -0.09]). This result was slightly in the opposite direction of the original finding and it is incompatible with the hypothesis that negative features are weighted more strongly when rejecting options and positive   ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "ORDINARY LANGUAGE (Knobe, 2003, Study 1)", "text": "Knobe (2003) investigated whether helpful or harmful side effects were differently U n e d i t e d M a n u s c r i p t 61 perceived to be intentional. Consider, for example, an agent who knows that their behavior will have a particular side effect, but does not care whether the side effect does or does not occur. If the agent chooses to go ahead with the behavior and the side effect occurs, do people believe that the agent brought about the side effect \u200b intentionally\u200b ? Knobe (2003) had participants read vignettes about such situations and found that participants were more likely to believe the agent brought about the side effect intentionally when the side effect was harmful compared to when it was helpful. 82% of participants in the harm condition said that the agent brought about the side-effect intentionally, whereas 23% in the help condition said that the agent brought about the side-effect intentionally (\u200b \u03a7\u200b 2\u200b (1, \u200b N\u200b = 78) = 27.2, \u200b p\u200b < 0.001, \u200b d\u200b = 1.45, 95% CI [0.79, 2.77]). Agents who brought about harmful side effects were also rated as being more blameworthy than agents who brought about helpful side effects were rated as being praiseworthy \u200b t\u200b (120)  Ratings of intentionality in the harm and help conditions were compared for the direct replication using a 7-point scale rather than a dichotomous judgment. In the aggregate replication sample (\u200b N\u200b = 7,982), participants in the harm condition (\u200b M\u200b = 5.34, \u200b SD\u200b =1.94) said that the agent brought about the side-effect intentionally to a greater extent than did participants in the help condition (\u200b M\u200b = 2.17, \u200b SD\u200b = 1.69; \u200b t\u200b (7,843.86) = 78.11, \u200b p\u200b < 1.68e-305, \u200b d\u200b = 1.75, 95% CI   Agents who brought about harmful side effects were rated as being more blameworthy (\u200b M\u200b = 6.03, \u200b SD\u200b = 1.26) than agents who brought about helpful side effects were rated as being praiseworthy (\u200b M\u200b = 2.54, \u200b SD\u200b = 1.60; \u200b t\u200b (7,553.82) = 108.15, \u200b p\u200b < 1.68e-305, \u200b d \u200b = 2.42, 95% CI [2.36, 2.48]). This is also consistent with the original result with a notably larger effect size (2.42 versus 1.55).", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Directionality and similarity: STUDIES OF SIMILARITY (Tversky & Gati, 1978, Study 2)", "text": "Tversky and Gati (1978) investigated the relationship between directionality and similarity. 144 participants made 21 similarity ratings of country pairs in which one country (e.g., U.S.A.) was pre-tested as more prominent than the other (e.g., Mexico). The pair was presented with either the more prominent country first (U.S.A.-Mexico) or the less prominent country first (Mexico-U.S.A.). Two versions of the survey with 21 pairs were created that presented the more prominent country first \"about an equal number of times\", with the same pair of countries being manipulated between-subjects. Results indicated that participant similarity ratings were higher when less prominent countries were displayed first compared to when more prominent countries were displayed first, \u200b t\u200b (153)   For replication, participants were randomly assigned to one of the two counterbalancing conditions as described above, and were randomly assigned to rate either similarities or differences between the two countries. Following the original study, we considered the similarity and difference judgements as two independent samples. Therefore, each site has about half as much data for its critical test as other effects. The similarity ratings were the primary test for direct replication, difference ratings were a secondary analysis. On the aggregate similarities replication sample (\u200b N\u200b = 3,549), we created an asymmetry score for each subject, calculated as the average similarity for comparisons where the prominent country appeared second minus the average for the comparisons where the prominent country appeared first. Across participants, the asymmetry score was not different from zero (\u200b t\u200b (3,548 Table 2 presents the original study effect size, median effect size of replication studies, weighted means of replication effect sizes with 95% confidence intervals after pooling data of all samples, and proportion of samples that rejected the null hypothesis in the expected direction, rejected the null hypothesis in the unexpected direction, or did not reject the null hypothesis.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Results", "text": "U n e d i t e d M a n u s c r i p t", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "65", "text": "Effects are ordered from the largest global replication effect size consistent with the original study first to the smallest or opposite direction effects. Importantly, we separated those studies that had shown cultural differences in original research into two rows to avoid aggregating results when effects might be anticipated in one sample and not another. However, the differences observed between samples in the original research may not be expected to replicate between our aggregate comparisons across many cultural contexts. As such, we avoid drawing conclusions about replication or not of original cultural differences beyond what is discussed in the individual finding reports and in our aggregate observation of variability across samples. Overall, 14 of the 28 effects (50%) showed significant evidence in the same direction as the original finding, 1 was weakly consistent (4%), and 13 (46%) showed a null effect or The key correlation comparison had a p\u200b -value of .02 in the same direction as the original study, and a mean difference in perceived intentionality between the experimental conditions was not replicated (\u200b p\u200b = .457). In the original study, the mean difference was accounted for by the difference in correlations.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "9", "text": "For the four original studies that used two samples to make cultural comparisons, we defined the positive direction using the effect size observed in the original sample that was more western, educated, industrialized, rich, and democratic.included WEIRD and less WEIRD samples, \"consistent\" is defined as the WEIRD samples in the replications showing a significant effect in the same direction as the WEIRD sample in the original study.  ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Variation Across Samples and Settings", "text": "Our central interest was the variation in effect estimates across all samples and settings. In a linear mixed model with samples and studies as random effects, we compared the intra-class correlation of samples across effects (ICC = 0.782) which was quite large, with the intra-class U n e d i t e d M a n u s c r i p t 68 correlation of effects across samples (ICC = 0.004) which was near zero. In other words, to predict effect sizes across the 28 findings and dozens of samples studied here, it is very useful to know the effect being studied and barely useful to know the sample in which it is being studied. Next, we examined whether \u200b specific\u200b effects are sensitive to variation in sample or setting. For each of the 28 replication studies, we examined variability in effect sizes using a random effects meta-analysis (with restricted maximum likelihood as estimator for the between-study variance) and established heterogeneity estimates -Tau, Q and I\u200b 2\u200b -to determine if the amount of variability across samples exceeds that expected by measurement error. Because the study procedures are nearly identical (except for language translations), variation exceeding measurement error is likely to be due to effects of sample or setting, and interactions between samples and the materials. Eleven of the 28 effects (39%) showed significant heterogeneity with the Q-test (\u200b p\u200b < .001). Notably, of those showing such variability, 8 were among the 10 largest effect sizes. Only one of the non-significant replication effects showed significant heterogeneity using Q (Van Lange et al., 1997). The I\u200b 2\u200b statistic indicated substantial heterogeneity for some of the tests, with 10 (36%) showing at least medium heterogeneity (I\u200b 2\u200b 50%), and two showing heterogeneity larger than 75% ( Huang et al., 2014 andKnobe, 2003; see Table 3). Note, however, that estimation of heterogeneity is rather imprecise, as evidenced by many large confidence intervals of I\u200b 2\u200b , particularly for the cases with low estimates of heterogeneity. Ten of the 14 smaller I\u200b 2\u200b effects had a lower bound of 0. Also, the I\u200b 2\u200b statistic increases if sample size increases, so the large samples may be an explanation for the large I\u200b 2\u200b statistics that were observed (R\u00fccker, Schwarzer, Carpenter, & Schumacher, 2008). As in the first Many Labs project ( Klein et al., 2014), heterogeneity was more likely to occur for large effects than small U n e d i t e d M a n u s c r i p t 69 effects. The Spearman rank-order correlation between aggregate effect size of the pooled analysis and I\u200b 2 \u200b values is \u200b r \u200b = 0.56. Finally, with Tau, only one effect ( Huang et al., 2014) showed a substantial standard deviation among effect sizes (0.24), and 8 others showed modest heterogeneity near 0.10. Most of the effects, 19 of 28 (68%) showed near zero heterogeneity as estimated by Tau. Overall, this indicates that many effects showed minimal heterogeneity and, when it was observed, it was quite small. Table 3 summarizes moderation tests between lab and online samples. Just one result showed a significant difference between lab and online samples (Zhong & Liljenquist, 2006). WEIRDness score was higher or lower than the observed WEIRDness mean score across samples (see Figure 4). Table 3 also presents heterogeneity statistics for comparing WEIRD and less WEIRD cultures. For 13 of the 14 effects that were reliable and in the same direction as the original study, the finding was observed for both WEIRD and less WEIRD samples with similar effect magnitudes. The only exception was Huang et al. (2014) for which less WEIRD samples showed no overall effect and wide variability across samples. This is relatively consistent with the original study in which Hong Kong participants showed an effect in the opposite direction as U.S. participants presumably because observed differences in whether wealth people tended to live in the North or South between the samples. It is likely that there is wide variability in whether wealthy people tend to live in the North or South of the many different settings within WEIRD and less WEIRD samples of the present study producing this high observed variability. Among the 14 effects that were near null in the aggregate, there was little evidence for the original finding in either WEIRD or less WEIRD samples. However, for Savani et al., both WEIRD and less WEIRD samples were in the direction of the original less WEIRD sample. Ultimately, just three effects ( Huang et al., 2014, Knobe, 2003, and Norenzayan et al., 2002) showed significant evidence for moderation by WEIRDness after correcting for multiple comparisons. However, for Norenzayan et al., the cultural difference was the inverse of the original result though their original study did not have a theoretical commitment regarding the cultural differences in the condition effect that we tested. Norenzayan et al. focused on rule-based responses across conditions, and predictions that their European American sample U n e d i t e d M a n u s c r i p t 71 would show greater rule-based responses than the East Asian sample within each condition (see Footnote 7).", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Influence of task order", "text": "The order of presentation could moderate effect sizes. Across the 30 minute session, effects may weaken if participants tire or if earlier procedures interfere with later procedures. We did not observe this in prior Many Labs investigations with the same design (Ebersole et al., 2016; Klein et al., 2014), but it remains a potential moderator. Order of administration was randomized, enabling a direct test of this possibility. Figure 3 shows each effect size in rank order across all locations from 1 (presented first) to 13 or 15 (presented last in its slate). Table 4 shows the aggregate effect size, effect size when the study was administered first, and effect size when the study was administered last. Across the 28 findings, we observed little systematic evidence that effects are stronger (or weaker) when administered first compared to last. Also, there was no evidence of linear, quadratic or cubic trends by task order (see supplements for analytic details: \u200b https://osf.io/z8dqs/\u200b ). Considering all task positions for all 28 findings, the mean effect size fell outside of the 95% confidence interval for 29 of the 394 finding-position estimates (7.4%) suggesting that there may be some order effects. However, the distribution of those unusual results appears to be relatively random across findings and positions (Figure 3). U n e d i t e d M a n u s c r i p t", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "73", "text": "Authors of four of the original articles ( Alter et al., 2007;Giessner & Schubert, 2007;Miyamoto & Kitayama, 2002;Schwarz et al., 1991) noted \u200b a priori\u200b that their findings may be sensitive to order of administration. However, none of these showed evidence for systematic variation in effect magnitudes by task order. It is still possible that there are specific order effect influences, such as when a particular procedure immediately precedes another particular procedure; but these analyses confirm that the findings, in the aggregate, are robust to task order and, particularly, that task order cannot account for observation of null effects for any of the non-replicated results.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Discussion", "text": "We conducted preregistered replications of 28 published results with protocols that were peer reviewed in advance with data from about 125 samples, including thousands of participants from locations around the world. Using conventional statistical significance (\u200b p\u200b < 0.05), fifteen (54%) of the replications provided evidence in the same direction and statistically significant consistent with the original finding. With a strict significance criterion (\u200b p\u200b < 0.0001), fourteen (50%) provide such evidence reflecting the extremely high powered design (for Inbar et al., 2009 the replication \u200b p\u200b -value was 0.02). Seven (25%) of the replications had effect sizes (Cohen's \u200b d\u200b or q\u200b ) larger than the original finding and 21 (75%) had effect sizes smaller than the original finding. In WEIRD samples, the median Cohen's \u200b d\u200b effect size for original findings was 0.60 and for replications was 0.15 indicating a substantial decline (Open Science Collaboration, 2015). 10 Sixteen replications (57%) had small effect sizes (< .20) and 9 (32%) were in the opposite direction from the original finding. Three of these had an aggregate replication effect size that 10 These medians exclude the two studies that used Cohen's \u200b q\u200b for effect size estimates. Including those, despite the different scaling of \u200b d\u200b and \u200b q\u200b , yields similar medians of 0.60 and 0.09 respectively. U n e d i t e d M a n u s c r i p t 74 was \u200b significantly\u200b in the opposite direction Schwarz, et al., 1991;Shafir, 1993) at \u200b p\u200b < 0.05 but only one at \u200b p\u200b < 0.0001 (Shafir, 1993). There is no simple decision rule for declaring success or failure in replication or for detecting positive results ( Benjamin et al., 2018;Camerer et al., 2018;Open Science Collaboration, 2015). In Table 5, we show a variety of possible decision criteria to decide whether the observed global effect size successfully replicated the original finding. Two used the replication sample size either with a loose criterion of \u200b p\u200b < .05 (54% success rate) or a strict criterion of \u200b p\u200b < .0001 (50% success rate). The others consider what the \u200b p\u200b -value would have been for the observed effect size if it had been obtained with the original study sample size (41% success rate), 2.5x the original study sample size (Simonsohn, 2015; 44% success rate), or (c) 50 participants per group --a reasonably large sample compared to historical trends (Fraley & Vazire, 2014; 33% success rate). Nine of the effects (32%) were successful replications across all criteria and 13 (46%) were unsuccessful replications across all criteria. Six findings (21%) 11 varied in replication success depending on the criteria usually because the replication effect size was substantially smaller than the original effect size. The final column in Table 5 provides the sample size needed to detect the original finding with observed global effect size of the replications when alpha = 0.05 and power is 0.80. Findings that were highly replicable across all criteria were relatively large effect sizes and relatively efficient to investigate with modest samples (\u200b N\u200b 's 12 to 54 and one 200). Replicable findings that had somewhat weaker effect sizes in general or compared to the original study need more substantial sample sizes to study efficiently (\u200b N\u200b 's 200 to 2,184). Findings that were in the same direction as the original study but 11 \u200b Replication success could not be computed for three criteria for one finding because of the test used ( Savani et al., 2010) and for the 50 participants per group criterion for four others because of the test used. For simplicity, we considered only computed tests for this summary. U n e d i t e d M a n u s c r i p t 75 too weak to reject the null-hypothesis of no effect with our large samples would need massive samples to reject the null-hypothesis (\u200b N\u200b 's 6,283 to 313,958). Finally, null-hypothesis of the 10 findings that had effect sizes of 0 or in the opposite direction of the original cannot be rejected not matter what sample size is used. The high proportion of failures to replicate with extremely large samples and weaker effect sizes compared to original studies is consistent with the accumulating evidence in systematic replication studies (Camerer et al., 2016(Camerer et al., , 2018Ebersole et al., 2016;Klein et al., 2014a;Open Science Collaboration, 2015). We cannot identify whether these are due to errors in replication design, \u200b p\u200b -hacking in original studies, or publication bias with selecting for positive results despite pervasive low-powered research. However, it is notable that surveys and prediction markets with researchers predicting and betting on whether these studies would replicate were effective at predicting replication success. For example, the correlation between market price and replication success for Many Labs 2 studies was 0.755. These results are reported in a separate paper ( Dreber et al., 2018), and replicate other studies using prediction markets and surveys to predict replication success (Camerer et al., 2016(Camerer et al., , 2018Dreber et al., 2016). In any case, these findings provide further justification for improving transparency of research ( Miguel et al., 2014;Nosek et al., 2015), and preregistering studies to make all findings discoverable even if they are not published and preregistering analysis plans to make clear the distinction between confirmatory tests and exploratory discoveries for improving statistical inference ( Nosek et al., 2018;Wagenmakers et al., 2012). The main purpose of the investigation was to assess variability in effect sizes by sample and setting. It is reasonable to expect that many psychological phenomena are moderated by U n e d i t e d M a n u s c r i p t 76 variation in sample, setting, or procedural details, and that this may impact reproducibility (Henrich et al., 2010;Klein et al., 2014aKlein et al., , 2014bMarkus & Kitayama, 1991;Schwarz & Strack, 2014;van Bavel, Mende-Siedlecki, Brady, & Reinero, 2016). However, while calculations of intra-class correlations showed very strong relations of effect sizes across the findings (ICC = 0.782), they showed near zero relations of effect sizes across samples (ICC = 0.004). Sensibly, knowing the effect being studied provides a lot more information on effect size than knowing the sample being studied. Just 11 of the 28 effects (39%) showed significant heterogeneity with the Q-test, and most of these were among the effects with the largest overall effect size. Only one of the near zero replication effect (Van Lange et al., 1997) showed significant heterogeneity with the Q-test. In other words, if no effect was observed overall, there was also very little evidence for heterogeneity among samples. Using the \u200b I\u200b 2\u200b statistic, for approximately one third of all effects being studied (36%) at least medium heterogeneity across samples was found, but almost all evaluations of heterogeneity had high uncertainty (i.e., wide confidence intervals). Taken at face value, the \u200b I\u200b 2 statistics in Table 3 indicate that heterogeneity in samples is high for some of the findings, even when there is little evidence for an effect. For example, for Zaval et al. (2014), the main effect was not distinguishable from zero and 89% of the individual samples showed non-significant effects, close to expectation of samples drawn from a null distribution, and yet, the \u200b I\u200b 2\u200b is 37%. However, an average effect size of 0 together with a majority of studies with null results can co-exist with strong heterogeneity, as measured with \u200b I\u200b 2\u200b (\u200b https://osf.io/frbuv/\u200b ). \u200b I\u200b 2\u200b compares variability in the dependent variable across studies with variability within studies. With increasing power of primary studies, \u200b I\u200b 2\u200b will tend toward 100% if there is any evidence for U n e d i t e d M a n u s c r i p t 77 heterogeneity in the sample no matter how small the effect. As such, these estimates likely reflect the extremely large sample sizes rather than the amount of heterogeneity in absolute terms. By comparison, the estimates for Tau in Table 3 indicate a small standard deviation in effect sizes for all studies except one (Tau = 0.24; Huang et al., 2014). In fact, 19 of the 28 (68%) had an estimated Tau near 0 indicating minimal heterogeneity and 8 (29%) had an estimated Tau near .10 indicating a small amount of heterogeneity. This illustrates the key finding for this study. For some effects heterogeneity across samples is near zero. It is not so surprising that this is the case for effects that failed to replicate in general, but it was also occasionally observed for successful replications. More importantly, even among successful replications, when heterogeneity was observed, it was relatively weak. As a consequence, at least for the variation investigated here, heterogeneity across samples does not provide much explanatory power for failures to replicate. Estimates of average effect size and effect size heterogeneity may have been affected by imperfect reliabilities of instruments measuring the outcome variables. For instance, Hunter and Schmidt (1990) show how imperfect reliabilities attenuate effect size estimates and suggest correcting for these imperfections when estimating effect size. As both original and replication studies did not correct for these imperfect reliabilities, systematic differences in effect size estimates between original and replication studies cannot be explained by imperfect reliabilities, unless the measurement instruments were systematically much less reliable in the replication than in the original studies; we have no evidence that this is the case. Differences across labs in reliabilities of measurement instruments may also result in overestimation of effect size U n e d i t e d M a n u s c r i p t 78 heterogeneity in case of a true non-zero effect size. Insofar as these differences exist, our results likely overestimate heterogeneity as our analyses do not take imperfect reliabilities of variables into account. For 12 of the 28 findings, moderators or sample subsets that may be necessary to observe the effect were identified \u200b a priori \u200b by original authors or other experts during the Registered Report review process. These effect-specific analyses were reported with the individual effects. For 7 of those 12, the pooled result was null or in the opposite direction of the original; for the other 5, the pooled results showed evidence for the original finding. Among the 12, just one (8% of the total; Hauser et al., 2007, Trolley Dilemma 1) showed evidence consistent with the hypothesized moderator/subset, and two (17%) showed weak or partial evidence (Miyamoto & Kitayama, 2002;Risen & Gilovich, 2008). The other nine (75%) showed little evidence that narrowing the datasets to the samples and settings deemed most relevant to testing the hypothesis had impact on the likelihood of observing the effects or their effect magnitude. This does not mean that moderating effects do not occur, but it may mean that psychological theory is not yet advanced enough to predict them reliably. Another possible moderating influence, unique to the present design, was task order. Participants completed their slate of 13 to 15 effects in a randomized order. It is possible that tasks completed later in the sequence would be influenced by tasks completed earlier in the sequence, either because of the specific content of the task, or because of interference, fatigue, or other order-related influences (Ferguson, Carter, & Hassin, 2014;Kahneman, 2016;Schnall, 2014). Contrary to this prediction, we observed little evidence for systematic order effects for the 28 findings investigated. This replicates the lack of evidence for task order effects observed in U n e d i t e d M a n u s c r i p t 79 Many Labs 1 ( Klein et al., 2014) and Many Labs 3 ( Ebersole et al., 2016). Across 51 total replication tests (28 reported here; 13 in Klein et al., 2014, and10 in Ebersole et al., 2016) we have observed little evidence for reliable effects of task order. The idea that completing a study first, in the middle, or at the end of a sequence has an impact on the magnitude of the observed effect is appealing and, so far, unsupported. The same is true for effects of administration in lab versus on-line. Since the Internet became a source for behavioral research, there has been interest in the degree to which lab and on-line results are consistent with one another (Birnbaum, 2004;Dandurand, Shultz, & Onishi, 2008;Hilbig, 2016). As with task order, across Many Labs projects we have observed little evidence for an effect of mode of administration. There may be conditions under which lab versus online administration is consequential, but we did not observe meaningful evidence for its impact. Finally, we included an exploratory analysis of the moderating influence of WEIRD versus less WEIRD cultural differences. We sampled from 125 highly heterogeneous sources (64 Slate 1, 61 Slate 2) to maximize the possibility of observing variation in effects based on sample characteristics--39 U.S. samples and 86 samples from 35 other countries and territories. Ultimately, just three effects ( Huang et al., 2014;Knobe, 2003;Norenzayan et al., 2002) showed compelling evidence for differences between our WEIRD and less WEIRD samples. However, our approach characterized cultural differences at the most general level possible--a dichotomy of WEIRDness--and ignored the rich diversity within that categorization.  illustrates the highly skewed distribution. Scores > 0.70 were categorized as WEIRD, the rest were less WEIRD. Our summary analyses also do not address the possibility of highly specific regional variations such as differences between U.S. and British samples, nor did they examine why differences were observed. Nor do these analyses investigate many interesting sampling moderators available in this dataset such as individual differences, gender, and ethnicity. Some moderating influences could be evaluated using the present dataset; others will require new data collections to test. Also, a true examination of WEIRDness would need to more deliberately vary sampling across all dimensions --Western, Educated, Industrialized, Rich, and Democratic. Further analyses of the present dataset may inspire hypotheses to test in future studies.  ", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Implications", "text": "It is practically a truism that human behavior is contingent on the cultural and personal characteristics of the participants under study and the setting in which they are studied. The depth with which this idea is embedded in present psychological theorizing is illustrated by the appeals to \"hidden moderators\" as counterclaims to failures to replicate without empirically testing whether such moderators are operative (Baumeister & Vohs, 2016;Crisp, Miles, & Husnu, 2014;Gilbert et al., 2016;Ramscar, Shaoul, & Baayen, 2015;Schwarz & Clore, 2016;Stroebe & Strack, 2014;van Bavel et al., 2016). The present study suggests that dismissing failures to replicate as a consequence of such moderators without conducting independent tests of the hypothesized moderators is unwise. Collectively, we observed some evidence for effect-specific heterogeneity, particularly for larger effects, occasional evidence for cultural variation, and little evidence for procedural factors such as task order and lab versus online administration. There have been a variety of failures to replicate effects that were quite large in the original investigation (e.g., Doyen, Klein, Pichon, & Cleeremans, 2012;Hawkins, Fitzgerald, & Nosek, 2015;Johnson, Cheung, & Donnellan, 2014;Hagger et al., 2016). If effects are highly contingent on the sample and setting then they could be large and easily detected with some samples and negligible with other samples. We did not observe this. Rather, evidence for moderation or heterogeneity was mostly observed in the large, consistently detectable effects. Further, we observed some heterogeneity between samples, but \u200b a priori\u200b predictions (e.g., original authors predictions of moderating influences) and prior findings (e.g., previously observed cultural differences) were minimally successful in accounting for it. For these findings U n e d i t e d M a n u s c r i p t 82 at least, it appears that the cumulative evidence base has not yet matured to predict moderating influences reliably. Simultaneously, there is accumulating evidence that researchers can predict the likelihood that the effect of interest will replicate or not (Camerer et al., 2016(Camerer et al., , 2018Dreber et al., 2016Dreber et al., , 2018. For many multi-study investigations, a common template is to identify an effect in a first study, and then report evidence for a variety of moderating influences in follow-up studies. A pessimistic interpretation would suggest that this template may be a consequence of practices that inflate the likelihood of false positives. Consider the context in which positive results are perceived as more publishable than negative results (Greenwald, 1975), and common analytic practices may inadvertently increase the likelihood of obtaining false positives (Simmons et al., 2011). In a program of research, researchers might eventually obtain a significant result for a simple effect and call that Study 1. In follow-up studies, the authors might fail to observe the original effect and then initiate a search for moderators. Such post hoc searches necessarily increase the likelihood of false positives, but finding one may simultaneously reinforce belief in the original effect despite failing to replicate it. That is, identifying a moderator may feel like one is unpacking the phenomenon and explaining why the main effect \"failed\". An ironic consequence is that the identification of a moderator may increase confidence and decrease credibility of the effect simultaneously. Investigating moderating influences is much harder than presently appreciated in practice. For one, a \u200b 2 x 2 x 2\u200b ANOVA has a nominal false positive rate of ~30% for one or more of its seven tests (1 -0.95^7). Correcting for multiple tests in multivariate analyses is rare (Cramer et al., 2016). Also, typical study designs are woefully underpowered for studying moderation (Frazier, Tix, & Barron, 2004;McClelland, U n e d i t e d M a n u s c r i p t 83 1997), perhaps because researchers intuitively overestimate the power of various research designs (Bakker, Hartgerink, Wicherts, & van der Maas, 2016). The combination of low power and lack of correction for multiple tests means that every study offers ample opportunity for seeming to detect moderating influences that are not there. Ultimately, the main implication of the present findings is a plain one --it is not sufficient to presume moderating influences to account for variation in observed results of a phenomenon. Invocation of cultural, sample, or procedural variation as an account for differences in observed effects could be a reasonable hypothesis, but is not a credible hypothesis until it survives confrontation with a confirmatory test (Nosek, Ebersole, DeHaven, & Mellor, 2018).", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Limitations", "text": "The present study has the strength of very large samples collected from a wide variety of sources and cultures. Nevertheless, the generalizability of these results to other psychological findings is unknown. Here, 50% of the examined findings reproduced the original results, roughly consistent with other large-scale investigations of reproducibility (Camerer et al., 2016(Camerer et al., , 2018Ebersole et al., 2016;Klein et al., 2014;Open Science Collaboration, 2015). However, the findings selected for replication were not a random sample of any definable population, nor was it a large sample of findings. It may be surprising that just 50% of findings reproduced under these circumstances (original materials, peer review in advance, extremely high power, multiple samples), but that does not mean that 50% of all findings in psychology will reproduce, or fail to reproduce, under similar circumstances. This study has the advantage over the prior work by having many tests and large samples for relatively precise estimation. Nevertheless, the failures to replicate do not necessarily mean that the tested hypotheses are incorrect. The lack of effect may be limited to the particular procedural conditions examined here. Future theory and evidence will need to account for why the effects are not observed in these circumstances if they are replicable in others. Conversely, the successful replications add substantial precision for effect estimation and extend the generalizability of those phenomena across a variety of samples and settings.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Data availability", "text": "The amassed dataset is very rich for exploring the individual effects, potential interactions between specific effects, and alternate ways to estimate heterogeneity and analyze the aggregate data. Our analysis plan focused on the big picture and not, for example, exploring potential moderating influences on each of the individual effects. These are worthy analyses, but beyond the scope of a single paper. Follow-up investigations on these data could provide substantial additional insight. For commentaries solicited by \u200b Advances in Methods and Practices in Psychological Science\u200b we leveraged the extremely high-powered design of this study to demonstrate the productive interplay of exploratory and confirmatory analysis strategies. Commentators received a third of the dataset for analysis. Upon completion of the exploratory analysis, the analytic scripts were registered and applied to the holdout data for a mostly confirmatory test . Analysts' decisions could be influenced by advance observation of the summary results in this paper, but use of the holdout sample reduces other potential biasing influences. Finally, the full dataset (plus the portions used for the U n e d i t e d M a n u s c r i p t 85 exploratory/confirmatory commentaries) and all study materials are available at https://osf.io/8cd4r/\u200b so that other teams can use it for their own investigations.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Conclusion", "text": "Our results suggest that variation across samples, settings, and procedures has modest explanatory power for understanding variation in effects for 28 findings. These results do not indicate that moderating influences never occur. Rather, they suggest that hypothesizing a moderator to account for observed differences between contexts is not equivalent to testing it with new data. The Many Labs paradigm allows testing across a broad range of contexts to probe the variability of psychological effects across samples. Such an approach is particularly valuable to understanding the extent to which given psychological findings represent general features of the human mind.  Political ideology item changed from U.S.-centric \"liberal-conservative\" to regionally appropriate terms for left-right; Simplified analysis strategy None 5 Affect and Risk ) Original may have been paper-pencil None 6 Priming consumerism ( Bauer et al., 2012) None known None", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "7", "text": "Correspondence Bias (Miyamoto & Kitayama, 2002) Original paper-pencil; Names and location altered to be familiar to each sample; Essay prompt matched to legal status of capital punishment in nation; Included 10 second lag to increase likelihood of reading essay; Removed \"low diagnositicity\" study conditions None Original paper-pencil, tested difference with 10 sites conducting this task on paper-pencil; Matched markets with location of data collection; Updated pictures to modern smartphones None Original paper-pencil. Original counterbalanced the order of parents, the replication did not. Estimated effect size directly from the key Z test rather than estimating effect size with a logistic regression model.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "26", "text": "Priming warmth ( Zaval et al., 2014) Excluded original question about current temperature at the start of the study with 10-minute delay to starting actual study Excluding participants that made errors in sentence unscrambling was not preregistered, but decided a priori on recommendation of original authors.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "27", "text": "Intentional Side Effects (Knobe, 2003) Original may have been paper-pencil; Dependent variable changed from yes/no response to 7-point agreement scale None", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "28", "text": "Direction and Similarity (Tversky & Gati, 1978) Original was likely paper-pencil; Nations updated: Ceylon to Sri Lanka, West Germany to Germany, and U.S.S.R. to Russia Additional mixed models reported in supplement Notes: Additional description and supplementary analyses are available in Supplementary Notes (https://osf.io/4rbh9/). Full description of known differences from original study appear in the preregistered protocol such as notation of additional experimental conditions or outcome variables that were part of original study but not included in the replication (https://osf.io/ejcfw/). Unless noted otherwise, differences from original study were suggested by original authors or reviewed and approved during peer review. All studies differed in sample and setting of data collection from the original including the administration of studies sequentially in a slate. This is evaluated directly in the results section.  Notes: All effect sizes (ES) presented in Cohen's d units except for Schwarz and Inbar for which Cohen's q is provided. 95% CIs for original effect sizes used cell sample sizes when available and assumed equal distribution across conditions when not available. For original studies that observed a difference between WEIRD and a particular less WEIRD sample, we present summary results for WEIRD and all less WEIRD samples separately to avoid potentially misrepresenting replication success within subsamples. Figure 2 plots WEIRD and less WEIRD distributions of effects across all studies. U n e d i t e d M a n u s c r i p t  Disgust & Homophobia (Inbar et al., 2009) 0.05 0.00 55.80 58.00 0.56 3% 0%, 30% 0.00 2.89 0.09 0% 0%, 29% 0.00 0.18 0.67 5% 0%, 31% Assimilation and Contrast ( Schwarz et al., 1991) -0.07 0.10 60.39 58.00 0.39 15% 0%, 33% 0.10 0.61 0.44 17% 0%, 35% 0.10 0.00 0.97 16% 0%, 34% Cohen's d Effect Size Correspondence Bias (Miyamoto & Kitayama, 2002) 1.82 0.00 235.65 57.00 <.001 65% 46%, 73% 0.00 1.47 0.23 64% 45%, 72% 0.00 2.83 0.09 64% 45%, 74% Intentional Side Effects ( Knobe, 2003) 1.75 0.14 631.72 58.00 <.001 93% 92%, 97% 0.10 26.43 <.001 91% 87%, 95% 0.14 2.55 0.11 93% 91%, 97% Trolley Dilemma 1 ( Hauser et al., 2007) 1.35 0.10 131.24 58.00 <.001 54% 32%, 66% 0.10 4.80 0.03 51% 27%, 64% 0.10 0.13 0.72 55% 32%, 67% False Consensus 1 ( Ross et al., 1977) 1.18 0.00 65.54 58.00 0.23 16% 0%, 41% 0.00 3.36 0.07 12% 0%, 38% 0.00 0.26 0.61 18% 0%, 43% Moral Typecasting (Gray & Wegner, 2009) 0.95 0.10 203.30 59.00 <.001 73% 62%, 83% 0.10 6.02 0.01 71% 58%, 81% 0.10 0.52 0.47 71% 59%, 82% False Consensus 2 ( Ross et al., 1977) 0.95 0.00 100.19 57.00 <.001 43% 18%, 62% 0.00 0.00 0.97 44% 19%, 63% 0.00 0.17 0.68 46% 21%, 65% Intuitive Reasoning ( Norenzayan et al. 2002) 0.86 0.10 156.75 56.00 <.001 66% 54%, 81% 0.10 20.58 <.001 55% 36%, 73% 0.10 0.69 0.41 67% 55%, 81% Less is Better (Hsee, 1998) 0.78 0.10 158.41 56.00 <.001 65% 49%, 77% 0.10 4.68 0.03 63% 46%, 75% 0.10 1.69 0.19 65% 49%, 77% Framing (Tversky & Kahneman, 1981) 0.40 0.00 55.20 54.00 0.43 6% 0%, 36% 0.00 1.46 0.23 3% 0%, 37% 0.00 0.20 0.66 7% 0%, 38% Direction & SES (Huang et al., 2014) 0.40 0.24 626.26 63.00 <.001 89% 84%, 92% 0.22 13.01 <.001 87% 81%, 91% 0.24 1.64 0.20 89% 84%, 92% Moral Foundations ( Graham et al., 2009) 0.29 0.09 175.26 59.00 <.001 64% 49%, 75% 0.09 0.25 0.62 65% 49%, 75% 0.09 1.26 0.26 65% 49%, 76% Tempting Fate (Risen & Gilovich, 2008) 0.18 0.00 87.82 58.00 0.01 36% 6%, 54% 0.00 1.61 0.20 34% 3%, 53% 0.00 0.53 0.47 37% 7%, 55% Trolley Dilemma 2 ( Hauser et al., 2007) 0.25 0.00 60.40 59.00 0.42 12% 0%, 33% 0.00 0.90 0.34 10% 0%, 34% 0.00 0.14 0.71 11% 0%, 31% Priming consumerism ( Bauer et al., 2012) 0.12 0.00 63.78 53.00 0.15 12% 0%, 49% 0.00 0.04 0.85 14% 0%, 50% 0.00 0.30 0.58 15% 0%, 51% Incidental Anchors (Critcher & Gilovich, 2008) 0.04 0.00 64.88 58.00 0.25 6% 0%, 43% 0.00 0.11 0.75 8% 0%, 44% 0.00 1.17 0.28 4% 0%, 41% Social Value Orientation (Van Lange et al., 1997) -0.03 0.00 103.56 53.00 <.001 50% 28%, 68% 0.00 1.15 0.28 50% 28%, 68% 0.00 1.15 0.28 49% 26%, 67% Moral Cleansing (Zhong & Liljenquist, 2006) 0.00 0.00 65.59 51.00 0.08 22% 0%, 52% 0.00 1.17 0.28 21% 0%, 52% 0.00 9.15 <.001 4% 0%, 46% Position & Power (Giessner & Schubert, 2007) 0.03 0.00 62.87 58.00 0.31 3% 0%, 42% 0.00 0.00 0.96 5% 0%, 43% 0.00 6.19 0.01 4% 0%, 35% Direction and Similarity (Tversky & Gati, 1978) 0.01 0.00 15.33 48.00 0.99 0% 0%, 0% 0.00 0.42 0.52 0% 0%, 0% 0.00 0.12 0.73 0% 0%, 0% SMS & Well-Being ( Anderson et al., 2012) -0.04 0.00 55.09 58.00 0.58 2% 0%, 30% 0.00 0.83 0.36 2% 0%, 30% 0.00 3.21 0.07 0% 0%, 16% Priming warmth ( Zaval et al., 2014) -0.03 0.10 72.96 46.00 0.01 37% 8%, 63% 0.10 0.76 0.38 37% 8%, 63% 0.10 0.50 0.48 40% 11%, 64% Structure and goal-pursuit ( Kay et al., 2014) -0.02 0.00 33.95 51.00 0.97 0% 0%, 2% 0.00 3.10 0.08 0% 0%, 0% 0.00 2.06 0.15 0% 0%, 0% Incidental disfluency  -0.03 0.00 59.46 65.00 0.67 0% 0%, 27% 0.00 1.38 0.24 0% 0%, 27% 0.00 0.91 0.34 0% 0%, 21% Choosing or Rejecting (Shafir, 1993) -0.13 0.00 51.67 40.00 0.10 26% 0%, 52% 0.00 0.55 0.46 26% 0%, 53% 0.00 0.14 0.71 25% 0%, 50% Affect and Risk  -0.08 0.00 50.75 59.00 0.77 0% 0%, 21% 0.00 0.28 0.60 0% 0%, 22% 0.00 0.31 0.58 0% 0%, 25% Actions are Choices ( Savani et al. 2010) -0.18 0.00 155.49 56.00 <.001 64% 47%, 76% 0.00 3.69 0.05 62% 43%, 74% 0.00 0.61 0.44 65% 48%, 77% Notes: ES = Global Effect Size, repeated from Table 4 for comparison with Tau. All effect sizes based on Cohen's d units except for Schwarz and Inbar for which Cohen's q was used. Q(mod) have 1 df. Bonferroni correction for multiple comparisons suggests alpha=.004 (slate 1) and alpha = .003 (slate 2). Italics used for significant moderators. Random effects meta-analyses were conducted using the R package metafor (Viechtbauer, 2010). Between-study variance was estimated using REML. U n e d i t e d M a n u s c r i p t  (Giessner & Schubert, 2007) 0.03 -.01, .08 0.01 -.18, .19 -0.02 -.20, .15 Direction and Similarity (Tversky & Gati, 1978) 0.01 -.02, .04 0.13 -.01, .26 Intentional Side Effects (Knobe, 2003) 78 7982 1.75 Welch Two Sample t-test < 1E-10 < 1E-10 < 1E-10 < 1E-10 < 1E-10 Two Sided Fisher's Exact Test < 1E-10 < 1E-10 < 1E-10 < 1E-10 NA", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "20", "text": "False Consensus 1 ( Ross et al., 1977) 80 7205 1.18 Welch Two Sample t-test < 1E-10 < 1E-10 6.98E-06 < 1E-10", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "5.18E-08 26", "text": "Moral Typecasting (Gray & Wegner, 2009  Replication success criteria p-values are bold if they meet criteria for successful replication. Replication success criteria p-values are in italics if the replication global effect size was in the opposite direction of the original WEIRD sample. Replication success could not be determined based on the original study sample size, 2.5x original study sample size, and 50/group for Savani et al. (2010) because this information could not be computed for the test used in this effect. Replication success could not be determined based on 50/group if the test was a Two Sided Fisher's Exact Test (4 findings), because this would require making strong assumptions on how the sample size per group is distributed in the 2x2 frequency table. Power analyses are conducted using the Cohen's d and Cohen's q values of the replication effect sizes. Note that if another effect size was used in the original study (e.g. correlation, odds ratio, proportion), these were transformed to Cohen's d values. The last column shows the sample size needed to detect a significant effect in the same direction of the original finding for the observed global effect size with alpha = .05 and power = .80. Cells are marked NA if the global effect size was in the opposite direction of the original finding.", "title": "Many Labs 2: Investigating Variation in Replicability Across Sample and Setting", "file_name": "Klein et al. - Many Labs 2 Investigating Variation in Replicabil.pdf"}
{"section": "Abstract", "text": "Marketers often claim to be part of an exclusive tier (e.g., \"top 10\") within their competitive set. Although recent behavioral research has investigated how consumers respond to rank claims, prior work has focused exclusively on claims having a numerical format. But marketers often communicate rankings using percentages (e.g., \"top 20%\"). The present research explores how using a numerical format claim (e.g., \"top 10\" out of 50 products) versus an equivalent percentage format claim (e.g., \"top 20%\" out of 50 products) influences consumer judgments. Across five experiments, the authors find robust evidence of a shift in evaluations whereby consumers respond more favorably to numerical rank claims when set sizes are smaller (i.e., <100) but more favorably to percentage rank claims when set sizes are larger (i.e., >100), even when the claims are mathematically equivalent. They further show that this change in evaluations occurs because consumers commit format neglect when making their evaluations by relying predominantly on the nominal value conveyed in a rank claim and insufficiently accounting for set size.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Theoretical Framework", "text": "In this research, we provide evidence for the differential impact of equivalent numerical (e.g., Product X is in the top 10 out of 50 products) versus percentage (e.g., Product X is in the top 20% out of 50 products) rank claim formats on consumer judgments and decisions. According to the principle of description or frame invariance (e.g., Arrow 1982;Tversky and Kahneman 1986), the same position on a ranked list should produce the same evaluation irrespective of which format is used. If this were the case, there should be no difference in how the aforementioned claims (i.e., top 10 vs. top 20% out of 50 products) are evaluated, despite their distinct formats. How- ever, others have shown that the format of information presen- tation affects judgments (Gigerenzer 2015;Gigerenzer et al. 2009). For example, Gigerenzer et al. (2009) document that communicating mathematically equivalent predictions as odds ratios versus percentage probabilities affects risk perceptions. In the context of ranked lists, we posit that the size of the set in which an item is being judged (e.g., out of 50 products) will influence whether a numerical claim or an equivalent percentage claim engenders more favorable evaluations. Spe- cifically, we expect equivalent rank claims that use a numer- ical (vs. percentage) format to elicit more positive evaluations when the set size is relatively small. However, when the set size is relatively large, we expect evaluations to shift such that percentage (vs. numerical) claims will lead to more positive evaluations. We attribute this shift in evaluations to format neglect, a novel bias in which consumers fail to fully account for claim format because they infer that nominal value is more important to their evaluations than set size (even though both components should be considered in conjunction). As we dis- cuss subsequently, our theorizing enables us to delineate an exact set size that serves as the inflection point for the shift in evaluations between equivalent numerical claims and per- centage claims. In addition to demonstrating how and why format neglect influences evaluations, we also demonstrate two ways in which this bias can be eliminated. Next, we summarize the extant literature on ranking effects and discuss why consumers might commit format neglect when making their judgments.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Ranking Effects", "text": "Prior research on rankings has documented several heuristics that affect consumer evaluations. For example, Isaac and Schindler (2014) find that rank improvements resulting in a ranked item entering a new round-number tier, such as the \"top 10\" (e.g., moving from 11 to 10), are viewed more favorably than equivalent rank improvements that do not span tiers (e.g., moving from 12 to 11 or from 10 to 9). This \"top-ten effect\" stems from a subjective categorization process that does not properly account for equivalent changes in rank. In a similar vein, Leclerc, Hsee, and Nunes (2005) document a \"ranking effect\" in which consumers rely primarily on favorable numer- ical rankings within a particular list when making product decisions, without fully considering the prominence or status of the ranked list itself. For example, consumers in one study were willing to pay more for the highest-ranked automobile model in a lower-status category (e.g., the Volkswagen Passat V6) than the lowest-ranked model in a higher-status category (e.g., the Audi 4). Taken together, these findings suggest that when processing rank-related claims, consumers' evaluations and/or decisions can be biased because they do not equally consider or accord the same importance to each piece of infor- mation that they receive. We extend this stream of research by documenting a novel bias-format neglect-that differentially influences how con- sumers evaluate rank list claims. If consumers were to fully take claim format into account when making their evaluations, they would need to integrate the nominal value expressed in the claim with set size. We propose that instead of doing this, consumers insufficiently factor in set size and overrely on nominal values when making their evaluation, thereby commit- ting format neglect. In support of our position that consumers insufficiently account for set size, we next discuss a related bias-base rate neglect-that has produced evidence that deci- sion makers often fail to fully utilize information that is both relevant and available to them at the time of their decision.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Base Rate Neglect", "text": "Extensive research suggests that when making judgments, con- sumers often give greater importance to focal and specific information about a case while neglecting other relevant gen- eral information (e.g., Bar-Hillel 1980;Kahneman and Tversky 1973;Lyon and Slovic 1976;Nisbett and Ross 1980;Yan and Sengupta 2013). One such class of errors occurs when consu- mers neglect base rates. Base rate neglect occurs when partici- pants rely more on a salient, individuating feature or characteristic and disregard or discount the more general piece of information about the population as a whole. In a seminal article, Nisbett and Ross (1980) showed that when participants were provided the description of a target person (i.e., specific case-related information) along with the proportion of people by profession in the total population (i.e., general base-rate information), judgments were not significantly influenced by the relative incidence of these professions in the population. For example, when a target person was described as \"short, slim, and likes to read poetry,\" participants were likely to pre- dict that the target was a professor rather than a truck driver despite the greater incidence of truck drivers (vs. professors) in the overall population. Consumers' failure to sufficiently account for relevant information has been demonstrated in many contexts. For instance, when making probability judgments, consumers exhi- bit a variation of base rate neglect called ratio bias or denomi- nator neglect (Reyna and Brainerd 2008), in which they pay greater attention to the number of times a target event has occurred and fail to fully consider the number of opportunities for the event to occur. As a result, when evaluating the like- lihood of randomly drawing a red jelly bean from a tray, con- sumers believe their chances to be higher if the tray contains 10 red and 90 white jellybeans versus 1 red and 9 white jelly beans (Pacini and Epstein 1999). This bias occurs because consumers rely too much on the number of red jelly beans (the numerator) and too little on the total number of jelly beans (the denomi- nator) in the tray. We believe that a similar mechanism may be at play when consumers encounter rank claims. Specifically, we anticipate that consumers may insufficiently account for the set size when making evaluations because they believe this information is less important than nominal value, despite the fact that both pieces of information should be considered together to appro- priately account for the rank claim's format. Set size may be perceived as less diagnostic to consumers' evaluations because it is less dynamic across items (i.e., set size is the same for each item in a set, whereas nominal value is not) and over time, which makes it appear less focal or specific to the item that is being judged. Although our proposed mechanism of format neglect resem- bles base rate neglect, it also differs from previous instantia- tions of base rate neglect in several ways. For example, whereas denominator neglect (e.g., Pacini and Epstein 1999;Reyna and Brainerd 2008) constitutes a case of base rate neglect in which both the numerator and the denominator vary simultaneously (1/10 vs. 10/100), format neglect occurs even when the denominator (set size) remains constant and only the numerator (nominal value) and claim format are varied. Furthermore, although prior research has implied that base rate neglect often arises from attentional oversight (e.g., Landman and Manis 1983), we propose that format neglect is an evalua- tion bias. In other words, it is not necessarily the case that consumers fail to notice set size or are unable to correctly recall the set size conveyed in a claim. Rather, our contention is that consumers do not fully consider the implications of set size when producing their evaluations because they think it is rel- atively unimportant, resulting in format neglect. It is crucial to note that although consumers' failure to insufficiently account for set size when evaluating rank claims might be considered an instantiation of base rate neglect, this by itself is inadequate to develop our hypotheses. In other words, although base rate neglect certainly contributes to for- mat neglect, the two are not synonymous. Take the example of \"top 10 out of 50\" and \"top 20% out of 50,\" which are equiv- alent numerical and percentage claims. Whereas prior research on base rate neglect might predict that consumers will rely relatively less on set size, the base rate neglect literature is agnostic as to how evaluations of \"top 10\" versus \"top 20%\" might differ when set sizes are equivalent, as may be the case in our research paradigm. Unlike denominator neglect, the values of 10 and 20% are not comparable given their different formats. Because of the inadequacy of base rate neglect alone to inform our prediction that consumers will overrely on nominal values, we draw from prior research exploring how consumers inter- pret and use percentages.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Biases in Evaluating Percentages", "text": "When evaluating rank claims of different formats, one might expect consumers to rely on processing ease. Because con- sumers find whole numbers more intuitive than percentages (Kruger and Vargas 2008;Parker and Leinhardt 1995), numerical claims may be more cognitively fluent and easier to process than percentage claims. Because fluency gener- ally increases liking (Reber, Winkielman, and Schwarz 1998), numerical claims might be expected to produce more positive evaluations than equivalent percentage claims. As such, \"top 10\" and perhaps even \"top 30\" would be pre- ferred over \"top 20%.\" We suspect, however, that numerical and percentage rank claims will not differ substantially from each other in terms of processing ease as consumers are likely to be experienced at processing both types of formats. Instead, we predict that con- sumers will overrely on the nominal value conveyed in a rank claim (e.g., 10 in \"top 10 out of 50\" or 20 in \"top 20% out of 50\"). Our prediction is consistent with several known biases involving percentages, in which consumers rely heavily on nominal values while ignoring or underweighting other diag- nostic information. For example, Bagchi and Ince (2016) showed that when evaluating the accuracy of probabilistic fore- casts (e.g., 70% vs. 30% chance of occurrence), consumers erroneously judge the estimate with the higher nominal value (70 vs. 30) as being more accurate. Consumers seem to rely chiefly on the nominal value of the forecast and infer that the magnitude of this value corresponds with the forecaster's con- fidence in his or her prediction. Likewise, extensive research in pricing contexts has shown that consumers overrely on nominal values when encountering percentages. For example, Chen and Rao (2007) found that when offered multiple discounts (e.g., 10% off followed by 20% off), consumers simply add the nominal values, leading to an overestimation (30% vs. 28%). Relatedly, Chen et al. (2012) demonstrated that when evaluating promotions, consu- mers prefer a bonus pack over an economically equivalent price discount when both are expressed as percentages. For example, they found that consumers favor a bonus pack quan- tity increment of 50% over an economically equivalent price discount of 33.3% because they directly compare the nominal values of 33.33 with 50. Kruger and Vargas (2008) showed that when subjectively judging the distance between two numbers whose difference is expressed in percentages, consumers are influenced by the nominal values conveyed in the percentages. For example, when comparing the numbers 1,500 and 1,000, consumers judged 1,500 to be much larger than 1,000 when 1,500 was presented as being 50% more than the latter, versus if 1,000 were described as being 33.3% less than the former. Although this result may be partially a function of the different frames (i.e., \"more than\" vs. \"less than\"), the authors suggested that participants' overreliance on nominal values (i.e., 50 vs. 33.3) was also a contributing factor. Collectively, this research shows that consumers tend to overrely on nominal values when making percentage-based judgments. Whereas many previously identified biases in the processing of percentage information have been attributed to attentional oversight or calculation complexity (e.g., Bagchi and Ince 2016;Chen et al. 2012;Davis and Bagchi 2018;Kruger and Vargas 2008), we posit that consumers believe that nominal value is more important to the evaluation task than other infor- mation contained in the rank claim. As a result, consumers will overrely on nominal values when making their evaluation. Because smaller nominal values typically indicate superior ranks, we expect consumers to judge an item represented by a smaller nominal value more favorably than an equivalent (or nearly equivalent) item with a larger nominal value. Thus, consumers will react more favorably to claims using smaller nominal values regardless of format. For example, \"top 10\" will be judged as better than an equivalent rank of \"top 20%\" because the number 10 is smaller than 20.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Hypothesis Development", "text": "Because numerical ranks and percentage ranks have different mathematical properties, they often require different nominal values to denote an equivalent position in a set. Furthermore, while the nominal value of a numerical rank claim is an abso- lute measure of an item's position in a set (e.g., top 10 out of 50), it is a relative measure in a percentage claim (e.g., top 20% out of 50). As a result, the same relative position in a set can lead to numerical claims with very different nominal values as a function of set size. For example, when expressed as a per- centage claim, a product ranked in the top 20% would have the same nominal value (i.e., 20) irrespective of whether there were 50 or 200 products in its set. However, this product would have a nominal value of 10 or 40 when expressed as a numerical claim, depending on if there were respectively 50 or 200 prod- ucts in the set. Given that percentages are, by definition, proportions with respect to a set of 100, we propose that a set size of 100 will act as the inflection point for our proposed effect as this is the point where numerical and percentage values converge (e.g., top 20% out of 100 is equivalent to top 20 out of 100). In accor- dance with format neglect, consumers who rely relatively more on nominal value but relatively less on set size should evaluate numerical and percentage claims similarly when set size is equal to 100. However, when an item is part of a small set with fewer than 100 items, the same objective position in the ranked list-say, 10 out of 50-will have a smaller nominal value when it is expressed numerically (top 10 out of 50) than when a percent- age format is used (top 20% out of 50). Given our prior theo- rizing, we expect a numerical format to elicit more positive evaluations relative to a percentage format when the set size is smaller than 100. Conversely, a different pattern of effects will emerge when the set size is larger than 100. In these cases, the same objective position in the ranked list-say, 10 out of 200-will have a larger nominal value when it is expressed numerically (top 10 out of 200) than when a percentage format is used (top 5% out of 200). Thus, we propose that the effect of claim format on consumer evaluations will depend on set size. Stated formally, we hypothesize the following: H 1a : Consumers evaluate a ranked item that is part of a set of more than 100 items more favorably when it is described with a percentage (vs. numerical) rank claim format. H 1b : Consumers evaluate a ranked item that is part of a set of less than 100 items more favorably when it is described with a numerical (vs. percentage) rank claim format. H 1c : Consumers evaluate a ranked item that is part of a set of exactly 100 items equally favorably when it is described with a numerical or percentage rank claim format. While our H 1a-c delineate when consumer evaluations will diverge or converge, H 2 explains why these effects occur. As previously discussed, we contend that the underlying mechan- ism for these shifts in consumer evaluations is format neglect, a novel bias that emerges when consumers encounter rank claims. In essence, format neglect is the net result of consumers relying too much on nominal value and too little on set size. Stated formally: H 2 : Consumers rely more (less) on an item's nominal value (set size) when evaluating a rank claim because they consider it more (less) important to their evaluations.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Overview of Studies", "text": "We report findings from five experiments that document an interaction between rank claim format and set size on consumer evaluations and demonstrate that this interaction arises because of format neglect. Experiment 1 provides initial support for our main thesis (H 1a and H 1b ) in a laboratory-controlled environ- ment. This study also provides evidence for H 2 , the existence of format neglect, by showing that participants rely on nominal values more than set sizes when formulating their evaluations. Experiment 2 provides further support in favor of format neglect as the underlying mechanism and corroborates our claim that the inflection point for this effect occurs when set size is 100 (H 1c ). In Experiments 3 and 4, we identify inter- ventions that can be used to debias consumers. These theoreti- cally derived interventions also provide additional support for our proposed format neglect mechanism (H 2 ) by demonstrating that when the importance of set size on evaluations is high- lighted perceptually (Experiment 3) or cognitively (Experiment 4), the effects found in our earlier experiments are attenuated. Finally, Experiment 5 is a field experiment conducted at a cheese shop over a 12-week period. It demonstrates our basic effect in the context of actual purchasing behavior, thereby enhancing the external validity of this work. In Web Appendix A, we report findings from four additional studies that demonstrate further robustness and generalizability of our findings, while highlighting other approaches for debias- ing. Experiment 6 shows that even marketing professionals who are likely to have considerable familiarity with rank claims are susceptible to format neglect. Experiments 7 and 8 show that format neglect applies to nonmarketing communica- tions and that claim format can even influence judgments about a student's academic performance. Experiments 8 and 9 pro- vide evidence that debiasing may be possible if participants are forced to consider both percentage and numerical format simul- taneously, either by converting one format to the other (Experi- ment 8) or by jointly evaluating a numerical claim and a percentage claim (Experiment 9).", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Experiment 1", "text": "In Experiment 1, we vary set size but equate the relative favor- ability of an item in the set, irrespective of whether participants encounter a numerical or percentage rank format. If partici- pants do in fact rely more (less) on nominal value (set size) when making their evaluations, we should find that percentage rank claims are favored when set size is large (i.e., > 100), but also that numerical rank claims are favored when set size is small (i.e., < 100). Experiment 1 also aims to provide support for our format neglect mechanism by comparing the extent to which participants consider nominal values and set size when making judgments.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Method", "text": "Experiment 1 was a lab study conducted with 233 students at a large public university in the U.S. (50.2% female; average age \u00bc 20.69 years, SD \u00bc 1.34) who participated in exchange for course credit. The study involved a 2 (claim format: numerical rank, percentage rank) \u00c2 2 (set size: small, large) between- participants design. Participants read a brief summary of the sales performance of a particular product, GLS, relative to the other products in its category on Amazon. Depending on set size condition, participants were informed that there were 50 products in GLS's product category (small set size) or 500 products (large set size). Participants in the numerical format conditions who were also assigned to the small set size condi- tion learned that GLS was among the top 20 of the 50 products in its category, whereas those assigned to the large set size condition learned that GLS was among the top 200 of the 500 products in its category. Participants in the percentage format conditions learned that GLS was among the top 40% of products in its category irrespective of the set size. For the wording of stimuli in Experiment 1 and all subsequent studies, see Web Appendix B. After reviewing the description of GLS's sales performance, participants indicated how well GLS was performing on an unnumbered sliding scale (0 \u00bc \"not very well,\" and 100 \u00bc \"very well\"). On a new screen, we asked participants to explain in an open-ended text box what information they used to determine how well GLS was performing. Participants advanced to another screen where they stated how much they had considered rank information (i.e., nominal value) and the number of products in GLS's category (i.e., set size) when evaluating GLS's performance (1 \u00bc \"not at all,\" and 9 \u00bc \"very much\"). Participants' answers revealed the relative importance they placed on rank information versus set size when evaluat- ing the product. Afterward, participants proceeded to another screen where they were asked to recall GLS's rank (i.e., its nominal value) and the number of products in its category (i.e., its set size) by entering these numbers in text boxes.  Figure 1.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Results and Discussion", "text": "A mixed ANOVA with self-reported consideration of nom- inal value and set size measured within-participants revealed a main effect of measure (F (1,229) \u00bc 83.97, p < .001, Z p 2 \u00bc .27), which indicates that participants were more likely to consider nominal value (M \u00bc 6.77, SD \u00bc 1.86) than set size (M \u00bc 5.13, SD \u00bc 2.46). None of the interactions were significant (all ps > .34). As a complement to this self-reported measure, we exam- ined the rationales provided by participants for the evaluations they had provided. Two independent coders, blind to condition, decided whether each rationale indicated that the participant had considered the product's rank (i.e., the nominal value) during the evaluation process (1 \u00bc yes, 0 \u00bc no). They also determined whether each rationale indicated that the partici- pant had considered the number of products in GLS's category (i.e., set size) (1 \u00bc yes, 0 \u00bc no). Coder agreement was an acceptable 84% (Noseworthy, Di Muro, and Murray 2014; Townsend and Liu 2012) and differences were resolved by discussion between the coders. Consistent with our hypothesis that consumers rely more on the nominal value in a rank claim when making an evaluation (H 2 ), 57.5% of participants' ratio- nales indicated that they had considered the product's nominal value whereas only 22.3% of rationales indicated that they had considered the product's set size (w 2 (1) \u00bc 65.61, p < .001), and this disparity was observed across conditions (all ps < .01). Unlike these measures, recall rates of nominal value and set size were universally high (i.e., >75%) across conditions. Taken together, these results support our proposal that con- sumer evaluations may be driven by perceived importance and not by recall accuracy.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Experiment 2", "text": "Although Experiment 1 provided support for H 1a , H 1b , and H 2 , it did not test whether the inflection point for the observed shift in evaluations between numerical and percentage rank claims occurs when set sizes have 100 items (H 1c Figure 1. Effectiveness of numerical versus percentage rank claims depends on set size (Experiment 1). Notes: Higher numbers indicate more favorable evaluations. Participants eval- uated a product's sales performance more (less) favorably when its perfor- mance was described using a numerical rank claim versus an identical percentage rank claim if the product was a member of a small (large) set of products. numerical or percentage format, and the number of brands in its industry (i.e., its set size). There were ten between-participant conditions in Experi- ment 2. Four of the conditions emulated the design of Experi- ment 1, in that the actual rank of Brand TFN was equally favorable across these conditions but claim format (i.e., numerical vs. percentage) and set sizes varied (i.e., either greater or less than 100). Those in the numerical format con- dition learned that TFN had been ranked in the top 10 of 40 brands (small set size) or the top 100 of 400 brands (large set size), whereas those in the percentage format condition learned that TFN had been ranked in the top 25% of 40 (small set size) or 400 brands (large set size). To test our proposition that a set size of 100 serves as an inflection point where consumers are indifferent between equivalent numerical claims and percentage claims, we added two inflection point conditions that had either a numerical format (i.e., top 25 of 100 brands) or a percentage format (i.e., top 25% of 100 brands). Based on the numbers we used, Brand TFN was in the top 25% of brands in its industry across all six of these experimental conditions. So that we could test whether the inflection point of 100 was robust irrespective of claim favorability, we included four more inflection point conditions. The first two of these extra condi- tions had either a numerical format (i.e., top 10 of 100 brands) or a percentage format (i.e., top 10% of 100 brands) and were superior to the six other claims. The last two conditions had either a numerical format (i.e., top 40 of 100 brands) or a percentage format (i.e., top 40% of 100 brands) and were infer- ior to the six other claims. In addition to establishing the robust- ness of 100 as an inflection point, these paired conditions enable us to compare equivalent rank claims in which nominal value and set size remain constant and only format is manipu- lated. This is useful as it helps us rule out the possibility that rank format may influence consumer evaluations independent of either nominal value or set size. After reviewing rank information about TFN, participants in all conditions were asked to indicate, in their opinion, how well TFN was performing in its industry (1 \u00bc \"not very well,\" and 10 \u00bc \"very well\"), the extent to which TFN was one of the best in its industry (1 \u00bc \"not one of the best,\" and 10 \u00bc \"one of the best\") and if they would consider buying the brand if they were shopping in that industry (1 \u00bc \"would not consider,\" and 10 \u00bc \"would consider\"). These three questions were combined to form a single evaluation measure (a \u00bc .91).", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Results and Discussion", "text": "We separated our analysis into two main parts. First, we focused on the six conditions that were equally favorable (i.e., top 25%) but in which claim format and set size were varied. Thus, we estimated a 2 (claim format: numerical rank, percentage rank) \u00c2 3 (set size: small, inflection, large) between-participants ANOVA on participants' evaluation of TFN performance. There was no main effect of claim format (F (1, 462 More importantly, we also observed a significant interaction between set size and claim format (F (2, 462) \u00bc 4.03, p \u00bc .02, Z p 2 \u00bc .02). Planned contrasts revealed that among participants in the small set size conditions (i.e., 40), those who encountered the numerical (i.e., top 10) claim evaluated TFN more favor- ably (M \u00bc 8.15, SD \u00bc 1.76, N \u00bc 77) than participants who encountered the equivalent percentage (i.e., top 25%) claim (M \u00bc 7.58, SD \u00bc 1.76, N \u00bc 87); F (1, 462) \u00bc 4.07, p < .05). Conversely, among participants in the large set size conditions (i.e., 400), those who encountered the numerical (i.e., top 100) claim evaluated TFN less favorably (M \u00bc 7.06, SD \u00bc 2.39, N \u00bc 74) than participants who encountered the identical per- centage (i.e., top 25%) claim (M \u00bc 7.63, SD \u00bc 1.72, N \u00bc 82); F (1, 462) \u00bc 3.94, p < .05). These results are consistent with the findings of Experiment 1. However, in the two equivalent inflection point conditions (when set size was equal to 100), evaluations provided by participants who encountered the numerical (i.e., top 25) claim (M \u00bc 7.75, SD \u00bc 1.52, N \u00bc 70) did not differ from evaluations provided by participants who saw the identical percentage (i.e., top 25%) claim (M \u00bc 7.83, SD \u00bc 1.47, N \u00bc 78); F (1, 462) \u00bc .07, p > .79). These results appear in Figure 2. Notes: Higher numbers indicate more favorable evaluations. Participants eval- uated a brand more (less) favorably if its rank was described using a numerical claim versus an identical percentage claim if the brand was a member of a small (large) set of brands. However, at the inflection set size of 100 brands, parti- cipants evaluated the brand the same irrespective of whether its rank was described using a numerical claim or an identical percentage claim. In the second part of our analysis, we again considered the same two inflection point conditions (i.e., top 25 of 100, and top 25% of 100), as well as the four other inflection point conditions. This resulted in three pairs of conditions with set sizes equal to 100 and the same nominal value for the numer- ical and percentage conditions. The nominal values in these pairs of conditions were 10, 25, and 40, respectively. We esti- mated a 2 (claim format: numerical rank, percentage rank) \u00c2 3 (nominal value: 10, 25, or 40) between-participants ANOVA on the evaluation of TFN. Results revealed a main effect of nominal value (F (2, 458)  Experiment 2 demonstrates that the effect of rank claim format on consumer evaluations depends on set size but also shows that this effect is absent when set size is equal to 100. Thus, this experiment corroborates our hypothesis (H 1c ) that a set size of 100 acts as an inflection point for our effect. In the next two experiments, we discuss interventions that can be used to debias consumers even when set sizes are smaller or larger than 100.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Experiment 3", "text": "Whereas we have examined identical numerical rank and per- centage rank claims in our experiments thus far, in Experiment 3 we test whether inferior numerical rank claims are preferred over superior percentage rank claims when set sizes are small. In addition, as we have demonstrated in multiple contexts how set sizes influence the assessments of rank claims as a function of format, hereinafter we focus on either a small or large set. This enables us to provide more focused and nuanced support for our theorizing. In Experiment 3, we use a small set, and in Experiment 4 we use a large set. Prior research has suggested that perceptual cues can change the perceived importance of information, which in turn affects consumers' reliance on this information when making judg- ments. For example, Monga and Bagchi (2012) found that in situations in which both numbers and units are presented together, highlighting numbers (units) perceptually leads to greater reliance on one versus the other in judgments. Like- wise, in the context of risk perception, Stone and colleagues (Stone, Yates, and Parker 1997;Stone et al. 2003) showed that when graphical formats are used to convey ratio information, highlighting the numerator or the denominator increases con- sumers' reliance on this information. If our theorizing is cor- rect, then making set size seem more important through its visual presentation is likely to increase consumers' reliance on set size, which should mitigate reliance on the nominal value information and thereby attenuate format neglect. There- fore, in Experiment 3, participants encounter inferior numerical rank claims and/or superior percentage claims, but we manip- ulate set size importance by varying whether set size informa- tion is underlined, bolded, and presented in a larger and different-colored font than rank information.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Method", "text": "We conducted Experiment 3 with 300 U.S. participants (54.3% female; average age \u00bc 35.29 years, SD \u00bc 9.35) recruited using MTurk. The study involved a 2 (claim format: numerical rank, percentage rank) \u00c2 2 (importance of set size information: high, low) between-participants design. Participants were asked to read a brief advertisement for a (fictional) library, Midtown Library. Participants were informed that Midtown Library was ranked in either the top 10 (numerical rank claim) or the top 30% (percentage rank claim) of the 20 libraries in its greater metropolitan area by Interlibrary Magazine. Unlike our previ- ous studies, these claims were nonequivalent; being in the top 10 (out of 20) is an inferior claim (top 50%) relative to the percentage claim of being in the top 30% (in top 6). In conditions with low set size importance, rank information was underlined, bolded, and presented in a red font that was larger than the other text in the claim. However, in those con- ditions in which set size importance was high, set size infor- mation (instead of rank information) was underlined, bolded, and presented in a larger red font. After reviewing the advertising claim for Midtown Library, participants were asked two questions that served as our key dependent variables. Specifically, they were asked to indicate, in their opinion, how well Midtown had performed (1 \u00bc \"not very well,\" and 11 \u00bc \"very well\") and how well it is likely to perform in the future (1 \u00bc \"not very well,\" and 11 \u00bc \"very well\"). These two questions were combined to form a single evaluation measure (r \u00bc .83). An assumption of this study is that underlining, bolding, and presenting a portion of text in a larger, different-colored font increases its perceived importance. To confirm this, we asked participants (on a new screen toward the end of the study) to rate the extent that performing each of these actions (under- lining, bolding, using a different font color, and increasing the font size) affects the perceived importance of a portion of text (1 \u00bc \"decreases perceived importance,\" 5 \u00bc \"does not affect perceived importance,\" and 9 \u00bc \"increases perceived importance\"). After combining the four items into a composite measure (a \u00bc .87), we found that the mean perceived impor- tance rating (M \u00bc 7.32, SD \u00bc 1.35, N \u00bc 300) was significantly higher than the scale midpoint (t (299) \u00bc 29.85, p < .001). This confirms that our visual presentation manipulation increased the perceived importance of the target item. On a new screen, participants were asked to recall Midtown Library's rank (i.e., its nominal value) and the number of libraries in its area (i.e., its set size) by entering these numbers in text boxes. Finally, participants were asked to indicate the extent to which they considered themselves a library expert on a nine-point scale (1 \u00bc \"not at all,\" and 9 \u00bc \"very much\"). Inclusion of self-reported library expertise as a covariate had no impact on our results; therefore, we will not discuss it further.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Results and Discussion", "text": "A 2 (claim format) \u00c2 2 (set size importance) between- participants ANOVA on the composite measure revealed neither a main effect of claim format (F (1, 296) \u00bc 1.85, p > 18, Z p 2 < .01) nor set size importance (F (1,296) \u00bc .93, p > .33, Z p 2 < .01). However, we observed a significant interaction between claim format and set size importance (F (1, 296) \u00bc 4.21, p \u00bc .04, Z p 2 \u00bc .01). Planned contrasts revealed that when set size importance was low, those who encountered the numer- ical rank (i.e., top 10) claim evaluated Midtown Library more favorably (M \u00bc 8.78, SD \u00bc 1.88, N \u00bc 67) than those who encountered the objectively superior percentage rank (i.e., top 30%) claim (M \u00bc 8.06, SD \u00bc 1.97, N \u00bc 79); (F (1, 296) \u00bc 5.65, p < .02). However, when set size importance was high, those who encountered the numerical rank (i.e., top 10) claim evaluated Midtown Library no differently (M \u00bc 8.15, SD \u00bc 1.71, N \u00bc 75) than those who encountered the objec- tively superior percentage rank (i.e., top 30%) claim (M \u00bc 8.29, SD \u00bc 1.66, N \u00bc 79); F (1, 296) \u00bc .24, p > .62. These results appear in Figure 3. As in our previous studies, recall rates of nominal value and set size were universally high (i.e., >75%) across conditions.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Experiment 4", "text": "In Experiment 4, we manipulate set size importance by asking participants to type in the set size that they had been shown before providing an evaluation. If our theorizing is correct, cognitively reinforcing set size in this way will communicate to participants the importance of set size and should attenuate any differences we observe in the evaluations of those who encounter a numerical versus percentage rank claim. Because Experiment 3 focused on a small set context, in Experiment 4 we test the effectiveness of this proposed debiasing interven- tion in the context of a large set size. We also directly measure the perceived importance of nominal value and set size. We predict that the perceived importance of nominal value will exceed that of set size only when set size importance is low.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Method", "text": "We conducted Experiment 4 with 362 U.S. participants (45.6% female; average age \u00bc 35.36 years, SD \u00bc 11.17) recruited using MTurk. The study involved a 2 (claim format: numerical rank, percentage rank) \u00c2 2 (relative importance of set size information: high, low) between-participants design. Partici- pants were informed that they were deciding whether to invest in the Bantam mutual fund and had consulted the website Fund Tracker.com, which ranks mutual funds. Those randomly assigned to the numerical rank condition learned that the Ban- tam Fund had been ranked as one of the top 150 mutual funds in its class. Participants in the percentage rank condition learned that the Bantam Fund had been ranked as one of the top 30% of mutual funds in its class. In addition, all participants were given identical information about set size (\"Number of mutual funds in the Bantam Fund's class: 500\"). Participants in the high-set-size-importance condition were asked to review the information they had been provided and to enter (in a text box) the number of mutual funds in Bantam Fund's class. Those in the low-set-size-importance condition were not asked to input set size information into a text box. We expected that the act of entering set size would cognitively reinforce set size and increase importance accorded to it. Subsequently, all participants evaluated the Bantam Fund by responding to three ten-point items (1 \u00bc \"not likely to be a good investment/not likely to generate a positive return/not likely to consider purchasing,\" and 10 \u00bc \"likely to be a good investment/likely to generate a positive return/likely to con- sider purchasing\"), which served as our key dependent vari- ables. These three items were combined to form a single evaluation measure (a \u00bc .89). Finally, participants were asked to recall the Bantam Fund's rank (i.e., its nominal value) and the number of mutual funds in its class (i.e., its set size) by entering these numbers in text boxes. Participants advanced to another screen where they indi- cated how much they had relied on rank information (i.e., nominal value) and the number of other mutual funds in Ban- tam's class (i.e., set size) when evaluating the Bantam Fund (1 \u00bc \"not at all,\" and 7 \u00bc \"very much\"). This question enabled us  Figure 3. Set size importance attenuates format neglect n small sets (Experiment 3). Notes: Higher numbers indicate more favorable evaluations. When set size importance was low, participants evaluated a library more favorably if it used an inferior numerical rank claim versus a superior percentage rank claim. How- ever, this bias was eliminated when set size importance was high. In all cases, the library was part of a small group of libraries (set size \u00bc 20). to directly assess whether the relative importance of nominal value versus set size differed across conditions.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Results and Discussion", "text": "A 2 (claim format) \u00c2 2 (set size importance) between- participants ANOVA on the composite evaluation measure revealed a main effect of claim format (F (1,358) \u00bc 4.08, p \u00bc .044, Z p 2 \u00bc .01) but no main effect of set size importance (F (1,358) \u00bc .04, p > .83, Z p 2 < .01). More germane to our theorizing, we observed a significant interaction between claim format and set size importance (F (1,358) \u00bc 9.02, p < .01, Z p 2 \u00bc .03). Planned contrasts revealed that when set size importance was relatively low, those who encountered the percentage rank (i.e., top 30%) claim evaluated the Bantam Fund more favor- ably (M \u00bc 8.09, SD \u00bc 1.43, N \u00bc 89) than those who encoun- tered the equivalent numerical rank (i.e., top 150) claim (M \u00bc 7.11, SD \u00bc 2.22, N \u00bc 92); (F (1,358) \u00bc 12.65, p < .001, Z p 2 \u00bc .03). However, when set size importance was high, those who encountered the percentage rank (i.e., top 30%) claim evalu- ated the Bantam Fund no differently (M \u00bc 7.47, SD \u00bc 1.90, N \u00bc 86) than those who encountered the numerical rank (i.e., top 150) claim (M \u00bc 7.66, SD \u00bc 1.76, N \u00bc 95; F (1,358) \u00bc .48, p > .48, Z p 2 < .01). These results appear in Figure 4. We then conducted a 2 (importance measure: rank, set size) \u00c2 2 (claim format: numerical rank, percentage rank) \u00c2 2 (set size importance: low, high) mixed ANOVA, with self-reported importance of nominal value versus set size measured within- participants. We detected a main effect of measure (F (1, 358) \u00bc 15.39, p < .001, Z p 2 \u00bc .04), which indicates that participants considered nominal value (M \u00bc 5.82, SD \u00bc 1.34) to be more important than set size (M \u00bc 5.46, SD \u00bc 1.48). There was also a significant interaction between importance measure and set size importance (F (1,358) \u00bc 31.61, p < .001, Z p 2 \u00bc .08). Parti- cipants in the low-set-size importance condition considered nominal value (M \u00bc 6.06, SD \u00bc 1.07, N \u00bc 181) to be more important than set size (M \u00bc 5.17, SD \u00bc 1.60; F (1,358) \u00bc 45.61, p < .001, Z p 2 \u00bc .11). However, participants in the high-set-size importance condition considered nominal value (M \u00bc 5.58, SD \u00bc 1.53, N \u00bc 181) and set size (M \u00bc 5.74, SD \u00bc 1.30) to be equally important (F (1, 358) \u00bc 1.44, p > .23, Z p 2 < .01. No other main effects or interactions were significant (all ps > .10). These results give us confidence that our cognitive rein- forcement manipulation affected perceived importance in the hypothesized direction. Unlike ratings of perceived impor- tance, recall rates of nominal value and set size were again universally high (i.e., >75%).", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Experiment 5", "text": "Our first four experiments provided extensive support for our proposed effect and the underlying format neglect mechanism. Experiment 5 tests whether the use of a numerical rank claim versus an equivalent percentage rank claim affects actual pur- chase behavior. In this field experiment, a product that is part of a large set (i.e., >100) is described using a numerical rank claim or a percentage rank claim. We predicted that real shop- pers will purchase the product more often when it is described using a percentage rank claim versus an equivalent numerical rank claim.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Method", "text": "Experiment 5 was conducted at a cheese shop (Beecher's Handmade Cheese) in a popular tourist location (Pike Place Market) of a large U.S. city (Seattle). At the store's walk-up counter, patrons can order prepared foods (e.g., macaroni and cheese) and/or purchase wrapped cheese blocks from a large display case containing 67 different cheeses. Next to each cheese in the display case is a 4.5-inch by 2.5-inch sign that states the cheese's name and includes other descriptive infor- mation (e.g., flavor, production process). The shop's manage- ment team allowed us to conduct a 12-week field experiment in which we varied the information provided in the display case about a particular Camembert-style cheese known as Cirrus. Cirrus was chosen for this experiment because it had recently received an award for technical excellence and aesthetic qual- ity at the 2017 American Cheese Society competition that could accommodate our desired rank format manipulation. The standard Cirrus sign in the store's display case (the control condition in our study) contained information about how and where the cheese was produced, along with the cheese's retail price ($9.95 per block). We created two new versions of the Cirrus sign containing numerical or percentage rank claims that were equivalent and truthful. The numerical rank claim stated, \"Of the 2,024 entrants in the 2017 American Cheese Society competition, only 411 were selected to receive awards for technical excellence and aesthetic quality. We're Notes: Higher numbers indicate more favorable evaluations. When set size importance was low, participants evaluated a mutual fund more favorably if it used a percentage rank claim versus an equivalent numerical rank claim. However, this bias was eliminated when set size importance was high. In all cases, the mutual funds were part of a large group of mutual funds (set size \u00bc 500). proud that Cirrus was one of the 411 cheeses to receive an award.\" The percentage rank claim stated, \"Of the 2,024 entrants in the 2017 American Cheese Society competition, only 20% were selected to receive awards for technical excel- lence and aesthetic quality. We're proud that Cirrus was one of the 20% of cheeses to receive an award.\" This field experiment was conducted over 12 consecutive weeks between July and October 2017. For the first two weeks and the last two weeks of the experiment, the control sign (without any American Cheese Society rank information) was displayed. For the eight weeks in between, we rotated the numerical rank signs and percentage rank signs on Sundays, prior to the store opening, on a predetermined weekly or bi- weekly basis. In total, the no rank (control) sign, numerical rank sign, and percentage rank sign were each displayed for 28 days (i.e., four weeks). Unit sales of Cirrus, as well as overall sales of all cheeses in the display stand (for use as a potential covariate), were tracked on a daily basis.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Results and Discussion", "text": "We analyzed the field data from Experiment 5 in two different ways, with and without a log transformation, and obtained similar results. Because these data were not positively skewed (skewness \u00bc .92), we use and report the untransformed mea- sure (values of skewness between \u00c02 and \u00fe2 are considered normal; George and Mallery 2010). A one-way ANOVA of experimental condition (no rank, numerical rank, percentage rank) on the daily unit sales of Cirrus cheese returned a signif- icant result (F (2, 81) \u00bc 9.71, p < .001, Z p 2 \u00bc .19). Compared with the no-rank condition (M \u00bc 2.39 units, SD \u00bc 1.73), sig- nificantly more Cirrus cheese blocks were sold per day when either the percentage rank (i.e., top 20%) sign (F (1, 81) \u00bc 19.41, p < .001) or the numerical rank (i.e., top 411) sign was dis- played (F (1, 81) \u00bc 4.71, p < .04). More germane to our theoriz- ing, however, significantly more Cirrus cheese blocks were sold on the days when the percentage rank sign was displayed (M \u00bc 4.86 units, SD \u00bc 2.49), as compared with the numerical rank sign (M \u00bc 3.61 units, SD \u00bc 1.99; F (1, 81) \u00bc 5.00, p < .03). In addition, we tested whether an equal proportion of total Cirrus cheese blocks was sold on the percentage-rank days compared with the numerical-rank days. In contrast, we found that a total of 136 Cirrus cheese blocks were sold on the 28 days when a percentage rank sign was displayed but only 101 Cirrus cheese blocks were sold on the 28 days when a numer- ical rank sign was displayed. These proportions (57.4% vs. 42.6%, respectively) differ significantly from 50% (w 2 (1) \u00bc 5.17, p \u00bc .023). Finally, to test whether our results were observed even after partitioning out daily variation in store traffic, we conducted an analysis of covariance (ANCOVA) of experimental condition on the daily dollar sales of Cirrus cheese, with the daily dollar sales of all other cheeses included as a covariate. We reasoned that daily variation in store traffic would be captured by dif- ferences in overall cheese sales per day. This ANCOVA returned a significant effect of experimental condition on Cirrus dollar sales (F (2, 80) \u00bc 10.11, p <.001, Z p 2 \u00bc .20). The covariate (i.e., dollar sales of all other cheeses) was also a significant predictor of Cirrus dollar sales (F (1, 80) \u00bc 9.95, p <.01, Z p 2 \u00bc .11). Compared with the no-rank condition, significantly more Cirrus cheese blocks were sold per day when either the percentage rank (i.e., top 20%) sign (F (1,80) \u00bc 20.08, p < .001) or the numerical rank (i.e., top 411) sign was dis- played (F (1, 80) \u00bc 6.42, p < .02). Furthermore, more Cirrus cheese blocks were sold on the days when the percentage rank sign was displayed as compared with the numerical rank sign (F (1, 80) \u00bc 3.71, p < .06). The results of Experiment 5 indicate that equivalent numer- ical versus percentage rank claims influence consumers' actual purchase behavior. We found that consumers were more likely to purchase a product that is part of a large set when it was described using a percentage claim. Although this experiment represents an extreme test of our proposed effect given the large difference in nominal values between conditions (i.e., 20 vs. 411), it is important to note that these numbers were factually accurate and were obtained from the 2017 American Cheese Society competition. Our results suggest that consu- mers utilize available rank claim information to make real purchase decisions, and that equivalent numerical versus per- centage rank claims can differentially affect consumers in a manner consistent with format neglect. Thus, we believe that Experiment 5 provides a useful demonstration of the external validity of this research.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "General Discussion", "text": "In this research, we investigate how the claim format used to convey an item's position on a ranked list influences consumer evaluations. Although recent research examining consumer response to rankings and ranked list claims has identified sev- eral psychological factors that influence consumers' evaluation of a ranked item (e.g., Isaac, Brough, and Grayson 2016;Isaac and Schindler 2014;Leclerc, Hsee, and Nunes 2005), it is unclear whether equivalent claims using a numerical (e.g., \"top 10,\" \"top 20\") versus percentage (e.g. \"top 3%,\" \"top 10%\") format will be evaluated differently. Given that both claim formats are frequently observed in marketing communications, understanding how different rank claim formats influence con- sumer evaluations is theoretically and managerially consequential. Across nine experiments (including those reported in Web Appendix A) that span multiple settings and contexts (includ- ing a field study), we find converging evidence of a shift in evaluations whereby consumers respond more favorably to numerical rank claims when set sizes are smaller (i.e., <100) but more favorably to percentage rank claims when set sizes are larger (i.e., >100), even when the claims are mathematically equivalent. To better assess the magnitude and robustness of this effect, we conducted a meta-analysis of claim format and set size on consumer evaluations across our experiments, using a statistical tool developed by McShane and B\u00f6ckenholt (2017) for single-paper meta-analyses. For Experiments 1-5, the meta-analysis revealed significant contrasts when set sizes were large (estimate \u00bc .83, SE \u00bc .20; z \u00bc 4.21, p < .001) or small (estimate \u00bc .94, SE \u00bc .21; z \u00bc 4.50, p < .001), as well as a significant interaction (estimate \u00bc 1.77, SE \u00bc .29; z \u00bc 6.16, p < .001). A second meta-analysis that included the four experiments reported in the Web Appendix A revealed similarly strong effects. See Web Appendix D for additional detail on these meta-analyses. Despite the robustness of our findings, some caveats are in order. While both numerical and percentage ranks are used frequently, ranked lists are not available for all categories of products and services, which may limit the utility of our find- ings. For example, ranked lists are more popular in certain contexts (e.g., restaurants, hotels, travel locations, cities to live in or visit, universities to attend, firms to work for) and for certain offerings (e.g., cars, laptops). Furthermore, although we focus exclusively on rank claims in this research, consu- mers are likely to use other information to assess the relative performance of products and services. For example, they may use consumer or expert reviews or ratings in conjunction with or in place of rankings to guide their decision making. These alternative sources may limit the situations in which our findings can be directly applied. In addition, although we examined many different small set sizes (i.e., 20, 40, and 50 items) and large set sizes (i.e., 200, 300, 400, 500, 2,024) in the nine studies described in this article and Web Appendix A, we did not systematically examine set sizes that were close to the inflection point of 100 (e.g., 75 vs. 125 items). Thus, we cannot state definitively how consumers will respond to such set sizes. Finally, there may be disconti- nuities at certain ranks where an interaction between set size and claim format does not occur. For example, it may be the case that the numerical rank of 1 is always judged as being superior to its percentage rank counterpart, irrespective of set size, because being in the first position on a list holds special significance that attenuates our effect. Future research might investigate such moderators.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Theoretical Contributions", "text": "At a theoretical level, the present research makes four discrete contributions. First, whereas existing research has focused exclusively on numerical rank claims, we introduce the con- cept of percentage rank claims to the academic literature. Second, we elucidate how different rank claim formats (i.e., numerical vs. percentage claims) influence consumer evalua- tions. Our experiments indicate that even when an item's objective rank is unchanged, the use of a percentage claim format versus a numerical claim format can dramatically alter consumer evaluations. Furthermore, even inferior ranks may be evaluated more favorably relative to superior ranks depending on how they are communicated. For example, in Experiment 3, we found that a library ranked among the top 10 out of 20 libraries was evaluated more positively than a library ranked in the top 30% although the latter is objectively superior (top 6). Third, we uncover a novel bias-format neglect-that explains the shift in evaluations that we observe. We show that format neglect results from two related biases that occur when consumers evaluate rank claims: (1) the insufficient utilization of set size information and (2) the overreliance on nominal values that convey an item's rank. Fourth, by bridging the literatures on base rate neglect and the processing of percentage information, we provide insights on how consumers integrate nominal value and set size informa- tion in their evaluations and document consequences of this integration process. Next, we discuss how format neglect relates to and differs from base rate neglect. Prior research has identified conditions under which con- sumers underweight general, base-rate information when making predictions (e.g., Kahneman and Tversky 1973;Nisbett and Ross 1980). We extend this research and demonstrate that a phenomenon akin to base rate neglect-the underutili- zation of set size-emerges in ranking contexts. Unlike pre- vious work on base rate neglect, in which the bias occurs because consumers neglect base rates of different magnitudes and instead treat them as if they were equivalent (e.g., BarHillel 1980;Kahneman and Tversky 1973;Lyon and Slovic 1976;Nisbett and Ross 1980), we find that consumers make inconsistent judgments related to rank claims even when base rates (i.e., set sizes) are the same. For example, we show that consumers differentially evaluate an item ranked in the \"top 10\" versus the \"top 20%\" out of 50, in spite of the base rate (set size) being identical (i.e., 50) across claims. While prior research on base rate neglect might have anticipated that con- sumers would rely relatively less on set size when encounter- ing a rank claim, this literature is agnostic as to whether evaluations of the \"top 10\" versus \"top 20%\" claims in this example would differ. The second bias, which states that consumers will overrely on nominal values when making evaluations, is also a critical component of format neglect. According to this bias, \"top 10\" will be evaluated more favorably than \"top 20%\" because \"10\" is a smaller number, and small numbers are typically more favorable in the contexts of rankings. However, when set sizes are larger (e.g., 500), overreliance on nominal values predicts the opposite-a percentage claim should be judged more favorably. This is because a percentage rank claim of top 20% would correspond to a numerical rank of top 100, and \"20\" is a smaller number. Of course, this prediction is con- tingent on consumers insufficiently accounting for set size when making their evaluation. Thus, both the underutilization of set size and the overreliance on nominal values are needed for format neglect to emerge. Our research not only explains how format neglect (i.e., the overreliance on nominal values and the underutilization of set size) causes this shift in evaluations but also reveals when such a shift is less likely to occur. As we demonstrate, the inflection point for the shift in evaluations we observe is a set size of 100 items. For set sizes smaller than 100, using a numerical rank elicits more positive evaluations compared to an equivalent percentage rank; however, for set sizes larger than 100, a per- centage rank elicits more positive evaluations. Our work also enriches knowledge in the area of numerical cognition by identifying a new context in which consumers process relatively complex forms of numerical information incompletely or inaccurately (e.g., Albert 2003;Davis 2012, 2016;Bagchi and Ince 2016;Chen et al. 2012;Chen and Rao 2007;Delvecchio, Krishnan, and Smith 2007;Gamliel and Peer 2017;Kruger and Vargas 2008;Tsiros and Chen 2017;Tsiros and Hardesty 2010). Extending prior research, which has mostly been limited to the areas of prob- abilities (e.g., Albert 2003;Bagchi and Ince 2016;Gigerenzer 2015), ratios (e.g. Gamliel and Peer 2017;Tsiros and Chen 2017) and price discounts (e.g., Cai, Bagchi, and Gauri 2016;Chen et al. 2012;Chen and Rao 2007;Delvecchio, Krishnan, and Smith 2007;Kruger and Vargas 2008;Tsiros and Hardesty 2010), we provide the first demonstration that consumers may be biased in their evaluations of rank claims as a result of the claim format employed. Our results show that consumers' overreliance on nominal values when encountering percent- age information emerges in a context unrelated to probability or pricing. Future work might study whether other contexts are simi- larly susceptible to the effects that we observe in rank claims. For example, consider research on food and restrained eating (e.g., Redden and Haws 2013;Scott et al. 2008). The same information (e.g., calorie intake) can be communicated with numerical values (500 calories) or percentages (25% of daily intake, assuming a recommended daily intake of 2,000 cal- ories). It may be the case that presenting this information in one format versus another leads to different inferences. For example, if consumers overrely on nominal values when mak- ing magnitude judgments, 500 calories may be perceived as much larger. This effect might reverse for other nutrients, for which the recommended daily intake may be lower (e.g., Vitamin C has a recommended daily intake of 80 milligrams). In addition, it may be the case that prevention versus promo- tion orientations or gain versus loss frames are more compa- tible with a particular format. We leave all of this for future research to examine.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Managerial and Public Policy Implications", "text": "The present work has important implications in the managerial and public policy realms as well. Given the abundance of information to which we are exposed in our daily lives, we are often unable to process it fully. Ranked lists are useful because they enable consumers to efficiently compare con- sumption options (e.g., top televisions, best cars). This research shows that relatively subtle features of rank list claims, such as claim format and set size, may bias how we evaluate items referenced in rank claims. Of course, firms often aim to use rankings as a vehicle to promote the positive aspects of their products and services. This research illustrates that certain rank claim formats may at times help firms depict their products in a more favorable light depending on the set size of the item being promoted. Our findings suggest that marketers may potentially boost how consumers evaluate their products and services by merely selecting the appropriate rank claim format for a particular set size. Firms should abide by the following clear and prescriptive managerial guidelines if their goal is to maximize consumer evaluations-when set size is smaller than 100, use nominal rank claims, but when set size is greater than 100, use percent- age rank claims. The effects documented in this research can be implemented by marketers in any communications that refer- ence the rank of their products or services, including their advertising, company website, or social media. In addition, marketers may sometimes have the ability to alter the set size used in their claims (e.g., top Chinese restau- rants in SoHo vs. top Chinese restaurants in Manhattan). Given the underutilization of set size information that we observed, it may be advantageous for marketers to focus on narrow (vs. broad) sets in their communications, because nominal values (irrespective of format) are typically smaller (i.e., the rank will be superior) for narrow sets. Future research might consider this possibility. A more charitable application of our results is that firms, public policy makers, and other entities can become better equipped to objectively convey rank information to consumers. Given the widespread use of ranked lists, as well as consumers' dependence on ranked lists to make decisions, it is important to educate consumers on how to avoid potential pitfalls in evalu- ating information about rankings and to draw their attention to those ranking aspects that may unknowingly be underempha- sized in the evaluation process. For example, if consumers become aware of how different rank formats influence their judgments, they may be able to devise strategies to avoid fall- ing prey to format neglect. Indeed, we identify and test several debiasing interventions that consumers and/or policy makers might implement to reduce the impact of claim format. It is worth mentioning that, in all our experiments, we pro- vide respondents full information about the rank claims (i.e., the nominal value and set size) and still obtain robust support for format neglect. However, marketers are not required to reveal all this information in their claims-for example, they may only provide nominal value information bereft of the set size it is drawn from. Doing so may exacerbate the effects of format neglect. Indeed, because managers can strategically decide which format to use to showcase their offerings and may have an incentive to use approaches that present their product in a seemingly better light, it is important for them to be aware of the potential ethical repercussions of engaging in such practices given the robustness of this bias. Public policy makers should also be aware that format neglect emerges even when full information about nominal value and set size is presented; thus, the use of interventions to debias consumers such as those documented in the present research may be beneficial for consumers. More broadly, numerical and percentage formats are often used interchangeably outside of ranking contexts. Thus, our findings may be utilized by public policy watchdogs to illus- trate and warn consumers about the fallacy of description or frame invariance. Increasing consumers' awareness of format neglect could potentially improve their decision making not only when evaluating rank claims but also when judging other types of information (e.g., nutritional information, financial information) that are commonly represented by both numerical and percentage formats.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Acknowledgments", "text": "The authors thank Alicia Curiel, Kurt Dammeier, Melissa Hartman, Nicole Lamb, and Julie Riendl at Beecher's Handmade Cheese for their help with Experiment 5, and Sharon Ideguchi and Shambhavi Mehrotra for their research assistance.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Author Contributions", "text": "The authors contributed equally to this work and are listed in reverse alphabetical order.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Area Editor", "text": "Wayne Hoyer served as area editor for this article.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Declaration of Conflicting Interests", "text": "The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Funding", "text": "The author(s) received no financial support for the research, authorship, and/or publication of this article.", "title": "Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments", "file_name": "Sevilla et al. - 2018 - Format Neglect How the Use of Numerical Versus Pe.pdf"}
{"section": "Create an Analysis Plan", "text": "Researchers have many decisions to make when conducting a study and analyzing the data. Which data points should be excluded? Which conditions and outcome variables are critical to assess? Should covariates be included? What variables might moderate the key relationship? For example, Carp (2012a) found that among 241 studies using functional magnetic resonance imaging (fMRI) there were 223 unique combinations of data cleaning and analysis procedures (e.g., correction for head motion, spatial smoothing, temporal filtering). The inordinate flexibility in analysis options provides researchers with substantial degrees-of- freedom to keep analyzing the data until a desired result is obtained; Carp (2012b) reports that when using the over 30,000 possible combinations of analysis methods on a single neuroimaging experiment, 90.3% of brain voxels differed significantly between conditions in at least one analysis. This flexibility could massively inflate false positives (Simmons et al., 2011;Wagenmakers, Wetzels, Borsboom, van der Maas, & Kievit, 2012). The best defense against inflation of false positives is to reduce the degrees of freedom available to the researcher by writing down, prior to analyzing the data, how the data will be analyzed. This is the essence of confirmatory data analysis ( Wagenmakers et al., 2012). The key effect of committing to an analysis plan in advance is to preserve the meaning of the p- values resulting from the analysis. The p-value is supposed to indicate the likelihood that these data would have occurred if there was no effect to detect. This interpretation is contingent on how many tests on the data were run and reported. Once the data have been observed, the universe of possible tests may be reduced to those that appear to be differences, and tests that do not reveal significant effects may be ignored. Without an a priori analysis plan, the extent to which the likelihood of a false positive has occurred is entirely unknown. Writing down an analysis plan in advance stimulates a more thorough consideration of potential moderators and controls as well as a deeper involvement with the previous research and the formulated theories. By committing to a prespecified analysis plan one can avoid common cognitive biases (Kunda, 1990;Nosek et al., 2012). This approach also allows researchers to be open about and rewarded for their exploratory research ( Wagenmakers et al, 2012), and highlights the value of conducting pilot research in order to clarify the qualities and commitments for a confirmatory design.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Determine Data Collection Start and Stop Rules", "text": "It is not uncommon for researchers to peek at their data and when just shy of the \"magical\" alpha = .05 threshold for significance to add participants to achieve significance (John, Loewenstein, Prelec, 2012). This is problematic because it inflates the false-positive rate (Simmons et al., 2011). Likewise, particularly with difficult to collect samples (e.g., infant studies, clinical samples), there can be ambiguity during pilot testing about when a study is ready to begin. A few particularly \"good\" participants might be promoted to the actual data collection if the status of piloting versus actual data collection is not clear. Defining explicit data collection start and stop rules is effective self-protection against false positive inflation. These could be defined as a target number of participants per condition, a target period of time for data collection, as a function of an a priori power analysis, or by any other strategy that removes flexibility for deciding when data collection begins and ends (Meehl, 1990). Some journals, such as Psychological Science, now require disclosure of these rules.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Register Study and Materials", "text": "Many studies are conducted and never reported. This \"file-drawer effect\" is a major challenge for the credibility of published results (Rosenthal, 1979). Considering only the likelihood of reporting a null result versus a positive result and ignoring the flexibility in analysis strategies, Greenwald (1975) estimated the false-positive rate to be greater than 30%. However, it is difficult to imagine that every study conducted will earn a full write-up and published report. A more modest solution is to register every study at the onset of data collection in a public registry. Registration involves, at minimum, documentation of the study design, planned sample, and research objectives. Registration ensures that all conducted studies are discoverable, and facilitates the investigation of factors that may differentiate the universe of studies conducted from the universe of studies published. Public registration of studies is required by law in the United States for clinical trials (De Angelis et al., 2004) and is a pre-condition for publication in many major medical journals. The 2013 Declaration of Helsinki, a possible bellwether of ethical trends, recommends that this requirement be extended to all studies involving human participants (http://www.wma.net/en/30publications/10policies/b3/). This movement towards more transparency of all research can improve accessibility of findings that did not reach the published literature in order to evaluate potential biases in publishing and aggregate all evidence for a phenomenon. A common concern about public registration is that one's ideas may be stolen by others before the research is completed and published. Registration actually certifies the originator of ideas with a time and date stamp. But, for the cautious researcher, some modern registries allow researchers to register studies privately and then reveal the registration later (e.g., https://osf.io/ described later).", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Perform Confirmatory Analyses First", "text": "For confirmatory analyses to retain their interpretability, they must be conducted and reported in full. Consider, for example, pre-registering 20 unique tests and reporting the single test that achieved a p-value below .05. Selective reporting renders a confirmatory analysis plan irrelevant. Likewise, a confirmatory analysis plan does not eliminate interpretability challenges of multiple comparisons. So, disclosing all 20 registered tests does not make the one significant result less vulnerable to being a false positive. The key for registering an analysis plan is that it constrains the initial analyses conducted and makes clear that any potential Type 1 error inflation is limited to those confirmatory analyses. In the ideal confirmatory analysis, the analysis script is created in advance and executed upon completion of data collection. In some cases, this ideal will be difficult to achieve. For example, there may be honest mistakes in the pre-registration phase or unforeseen properties of the data -such as non-normal data distributions or lack of variation in a key variable -that make deviations from the original analysis plans necessary. Having an analysis plan -with whatever degree of specificity is possible -makes it easy to clarify deviations from a strictly confirmatory analysis; explanations of those deviations makes it easier to judge their defensibility.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Conduct Exploratory Analysis for Discovery, not for Hypothesis Testing", "text": "Exploratory analysis is a valuable part of data analysis (Tukey, 1977). Much of progress in science is through accidental discovery of questions and hypotheses that one did not think to have in advance (Jaeger & Halliday, 1998). The emphasis on confirmatory designs does not discourage exploratory practice. Rather, it makes explicit the difference between outcomes resulting from confirmatory and exploratory approaches. In exploratory analysis, inductive reasoning is used to form tentative a posteriori hypotheses that explain the observations (Stebbins, 2001). Popper (1959) proposed that a hypothesis that is derived from a given set of observations cannot be falsified by those same observations. As Popper noted \"a hypothesis can only be empirically tested-and only after it has been advanced\" (p.7). Making explicit the distinction between confirmatory and exploratory analysis helps clarify the confidence in the observed effects, and emphasizes the fact that effects from exploratory analysis require additional investigation.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Test Discoveries with Confirmatory Designs", "text": "With discovery in hand, the temptation for publication is understandable. Replication offers only the dreaded possibility of \"losing\" the effect ( Nosek et al., 2012). There may be no palliative care available other than to point out that while many exploratory results are opportunities to develop hypotheses to be tested, they are not the hypothesis tests themselves. The long-term view is that it is better to learn quickly that the effect is irreproducible than to expend yours and others' resources on extensions that falsely assume its veracity. Following a discovery with a high-powered confirmatory test is the single best way to enhance the credibility and reproducibility of research findings. This point is not universal. There are instances for which the effects in exploratory analysis are estimated with such precision that it is highly unlikely that they are chance findings. However, the circumstances required for this are uncommon in most psychological applications. The most common cases are studies with many thousands of participants. Even in these cases, it is possible to leverage chance and exaggerate results. Further, data collection circumstances may not be amenable to conducting a confirmatory test after an exploratory discovery. For example, with extremely hard-to-access samples, the effort required to conduct a confirmatory test may exceed available resources. It is simply a fact that clarifying the credibility of findings can occur more quickly for some research applications compared with others. For example, research on the development of infant cognition often requires laborious laboratory data collections with hard to reach samples. Adult personality investigations, on the other hand, can often be administered via the Internet to hundreds of people simultaneously. The former will necessarily accumulate information and knowledge more slowly than the latter. These constraints do not exempt research areas from the tentativeness of exploratory results and the need for confirmatory investigations. Rather, because of practical constraints, some research applications may need to tolerate publication of more tentative results and slower progress in verification.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Keep Records of Analyses", "text": "Some challenges to reproducibility are more a function of deficient record-keeping than analysis and reporting decisions. Analysis programs, like SPSS, provide easy-to-use point-and- click interfaces for conducting analyses. The unfortunate result is that it can be very easy to forget the particulars of an analysis if only the output persists. A simple solution for increasing reproducibility is to retain scripts for exactly the analyses that were conducted and reported. Coupled with the data, re-executing the scripts would reproduce the entire analysis output. This is straightforward with script-based analysis programs like R, STATA and SAS, but is also easy with SPSS by simply generating and saving the scripts for the conducted analyses. Taking this simple step also offers practical benefits to researchers beyond improved reproducibility. When analysis procedures are carried out without the use of scripts, adding new data points, revising analyses, and answering methodological questions from reviewers can be both time-consuming and error-prone. Using scripts makes these tasks incomparably simpler and more accurate.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Share and Review Data and Analysis Scripts Among Collaborators", "text": "Science is often done in teams. In most cases, team members have some specialization such as one member developing the research materials and another conducting analysis. In most cases, all members of a collaboration should have access to all of the research materials and data from a study. At minimum, shared access ensures that any member of the team could find the materials or data if other team members were not available. Moreover, sharing materials and data increases the likelihood of identifying and correcting errors in design and analysis prior to publication. For example, in software development, code review -the systematic evaluation of source code -is common practice to fix errors and improve the quality of the code for its purpose and reusability (Kemerer & Paulk, 2009;Kolawa & Huizinga, 2007). Such practices are easy to incorporate into scientific applications, particularly of analysis scripts, in order to increase confidence and accuracy in the reported analyses. Finally, sharing data and analysis scripts with collaborators increases the likelihood that both will be documented so that they are understandable. For the data analyst, it is tempting to forgo the time required to create a codebook and clear documentation of one's analyses because -at the moment of analysis the variable names and meaning of the analysis are readily available in memory. However, six months later when the editor requires additional analysis, it can be hard to recall what VAR0001 and VAR0002 mean. Careful documentation of analyses and methods, along with data codebooks increase reproducibility by making it easier for someone else, including your future self, to understand and interpret the data and analysis scripts (Nosek, 2014). Sharing with collaborators is a means of motivating this solid practice that otherwise might feel dispensable in the short-term, but becomes a substantial time saver in the long-term.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Archive Materials and Data", "text": "Wicherts, Borsboom, Kats and Molenaar (2006) tried to obtain original datasets from 249 studies in order to reproduce the reported results. They found that the major barrier to reproducibility was not errors in the datasets; it was not being able to access the dataset at all. Just 26% of the datasets were available for reanalysis. In a more recent case, Vines and colleagues (2013) found that just 23% of 516 requested datasets were available, and the availability of datasets declined by 7% per year over the 20-year period they studied. Further, Vines and colleagues observed that the working rate of email addresses of corresponding authors fell by 7% per year over the same span. In sum, reproducibility and reanalysis of data is most threatened by the gradual loss of information through the regular laboratory events of broken machines, rotating staff, and mismanagement of files. The potential damage for one's own research and data management are substantial. Researchers routinely return to study designs or datasets as their research programs mature. If those materials and data are not well maintained, there is substantial loss of time and resources trying to recover prior work. Considering the substantial resources invested in obtaining the data and conducting the research, these studies reveal a staggering degree of waste of important scientific resources. It does not need to be this way. There are now hundreds of repositories available for archiving and maintaining research materials and data. If researchers adopt the strategy of sharing research materials and data among collaborators, then it is a simple step to archive those materials for purposes of preservation and later recovery.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Disclose Details of Methods and Analysis", "text": "The APA Manual, the style guide for report writing, suggests that methods sections need to report sufficient detail so that a reader could reasonably replicate the study ( Even a cursory review of published articles reveals that these norms are rarely met in modal research practices. And, yet, complete methodology and analysis description is vital for reproducibility. In the ideal report, a reader should be able to identify the conditions necessary to conduct a fair replication of the original research design, and have sufficient description of the analyses to reproduce them on the same or a new dataset. Without full description, the replication will inevitably contain many unintended differences from the original design or analysis that could interfere with reproducibility. There are occasions in which some critical elements of a research design will not fit into a written report -either because of length restrictions or because the design elements cannot be described in words effectively. For both, there are readily available alternatives. Supplementary materials, which most journals now support online during review and after publication, allow more comprehensive descriptions of methodology. Photo or video simulations of research designs can clarify key elements that are not easy to describe. What should be included in methods descriptions will vary substantially across research applications. An example of guidelines for effective reporting of methods and results was developed by the Research Committee at the Tilburg University Social Psychology Department (2013; http://www.academia.edu/2233260/Manual_for_Data_Sharing_-_Tilburg_University). While preparing comprehensive descriptions of research methods may add to the time required to publish a paper, it also has the potential to increase the impact of the research. Independent scientists interested in replicating or extending the published findings may be more likely to do so if the original report describes the methods thoroughly. And detailed methodological reporting increases the chances that subsequent replication attempts will faithfully adhere to the original methods, increasing the odds that the findings are replicated and the original authors' reputations enhanced.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Follow Checklists for Good Reporting Practices", "text": "The APA manual provides specific guidance for style and general guidance for content  Table 1.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Share Materials and Data with the Scientific Community", "text": "When Wicherts et al. (2006) received just 26% of requested datasets of published articles, they speculated that the low response rate was primarily a function of the time and effort it takes for researchers to find, prepare, and share their data and code books after publication. It is also possible that some were reluctant to share because the present culture perceives such requests as non-normative and perhaps done in effort to discredit one's research. Explicit, widespread embrace of openness as a value for science may help neutralize this concern. More directly to the point, when materials and data are archived from the start of the research process, it will be much easier for researchers to adhere to data-sharing requests. Some archiving solutions make it trivially easy to move a private repository into public or controlled access. Researchers who shared their materials and data with collaborators in a web-based archive can select which of those materials and data to release to the public. This may be particularly helpful for addressing the file-drawer effect. For those studies that researchers do not intend to write up and publish, their presence in a registry and public access to the materials and data ensures their discoverability for meta-analysis and assists researchers investigating similar questions in informing their research designs. Sharing research materials and data is not without concern. First, researchers may be concerned about the amount of work that will be required from them once method and data sharing becomes the standard. However, if researchers incorporate the expectation of sharing materials and data with collaborators, and potentially more publicly, into their daily workflow, sharing becomes surprisingly easy and encourages good documentation practices that assists the researcher's own access to the materials and data in the future. This may even save time and effort in the long run. Second, some data collections require extraordinary effort to collect and are the basis for multiple publications. In such cases, researchers may worry about the cost:benefit ratio of effort expended to obtain the data against the possibility of others' using the data before they have had sufficient time to develop their own published research from it. There are multiple ways to address this issue including: (a) releasing the data in steps exposing only the variables necessary to reproduce published findings; (b) establishing an embargo period during which the original authors pursue analysis and publication, but then open the data to others following that; or (c) embracing the emerging evidence that open data leads to greater scientific output and impact (Piwowar & Vision, 2013). Further, there are now journals such as the Journal of Open Psychology Data (http://openpsychologydata.metajnl.com/) and organizational efforts like Datacite (http://www.datacite.org/) that aim to make datasets themselves citable and a basis for earning reputation and citation impact. Finally, the most potent concern is protecting participant privacy with human participant research. At all times, the individual researcher bears fundamental responsibility to meet this ethical standard. Data sharing cannot compromise participants' rights and well-being. For many research applications, making the data anonymous is relatively easy to do by removing specific variables that are not essential for reproducing published analyses. For other research applications, a permissions process may be needed to obtain datasets with sensitive information. In summary, reproducibility will be maximized if the default practice for materials and data is to share them openly. Restrictions on open data are then the exceptions to the default practice. There are many defensible reasons for closing access, particularly to data. Those reasons should be made explicit in each use case.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Report Results to Facilitate Meta-Analysis", "text": "A single study rarely settles a scientific question. Any single finding could be upwardly or downwardly biased (i.e., larger or smaller than the true effect, respectively) due to random or systematic sources of variance. Meta-analysis addresses this concern by allowing researchers to model such variance and thereby provides summary estimates worthy of increased confidence. However, if the sources used as input to meta-analyses are biased, the resulting meta-analytic estimates will also be biased. Biased meta-analytic findings are especially problematic because they are more likely than primary studies to reach scientific and practitioner audiences. Therefore, they affect future research agendas and evidence-based practice (Kepes & McDaniel, 2013). Individual researchers can facilitate effective aggregation of research evidence by (a) making their own research evidence -published and unpublished -available for discovery by meta-analysts, and (b) structuring the results reports so that the required findings are easy to find and aggregate. The first is addressed by following the archiving and sharing steps described previously. The second is facilitated by ensuring that effect sizes for effects of interest and all variable pairs are available in the report or supplements. For example, authors can report a correlation matrix, which serves as an effect size repository for a variety of variable types ( Dalton et al., 2012).", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Replicate-and-Extend", "text": "The number of articles in psychology explicitly dedicated to independent, direct replications of research appears to be 1% or less of published articles (Makel, Plucker & Hegarty, 2012). It would be easy to conclude from this that psychologists do not care about replicating research, and that journals reject replication studies routinely because they do not make a novel enough contribution. However, even when researchers are skeptical of the value of publishing replications, they may agree that replication-and-extension is a profitable way to meet journals' standards for innovation while simultaneously increasing confidence in existing findings. A great deal of replication could be carried out in the context of replicate-and-extend paradigms ( Nosek et al., 2012;Roediger, 2012). Researchers may repeat a procedure from an initial study within the same paper, adding conditions or measures, but also preserving the original design. For example, a Study 2 might include two conditions that replicate Study 1 (disgust prime and control), but also add a third condition (anger prime), and a second outcome measure. Thus, Study 2 offers a direct replication of the Study 1 finding with an extension comparing those original conditions to an anger prime condition. This provides greater certainty about the reproducibility of the original result than a Study 2 that tests the same hypothesis after changing all the operationalizations.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Participate in Crowdsourced Research Projects", "text": "The prior section alluded to the fact that some challenges for reproducibility are a function of the existing culture strongly prioritizing innovation over verification (Nosek et al., 2012). It is not worth researchers' time to conduct replications or confirmatory tests if they are not rewarded for doing so. Similarly, some problems are not theoretically exciting, but would be practically useful for developing standards or best practices for reproducible methodologies. For example, the scrambled sentence paradigm is used frequently to make particular thoughts accessible that may influence subsequent judgment (e.g., Bargh, Chen, & Burrows, 1996). Despite being a frequently used paradigm, there is no direct evidence for which procedural features optimize the paradigm's effectiveness, and there is great variation in operationalizations across studies. Optimizing the design would be very useful for maximizing power and reproducibility, but conducting the required studies would be time consuming with uncertain reward. Finally, some problems are acknowledged to be important, but are too large to tackle singly. It is difficult for individual researchers to prioritize doing any of these when confronted with the competitive nature of getting a job, keeping a job, and succeeding as an academic scientist. One solution for managing these incentive problems is crowdsourcing. Many researchers can each contribute a small amount of work to a larger effort. The accumulated contribution is large, and little risk is taken on by any one contributor. For example, the Reproducibility Project: Psychology is investigating the predictors of reproducibility of psychological science by replicating a large sample of published findings. Almost 200 researchers are working together with many small teams each conducting a replication following a standardized protocol (Open Science Collaboration, 2012a, in press). Another approach is to incorporate replications into teaching. This can address the incentives problem and provide pedagogical value simultaneously (Frank & Saxe, 2012;Grahe et al., 2012). The CREP project (https://osf.io/wfc6u/) identifies published research for which replication could be feasibly incorporated into undergraduate methods courses. Also, the Archival Project (http://archivalproject.org/) integrates crowdsourcing and pedagogical value with a crowdsourced effort to code articles to identify the rates of replications and characteristics of methods and results in the published literature.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Request Disclosure as a Peer Reviewer", "text": "Individual researchers can contribute to promoting a culture of reproducibility by adapting their own research practices, and also by asking others to do so in the context of their role as peer reviewers. Peer reviewers have influence on the articles they review and, in the aggregate, on editors and standard journal practices. The Center for Open Science (http://cos.io/) maintains a standard request that peer reviewers can include in their reviews of empirical research to promote a culture of transparency: \"I request that the authors add a statement to the paper confirming whether, for all experiments, they have reported all measures, conditions, data exclusions, and how they determined their sample sizes. The authors should, of course, add any additional text to ensure the statement is accurate. This is the standard reviewer disclosure request endorsed by the Center for Open Science [see http://osf.io/hadz3]. I include it in every review.\" Including this as a standard request in all reviews can (a) show the broad interest in making the disclosure standard practice, and (b) emphasize it as a cultural norm and not an accusatory stance toward any individual. A culture of transparency works best if all members of the culture are expected to abide by it.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Implementing These Practices: An Illustration with the Open Science Framework", "text": "There are a variety of idiosyncratic ways to implement the practices discussed in this chapter. Here we offer an illustration using an open source web application that is maintained by the Center for Open Science called the Open Science Framework (OSF; http://osf.io/). All of the practices summarized can be supported by the OSF. Organize a research project. The research workflow in the OSF begins with the creation of a project. The creator provides the title and description, uploads files, write documentation via the wiki, and add contributors. Users can create project components to organize the project into conceptual units. For example, a survey research project might include one component for study design and sampling procedures, another for survey instruments, a third for raw data, a fourth for data analysis, and a fifth for the published report. Each component has its own list of contributors and privacy settings. For example, the lead investigators of a project may decide to grant access to the data coding components to research assistant collaborators, but to deny those collaborators permission to modify the data analysis components. Create an analysis plan. Once the investigator has organized her project and added her contributors, she might then add her analysis plan. The investigator might create a new component for the analysis plan, upload analysis scripts and sample codebooks, and write a narrative summary of the plan in the component wiki. Register study and materials. Once the investigator is ready to begin data collection, she might next register her study and materials. Materials are often used between studies and may evolve; registration at this point ensures that the exact materials used in the study are preserved. To do so, she would click a button to initiate a registration and provide some description about what is being registered. Once created, this registration becomes a frozen copy of the project as it existed at the moment it was registered. This frozen copy is linked to the project, which the researchers may continue to edit. Thus, by creating a registration, the investigator can later demonstrate that her published analysis matched her original plan--or, if any changes were necessarily, detail what was changed and why. Keep records of analyses. As the research team collects data and conducts analysis, the tools used to generate the analysis and records of how those tools were used can be added to the data analysis component of the project. These might include analysis or data cleaning scripts written using Python, R, or SPSS, quality checking procedures, or instructions for running these scripts on new data. The OSF records all changes made to project components, so the research team can easily keep track of what changed, when it changed, and who changed it. Prior versions are retained and recoverable. Share materials and data. At any point during the research life cycle, the team may choose to make some or all of their work open to the public. OSF users can make a project or one of its components public in a single step: clicking on the \"Make Public\" button on the dashboard of each project. Researchers can also independently control the privacy of each component in a project; for example, an investigator may decide to make her surveys and analysis plan public, but make her raw data private to protect the identities of her research participants. Replicate and extend. Once the investigator's project is complete, independent scientists may wish to replicate and extend her work. If the original investigator made some or all of her work public, other OSF users can create an independent copy (or a \"fork\") of her project as a starting point for their own investigations. For example, another OSF user might fork the original researcher's data collection component to use her surveys in a new study. Similarly, another researcher planning a meta-analysis might fork the original raw data or data analysis components of several OSF projects to synthesize the results across studies. The source project/component is maintained, creating a functional citation network--the original contributors credit is forever maintained.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Closing", "text": "We started this chapter concerning how to improve reproducibility with a question: \"What can I do?\" We intend the suggestions made in this chapter to provide practical answers to that question. When researchers pursue open, reproducible practices they are actively contributing to enhancing the reproducibility of psychological research, and to establishing a culture of \"getting it right\" (Nosek et al., 2012). Though adhering to these suggestions may require some adaptation of current practices by the individual researcher, we believe that the steps are minor, and that the benefits will far outweigh the costs. Good practices may be rewarded with general recognition, badges (https://osf.io/tvyxz/), and enhanced reputation, but ultimately the reward will be having contributed to a cumulative science via reproducible findings.", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Glossary", "text": "Confirmatory research: Research in which data is gathered to test a priori hypotheses. Exploratory research: Research in which data is gathered to determine whether interesting a posteriori hypotheses might be generated from the data. File-drawer effect: The bias introduced into the scientific literature by a tendency to publish positive results but not to publish negative results. Meta-analysis: The use of statistical methods to combine results of individual studies. Power: The probability that the test will reject the null hypothesis when the alternative hypothesis is true. Pre-registration: Registering which variables will be collected, how many participants will be tested, and how the data will be analyzed before any participants are tested. Table 1 Increasing the reproducibility of psychological research across the research lifecycle ", "title": NaN, "file_name": "Bosco et al. - 2016 - Maximizing the Reproducibility of Your Research.pdf"}
{"section": "Abstract", "text": "These first ten authors contributed equally to this work.", "title": "Evaluating the Replicability of Social Science Experiments in Nature and Science", "file_name": "Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "2", "text": "Being able to replicate scientific findings is crucial for scientific progress [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15] . We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 2015. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered with sample sizes on average about 5 times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62%) studies, and the effect size of the replications is on average about 50% of the original effect size. Replicability varies between 12 (57%) and 14 (67%) for complementary replicability indicators. Consistent with these results, the estimated true positive rate is 67% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71% suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. We furthermore find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone. To what extent can we trust scientific findings? The answer to this question is of fundamental importance 1-3 , and the reproducibility of published studies has been questioned in many fields 4-10 . Until recently, systematic evidence has been scarce 11-15 . The Reproducibility Project: Psychology 12 (RPP) put the question of scientific reproducibility at the forefront of scientific debate 16-18 . The RPP replicated 100 original studies in psychology, and found a significant effect in the same direction as the original for 36% of the 97 studies reporting \"positive findings\" 12 . The RPP was followed by the Experimental Economics Replication Project (EERP) which replicated 3 18 laboratory experiments in economics and found a significant effect in the same direction as the original studies for 61% of replications 13 . Both the RPP and the EERP had high statistical power to detect the effect sizes observed in the original studies. However, the effect sizes of published studies may be inflated even for true positive findings due to publication or reporting biases [19][20][21] . As a consequence, if replications were well-powered to detect effect sizes smaller than those observed in the original studies, replication rates might be higher than those estimated in the RPP and the EERP. We provide evidence about the replicability of experimental studies in the social sciences published in the two most prestigious general science journals, Nature and Science (the Social Sciences Replication Project; SSRP). Articles published in these journals are considered exciting, innovative, and important. We include all experimental studies published between 2010 and 2015 that (i) test for an experimental treatment effect between or within subjects, (ii) test at least one clear hypothesis with a statistically significant finding, and (iii) were performed on students or other accessible subject pools. Twenty-one studies were identified to meet these criteria. We used the following three criteria in descending order to determine which treatment effect to replicate within each of these 21 papers: (a) select the first study reporting a significant treatment effect for papers reporting more than one study, (b) from that study, select the statistically significant result identified in the original study as the most important result among all within and between subjects treatment comparisons, and (c) if there was more than one equally central result, randomly select one of them for replication. The interpretation of which was the most central and important statistically significant result within a study in criteria (b) above was made by us, and not by the original authors. (See Supplementary Information, section 1 and Tables S1-S2 for details.) 4 To address the possibility of inflated effect sizes in the original studies, we used a high-powered design and a two-stage procedure for conducting the replications. In Stage 1 we had 90% power to detect 75% of the original effect size at the 5% significance level in a two-sided test. If the original result replicated in Stage 1 (a two- sided p-value < 0.05 and an effect in the same direction as in the original study), no further data collection was carried out. If the original result did not replicate in Stage 1, we carried out a second data collection in Stage 2 to have 90% power to detect 50% of the original effect size for the first and second data collections pooled. The motivation for having 90% power to detect 50% of the original effect size was based on the replication effect sizes in the RPP being on average about 50% of the original effect sizes (12) (see Supplementary Information, section 1, for details; the average relative effect size of the replications in the EERP was 66% 13 ). On average, replication sample sizes in Stage 1 were about three times as large as the original sample sizes and replication sample sizes in Stage 2 were about six times as large as the original sample sizes. All of the replication and analysis plans were made publicly known on the project website, pre-registered at the OSF and sent to the original authors for feedback and verification prior to data collection (see Supplementary Information, section 1, for details and see individual replication reports for methodological details and reporting of any deviations in the protocols from the original studies). There is no universally agreed upon criterion for replication 12, 22-25 , but our power analysis strategy is based on detecting a significant effect in the same direction as the original study using the same statistical test. As such, we treat this as the primary indicator of replication and refer to it as the statistical significance criterion. This approach is appealing for its simplicity as a binary measure of replication, but does not In Stage 1 we find a significant effect in the same direction as the original study for 12 replications (57.1%) ( Fig. 1a and Table S3). When we increase the statistical power further in Stage 2 ( Fig. 1b and Table S4), 2 additional studies replicate based on this criterion. By mistake, a second data collection was carried out for one study replicating in Stage 1, and we therefore also include this study in the Stage 2 results to base our results on all the data collected. This study does not replicate in Stage 2. This may suggest that replication studies should routinely be powered to detect at least 50% of the original effect size, or that one should use a lower p-value threshold than 0.05 for not continuing to Stage 2 in our two-stage testing procedure. Based on all data collected, 13 (61.9%) studies replicate after Stage 2 using the statistical significance criterion. The mean standardized effect size (correlation coefficient, r) of the replications is 0.249, compared to 0.460 in the original studies (Fig. S4). This difference is significant (Wilcoxon signed-ranks test, z = 3.667, p < 0.001, n = 21), and the mean relative effect size of the replications is 46.2%. For the 13 studies that replicated, the mean relative effect size is 74.5%, and for the 8 studies that did not replicate, the mean 6 relative effect size is 0.3%. It is not surprising that the mean relative effect size is smaller for the non-replicating effects than replicating effects as these are correlated indicators. However, it is notable that, even among the replicating effects, effect sizes for the replications were weaker than the original findings and, for the non-replicating effects, the mean effect sizes were approximately zero. We also combined the original result and the replication in a meta-analytic estimate of the effect size. As seen in Fig. 1c, 16 studies (76.2%) have a significant effect in the same direction as the original study in the meta-analysis. However, the meta-analysis assume that the results of the original studies are not influenced by publication or reporting biases making the meta-analytic results an overly optimistic indicator compared to criteria focused on the replication evidence 12 . A team recently suggested that the p-value threshold for statistically significant findings should be lowered from 0.05 to 0.005 for new discoveries 30 . In a replication context it would be relevant to apply this stricter threshold to meta-analytic results. In this case, the meta- analysis leads to the same conclusions about replication as our primary replication indicator (i.e., 13 studies or 61.9% have a p-value < 0.005 in the meta-analysis). It is obvious that the 13 successful replications would achieve p < 0.005 when the original and replication results were pooled, but this criterion could have also included replications that did not achieve p < 0.05 but were in the right direction and were combined with an original study with particularly strong evidence. A complementary replication criterion is to count how many replicated effects lie in a 95% prediction interval (26), which takes into account the variability in both the original study and the replication study. Using this method, 14 effects replicate (66.7%; see Fig. 2a and Supplementary Information, section 2, for details). This method yields 7 the same replication outcome as the statistical significance criterion for 20 of the 21 studies. The Small Telescopes approach estimates whether the replication effect size is significantly smaller than a \"small effect\" in the original study with a one-sided test at the 5% level. A small effect is defined as the effect size the original study would have had 33% power to detect. Following the Small Telescopes approach 25 12 studies (57.1%) replicate (see Fig. 2b and Supplementary Information, section 2, for details). One replication has a significant effect in the same direction as the original study, but the effect size is significantly smaller than a small effect as defined by the Small Telescopes approach. This is the only difference compared to the statistical significance criterion. Another way to represent the strength of evidence in favor of the original result versus the null hypothesis of no effect is to estimate the Bayes factor 24, 27, 31-32 . The Bayes factor compares the predictive performance of the null hypothesis against that of an alternative hypothesis in which the uncertainty about the true effect size is quantified by a prior distribution. The prior distributions were first set to their generic defaults; they were then folded across the test value so that all prior mass is consistent with the direction of the effect from the original study, thereby implementing a Bayesian one- sided test (see the Supplementary Information, section 2, for details). For example, the replication of Pyc and Rawson yielded a one-sided default Bayes factor of BF+0 = 6.8, meaning that the one-sided alternative hypothesis outpredicted the null hypothesis of no effect by a factor of almost 7. The one-sided default Bayes factor exceeds 1, providing evidence in favor of an effect in the direction of the original study for the 13 (61.9%) studies that replicated 8 according to our primary replication indicator (Fig. 3). This evidence is strong to extreme for 9 (42.9%) studies. The default Bayes factor is below 1 for 8 (38.1%) studies providing evidence in support of the null hypothesis; this evidence is strong to extreme for 4 (19.0%) studies. In additional Bayesian analyses, we use an errors-in-variables mixture model (28) to estimate the true positive rate in the total sample (see the Supplementary Information, section 2 and Fig. S5 for details). The estimated true positive rate is 67% (Fig. S5), which is close to the other replicability estimates. The mixture model also estimates that the average relative effect size of true positives is 71% (Fig. S5) suggesting that the original studies overestimated the effect sizes of true positives. We also estimate peer beliefs about replicability using surveys and prediction  Tables S5-S6 for more details). The prediction market beliefs and the survey beliefs are highly correlated, and both are highly correlated with a successful replication (Fig. 4 and Fig. S7). That is, in the aggregate, peers were very effective at predicting future replication success. In the RPP 12 and the EERP 13 , replication success was negatively correlated with the p-value of the original study, suggesting that original study p-values might be a predictor of replicability. We also find a negative correlation between the p-value of the original study and replication success, although it is not significant (Spearman 9 correlation coefficient -0.405, p = 0.069, n = 21); the estimate is in between the correlations found in the RPP (-0.327) and the EERP (-0.572) (Table S7). That peers are to some extent able to predict which studies are most likely to replicate suggests that there are features of the original studies that journals or researchers can use in determining ex ante whether a study is likely to replicate. The results from the RPP, EERP, and SSRP taken together suggest that the p-value of the original study is one such important determinant of replication. The SSRP with n = 21 studies is too small to reliably test determinants of replications, but pooling the results of all large scale replication projects may offer a higher powered opportunity to explore moderators of replication. To summarize, we successfully replicated 13 of 21 findings from experimental social and behavioral science studies published in Science or Nature between 2010 and 2015 based on the statistical significance criterion with very high-powered studies compared to the RPP 12 and the EERP 13 . This number is larger than the RPP's replication rate and similar to the EERP's replication rate (Fig. S9). However, the small sample of studies and different selection criteria makes it difficult to draw any interpretation confidently in comparison with those studies. We can conclude, however, that increasing power substantially is not sufficient to reproduce all published studies. Also, we observe that the conclusions across binary replication criteria converge with increased statistical power. The Small Telescopes and the 95% prediction interval indicators drew different conclusions on only one of the replications compared to the statistical significance criterion. Considering statistical significance and effect sizes simultaneously, we observe two major outcomes. First, even among successful replications, estimated effect sizes 10 were smaller than the original study. For the 13 studies that replicated according to the statistical significance criterion the replication effect sizes were about 75% of the original effect size. This provides an estimate of the overestimation of effect sizes of true positives in original studies. The Bayesian mixture model corroborates this result yielding an estimate of the relative effect size of true positives of 71%. This implies that meta-analyses of true positive findings will overestimate effect sizes on average. This finding bolsters evidence that the existing literature contains exaggerated effect sizes because of pervasive low powered research coupled with bias selecting significant results for publication. 8,12 Also, if this finding generalizes to the literatures investigated by the RPP and the EERP, it suggests that the statistical power of these two projects, where the sample sizes were determined to obtain 90% power to detect the original effect size, was de-facto smaller than intended. This would imply that the replication rates, based on the statistical significance criterion, were underestimated in these studies consistent with those authors' speculation. Second, among the unsuccessful replications, there was essentially no evidence for the original finding. The average relative effect size was very close to zero for the eight findings that failed to replicate according to the statistical significance criterion. The expected relative effect size for a sample of false positives is zero, but this observation does not demand the conclusion that the eight original findings were false positives. Another possibility is that the replication studies failed to implement necessary features of the protocol to detect the effect 17 . We cannot rule out this alternative, but we also do not have evidence for necessary features missing from the replications that would reduce the observed effect sizes to zero. Indeed, it would be surprising but interesting to identify an unintended difference that completely 11 eliminated the effect rather than just reduce the effect size. One suggested indicator for whether differences between studies are a likely cause for bias, is the endorsement of the original authors. 17 In the current project, we took extensive efforts to ensure that the replications would be as close as possible to the originals. All of the replications but one were designed with the collaboration of the original authors (for one replication the original authors did not respond to our queries). And, all of the reviewed replications but one were approved by the original authors. However, none of this implies that original authors agree with the final outcomes or interpretation. For example, changes in planned implementation or insights after observing the results could lead to different interpretations of the replication outcome and ideas for subsequent research to clarify the understanding of the phenomenon. See Supplementary Information, section 1 and the posted replication reports for each study for more details including follow-up comments from original authors if provided. Another hypothesis that could account for replication failures, at least partly, is the result of chance, such as a large degree of heterogeneity in treatment effects in different samples 17 . However, such heterogeneity would not affect the average relative effect size of replications, as replications would be as likely to over-as underestimate original effect sizes. It cannot therefore explain why the average effect sizes of our replications is only about 50% of the original effect sizes. Furthermore, the strong correlation between the peer predictions and the observed replicability is discordant with the possibility that replication failures occurred by chance alone. That is, researchers appear to have identified a priori systematic differences between the studies that replicated and those that did not. This capacity to predict the replicability of effects is a reason for optimism that methods will emerge to anticipate reproducibility 12 challenges and guide efficient use of replication resources toward exciting but uncertain findings. The observed replication rate of 62%, based on the statistical significance criterion, adds to a growing pool of replicability rates from a variety of systematic replication efforts with distinct selection and design criteria: the RPP 12 (36%, n = 100 studies), the EERP 13 (61%, n = 18 studies), Many Labs 1 11 (77%; n = 13 studies), Many Labs 2 15 (50%, n = 28 studies), and Many Labs 3 14 (30%, n = 10 studies). It is too early to draw a specific conclusion about the reproducibility rates of experimental studies in the social and behavioral sciences. Each investigation has a relatively small sample of studies with idiosyncratic inclusion criteria and unknown generalizability. But, the diversity in approaches provides some confidence that considering them in the aggregate may provide more general insight about reproducibility in the social- behavioral sciences. As a descriptive and speculative interpretation of these findings in the aggregate, we believe that reasonable lower and upper bound estimates are 35% and 75% for an average reproducibility rate of published findings in social and behavioral sciences. Accumulating additional evidence will reveal if there are systematic biases in these reproducibility estimates themselves. When assessing reproducibility we are interested in both the systematic bias in the estimated effect sizes of original studies and the fraction of original hypotheses that are directionally true. The average relative effect size of 50% in the SSRP is a direct estimate of the systematic bias in the published findings of the 21 studies, as it should be 100% if original studies provide unbiased estimates of true effect sizes. This estimate assumes that there is no systematic difference in the effectiveness of implementing the study procedures or the appropriateness of testing circumstances between original and 13 replication studies. If both of those assumptions are true, then our data suggests that the systematic bias is partly due to false positives and partly due to the overestimated effect sizes of true positives. These systematic biases can be reduced by implementation of pre-registration of analysis plans to reduce the likelihood of false positives, and registration and reporting of all study results to reduce the effects of publication bias inflating effect sizes 34 . With notable progress on these practices, particularly in the social and behavioral sciences 35 , we predict that replicability will improve over time.", "title": "Evaluating the Replicability of Social Science Experiments in Nature and Science", "file_name": "Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Limitations", "text": "The SSRP is a small sample of studies with specific selection criteria for experimental studies from two high-profile journals. Work that is published in Nature and Science may be atypical to the field as a whole, and may have a stronger focus on novelty, which may also lead to greater -or lesser -editorial scrutiny. The small sample and selective criteria significantly reduce confidence in generalizing these findings to the social science literature more generally. Indeed, like all other research, replications require an accumulation of evidence across multiple efforts to identify and address sampling biases and to obtain increasingly precise estimates of replicability. This study adds to this accumulating literature with a focused, high-powered investigation of high-profile studies published in Nature and Science. Notably, with replication sample sizes about five times larger as the original studies, we get relatively precise estimates of the individual effects of these single replications and average relative effect sizes that are very similar to what was observed in RPP. Another important limitation is that for papers reporting a series of studies we only replicate one of those studies, and for studies testing more than one hypothesis we only replicate one hypothesis. Like prior large-scale replication projects, this study does not provide definitive insight on any of the original papers from which we designed replication studies. An alternative methodology would be to replicate all results within the selected study or all results within all studies in a paper reporting a series of studies. This would give more information from each replication and a more precise estimate of reproducibility of each study and paper. All investigations involve tradeoffs. The advantage of an in-depth examination of a paper is greater insight and precision of the reproducibility of its findings. The disadvantage is that many fewer findings can be investigated to learn about reproducibility of findings more generally. Some other findings reported in the original papers can be tested with the data available in our study's replications. We did not consider those secondary findings in this paper or in deciding the statistical power plans for the design. However, all of our data and materials are publicly posted as part of open science and will be available to other researchers who may want to pursue this issue further in follow-up work. The original authors in reviewing our paper and replication results have noted some limitations on the replications of their individual studies. These are discussed more in the Supporting Information (Section 1.2); and several of the original authors have also posted comments on the replications at OSF alongside our Replication Reports. For example, previously unidentified or inadvertent changes to the protocol may have affected replication success for some studies. Also, for papers reporting a series of studies we replicated the first study reporting a significant treatment effect. In some cases the original authors argue that other studies in their papers report more important results or use stronger research designs. 46,54 If the replicability of the first study systematically differ from the replicability of subsequent studies in a paper our 15 criteria for deciding which study to replicate will systematically over-or under-estimate replicability. Inspired by our replication, the original authors of Shah et al. 54 decided to carry out a replication study of their own on all of their five studies (with results posted at https:osf.io/vzm23/). They did replicate what they consider to be their most important finding, that scarcity itself leads to over-borrowing. They also failed to replicate study 1 in their paper consistent with our findings. Their approach of conducting replications of their own studies is admirable and provides additional insight and precision for understanding those effects. Five of our replications were carried out on Amazon Mechanical Turk (AMT) and for one of those ( Rand et al. 53 ), the original authors argue that increasing familiarity with economic game paradigms among AMT samples may have decreased the replicability of their result. It cannot be ruled out that changes in the AMT subject pool over time have affected our results, but we also note that the two other studies based on economic game paradigms and AMT data replicated successfully 43,50 . It would be interesting in future work to test if replicability differ for older versus newer studies or depends on the time that has elapsed between the original study and the replication. For the Sparrow et al. 55 replication, the original authors did not provide us with materials for the replication or feedback on our inquiries. This made it more difficult to replicate the experimental design of the original study. After the replication had been completed the original authors noted some design differences compared to the original study. These design differences are discussed further in the Supplementary Information,   16 and we cannot rule out that they impacted the replication result. This illustrates the importance of open access to all the materials of published studies for conducting direct replications and accumulating scientific knowledge. CIs of replication effect sizes in relation to small effect sizes as defined by the Small Telescopes approach (the effect size the original study would have had 33% power to detect). Effect sizes are standardized to correlation coefficients r and normalized so that 1 equals the original effect size. A study is defined as failing to replicate if the 90% confidence interval is below the small effect. According to the Small Telescopes approach 12 [57.1%; 95% CI = (34.1%, 80.2%)] studies replicate.  27 for the 21 replications. A default Bayes factor above 1 favors the hypothesis of an effect in the direction of the original paper and a default Bayes factor below 1 favors the null hypothesis of no effect. The evidence categories proposed by Jeffreys 31 are also shown in the Figure (from extreme support for the null hypothesis to extreme support for the original hypothesis). The default Bayes factor is above 1 and provide evidence in favor of an effect in the direction of the original study for the 13 (61.9%) studies that replicated according to the statistical significance criterion. This evidence is strong to extreme for 9 (42.9%) studies. The default Bayes factor is below 1 for 8 (38.1%) studies providing evidence in support of the null hypothesis; this evidence is strong to extreme for 4 (19.0%) studies. ", "title": "Evaluating the Replicability of Social Science Experiments in Nature and Science", "file_name": "Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "University of California-Berkeley", "text": "One of sociology's classic puzzles is how groups motivate their members to set aside self- interest and contribute to collective action. This article presents a solution to the problem based on status as a selective incentive motivating contribution. Contributors to collective action signal their motivation to help the group and consequently earn diverse benefits from group members-in particular, higher status-and these rewards encourage greater giving to the group in the future. In Study 1, high contributors to collective action earned higher status, exercised more interpersonal influence, were cooperated with more, and received gifts of greater value. Studies 2 and 3 replicated these findings while discounting alternative explanations. All three studies show that giving to the group mattered because it signaled an individual's motivation to help the group. Study 4 finds that participants who received status for their contributions subsequently contributed more and viewed the group more positively. These results demonstrate how the allocation of respect to contributors shapes group productivity and solidarity, offering a solution to the collective action problem. action literature: solidarity, productivity, and feelings of collective identity. The present research makes several contri- butions to the vast, multidisciplinary body of research on the collective action problem: (1) a theory that shows how the dynamics of status attainment in groups help solve the problem; specifically, how successful collective action and status for contributors stimulate one anoth- er; (2) empirical demonstrations of how groups reward those who sacrif ice for the group (Studies 1 and 2); (3) evidence that groups do so because they see contributors as group-moti- vated (Studies 1, 2, and 3); (4) elimination of alternative explanations (Studies 2 and 3); and (5) a demonstration that individuals who receive status for their contributions give more in the future, and they do so because that experience increases their group motivation (Study 4). I conducted a series of laboratory experi- ments to test predictions derived from the the- ory. In natural collective action settings, such as activist groups, where the reciprocal relation- ships between contribution and status are already under way, determining causality is dif- ficult. Experiments permit me to manipulate the critical variables of interest in isolation from other variables, allowing careful scrutiny of the validity of the theory's predictions with limit- ed concern that other, unmeasured variables (e.g., personality, skills, and occupational sta- tus) are actually responsible for observed rela- tionships between status and collective action contribution (Lucas 2003). Random assignment to experimental conditions helps control for the potential contaminating effects of individual differences. The present methodology thus pro- vides for strong initial tests, although the theo- ry's validity, in the end, is a function of the number, proportion, quality, and diversity of empirical tests supporting it.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "THEORY", "text": "There are many solutions to the collective action problem, reflecting different methodologies, theoretical approaches, and levels of analysis (for reviews, see Kollock 1998;Ledyard 1995;Oliver 1993;Yamagishi 1995). Perhaps the most intuitive solution is that groups provide selec- tive incentives to individuals to encourage con- tributions and to discourage free riding (Olson 1965). Examples of selective incentives range from the material to the social ( Clark and Wilson 1961), including paying blood donors, \"I Voted\" stickers given out to voters, tote bags and bumper stickers given to public-television donors, prosecution of tax evasion, and eco- nomic sanctions for violating international treaties. The solution I propose considers status in the group as an implicit, selective incentive. Olson (1965:60) notes that \"people are some- times also motivated by a desire to win prestige, respect, friendship, and other social and psy- chological objectives.\" I elaborate here on Olson's observation by theorizing a reciprocal relationship between status hierarchies and col- lective action. Status and other social and mate- rial benefits are allocated to individuals for contribution to collective action to the extent that individuals successfully signal their motivation to help the group. These status rewards in turn increase that motivation, leading to greater giv- ing and more positive views of the group. I define status as an individual's relative standing in a group based on prestige, honor, and deference (Berger, Cohen, and Zelditch 1972). Status hierarchies are typically conceptualized as relative and zero-sum. Research shows that group members tend to agree on the group's sta- tus hierarchy ( Anderson et al. 2006; Ridgeway and Walker 1995), evaluate higher status mem- bers more favorably than others (Foschi 1992), defer to them more often (Berger et al. 1972(Berger et al. , 1977Wagner, Ford, and Ford 1986), give them more chances to speak and act (Cohen 1994), and allocate more resources to them (Berger et al. 1985). In all these ways, being accorded high status in a group is valuable to individu- als. Furthermore, status research shows that group members voluntarily-and noncon- sciously (Rashotte and Webster 2005)-reorder status hierarchies based on new, salient, status- relevant information (Cohen 1994;Fisek, Berger, and Normon 1991;Markovsky, Smith, and Berger 1984; but see also Foschi 1992Foschi , 1996. 1", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "THEORETICAL SCOPE AND PROPOSITIONS", "text": "The scope of the theory encompasses collective action situations as conventionally defined, that is, situations where a public good that is (1) non-excludable (i.e., it benefits all group mem- bers) and (2) valued by all group members (3) requires costly contributions from individuals to be produced. Contributions to a public television funding drive would be within the scope, but investments in a corporation would not, because free riders are excluded from resulting benefits. I assume that individuals value higher status. This assumption is widely held in sociology (e.g., Sewell, Haller, and Portes 1969) and exper- imental research shows that people will forgo material profits to gain status ( Eckel 1996, 1998;Ball et al. 2001;Huberman, Loch, and Onculer 2004). Andreoni and Petrie (2004) demonstrate that there is greater giving to pub- lic goods when contributions are made public than when they are anonymous, suggesting that people value status in public good settings. Figure 1 presents the theory as a whole. Proposition 1: The more highly group members evaluate (1) the cost of an individual's con- tributions to the group, and (2) the benefits to the group of an individual's contribu- tions, then the more highly members will evaluate the individual's level of \"group motivation.\" I define individuals' group motivation as how much they value the group's interests relative to their own. Individuals perceived to have greater group motivation will be expected to sacrifice more for the group's benefit. Significantly, the group's estimate of an individual's group moti- vation is the product of both how much the group benefits and how much group members believe the contribution costs the individual. This is evaluated as a matter of how much the group believes the individual could contribute, that is, how much was sacrificed. For example, members of a club would assess an hour's vol- unteer work by a busy professional as indicative of more group motivation than the same amount of time donated by a retiree. Proposition 2: The greater an individual's per- ceived group motivation, the greater the individual's relative status standing. Ridgeway (1978,1982) shows that a mem- ber's status in a group depends not only on spe- cific contributions, but also on whether the individual conveys an impression of being con- cerned with group welfare. Related anthropo- logical f ield research and experimental", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "GROUPS REWARD INDIVIDUAL SACRIFICE--25", "text": "sion formation processes, then a second-order free- rider problem need not develop. economics research find that groups show greater esteem toward members who make greater contributions to the collective good. 2 Proposition 3: The greater an individual's sta- tus standing, the more group members will yield to the influence of the individual, cooperate with the individual, and give the individual gifts of greater value. Proposition 3 proposes several behavioral consequences of improved status standing. Past research on status characteristics theory con- sistently shows a strong and reliable relationship between status and influence ( Berger et al. 1972Berger et al. , 1977. Additionally, past theory and research suggest that material benefits accrue to higher status individuals ( Berger et al. 1985;Hardy and van Vugt 2006;Henrich and Gil-White 2001;Thye 2000). Here I reason that higher status people will be cooperated with more and receive gifts of greater value from group members because the respect that they command leads them to be perceived as more trustworthy and makes them less likely targets of self-interest- ed exploitation. Proposition 4: The greater the status an indi- vidual receives for past contributions, the more the individual's group motivation will increase and the more the individual will tend to view the group positively-includ- ing identifying with the group more and seeing it as having more solidarity and cohesion. Proposition 5: The greater an individual's group motivation, the more the individual will contribute to the group. Propositions 4 and 5 make new predictions about the effects of receiving status rewards on collective action contribution. They link the micro-level dynamic of rewarding contribu- tions with status to macro-level phenomena such as group productivity, identification, and solidarity. On the basis of these theoretical proposi- tions, I make the following derivations tested in Studies 1, 2, and 3: the more that individuals contribute to collective action, relative to other group members, the greater their status in the group and the more they will (1) exert influence over other group members, (2) enjoy coopera- tion from other group members, and (3) receive gifts from other group members. I also make the following derivations to be tested in Study 4: the greater the status that individuals receive for past contributions, the more they contribute to the group in the future and the more positive their perceptions of it. As shown in Figure 1, individuals' contribu- tions to collective action lead them to be viewed as more group motivated and, as a result, high- er status. Individuals who achieve higher status are then predicted to wield more influence, be cooperated with more, and receive gifts of greater value in interactions with other group members. Additionally, as group members express greater respect to high contributing individuals, high contributors tend to view the group more positively, as indicated by their greater motivation to help the group, feelings of group solidarity and cohesion, and identifica- tion with the group. Self-sacrifice for group goals thus earns a contributor a diversity of benefits, both social and material. Further, the theory outlines a \"virtuous\" cycle, wherein cost- ly contributions to group efforts signal one's concern for the group, leading to expressions of respect, which enhance one's motivation to help the group, thereby increasing subsequent con- tributions, and so on.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "STUDY 1: SOCIAL AND MATERIAL BENEFITS FOR CONTRIBUTION", "text": "I conducted a series of laboratory experiments to study the behavior of individuals in settings where group-and self-interest were at odds. In Study 1, I predict that group members will reward a higher contributor to collective action with higher status, greater influence, greater cooperation, and greater gift-giving. In addition, I predict that perceptions of a contributor's 26--AMERICAN SOCIOLOGICAL REVIEW 2 Ethnographic research shows contributions to public goods, such as food production (Lemonnier 1996;Price 2003) and military service (Chagnon 1988;Patton 1996), can determine status standing, as does research in organizational settings ( Flynn et al. 2006). Experimental research from evolutionary biology and economics finds that group members reward generous group members with resources group motivation will mediate the effect of con- tribution on status. In this study, participants first worked in a computer-mediated task (a Public Goods game) that created the structural conditions of the col- lective action problem (group and individual interests were at odds). Afterward, they were paired with a specific other group member, either a high or low contributor from the col- lective action setting. Participants rated their assigned partners on several survey questions designed to measure perceived group motivation and status. Following these ratings, participants and their assigned partners collaborated in a \"contrast-sensitivity\" task, a standard experi- mental setting used for assessing interpersonal influence in research on status characteristics theory (Moore 1965;Troyer 2001;Wagner et al. 1986). Next, participants and their partners were paired in two uniquely structured dyadic eco- nomic exercises reflecting different motives for possible non-cooperation. Finally, participants completed a gift-giving opportunity in which they divided a pool of resources between them- selves and their partners.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "METHODS", "text": "DESIGN AND PARTICIPANTS. The study features a two-condition (partner was a high/low contrib- utor), between-subjects design. Seventy-one undergraduates (44 women, 27 men) 3 at a large, private university participated in the study in return for pay, plus an option of extra credit in a sociology class. 4 Five participants reported either high levels of suspicion regarding the design or not paying attention at all in phase 2 of the study, so I excluded them from the analy- sis. Alternate analyses including suspicious par- ticipants produced qualitatively identical results. Eliminating participants who did not pay atten- tion or were heavily suspicious of the setting is standard in experimental research on influence (e.g., Wagner et al. 1986). PROCEDURE. Participants arrived at a com- puter classroom in groups of six at a time to par- ticipate in a \"Group Interaction Study.\" A research assistant seated participants at separate computer terminals with dividers arranged to prevent communication between participants. I introduced participants to the study via instruc- tions on their computer terminals. They were told that the researchers were interested in processes of group interaction in general and that they would participate in a series of group tasks, each with its own instructions. In actual- ity, each participant engaged with prepro- grammed, fictitious \"group members.\" PHASE 1: THE PUBLIC GOODS GAME. In phase 1 of the study, participants were introduced to a six-person Public Goods game. In the game, they were given an initial endowment ($5) to contribute to a public fund across five rounds. In each of the five rounds, participants simul- taneously decided how much of $1 they would contribute. The public good was then doubled and divided equally between all participants (Yamagishi and Kiyonari 2000). Thus, if every- one contributed their entire endowment to the public good, then everyone would receive $10 in study pay (6 \ud97b\udf59 $5 = $30, 2 = $60, / 6 = $10). But if no one contributed anything, every- one would leave with only their initial $5 endow- ment. Each person was presumably tempted to free-ride on the efforts of others, saving their $5 endowments for themselves, but also receiving their share of everyone else's contributions to the public good. This situation is a social dilemma and meets the mathematical definition of an \"N-Person Prisoner's Dilemma\" (Hardin 1971;Komorita 1976). During these rounds of the Public Goods game, the five simulated group members con- tributed an average of $.49 per round, ranging from a high contributor who gave $.95 on aver- age to a low contributor who gave $.05 on aver- age. These contribution levels are generally consistent with observed contribution levels in past public goods research (Ledyard 1995). At the conclusion of five rounds, the com- puter program displayed the total contributions and earnings of each group member.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "GROUPS REWARD INDIVIDUAL SACRIFICE--27", "text": "Participants were asked to write the information on formatted totals sheets provided at each ter- minal. Following the results, participants were told that in phase 2 of the study they would collaborate with another participant in a two- person group task. They were then \"randomly assigned\" one of the other five group members to be their partner. In fact, participants were randomly assigned either the second highest or the second lowest simulated contributor from phase 1. Before phase 2 started, participants were asked on their computers to indicate how well a series of qualities described their assigned partner using click-and-drag 100-point scales. As measures of perceived status, participants indicated how \"honorable,\" \"prestigious,\" and \"respected\" they perceived their pseudo-partner to be ( Ridgeway and Erickson 2000). To meas- ure perceived group motivation-the extent to which partners were perceived to value the group relative to themselves, and could there- fore be expected to act to benefit the group in the future-I asked participants to rate their partners on a series of individual trait dimen- sions (e.g., \"generous,\" \"selfish,\" and \"cooper- ative\"), as well as scales assessing more group-relevant qualities (how much the partner \"valued others\" in general, \"valued the group,\" and was a \"team player\"). 5 The anchors for all scales range from \"a great deal\" to \"not at all.\" PHASE 2: MEASURING INFLUENCE. In phase 2, participants worked with their assigned partners on a \"contrast-sensitivity task.\" During the task, participants and their fictional partners made initial decisions on which of two checkerboard designs on the screen contained more white area. After submitting their initial answers, par- ticipants were shown their (simulated) partner's choices and given the opportunity to change their choices to agree with those of the partner. The two designs in fact contained approximately equal amounts of white space. The partners were programmed to disagree with the partici- pant's initial choice on 20 of 25 trials. I measured the influence of the programmed partner as the proportion of trials in which par- ticipants switched from their initial decision to that of the partner. Following this task, I asked participants how motivated they were to do well on the task and whether they paid attention to their partner's answers. These are considered preconditions for interpersonal influence in past research using this setting (e.g., Berger et al. 1977;Troyer 2001). PHASE 3: COOPERATION AND GIFT-GIVING. Upon the completion of phase 2, all participants were given a packet of materials that included instruc- tions and three economic exercises. The instruc- tions indicated that their answers (in combination with their partner's answers) would lead to real money payoffs for themselves and their partner. The first exercise, the Greed game (Simpson 2003), features a symmetric payoff structure (see Table 1, panel A). In the game, participants profit from defecting on a partner who is expected to cooperate, but if their part- ner is expected to defect, then participants should be indifferent to the decision as it does not affect their own payoff. Thus, defection by participants in the game reflects a motivation to exploit their partner's cooperation. The Fear of Greed game (Kuwabara 2006) features a more complex, asymmetric payoff structure (see Table 1, panel B). In this game, participants profit from defecting on a partner who they expect will defect on them. If a part- ner is expected to cooperate, however, partici- pants should be indifferent to the cooperation decision because it does not affect their payoffs. At the same time, the partner faces an incentive to defect on the participant if the partner expects the participant to cooperate. Defection by par- ticipants in the game thus reflects fear of exploitation from the partner. I used these two games as measures of cooperation, rather than the classic Prisoner's Dilemma, to separately analyze the two motives for defection-fear of exploitation and greed-that are normally con- founded in the Prisoner's Dilemma (Simpson 2003). Finally, participants played a Dictator game with their partner, wherein they were asked how they would like to divide a pool of money ($3) with their partner. The person assigned to allo- cate money in the Dictator game (the dictator) is free to take the entire pool, so the amount of money allocated to the other person represents a simple, continuous measure of gift-giving. These three economic exercises provide behav- ioral measures of participants' motivation to exploit (the Greed game), trust (Fear of Greed game), and be generous (Dictator game) toward their assigned partner. 6 In sum, the experiment exposes participants to group members who vary greatly in their contributions to the group; measures participants' evaluations of their assigned partners; and assesses how much par- ticipants accept the influence of their partners, cooperate with them, and are generous toward them. These steps test predictions derived from Propositions 1 to 4. 7", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "PERCEPTIONS OF HIGH AND LOW CONTRIBUTORS.", "text": "I created composite measures for group moti- vation and status by averaging the results for each constituent item. Reliability analyses jus- tify the use of these composites as measures of common, underlying factors (Cronbach's alpha = .97 and .90, respectively). Table 2 gives results of t-tests comparing how participants rated their partner's group motivation and status depend- ing on whether the partner was a high or low GROUPS REWARD INDIVIDUAL SACRIFICE--29 Note: Participant chooses between rows, partner chooses between columns. Participant's payoff is given in the lower left of each cell and partner's is given in the upper right. 6 Participants were given no feedback on their partner's decisions after completing the economic exercises. At the completion of each session, I used a funnel debriefing procedure to assess levels of sus- picion regarding deception in the study. I carefully explained the goals of the research and the reasons for not revealing the true nature of the study. Finally, I paid participants and thanked them for their par- ticipation. Note: Standard deviations shown in parentheses. contributor. As the table shows, participants rated high contributing partners to be both more group motivated and higher status than low con- tributing partners. These data show that partic- ipants viewed partners who had contributed more to collective action in phase 1 as more motivated to help the group and as more respect- ed than lower contributing partners. MEDIATION ANALYSIS: THE IMPORTANCE OF PER- CEIVED GROUP MOTIVATION. To investigate whether participants awarded high-contributors status because their high contributions imply group motivation, I conducted a mediation analysis ( Baron and Kenny 1986) (results are given in Figure 2). First, regression analyses show that partner's contribution level predicts participant ratings of the partner's group moti- vation and status. Further, ratings of group moti- vation predict the partner's status. Finally, in a regression analysis including both contribution level and perceived group motivation as inde- pendent variables, only group motivation is sig- nificantly related to perceived status. This indicates that the effect of contribution level on the partner's status operates through per- ceptions of the partner's group motivation. This supports the idea that contributions to the pub- lic good increase group members' status to the extent that they increase perceptions of their group motivation. INFLUENCE. I next analyzed the effects of partner's contribution level on the influence the partner had over the participant during the con- trast-sensitivity task. I calculated the rate of partner's influence as the proportion of trials where participants and their programmed part- ner initially disagreed but participants decided to switch their answers to agree with those of their partner. As Table 2 shows, participants who interacted in the contrast-sensitivity task with a previously high contributor were influ- enced at a significantly higher rate than were participants who interacted with a previously low contributor.   Table 2 shows the results for the three economic exercises. In the Fear of Greed game, participants interacting with partners who were previously high con- tributors did not cooperate at significantly high- er rates than participants interacting with low contributors, although the difference was in the predicted direction. Results of participants' behavior in the Greed game conform to pre- dictions: participants interacting with partners who were high contributors in phase 1 of the study cooperated with them at higher rates than did participants interacting with partners who were previously low contributors. This result shows that participants behaved as though they had less desire to exploit high contributors than low contributors. I also found that participants allocated a sub- stantially higher proportion of the Dictator game pool to high contributing partners than to low contributing ones. This result suggests that par- ticipants felt a greater desire to be generous toward high contributors than toward low con- tributors.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "DISCUSSION", "text": "Results of Study 1 strongly support predictions derived from the theory. Participants perceived the simulated higher contributors as more group motivated and higher status than their lower contributing counterparts. The extent to which participants perceived their partners as group motivated mediates the effect of earlier contri- bution level on status standing. In addition, par- ticipants submitted more to the views of relatively high contributors than to low con- tributors. This is important in part because it suggests the promise of further social and mate- rial benefits for the high contributor. This study also shows the material benefits that accrue for contributions. Participants were more likely to cooperate with high contributors in a Greed game that measured participants' desire to exploit their partners for material gain. This result is especially impressive because the high contributing partners logically could be expect- ed to cooperate at higher rates and were there- fore more vulnerable to exploitation. Participants were also more generous toward previously high contributing partners in a Dictator game. In the Fear of Greed game, how- ever, participants did not cooperate at signifi- cantly higher rates with high contributors. In this game, participant defection indicates fear of exploitation by one's partner. This is somewhat surprising because participants could logically assume that partners who previously contributed at high levels to the public good would be less interested in exploiting them in the ensuing Fear of Greed game.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "STUDY 2: OBSERVERS ALSO REWARD CONTRIBUTION", "text": "Alternative explanations for the results of Study 1 are possible, such as an exchange-theoretic explanation based on direct reciprocity (Bienenstock and Bianchi 2004;Blau 1964;Homans 1974; for a review, see Molm and Cook 1995). It could be that high contributors to col- lective action in Study 1 earned greater status than low contributors, not because group moti- vation is a respected trait, but instead through a reciprocal exchange of resources for improved status. How can we discern an exchange-theoretic account from one based on the idea that giving to a group conveys group motivation, a char- acteristic that is considered meritorious? The two accounts diverge on whether an observer of apparently group motivated behavior would hold a high contributor in high esteem. The exchange-theoretic account argues that people give respect as direct reciprocity for resources received, and therefore would not give respect for contributions they do not benefit from. My theory predicts, however, that to respect indi- viduals for their contributions, people need only have reliable information regarding an individ- ual's group motivation and value the public good. For example, people who do not directly benefit from a charitable donation still respect those who give to charity.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "METHODS", "text": "To empirically test these two explanations against one another, I altered Study 1 in a few important respects. Most significantly, partici- pants no longer actively participate in a collec- tive action setting; instead, they simply observe the other five study participants. To maximize the contrast between high and low contribu- tion, participants in Study 2 were paired with either the highest or lowest contributor from the Public Goods game. Finally, I dropped the contrast-sensitivity task from the follow-up study because it involved much of the deception in the study, and therefore created relatively high rates of suspicion. Apart from these changes, all other aspects remained the same as in Study 1. The study features a two-condition (partner high/low contributor), between-partic- ipants design. Forty-one undergraduates (25 women, 16 men) at a large, private university participated in return for pay, plus an option of extra credit in a sociology class. 9 ", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "RESULTS", "text": "Composite measures for group motivation and status show acceptable levels of reliability (Cronbach's alpha = .98 and .78, respectively). Table 3 gives results of t-tests for all dependent measures in Study 2. Participants rated high contributing partners as significantly more group motivated and higher status than low con- tributing partners. As in Study 1, I investigated whether per- ceived group motivation mediates the relation- ship between contribution levels and perceptions of the partner's status. Figure 3 shows the results of this analysis. As in Study 1, partner's contri- bution level significantly predicts participants' ratings of their partner's group motivation and status. In a regression analysis with perceptions of partner's status as the outcome variable, and partner's contribution level and perceptions of the partner's group motivation as independent variables, only perceived group motivation has a significant effect. These findings suggest that the effects of contribution level on partner's status operated through perceptions of the part- ner's motivation to help the group. In the Fear of Greed game, participants coop- erated at higher rates with high contributors than they did with low contributors. This repli- cates a prior study that found high contributors to public goods were more trusted than low contributors (Barclay 2004). Results of partic- ipants' behavior in the Greed game also confirm predictions. Participants who interacted with high contributors cooperated with them at high- er rates than did participants interacting with low contributors. These results show that partici- pants behaved as though they had less desire to exploit, and less fear of exploitation from, high contributing partners. I also found that participants allocated a sub- stantially higher percentage of the Dictator game pool to high contributing partners than to low contributing partners. This result indicates that participants were more inclined to be generous toward high contributors than toward low con- tributors, which is consistent with past research on indirect reciprocity (Milinski, Semmann, and Krambeck 2002b;Semmann, Krambeck, and Milinski 2005;Wedekind and Milinski 2000).", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "DISCUSSION", "text": "The results of Study 2 support my claim that the status and material benefits earned by high con- tributors in Study 1 are attributable to the respect accorded to individuals who signal high group  Note: Standard deviations shown in parentheses. 9 One participant reported high levels of suspicion in the study and was removed from data analysis. Additionally, survey response results for six partic- ipants were lost due to a computer malfunction. Thus, analyses of all survey responses are limited to 34 par- ticipants, while analyses of behavior on the eco- nomic games reflect all 40 valid participants. motivation, rather than the result of a direct reciprocity process. These results further indi- cate that high contributors to collective action may enjoy social and material benefits even outside the group from others who do not direct- ly benefit from the collective action. This sug- gests broader benefits for contribution to collective action than just those available with- in the group, further highlighting how status may resolve the collective action problem.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "STUDY 3: THE SIGNIFICANCE OF SELF-SACRIFICE", "text": "I argue that contributions to collective action lead to increased status by signaling an indi- vidual's underlying group motivation. Alternative explanations are possible, however. Although the mediation analyses from Studies 1 and 2 support the group motivation account, studies in which a mediator is experimentally manipulated-rather than measured and ana- lyzed statistically-constitute stronger evidence for claims of mediation (Spencer, Zanna, and Fong 2005). Therefore, one purpose of Study 3 is to manipulate how group motivated an indi- vidual's contributions to collective action are (versus how much underlying resources they signal) in order to determine whether the social and material benefits documented in these first two studies are in fact attributable to the sig- naling of group motivation. Additionally, the theory presented here argues that contributions lead to greater perceived group motivation to the extent that they entail group benefit and self-sacrifice. In Studies 1 and 2, however, these two factors were confounded in the experimental manipulations of partner's contribution size. An additional purpose of Study 3, therefore, was to address this gap. To do so, I manipulated how self-sacrificing a con- tribution was while holding constant how group beneficial it was. This allowed me to study the effects of self-sacrifice on resulting status stand- ing independent of group benefit. To address these issues, Study 3 manipulat- ed the proportionality of the partner's contri- bution behavior (see Hardy and van Vugt 2006). Half of the participants were asked to evaluate a partner who contributed a high proportion of his available resources; the other half evaluat- ed a partner who contributed a more moderate proportion. In both cases, the actual amount contributed was held constant. This allowed me to manipulate how self-sacrificing a contribu- tion was, while holding constant how much it benefited the group.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "METHODS", "text": "Study 3 was identical to Study 1 except for a few changes. First, rather than all endowments being set at $5, at the beginning of the Public Goods game participants were told that group members would be randomly assigned different sized endowments, which would be known to all group members. Participants could contribute up to a fifth of their endowments in each of the five rounds of the game. As in Study 1, partic- ipants contributed to the public good through- out phase 1, rather than acting as observers as in Study 2. Participants were always assigned a $6 endowment. For half of the participants, the group member who would become their partner was assigned a $5 endowment; for the other half, their partner was assigned a $9 endowment. The partner always contributed a total of $4.35 to the public good. In phase 2, all experimental participants rated their partner on survey measures of status and group motivation. They then played the three economic exercises with their partner. As in Study 2, to reduce suspicion and the length of study sessions, I again did not use the contrast- sensitivity measure of influence from Study 1. The procedure is otherwise the same as in Study 1, featuring a two-condition (partner was a high versus moderate proportional contributor), between-participants design. Ninety-seven undergraduates (69 women, 28 men) at a large, private university participated in the study in return for pay, plus an option of extra credit in a sociology class.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "RESULTS", "text": "The composite measures for group motivation and status are reliable (Cronbach's alpha = .93 and .88, respectively). Table 4 gives results for all dependent measures in Study 3. Participants rated partners who contributed a high propor- tion of their resources to be significantly more group motivated and higher status than those who contributed a moderate proportion. These findings show that the greater apparent sacrifice made by the high proportional contributors earned them higher status and perceived group motivation. I again conducted a mediation analysis, this time of the relationships between partner's pro- portion contributed and participants' ratings of the partner's group motivation and status. Figure  4 shows that participants granted higher status to partners who contributed a higher proportion of their resources, to the extent that they were seen as more group motivated. Participants cooperated with high propor- tional contributors at higher rates in the Fear of Greed game, but not in the Greed game. High proportional contributors also were not allo- cated more resources in the Dictator game. These findings suggest that participants who interacted with high proportional contributing partners had less fear of exploitation, but they were not less inclined to exploit nor more gen- erous.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "DISCUSSION", "text": "Overall, the findings of Study 3 support the group motivation-signaling account of why par- ticipants granted partners high status for their contributions. Contributions that represented a higher proportion of a group member's available resources were more respected and viewed as indicating greater group motivation because they represented greater individual sacrifices for the group. The results of Study 3 are less pro- nounced than those of Studies 1 and 2, with two of three behavioral measures showing non- significant differences. While this may indicate limitations to the material benefits enjoyed by very generous group members, it may be  because Study 3 manipulated high versus mod- erate proportional giving, as opposed to the prior studies that manipulated contribution level at high versus low levels. It could be argued that the insignificant dif- ferences observed in the Greed and Dictator games are consistent with an exchange-theoretic account because the partner contributed the same absolute amount across the two condi- tions. Study 2, however, offers strong evidence against this account, and the smaller and less sig- nificant mean differences observed in Study 3 are more likely attributable to the smaller con- trast in partner's contribution behavior across conditions, as compared with Studies 1 and 2. The main effects of Study 3, combined with the mediation findings from Studies 1, 2, and 3, all support the theoretical claim that contribution to collective action earns a contributor status because it indicates the contributor's underlying motivation to help the group, a trait that others view as meritorious.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "STUDY 4: GAINING STATUS ENCOURAGES GROUP CONTRIBUTIONS", "text": "Studies 1, 2, and 3 establish part of my status theory of collective action. They show that group members (in these studies, experimental participants) reward other members (here, pro- grammed partners) for displaying group moti- vation by granting them status, deferring to them on tasks, cooperating with them, and being generous toward them. But, I have not yet closed the circle by demonstrating that status rewards for contribution in turn enhance group mem- bers' real motivations to help the group and, as a result, their future willingness to contribute. Status rewards for contribution should also engender positive feelings from the rewarded member toward the group-including identifi- cation, solidarity, and cohesion. Study 4 was designed to test these predic- tions. I changed the design of the previous stud- ies in a few important respects. Most significantly, after participants rated other group members, they received prefigured feedback on how the other group members rated them. I told half the participants that the other members held them in very high esteem, and half that the other members held them in moderate esteem. Participants then went through more rounds of the Public Goods game before answering sev- eral survey questions regarding their group motivation, identification with the group, per- ceptions of group solidarity, and perceptions of group cohesion. Importantly, the key ques- tion of this study is not how the experimental participants perceived their partners, but how they behaved in response to their partners' per- ceptions. Did receiving respect from others increase group motivation and contributions? ", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "METHOD", "text": "Study 4 is identical to Study 1 except for a few changes. First, following phase 1, all participants rated both the second highest and second low- est contributors' group motivation and status. 10 To minimize participant fatigue, I removed two of the group motivation questions and the three economic exercises. After rating the two group members, participants saw the average ratings that two other anonymous participants submit- ted about them so that they could \"know a lit- tle more about the perspective of the other participants.\" In fact these ratings were simu- lated. Participants were shown either high or moderate level ratings for how prestigious (rat- ings of 92 or 47), honorable (90.5 or 45.5), and respected (94.5 or 48.5) they were seen to be by the two other group members. Participants then continued to phase 2, in which they participat- ed in five more rounds of the Public Goods game. Following new rounds of the Public Goods game, participants answered four questions regarding their group motivation (e.g., \"How generous are you?\"), three questions measuring their perceptions of the group's solidarity (e.g., \"How much solidarity do you think the group had?\"), two questions regarding their degree of identification (e.g., \"How much do you identi- fy with the group?\"), and five questions regard- ing group cohesion, borrowed from past research on relational cohesion (Lawler andYoon 1996, 1998). 11 I reduced the group moti- vation composite from six to four items to streamline the survey (see the Appendix for the complete list of questions). The study features a two-condition (partici- pant received high/moderate status feedback), between-subjects design. Eighty-six under- graduates (60 women, 26 men) at a large, pri- vate university participated in return for pay, plus an option of extra credit in a sociology class. I dropped seven participants from data analysis for expressing suspicion regarding either the status feedback or simulated partici- pants and indicating that these suspicions affect- ed their contribution behavior in phase 2.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "RESULTS", "text": "I predicted that participants who were ran- domly assigned high status feedback would contribute more to the group afterward than would those who received moderate status feedback. I first tested this claim by creating a variable \"change in contribution,\" which represents the difference between partici- pants' average contribution before the ratings feedback and their average contributions after the feedback. Table 5 gives the means for change in contribution levels for the two con- ditions of the study. Participants in phase 2 increased their contributions when given high status feedback, but tended to decrease their contributions if given moderate status feed- back. I next conducted a mediation analysis to determine if participants' self-reported group motivation mediates the relationship between status rewards and change in contribution level. That is, do status rewards increase con- tributions by increasing participants' moti- vation to be generous toward the group? A reliability analysis justified making a com- posite for participants' self-reported group motivation (Cronbach's alpha = .88). Figure  5 shows the results of this analysis. First, as above, a regression analysis shows that the more highly participants were rated, the more they increased their contributions. Also, a regression analysis demonstrates that the more highly participants were rated, the more they described themselves as group motivat- ed (e.g., generous and cooperative). This is an important finding, showing that participants' evaluations of their own motivations are shaped by what they think others think of them. Finally, a full regression analysis sup- ports my claim that participants who received more praise increased their contributions 36--AMERICAN SOCIOLOGICAL REVIEW 10 As in Studies 1, 2, and 3, participants rated high contributing group members as significantly more group motivated and higher status than lower con- tributing members. 11 After these surveys, participants completed the PANAS survey of positive and negative affect (Watson, Clark, and Tellegen 1988). I find no sig- nificant effect of the study manipulation on report- ed positive or negative affect. This serves as evidence against the possible alternative explanation that expressions of respect increased contributions by engendering positive affect (e.g., Isen and Levin 1972). because the praise raised their own estimation of their motivation to help the group. 12 VIEWS OF THE GROUP: SOLIDARITY, COHESION, AND IDENTIFICATION. I created composites from the solidarity, cohesion, and identification ques- tions. Each set of questions shows at least acceptable levels of reliability (Cronbach's alpha = .75, .88, and .91, respectively). As shown in Table 5, participants who were given higher level status feedback perceived the group as having greater solidarity than did those who received moderate status feedback. Participants GROUPS REWARD INDIVIDUAL SACRIFICE--37 Note: Standard deviations shown in parentheses. Note: A dotted arrow indicates that a relationship is statistically insignificant (p < .05) in the full model. 12 I did not survey participants' group motivation until after all rounds of the Public Goods game were concluded to avoid creating a demand effect in favor of the behavioral prediction. That is, receiving status could have led individuals to feel greater group moti- vation and then behave consistent with their self- description via a \"commitment and consistency\" dynamic (Cialdini 2001). Placing measurement of the mediator at the end of the study, however, raises the possibility that status feedback somehow affected contribution directly, and only thereafter did partic- ipants perceive themselves as more group motivat- ed. Although the mediation analysis given in Figure  5 is consistent with my theory, the present design can- not completely eliminate this alternate interpreta- tion.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "\ud97b\udf59 \ud97b\udf59", "text": "\ud97b\udf59 given high status feedback also perceived the group as having greater cohesion than did those who received moderate status feedback, although this result only approaches marginal significance. Finally, participants who were given high status feedback also reported iden- tifying with the group more than did those who received moderate status feedback, although this result is marginally significant.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "DISCUSSION", "text": "Results of Study 4 confirm that status rewards for contribution to collective action encourage greater giving in the future. Importantly, the effects of status rewards on participants' con- tribution levels do not significantly interact with participants' prior levels of contribution (p > .90). This suggests that people will give more after receiving esteem from group members, and less if they do not, regardless of how much they gave in the past. We should consider this in light of Studies 1, 2, and 3, however, which show that low contributors are unlikely to receive high status feedback in the first place. The present research suggests that low con- tributors receive low levels of respect from group members, leading them to give less in the future, while high contributors receive high respect from group members, leading them to subsequently give more. If this pattern holds over time, groups may bifurcate into subgroups of contributors and free-riders as high contributors gain higher sta- tus, contribute more as a result, again achieve improved status, and so on. This dynamic would be at least superficially similar to Michels's (1915) \"Iron Law of Oligarchy,\" which describes a tendency for a subgroup of early contributors to assume positions of leadership in the group and take on the majority of contributions and decision making. This theoretical connection deserves further study. One could argue that status does not provide a solution to the collective action problem if this dynamic obtains; that is, if status feedback leads some to contribute more, but others to con- tribute less, then status does not have a uni- formly positive impact on group productivity. However, we should not expect solutions to the collective action problem to satisfy such a cri- terion. We observe neither maximal productiv- ity in groups nor uniformity in contributions across group members. The present research shows that collective action contributions earn individuals status and that this status affects further contribution patterns. Status thus moti- vates at least some observed contributions to collective action. Furthermore, the present research shows how status structures the dynam- ics of who gives, how much they give, and under what conditions. Although I follow the social psychology lit- erature in assuming that groups' status hierar- chies are zero-sum, this assumption deserves further investigation. It could be that groups vary in their general rates of granting respect for contributions, with some groups offering copi- ous respect for generous behavior and others offering less. If this is the case, the present research suggests that groups that grant more respect will engender greater productivity. Indeed, organizational research indicates that workers are more productive when they feel respected in their workplaces, and workplaces that offer greater respect to workers are more generally productive (Tyler and Blader 2000). Future research should further investigate the exact macro-level implications of the dynamics explored here (see, e.g., Kitts 2006). The results of Study 4 highlight the signifi- cance of Studies 1, 2, and 3 by showing that the receipt of status rewards for contributions to the group may be a key factor in sustaining group productivity among rewarded group members. Status rewards also influenced less tangible aspects of the group, such as feelings of solidarity and identification among group members, both of which have important effects on the experience of group membership. Further, status rewards affected contribution behavior by increasing participants' motiva- tions to help the group. This suggests a novel solution to what could be called the paradox of reward-driven generosity. This paradox follows from evidence that prosocial behavior is both seemingly sincere (e.g., anonymous giving, self- reports of altruistic motives, and cases of extreme altruistic sacrifices for non-kin) and responds to reputational incentives (e.g., people give more in public than in private and often tell others about their giving). Study 4 suggests that status rewards may affect prosocial behavior not in a forward-looking, rational calculus, but instead by operating at individuals' backs, rein- forcing past prosocial behavior and encourag-", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "38--AMERICAN SOCIOLOGICAL REVIEW", "text": "ing further giving. 13 In this way, status can encourage generous behavior, not through the prospect of future rewards, but through rein- forcement of past generous behavior. If this learning dynamic is nonconscious, it would help explain how prosocial behavior in humans could be experienced as sincere and selfless, while also being responsive to past status rewards. Study 4 offers support for such an account, although other explanations for the findings of this initial test remain possible. It is worth noting that the extent to which con- tributions to a group lead to greater respect likely depends on the cultural beliefs shared by group members. Specifically, the more group motivation is considered a meritorious trait within a given culture, the more contributions will tend to earn an individual improved status standing among groups within, or influenced by, that culture. For example, it has been speculat- ed that cultures could value individual achieve- ment and self-actualization to such an extent that self-sacrifice for others would be disrespected (Nietzsche [1887] 1989). That said, anthropo- logical research indicates that even highly com- petitive cultures, such as that of the Yanomam\u00f6 (Chagnon 1988), view individuals perceived as making valuable contributions to group efforts as higher status.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "CONCLUSIONS", "text": "Collective action research investigates how peo- ple overcome the temptation to free-ride on the efforts of others, deciding instead to contribute to public goods. Until recently, status was large- ly overlooked as a variable in this research. The theory I present here asserts that individuals who contribute to collective action receive social and material benefits for their efforts, in par- ticular, improved status in the eyes of other group members. The presence of these rewards helps demystify how groups solve collective action problems. Contributions to collective action earn respect from group members by signaling that contributors value the group's well-being over their own. Experimental results show that individuals who behave in more group-motivated ways in collective action set- tings are seen as higher status (Studies 1, 2, 3, and 4), have more influence over other group members (Study 1), are cooperated with more in subsequent interactions (Studies 1, 2, and 3), and receive greater gifts (Studies 1 and 2). Study 2 also shows that contributors to collec- tive action may enjoy social and material ben- efits from individuals outside the group as well. I also found evidence that collective action contributions yield status gains for the reason specified by the theory: they communicate a contributor's group motivation. An exchange- theoretic alternative explanation failed to account for the results of Study 2. Mediation analyses of the results of Studies 1, 2, and 3 sup- port the group motivation-signaling account of why collective action contributions lead to improved status. Most significantly, Study 3 demonstrates the mediating role of group moti- vation via a direct experimental manipulation of the extent of sacrifice shown by a given contri- bution. The theory also proposes that individuals receiving status rewards for collective action contribution will (1) develop more positive views of the group, (2) be more motivated to help the group, and (3) will contribute more as a result. Study 4 tests these claims and shows that status rewards for collective action contri- bution can stimulate individuals' subsequent contributions, identification, and solidarity. These findings underscore the value to a group of rewarding contributions and also support a solution to the paradox of reward-driven gen- erosity. Taken together, status rewards accrue to those who appear group-motivated in their behaviors, and group motivation in turn is enhanced by status rewards, generating more contributions in the future. This research points to a possible drawback of formal incentive systems designed to encour- age contributions to public goods. Formal incen- tive systems likely interfere with the informal incentives of status rewards because contribu- tions under threat of formal sanction (e.g., pay- ing taxes or obeying the law) do not necessarily reflect concern for the group. Therefore, they do not earn an individual status. Although effective at maximizing group contributions, formal GROUPS REWARD INDIVIDUAL SACRIFICE--39 13 Other theoretical accounts of how prosocial behavior could be experienced as sincere while also responsive to past reputational rewards emphasize evolution as the mechanism, that is, reputational gains contribute to the reproductive fitness of humans who behave prosocially (e.g., Frank 1987Frank , 1988. incentives may undermine a group's natural ten- dency to produce solidarity and identification among contributing members via status rewards. Indeed, recent research shows that formal sanc- tions can undermine trust and altruism between group members (Fehr and Rockenbach 2003;Mulder et al. 2006). The theory also suggests that the character of a group's status hierarchy may encourage or impede contributions from its members (see Kitts 2006). Groups with greater status mobil- ity (e.g., newly formed groups without set sta- tus hierarchies) might be more likely to offer substantial status rewards that promote future giving. For example, a union drive might be most effective at maximizing contributions of organizing time from activists when the group is new and the status hierarchy is still flexible, rather than later when relative position in the group is more determined. Organizations and other groups with rigid status hierarchies may struggle to promote maximal contributions from their members unless they provide some alter- native mechanism for intragroup status mobil- ity. Groups with highly unequal status hierarchies may also face impediments to group productivity, as large gaps between status posi- tions may discourage group members from con- tributing. The collective action problem is concerned with how social movements and other groups are organized and sustained, as well as the more fun- damental, Hobbesian question, why society? Why do individuals come together in groups to pursue collective goals, rather than pursuing purely selfish ones? Why are there groups, soci- eties, and cultures, rather than just a population of disconnected egoists? The status solution fits the collective action problem well in a few ways. The status incen- tive system does not require central, formal organization. The incentives rest in the regard members have for one another. This means that the system is not easily destroyed or under- mined and does not require any explicit man- agement or leadership to be maintained. Also, status incentives, unlike material ones, increase as the collective action becomes more difficult. As tasks require greater sacrifice from group members, contributing indicates even greater concern for the group. Lest readers take these results to imply a sim- ple quid-pro-quo between group and member that answers the collective action problem, I would underscore the importance of both per- ceived and actual group motivation in the inter- play of individual contributions and relative standing in a group's status hierarchy. These are critically social phenomena wherein group members carefully evaluate each other for signs of devotion to the group, far more than a sim- ple exchange. The collective action problem has engaged scholars for centuries before and after Hobbes. The answer to this question promises insights on the evolution of government and social insti- tutions, as well as the movements that might be devised to reform or even overthrow them. Hobbes's Leviathan, the collective force we wield in the creation and maintenance of social order, is not an explicit contract. Perhaps, though, there is an implicit social contract embedded in our minds. That social contract, our willingness to behave in basically prosocial ways and make sacrifices for the group's wel- fare, may stem fundamentally from our con- cern for what others think of us. Sartre ( [1943] 1955) famously observed that \"l'enfer, c'est les autres\" (\"hell is other peo- ple\"), partly because our reliance on what oth- ers think of us leaves us dependent on them for our own happiness. Because of this concern for others' esteem, we are privately tyrannized by others' opinions, even outside their presence, as even our views of ourselves are reflected to us through the eyes of many (Cooley 1902). I sug- gest here, however, that our concern for others' opinions of us is, in the end, responsible for much of our happiness because of the possibil- ities it grants: the opportunity for society, cama- raderie, and the production of public goods. Although we are pursued by our consideration of how others view us, at the end of the pursuit we find ourselves in groups, with the potential for solidarity with others.", "title": "24-AMERICAN SOCIOLOGICAL REVIEW", "file_name": "Willer - 2009 - Groups Reward Individual Sacrifice The Status Sol.pdf"}
{"section": "Abstract", "text": "We attempt to replicate 67 papers published in 13 well-regarded economics journals using author-provided replication files that include both data and code. Some journals in our sample require data and code replication files, and other journals do not require such files. Aside from 6 papers that use confidential data, we obtain data and code replication files for 29 of 35 papers (83%) that are required to provide such files as a condition of publication, compared to 11 of 26 papers (42%) that are not required to provide data and code replication files. We successfully replicate the key qualitative result of 22 of 67 papers (33%) without contacting the authors. Excluding the 6 papers that use confidential data and the 2 papers that use software we do not possess, we replicate 29 of 59 papers (49%) with assistance from the authors. Because we are able to replicate less than half of the papers in our sample even with help from the authors, we assert that economics research is usually not replicable. We conclude with recommendations on improving replication of economics research.", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Board of Governors of the Federal Reserve System et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Introduction", "text": "In response to McCullough and Vinod (2003)'s failed replication attempt of several articles in the American Economic Review (AER), then-editor of the AER Ben Bernanke strengthened the AER's data and code availability policy to allow for successful replication of published results by requiring authors to submit to the AER data and code replication files (Bernanke, 2004). Since the AER strengthened its policy, many of the other top journals in economics, such as Econometrica and the Journal of Political Economy, also started requiring data and code replication files. There are two main goals of these replication files: (1) to bring economics more in line with the natural sciences by embracing the scientific method's power to verify published results, and (2) to help improve and extend existing research, which presumes the original research is replicable. These benefits are illustrated by the policy-relevant debates between Krueger (1994, 2000) and Neumark and Wascher (2000) on minimum wages and employment; Hoxby (2000Hoxby ( , 2007 and Rothstein (2007) on school choice; Levitt (1997Levitt ( , 2002 and McCrary (2002) on the causal impact of police on crime; and, more recently, Reinhart and Rogoff (2010) and Herndon, Ash, and Pollin (2014) on fiscal austerity. In extreme cases, replication can also facilitate the discovery of scientific fraud, as in the case of Broockman, Kalla, and Aronow (2015)'s investigation of the retracted article by LaCour and Green (2014). This article is a cross-journal, broad analysis of the state of replication in economics. 1 We attempt to replicate articles using author-provided data and code files from 67 papers published in 13 well-regarded general interest and macroeconomics journals from July 2008 to October 2013. This sampling frame is designed to be more comprehensive across well- regarded economics journals than used by existing research. Previous work has tended to focus on a single journal, such as McCullough, McGeary, and Harrison (2006), who look at the Journal of Money, Credit and Banking (JMCB); McCullough and Vinod (2003), who attempt to replicate a single issue of the AER (but end up replicating only Shachar and Nalebuff (1999) with multiple software packages); or Glandon (2010), who replicates a selected sample of nine papers only from the AER. Using the author-provided data and code replication files, we are able to replicate 22 of 67 papers (33%) independently of the authors by following the instructions in the author- provided readme files. The most common reason we are unable to replicate the remaining 45 papers is that the authors do not provide data and code replication files. We find that some authors do not provide data and code replication files even when their article is published in a journal with a policy that requires submission of such files as a condition of publication, indicating that editorial offices do not strictly enforce these policies, although provision of replication files is more common at journals that have such a policy than at journals that do not. Excluding 6 papers that rely on confidential data for all of their results and 2 papers that provide code written for software we do not possess, we successfully replicate 29 of 59 papers (49%) with help from the authors. Because we successfully replicate less than half of the papers in our sample even with assistance from the authors, we conclude that economics research is usually not replicable. 2 Despite our finding that economics research is usually not replicable, our replication success rates are still notably higher than those reported by existing studies of replication in economics. McCullough, McGeary, and Harrison (2006) find a replication success rate for articles published in the JMCB of 14 of 186 papers (8%), conditioned on the replicators' access to appropriate software, the original article's use of non-proprietary data, and without assistance from the original article's authors. Adding a requirement that the JMCB archive contain data and code replication files the paper increases their success rate to 14 of 62 papers (23%). Our comparable success rates are 22 of 59 papers (37%), conditioned on our having appropriate software and non-proprietary data, and 22 of 38 papers (58%) when we impose the additional requirement of having data and code files. Dewald, Thursby, and Anderson (1986) successfully replicate 7 of 54 papers (13%) from the JMCB, conditioned on the replicators having data and code files, the original article's use of non-confidential data, help from the original article's authors, and appropriate software. Our comparable figure is 29 of 38 papers (76%).", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Board of Governors of the Federal Reserve System et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Methodology and Sampling Frame", "text": "Our sampling frame includes papers from 13 well-regarded macroeconomics and general interest economics journals: American Economic Journal: Economic Policy, American Eco- Quarterly Journal of Economics. We choose papers from these journals because of the rela- tive likelihood that such papers will have a policy effect and also influence future research. 3 We do not select these journals to single out a particular author, methodology, institution, or ideology. From our sample of journals, we browse for original research articles published in issues from July 2008 to October 2013. 4,5 Within these issues, we identify all papers with the following three characteristics: (1) an empirical component, (2) model estimation with only US data, and (3) a key empirical result produced by inclusion of US gross domestic product (GDP), published by the Bureau of Economic Analysis (BEA), in an estimated model. 6 We choose to focus on GDP because of its status as a standard macroeconomic statistic and its widespread use in research. 7 For each paper in this set, we attempt to replicate the key empirical results. 8 We focus on the key empirical results for two reasons: (1) replicating only the key results allows us to expand the sample to more papers, and (2) the key result of the paper is presumably what drove the paper's publication; robustness checks merely serve as confirming evidence. Defining a key result is subjective and requires judgmental decisions on our part. We attribute a key result of the paper to GDP when the authors themselves refer to GDP as driving a key result, or when a discussion of GDP is featured either in the abstract or prominently in the introduction of their work (or both). We also take key results as those that appear in figures and tables. 9 We find 67 papers that fit these criteria. Of these papers, 6 papers use proprietary data for all of the key results, so we do not include them in our replication exercise (Fisher and Peters, 2010;Alexopoulos, 2011;Alexopoulos and Cohen, 2011;Hall and Sargent, 2011;Bansak, Graham, and Zebedee, 2012;Gilchrist and Zakraj\u0161ek, 2012). If a subset of the key results could be obtained using non-proprietary data, then we attempt to replicate those results. For the remaining papers that use public data and are published in journals that maintain  (2006), we find that journal data and code archives are incomplete. Of the 35 papers that use public data and are published in journals that require data and code replication files, we obtain files for 28 papers (80%) from journal archives. For papers where we are unable to obtain replication data and code files from journal archive sites, either because the mandatory files are is missing or because the paper is not subject to a data availability policy, we check the personal websites of each of the authors for replication files. If we are unable to locate replication files online, then we email each of the authors individually requesting the replication files. 10 Of the 7 papers that use public data, are subject to a data and code policy, and do not have replication files on the journal's archive site, this procedure nets us one additional set of replication files. Therefore, we are unable to locate replication files for 6 of 35 papers (17%) that are published in journals that require submission of data and code replication files. For papers published in journals without a data and code availability policy and that use public data, we are unable to obtain data and code replication files for 15 of 26 papers (58%). We do not single out any paper or author that fails to comply with a journal's mandatory data and code policy. We therefore only report these summary statistics of compliance with data availability policies and only cite papers that we either successfully replicate, that use proprietary data, or where we have what appears to be a complete set of replication files in a software we do not possess. Our intention is to highlight the general state of replication files for published economics research, not to berate any given author, methodology, institution, or ideology. To determine whether a paper was subject to a data availability policy, we check the implementation dates of the journal data policies and compare them to the publication and submission dates of the published work. If the journal's website does not allow us to extract this information, then we query the editorial office as to when their data availability policy became effective. We do not ask the editorial offices whether a particular paper was subject to a data availability policy. Aside from papers with proprietary data, we find that journal data archives do not provide lists of potentially exempt papers. Therefore, we are unable to determine whether a paper is exempt for a reason other than using proprietary data, although we are not aware of reasons why journals would grant a paper a data and code exemption other than for proprietary data. The authors we query whose papers we believe are subject to a data availability policy yet whose replication files we are unable to locate do not volunteer whether their papers are exempt from the policy, and we do not ask the authors for this information. For the papers for which we are able to obtain data and code replication files, we attempt to replicate the key results of the paper using only the instructions provided in the author readme files. If the readme files are insufficient or if the replication files are incomplete (or both) and the paper is subject to a replication policy, then we email the corresponding author (if no corresponding author, then the first author) for either clarification or to request the missing files. If we do not receive a response within a week, then we query the second author, and so on, until all authors on the paper had been contacted. 11 We define a successful replication as when the authors or journal provide data and code files that allow us to qualitatively reproduce the key results of the paper. For example, if the paper estimates a fiscal multiplier for GDP of 2.0, then any multiplier greater than 1.0 would produce the same qualitative result (i.e., there is a positive multiplier effect and 11 If we already contacted the authors to request data or code but were having difficulty executing the code, then we only queried the authors whom we did not yet contact. We initiate contact with each author a maximum of one time. that government spending is not merely a transfer or crowding out private investment). 12 We define success using this extremely loose definition to get an upper bound on what the replication success rate could potentially be. 13 We allow for minimal re-working of the provided files, following the procedure of McCullough, McGeary, and Harrison (2006). 14 One dimension where we are unable to follow the authors exactly is the software ver- sion they use. To execute the replications, we make use of the following software version- available in the readme, we attempt to run the software version-operating system combina- tion specified by the authors. When the replication files fail to execute on a given software version-operating system combination, the author readme did not specify a particular soft- ware version-operating system combination, and it appeared that the data and code were complete, we email the authors to find out which combination they use.", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Board of Governors of the Federal Reserve System et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Results", "text": "This section presents summary statistics of our replication attempts. 16 12 This definition corresponds to a rating of three out of five, \"minor discrepancies,\" or better by Glandon (2010), and \"partially successful replication\" or better by McCullough, McGeary, and Harrison (2006). 13 This definition is less stringent than the definition for replication success of Harrison (2006, 2008) 14 McCullough, McGeary, and Harrison (2008), in their appendix, suggest that \"the author [whose study is being replicated] provides code such that data and code, when placed in the same subdirectory, will execute; and that the output from doing this also will be provided... and produces the results in his paper,\" which implies that replication files should contain the data and code that requires no re-working. If the code is clearly missing the ability to replicate results, then we do not attempt to re-code the procedures ourselves. 15 We check the replication results of a small sample of selected papers across different versions of Matlab for Windows and find very minor differences. None of the differences in results across different versions of Matlab are qualitatively significant. The web appendix lists the software version-operating system combination we use for each successfully replicated paper. 16 Interested readers can find detailed results for each paper we successfully replicate in the web appendix on Chang's website, https://sites.google.com/site/andrewchristopherchang/research.  Table 2 shows that our overall replication success rate is 29 of 67 papers (43%). Table 2, Panel B shows that we successfully replicate 23 of 39 papers (59%) from journals that require data and code replication files. This rate compares to 6 of 28 (21%) of the papers from journals that do not require such files, shown in Table 2, Panel C. These replication rates are similar when we only consider papers with publicly available data: we successfully replicate 23 of 35 (66%) of the papers from journals with mandatory data and code policies and 6 of 26 (23%) of the papers from journals without such policies. The presence of a mandatory data and code policy does not necessarily imply a causal relationship from the policy to successful replication. Authors select which journals to submit papers to, taking into account idiosyncratic journal policies such as mandatory submission of replication data and code. However, we find that it is significantly easier to replicate published research that comes from journals that require authors to submit their data and code. Table 3, Panel A provides explanations for why we are unable to replicate papers ac- cording to four broad classifications: \"missing public data or code,\" \"incorrect public data or code,\" \"missing software,\" or \"proprietary data.\" Panel B provides the breakdown for jour- nals that require data and code. Panel C shows the results for journals that do not require data and code. From Table 3, Panel A we find that we are unable to replicate 21 papers because of \"missing data or code,\" which constitutes the majority of our failed replications (55%). As we outline in our methodology, for each of these unsuccessful replications we attempt to secure data and code from the authors by visiting their personal websites, visiting the journal websites (when the journal requires authors to submit data or code), and sending email requests. We classify an unsuccessful replication as \"missing data or code\" when at least one of two events occur: (1) the replication code file(s) are clearly missing necessary author-written functions for a subset or all of the key results or (2) the replication data file(s) are missing at least one variable. If the replication data has a shorter data sample than reported in the paper, then we still attempt the estimation and do not necessarily classify the paper as \"missing data or code.\" We are unable to replicate 9 papers (24% of failed replications) because of \"incorrect data or code.\" We classify an unsuccessful replication as \"incorrect data or code\" when all variables are present in the dataset and the authors self-identify code for each of the key figures and tables we attempt to replicate. The author-provided code may finish executing and give different results or the code may not finish executing and still fall into this category. We believe we do not have the needed software to run two papers (Senyuz, 2011;Jermann and Quadrini, 2012) because we are unable to locate a necessary packaged function in our versions of the appropriate software, because of significant syntax changes between software versions, or because the authors declared that they use a particular software version and we are aware that our software would not be compatible. However, it is tricky differentiating be- tween an unsuccessful replication due to \"incorrect data or code\" or due to \"missing software.\" Because the implementation of packaged functions may differ across software versions even without syntax changes, we believe the number of failed replications we classify as \"missing software\" is a lower bound. It is possible that a paper we classify as \"incorrect data or code\" is actually replicable with the appropriate operating system-software combination, so some of the papers that we classify as \"incorrect data or code\" may belong in the \"missing software\" category. However, we cannot verify this statement without additional documentation. Table 4 shows our summary statistics for successful replications independent of the au- thors versus replications that were successful with the author's help. Overall, we find that contacting the authors marginally improves our success rate for replication. Of the 29 suc- cessful replications, we complete 22 without any help from the authors.", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Board of Governors of the Federal Reserve System et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Conclusion and Recommendations", "text": "In this article, we attempt to replicate 67 papers from 13 well-regarded economics journals using author-provided data and code replication files. Improving on existing work evaluating the state of replication in economics, our sampling frame is broader across different journals and covers a large number of original research articles. We replicate 22 of 67 papers (33%) by using only the authors' data and code files, and an additional 7 papers (for a total of 29 papers, 43%) with assistance from the authors. The most common cause of our inability to replicate findings is that authors do not provide files to the journal replication archives, which constitutes approximately half of our failed replication attempts (21 of 38 papers, 55%). Because we are able to replicate less than half of the papers in our sample, we conclude that economics research is generally not replicable. We now turn to some recommendations that we feel would improve the ability for re- searchers to replicate and extend published articles, largely echoing the recommendations of McCullough, McGeary, and Harrison (2006). \u2022 Mandatory data and code files should be a condition of publication. Our replication success rate is significantly higher when we attempt to replicate papers from journals that have a mandatory replication data and code submission policy. We believe that replication files need to encompass both data and code. As shown in Table 2, the data-only archives at Economic Journal and Journal of Applied Econometrics only allow for replication of 4 of 20 papers (20%) that use non-confidential data, compared to the replication success rate of 23 of 35 papers (66%) that use non-confidential data from journals that require both data and code. \u2022 An entry in the journal's data and code archive should indicate whether a paper without replication files in the journal's archive is exempt from the journal's replication policy. Among papers that we believe were subject to a mandatory data and code policy, we are unable to acquire replication files for 6 of 35 papers (15%) even after emailing, and often receiving a response from, the authors. However, we are unsure whether these six papers are exempt from their respective journal's mandatory data and code policies, and the authors did not volunteer whether their papers are exempt in response to our requests for replication files. Therefore, we suggest that journals include an exemption entry in their replication archives. This note in the replication archives would have four virtues: (1) it is low-cost for the journal, (2) it would save authors who are exempt from submitting replication files from needing to respond to queries about replication files, (3) it would save would-be-replicators from searching for replication files for papers that are exempt from the journal's policy, and (4) it would identify those authors who are not compliant with the journal's mandatory data and code policy. \u2022 Readme files should indicate the operating system-software version combination used in the analysis. We attempt to use the operating system-software version combination reported by the au- thors in their readme files, but we notice that very few readmes include the operating system- software version combination used to conduct their analysis. When we ask authors about the operating system or software version they use to run their models, most authors do not recall this information. Although it is not a focus of our paper, we notice minor discrepancies for a selected subset of papers when running programs on different versions of Matlab (although the discrepancies are not large enough to change the key qualitative results). \u2022 Readme files should contain an expected model estimation time. Many macroeconomic models are estimated with Bayesian (i.e., Markov Chain Monte Carlo) methods, which can take a considerable amount of processing time to execute even under the best of circumstances. We encountered a few instances where we believed an estimation was executing, only to find out weeks later that the programs were stuck in an infinite loop and were supposed to run in much less time. In addition, frequently programs are not written to optimize computation time and also frequently written without a progress bar, so there is no way to track the expected completion time of estimation. A low-cost alternative to a progress bar is simply writing the expected estimation time in the readme file. \u2022 Code that relies on random number generators should set seeds and specify the random number generator. Optimization algorithms often rely on a set of initial conditions, which are commonly speci- fied through a random number generator. For any research that relies on a random number generator, replication requires the same set of numbers that are generated in the published article. \u2022 Readme files should clearly delineate which files should be executed in what order to produce desired results. Occasionally, we are presented with replication data and code that requires files to be exe- cuted in a particular order to furnish published results. In cases where the execution order is critical but unspecified, we spend a considerable amount of time attempting to determine the proper order of execution and, in some cases, ultimately fail to do so. We now turn to two recommendations that will improve the ability of researchers to extend published work, in addition to merely replicating it. \u2022 Authors should provide raw data in addition to transformed series. While only the transformed data are needed to conduct replication of published results, raw data facilitate potential extensions of research. For example, raw data allow for the investigation of the effect that revisions to macroeconomic data have on previously published research, as in Croushore and Stark (2003) and Chang and Li (2015). \u2022 Programs that replicate estimation results should carry out the estimation. We notice that the replication files for a few papers run smoothly and exactly furnish the results of the tables and figures as published. However, oftentimes the results in tables and figures depend on a model's parameters being estimated. Some of these replication files, instead of estimating the models, take the relevant parameters as given to produce results in tables and figures. For verification of published results, and particularly for purposes of extending research, we assert that code that actually estimates the relevant models would be far more useful.  Barro and Redlick (2011) Baumeister and Peersman (2013) Canova and Gambetti (2010) Carey and Shore (2013) Chen, Curdia, and Ferrero (2012) Clark and McCracken (2010) Corsetti, Meier, and M\u00fcller (2012  Journal of Applied Econometrics requires data only. Economic Journal currently requires data and code, but the papers in our sample were not subject to a data and code policy according to the Economic Journal's editorial office.  ", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Board of Governors of the Federal Reserve System et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Abstract", "text": "aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.", "title": "Comment on \"Estimating the reproducibility of psychological science\"", "file_name": "Gilbert et al. - 2016 - Comment on Estimating the reproducibility of psyc.pdf"}
{"section": "PERMISSIONS", "text": "http://www.sciencemag.org/help/reprints-and-permissions Terms of Service Use of this article is subject to the is a registered trademark of AAAS. ", "title": "Comment on \"Estimating the reproducibility of psychological science\"", "file_name": "Gilbert et al. - 2016 - Comment on Estimating the reproducibility of psyc.pdf"}
{"section": "Abstract", "text": "Many political observers view get-out-the-vote (GOTV) mobilization drives as a way to increase turnout among chronic nonvoters. However, such a strategy assumes that GOTV efforts are effective at increasing turnout in this population, and the extant research offers contradictory evidence regarding the empirical validity of this assumption. We propose a model where only those citizens whose propensity to vote is near the indifference threshold are mobilized to vote and the threshold is determined by the general interest in the election. Our three-parameter model reconciles prior inconsistent empirical results and argues that low-propensity voters can be effectively mobilized only in high-turnout elections. The model is tested on 11 randomized face-to-face voter mobilization field experiments in which we specifically analyze whether subjects' baseline propensity to vote conditions the effectiveness of door-to-door GOTV canvassing. The evidence is consistent with the model and suggests that face-to-face mobilization is better at stimulating turnout among low-propensity voters in prominent elections than it is in quiescent ones.", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "A (Seemingly) Contradictory Literature", "text": "There are an infinite number of possible ways in which voting propensity may condition the relationship between GOTV contact and the decision to vote, but the four most prominent possibilities are exposited in Figure 1. Each of the panels in Figure 1 displays three lines. The upper black line is the hypothetical voting rate (y-axis) in the treat- ment group across levels of voting propensity (x-axis), and the lower gray line is the hypothetical voting rate in the control group. Note that the S-shaped functional form that characterizes the change in voting rates accounts for the fact that the values of the x-and y-axis are bounded by 0 and 1. The black dotted line is the difference between the voting rates between the treatment and control group, also known as the \"treatment effect.\" In sum, GOTV may be equally effective across all types of voters (Panel A), most effective among low-propensity voters (Panel B), among high-propensity voters (Panel C), or among those who fall in the middle of the voting propensity distribu- tion (Panel D). Extant research finds little evidence for the homoge- nous treatment effect hypothesis, but fails to agree on which of the heterogeneous treatment effects (i.e., Pan- els B-D) correctly characterizes the relationship between mobilization and voting propensity. Green and Gerber (2004) find that canvassing consistently boosts turnout among individuals who voted in the previous midterm election by twice as much as those who did not. While the experimental design utilized by Green and Gerber ensures campaign outreach is uncorrelated with turnout likeli- hood, their dichotomous measure of voting propensity is far too coarse to definitively support the high-propensity treatment effect model. At the very least, the dichoto- mous propensity measure only facilitates the estimation of linear relationships making it impossible to detect a curvilinear treatment effect. Hillygus (2005) uses panel survey data to analyze changes in a person's vote intention over the course of the 2000 U.S. presidential campaign, finding some evi- dence for the low-propensity treatment effect. She explains the result by pointing out that individuals who plan on voting cannot be mobilized by campaigns. However, like Green and Gerber (2004), she relies upon a dichotomous measure of voting propensity, thereby excluding a curvi- linear relationship. In contrast, Niven (2001,2004) di- rectly tests and finds support for the curvilinear treatment effect hypothesis. Niven argues that politically disengaged individuals will quickly forget campaign messages, while those who regularly vote will not require any persuasion to turn out. He reasons that mobilization can only work on people who lie between the two extremes (2001,338). Niven provides evidence for his claim by comparing the voter turnout of residents targeted by local campaign to the turnout of individuals not targeted across three dif- ferent subpopulations: (1) \"consistent voters,\" who voted the past three elections, (2) \"intermittent voters,\" who voted in some but not all of the past three elections, and ", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Control", "text": "Treatment Treatment Effect (3) \"seldom voters,\" who did not vote in any of the past three elections. In both studies, Niven finds that door-to- door canvassing is more effective at getting out the vote of those in the intermittent category.", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "A Theory of Contingent Mobilization", "text": "While it is tempting to view these varied findings as com- peting claims, we offer a single theoretical framework that anticipates all three heterogeneous treatment effects. In short, we argue that the type of voter for whom mobiliza- tion is effective is contingent on the electoral context. In a low-salience election, where few people in the electorate are either aware of or interested in the campaign, only high-propensity voters will be receptive to canvassers' blandishments to vote. Conversely, we expect the opposite in a high-salience election. When most people are aware of and interested in an upcoming election, it is mostly those at the low end of the voting propensity spectrum who have not committed to voting. Yet because campaign coverage is intense, even these people have some interest in the election outcome, making them more receptive to entreats to vote than they are in less salient elections. This leaves us with elections of middling interest, such as the typical high-status local election (e.g., mayor) or con- gressional race. Local news outlets devote some attention to the race, and it is likely that many people are at least aware of the upcoming election and have some interest in the outcome. High-propensity voters are aware and plan to vote, while low-propensity voters are unlikely to be swayed to show up at the polls. As a result, GOTV efforts are more likely to mobilize those who fall in the middle of the voting propensity spectrum in these races. To state our expectations formally, let V i denote whether an individual votes in the current election, P i is an individual's latent propensity to vote, M is the effect of any mobilization conducted by the campaign, G is the general interest among the electorate in the election, and V * is a latent variable that reflects an individual's deci- sion to vote in a particular election. Note that P i and V * are distinct but related constructs. One's propensity to vote is an enduring individual-level trait, while the deci- sion to vote is an episodic choice subject to short-term forces. Also note that P i , M, and G are exogenous to V i and V * (we will say more about this assumption below). We model an individual's decision to vote as a function of his or her underlying propensity and the effect of any GOTV efforts. An individual will vote in the upcoming election, V i , if V * surpasses a threshold of interest, which is dictated by G. Figure 2 graphically displays the expectations we derive from equation (1). Voter mobilization should be most clearly observed for those individuals whose propensity to vote places them near the threshold where they are indifferent to voting. The threshold is lower for elections with a great deal of general interest, such as presidential elections, and much higher for elections of little interest, such as school board races. Equation (1) predicts that campaigns should best be able to in- crease turnout among (a) low-propensity voters during tightly contested, high-profile elections (as depicted in Figure 1B); (b) high-propensity voters during uncompeti- tive, low-interest elections ( Figure 1C); and (c) moderate- propensity voters in elections of middling interest to the general public ( Figure 1D). 1 Thus, the theoretical model predicts that the relation- ship between voting propensity and mobilization should be an inverted-U shape in which the location of the peak of the curve depends on the value of G. In low-salience elections, the peak will be located at the higher end of the voting propensity scale; in medium-salience elections, it will be located near the middle of the propensity scale; and in high-salience elections, it will be located near the 1 Strictly interpreted, equation (1) predicts voter mobilization within a very narrow propensity range. However, there are two pri- mary reasons for expecting a parabolic mobilization effect over a broader range of voters. First and foremost, there will be individual- level idiosyncratic causes of voting that may have the effect of raising or lowering a person's likelihood of voting in the election. The equation and statistical analysis captures average trends rather than individual-level decisions. These individual-level causes will be captured in the estimator's error term. Second, voting propensity will always be measured with uncertainty. The particular point es- timate will be accurate within a given range, so mobilization might be observed over a wider range of individuals. V * = P + M and V = \ud97b\udf59 0 if V * \u2264 \u2212G 1 if V * > \u2212G(1)", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "FIGURE 2 Theoretical Model Predictions", "text": "Note: The y-axis, V * , is the latent likelihood of voting in a par- ticular election. The x-axis, Propensity, is a person's enduring baseline propensity for voting. M is the amount voter mobi- lization increases a person's propensity to vote in an election. G is a cut-point. If a person's propensity to vote is greater than G, then the person votes. The position of G along the y-axis is based upon the salience of the particular election. P 1 and P 2 represent the points at which mobilization can be effective by boosting the propensity to vote over the threshold for voting in two hypothetical elections.", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "FIGURE 3 Relationship between Propensity to Vote and GOTV Mobilization Predicted by Context-Dependent Theory of Mobilization", "text": "Propensity to Vote", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Treatment Effect", "text": "High Salience Medium Salience Low Salience lower end of the scale (see Figure 3). As discussed below, the same predictions are derived if we relax the parsimony of the model a bit by adding an error term to account for the probabilistic nature of human decision making (see equations 3 through 5 below). The model supplies an intuitive interpretation of how mobilization works. By personally encouraging people to vote, the campaign hopes to induce poll-avoidant sup- porters to behave like high-propensity voters. Yet cam- paigns will not have equal success attracting all low- propensity voters to the polls. Our model anticipates that campaigns successfully achieve this goal only among sup- porters whose voting propensity is near the threshold set by G. Assuming G = 0.50, then, a campaign that can expect to increase, on average, the probability an individ- ual votes by 7 percentage points (i.e., M = 0.07) will only have a decisive impact on those who have a 43-50% prob- ability of voting in the election without GOTV contact. 2 We believe our simple model offers a number of con- tributions beyond previous research. First, our model of mobilization builds upon considerable empirical ev- idence that the decision to vote is highly contingent on the electoral context. When the race is close and many people care about the outcome, more people decide to vote relative to races in which general interest is low (e.g., Cox and Munger 1989;Rosenstone and Hansen 1993). Our contribution is to note that the type of voter for whom mobilization is effective must also be contingent on electoral context. As general interest in the campaign increases, mobilization is more likely to reach inveterate nonvoters. Second, our model is highly consistent with how campaigns actually conceptualize voting behavior when crafting GOTV strategy. Campaigns attempt to mobilize individuals whom they believe will be most receptive to their appeals to vote. They do so by using government voting records to identify individuals who have voted in previous elections. Consequently, campaigns behave as if P i is exogenous to V * . Furthermore, a campaign's pri- mary means of boosting turnout is via mobilization, M. 3 Finally, the types of voters targeted by campaigns are af- fected by G. In high-salience elections, campaigns target unlikely voters out of the belief that everyone else is going to vote without their encouragement, whereas in low- salience elections they assume the opposite and focus on those voters who have reliably voted in the past (Malchow 2003). In short, our model, as stated in equation (1) and depicted in Figure 2, offers clear predictions about how mobilization interacts with individual voting propensi- ties and electoral context to affect turnout. Not only is our model parsimonious and intuitive, but it also ac- counts for empirical findings that heretofore have been considered inconsistent and contradictory. Moreover, as we will discuss in the next section, it is possible to mea- sure these parameters in a straightforward and objective fashion, surmounting a major obstacle that has beset at- tempts to test empirically previous formal models of voter turnout.", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Method", "text": "The theoretical model presented in equation (1) read- ily lends itself to an empirical model. By adding G to both sides of the equation and substituting, the mobiliza- tion model can be transformed so as to make the cutoff tractable. V * = P + M + G and V = \ud97b\udf59 0 if V * \u2264 0 1 if V * > 0 (2) When an individual's propensity to vote, P, exposure to mobilization activity, M, and the general salience of the election, G, push the underlying likelihood of voting, V * , above zero, then the person will vote. Otherwise, the in- dividual will abstain. The mobilization model now par- allels commonly used statistical models for dichotomous dependent variables. The parameterization of the models follows in a straightforward manner and is presented in equation (3): V * i = \ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 2 M i + \ud97b\udf59 3 G E + \u03b5 i and V i = \ud97b\udf59 0 if V * i \u2264 0 1 if V * i > 0 (3) where \u03b5 i is a random variable capturing idiosyncratic factors that influence V i but are unrelated to P i , M i , and G E . All of the quantities of interest in equation (3) vary across individuals and are measured at the individual level (subscripted i) except the salience of the election (subscripted E), which varies only across elections. Thus, standard errors should be much larger for the salience coefficient, \ud97b\udf59 3 , than the other terms. If one assumes \u03b5 i is distributed normally, equation (3) presents the probit model. Citizen i votes in election E if and only if V * i > 0. Hence, the probability i votes is: Pr(\ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 2 M i + \ud97b\udf59 3 G E > \u2212\u03b5 i ) = \ud97b\udf59(\ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 2 M i + \ud97b\udf59 3 G E ) (4) where \ud97b\udf59 is the cumulative normal distribution. The effect of being mobilized is then: \ud97b\udf59M i = \ud97b\udf59(\ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 3 G E + \ud97b\udf59 2 ) \u2212 \ud97b\udf59(\ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 3 G E )(5) Thus, \ud97b\udf59M i represents the effect of being mobilized on person i's probability of voting given i's propensity to vote and the general salience of the election. This function is graphed in Figure 3 and demonstrates the link between the context-dependent theory of mobilization and the estimator presented in equation (3).", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "The Need for Randomization", "text": "Strategic behavior on the part of campaigns and unob- served heterogeneity among individuals could cause M to be correlated with \u03b5, thereby biasing parameter es- timates. Campaigns target specific types of individuals, who, in turn, are not equally available to be contacted by campaigns. Political parties also carefully allocate mo- bilization dollars to particular races based upon compli- cated decision rules that are a function of G. Furthermore, it is possible that the people targeted and available to re- ceive contact from the campaign possess higher baseline propensities to vote and may be more receptive to blan- dishments to vote than people who are not available. Thus, strategic behavior on the part of campaigns and individ- ual psychology make observational data unsuitable to test our theory of context-dependent mobilization. 4 4 This problem can also be viewed through the lens of measure- ment error and is illustrated in the studies conducted by Hillygus (2005) and Niven (2001, 2004). Even though Hillygus uses panel data, it is possible that the type of individuals who report being contacted about voting by members in their community are the type of people who tend to vote anyway-despite the fact that they initially expressed disinterest in voting at the beginning of the cam- paign. In Niven's case, an alternative explanation of his findings is that his measure of intermittent voters is biased. Recall that he categorizes an individual as a high-propensity voter if the person voted in all of the three past elections. Yet it is likely the case that many individuals who typically vote in elections (i.e., true high- propensity voters) could not vote in a previous election because they were ill or away for vacation, causing them to be mislabeled as \"intermittent\" voters. Meanwhile, it is unlikely that enough true low-propensity voters would accidentally vote in an election so as to counterbalance the miscategorized high-propensity voters. As a result, Niven's measure of intermittent voters will be biased in the direction of finding a turnout boost. We surmount the potentially problematic behavior of campaigns and individuals by using randomized field ex- periments. Rather than allowing the campaign to decide whom to target, individual voters are randomly assigned to be contacted, M = 1, or not, M = 0. Since the assign- ment is exogenous and random, the treatment group (i.e., people to be contacted) and control group (i.e., people not to be contacted) should possess equal average propen- sities to vote. That is, E[Propensity T \u2212 Propensity C ] = 0 within each experiment. To have variance in election salience, G, experiments across a range of different elections need to be pooled together. While randomization ensures no correlation between mobilization activities and personal and con- textual attributes within experiments, the same cannot be said across experiments. If the proportion of subjects assigned to the control group were systematically smaller in tightly contested elections, then M would be correlated with G and, potentially, \u03b5. Our solution to this prob- lem is to force experiments to be evenly split between treatment and control groups, thereby eliminating any potential biases. For experiments containing more treat- ment subjects than control, we randomly select treatment subjects to exclude from the experiment to conduct the analysis with an even number of treatment and control subjects from the experiment. The same process is con- ducted for experiments \"overweighted\" with control sub- jects. Eliminating subjects from the analysis is inefficient, but since they are randomly selected it does not intro- duce bias. 5 In sum, the use of randomized experiments provides unbiased estimates of who is mobilized to vote and corrects a methodological problem in the existing literature.", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Average Treatment on the Treated Effect", "text": "Like all GOTV efforts, the campaigns in the experiments that we analyze were not able to reach everyone assigned to the treatment group. Some individuals did not answer the door, were not home, or no longer lived at the ad- dress listed in the voter files. The failure-to-treat problem does not bias the estimates of the empirical model dis- cussed in the last section, because random assignment en- sures that (within sampling variability) the treatment and control group have an equal proportion of contactable individuals. 6 However, it does mean that estimates from equation (3) measure the effect of assignment to treat- ment conditions or the overall effect of GOTV outreach on those that the campaign intended to treat (i.e., the ITT effect), and not the effect on those who were exposed to GOTV contact. The intent-to-treat effect is useful for eval- uating the effect of a program (i.e., given that a campaign makes outreach, who responds?), but not estimating the behavioral response of individuals to the actual program intervention (i.e., campaign contact). An intuitive way to measure the effect of GOTV con- tact would be to substitute treatment assignment, M, in equation (3) with an indicator for actual GOTV contact, C. Unfortunately, this approach risks introducing bias into the causal estimates since unobservable factors that cause individuals to be exposed to GOTV contact may also be correlated with voting behavior (see Arceneaux, Ger- ber, and Green 2006 for a demonstration of this point). Instead, it is more appropriate to rework the empirical model in equation (3) by including C as an endogenous function of M. This approach is akin to treating random assignment as an instrument for GOTV contact, which others have shown to be a valid way to estimate average- treatment-on-treated (ATT) effects (Angrist, Imbens, and Rubins 1996; Gerber and Green 2000). Wooldridge (2002, 477-78) offers a blueprint for this approach when both the dependent variable and endogenous explanatory vari- able are dichotomous, as they are in our case. Using our definitions for variables, the model is: V * i = \ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 2 C i + \ud97b\udf59 3 G E + \u03b5 1 V i = \u23a7 \u23a8 \u23a9 1 if V * i > 0 0 if V * i \u2264 0 C * i = \ud97b\udf59 0 + \ud97b\udf59M i + \u03b5 2 C i = \ud97b\udf59 1 if C * i > 0 0 if C * i \u2264 0(6) where the error terms (\u03b5 1 , \u03b5 2 ) \u223c bivariate normal and are independent of M. Because M is a random, exogenous variable it is independent of the error terms by design. In order to derive the likelihood function, one must obtain the joint distribution of (V, C) given M. Wooldridge (2002, 478) derives the likelihood function below: ln \ud97b\udf59 1 \ud97b\udf59(\ud97b\udf59M) \ud97b\udf59 \u221e \u2212\ud97b\udf59 M \ud97b\udf59 \ud97b\udf59 (\ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 2 C i + \ud97b\udf59 3 G E + \ud97b\udf59\u03b5 2 )/(1 \u2212 \ud97b\udf59 2 ) 1/2 \ud97b\udf59 \ud97b\udf59(\u03b5 2 )\u2202\u03b5 2 \u00d7 \ud97b\udf59 1 \u2212 1 \ud97b\udf59(\ud97b\udf59 M) \ud97b\udf59 \u221e \u2212\ud97b\udf59 M \ud97b\udf59 \ud97b\udf59 (\ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 2 C i + \ud97b\udf59 3 G E + \ud97b\udf59\u03b5 2 )/(1 \u2212 \ud97b\udf59 2 ) 1/2 \ud97b\udf59 \ud97b\udf59(\u03b5 2 )\u2202\u03b5 2 \ud97b\udf59 \u00d7 1 \ud97b\udf59(\ud97b\udf59M) \ud97b\udf59 \u2212\ud97b\udf59 M \u2212\u221e \ud97b\udf59 \ud97b\udf59 (\ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 2 C i + \ud97b\udf59 3 G E + \ud97b\udf59\u03b5 2 )/(1 \u2212 \ud97b\udf59 2 ) 1/2 \ud97b\udf59 \ud97b\udf59(\u03b5 2 )\u2202\u03b5 2 \u00d7 \ud97b\udf59 1 \u2212 1 1 \u2212 \ud97b\udf59(\ud97b\udf59 M) \ud97b\udf59 \u2212\ud97b\udf59 M \u2212\u221e \ud97b\udf59 \ud97b\udf59 (\ud97b\udf59 0 + \ud97b\udf59 1 P i + \ud97b\udf59 2 C i + \ud97b\udf59 3 G E + \ud97b\udf59\u03b5 2 )/(1 \u2212 \ud97b\udf59 2 ) 1/2 \ud97b\udf59 \ud97b\udf59(\u03b5 2 )\u2202\u03b5 2 \ud97b\udf59\ud97b\udf59(7) where \ud97b\udf59 = Corr(\u03b5 1 , \u03b5 2 ) and \ud97b\udf59 = probability density function for the normal distribution. Because V cannot be endogenous to C, this model is a special case of a \"recursive, simultaneous-equations\" sys- tem identified by Greene (2000, 852-53), and the bivariate probit can solve the likelihood equations to estimate the average treatment effect upon those treated (ATT). 7 ", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Measurement", "text": "In addition to avoiding bias from selection processes and unobserved heterogeneity, our analysis seeks to mini- mize bias stemming from measurement error. To that end, we follow the lead of Gerber and Green (2004) and Niven (2004), who use official voter turnout records to measure the dependent variable, thereby avoiding any bias from self-reported behavior. 8 Similarly, we use campaign records to measure both the assignment to treatment condition and actual contact-again avoiding 7 Bivariate probit likelihood surfaces can be full of saddle points and flat surfaces that make maximization unreliable (Freedman and Sekhon 2008). Our models converged with little difficulty and no signs of problematic local areas. The fact that the treatment-on- the-treated results (i.e., biprobit) mirror the intent-to-treat results (i.e., probit) and that linear two-stage least squares replicates the findings (not reported) gives us added confidence in the bivariate probit analysis. self-reporting bias. 9 Thus, our measures of V, M, and C are unproblematic. Estimating a person's propensity to vote, P, requires more care. Green and Gerber (2004) and Niven (2004) use official voter turnout records to remove measurement er- ror in the dependent variable, but they rely upon turnout in the past one or two elections as a measure of propen- sity to vote. 10 Consequently, their measures are coarse and unreliable, undermining the ability to make accurate inferences. Voting propensity can be modeled successfully by taking into account a more complete range of factors. By including common correlates of voting such as age, registration year, party registration, and a more complete voter history, one can derive relatively precise estimates of a person's propensity to vote. As a practical matter, we use factor analysis to collapse all this information into a single dimension measuring a person's propensity to vote, but the analysis is not particularly sensitive to precisely how propensity is modeled. 11 Two complications arise from this strategy. First, an individual's propensity to vote is obviously an estimate with greater uncertainty surrounding it than our objective measure of campaign contact. To account for this uncer- tainty, we employ a bootstrap procedure where propensity is reestimated for each sample drawn. Thus, the boot- strapped standard errors account for the variance in our propensity measure. This is an important innovation that improves upon the approach taken in previous research, which is to assume that an individual's voting propensity is measured with certainty. The second complication is that each propensity es- timate is experiment-specific and not comparable across experiments. The particular score generated using fac- tor analysis will depend entirely upon the data used in the estimation, which will vary across electoral settings. In order to make propensities comparable, we standard- ize each score so that propensity in each experiment has a mean equal to zero and a standard deviation of 1. This approach assumes the subject population in each experiment shares a mean and variance in propensity to vote. 12 The final term that requires estimation is the general salience of the election, G. Since people must expend some amount of effort to cast a vote, people are more likely to vote as the salience of the election increases (Cox and Munger 1989). Consequently, the level of turnout in an election is an unambiguous expression of the level of citizen interest in the campaign and offers the most direct measure of G. In order to avoid contaminating our measure of G with the effect of the treatment, we use the turnout in the control group (or placebo group) as an indicator of election salience. Because subjects were randomly assigned to the control group, it is appropriate to infer G for the GOTV target universe from turnout in the control group. 13 Table 1 presents the 11 experiments included in the anal- ysis. 14 Studies were included in the analysis when the data were readily available, the treatment provided was consistent across studies (i.e., face-to-face contact with a very simple \"Please vote\" message), and the experimental protocol was successfully executed. Data for a range of door-to-door canvassing experiments were collected for two reasons. First, the analysis requires variance in the general salience of the election, G. The experiments in- cluded feature turnout ranging from a high of 69% during the extremely close 2000 presidential campaign in Ore- gon to a low of 8% in an uncompetitive 2001 city council election in Columbus, Ohio (see Table 1, row 6).", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Data", "text": "Second, while randomized experiments offer unpar- alleled internal validity with regard to the manipulated 12 An alternative strategy would be to declare particular past elec- tions fixed points by which to construct benchmarks to compare across experiments. One could impose fewer modeling assump- tions by not pooling the data at all and measuring treatment effects for different voting propensities within each election. The diffi- culty in examining the role played by electoral salience, G, is the downside of such a strategy. 13 Since G is measured at the level of the election rather than the individual and turnout is highly correlated within elections, the number of observations for elections is much closer to the number of elections rather than the number of individuals. To account for this difference, bootstrapping occurs not only among individuals within an experiment but also between elections included in the analysis. The resulting standard errors are much more conservative than standard errors calculated simply using clustering or random effects. treatment, external validity is nearly always an open question to be answered with future research. Includ- ing as many experiments as possible ensures that the average treatment effect reported represents as many communities, elections, and campaigns as possible. The experiments were conducted in large cities like Denver and Minneapolis, medium-sized cities, and a rural area (Dos Palos). The targeted population includes predom- inantly college students (i.e., Eugene and Columbus), working-class family neighborhoods (e.g., Denver and St. Paul), and low-income neighborhoods (e.g., Bridgeport and Kansas City). While the settings are not fully repre- sentative of the United States, they embody characteristics of many communities. The pooled data set also contains a number of dif- ferent types of experiments. The protocol design (see Table 1, row 2), size (row 7), contact rate (row 9), and treatment effect (row 10 for ITT and row 11 for ATT) all vary across experiments. As mentioned above, to avoid possible correlation between assignment to the treatment condition and election-level factors, the data need to be winnowed so that the treatment and control groups are equally sized. The only two experiments meaningfully in- fluenced are New Haven and Dos Palos (see row 9). Thus, the results are unlikely to be an artifact of a particular experimental design. A subject's propensity to vote is measured with vary- ing degrees of precision across the sites. Voter history is abundant in sites like Kansas City and the minimum pos- sible in New Haven (see Table 1, row 12). When available, other variables such as age (row 13), year of registration (row 14), party (row 15), gender (row 16), the number of registered voters in the household, and geography were also used in calculating the propensity to vote. While more data would be preferable, recall that the extant literature on the subject measures propensity using one or three elections. Our measurement, while not perfect, makes use of all the available data and represents a considerable step forward.", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Results", "text": "The empirical estimates for the three parameters in our theoretical model are shown in Table 2. Regression results with three variables normally would be highly prone to omitted variable bias, but as we discussed in the last sec- tion, because mobilization is randomized in all of these studies, we obviate the need to include control variables. In the first column M is estimated with treatment as- signment as the measure of attempted mobilization (i.e., the intent-to-treat effect, equation 3). We adjust the es- timate of M for contact in the second column (i.e., the average-treatment-on-treated effect, equation 7). There are no surprises here. G is large, positive, and statistically significant, as it should be. Both M and P are positive and statistically significant, confirming that door-to-door canvassing increases the marginal probability of voting and that people's underlying propensity to vote is, in fact, actually predictive of whether they do vote. Finally, when we adjust for contact, the size of M increases, indicat- ing that individuals who are contacted by door-to-door canvassers are more likely to vote. Of greater interest to us is what happens to M across values of P given levels of G. We are able to assess this by plugging the estimates displayed in Table 2 into the normal probability distribution and graphing the mobi- lization effect along the voting propensity continuum for low-and high-salience elections. In order to avoid ex- trapolating effects that are not supported by our range of data, we use low and high values of G that fall within its observed bounds at the 10 th and 90 th percentile. We display the results of this exercise in Figure 4. As our theory predicts, door-to-door canvassing is most likely to boost turnout among high-propensity voters but not low-propensity voters in low-salience elections (G = .1). Conversely, in high-salience elections (G = .48), door-to- door canvassing has little effect on high-propensity voters because they are already committed to voting. Instead, it", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "FIGURE 4 Substantive Effects Generated by Empirical Model", "text": "Note: ATT effect estimated from bivariate probit model reported in Table 2, column 2. is among low-propensity voters that face-to-face contact can be an effective nudge to the polls. Also, the treatment effect follows a parabolic shape across individuals' propensity to vote, which supports the expectation that mobilization is most effective among those voters who are near the threshold for voting. The location of that threshold depends on the salience of the election. As Figure 4 makes plain, in low-salience elections, door-to-door canvassing has a de minimus ef- fect among low-end voters but its effect increases mono- tonically until it peaks among mid-high-end voters and then begins to taper off. The opposite takes place in high-salience elections. Here, mobilization has a sub- stantial effect even among low-end voters and that ef- fect increases until it peaks among mid-low-end voters. The mobilization effect, then, decreases precipitously and eventually has little effect among high-end voters. As a robustness check, we excluded studies from the analyses and re-estimated the empirical model. The re- sults we report are not sensitive to the inclusion of any particular study or set of studies. The results also hold up if we step outside of the probit model and include squared terms for P and interaction terms to capture the hypothesized relationships. Because these less parsimo- nious models with no theoretical motivation yield similar results, we conclude the probit model is not imposing the relationship on the data (see the appendix for tables and details). Two implications of our findings deserve comment. First, as the figures make clear, treatment-on-treated ef- fects range from 1.3 to 13.2 percentage points. Thus, the 7 to 10 percentage point average treatment effect from face- to-face contact reported by previous field experiments (cf. Green, Gerber, and Nickerson 2003) conceals a great deal of heterogeneity in response to campaign contact. Second, because these effects vary a great deal across individuals and electoral contexts, the cost per vote that a campaign can expect depends on whom they try to mobilize and in what election the mobilization takes place. To make this last point concrete, we present a cost- per-vote analysis in Table 3. The ingredients for this anal- ysis are straightforward: $ per vote = $HR (ATT * #DK * CR)(8) where $HR = hourly rate paid to canvassers, #DK = num- ber of door knocks that a canvasser can expect to com- plete in an hour, and CR = the contact rate. Obviously, these quantities will vary depending on the specific situ- ation, but we can nevertheless illustrate the implications of our empirical model by referring to the available data and inserting plausible numbers. We estimate the con- tact rate for low-, medium-, and high-propensity voters by regressing contact on vote propensity and calculating the probability that a voter will be contacted at a given propensity. 15 These estimates are shown in column 1 of Table 3. As one would expect, the probability of contact is positively correlated with voting propensity. The ATT ef- fect for each of these voters is displayed in columns 2 and 3 of Table 3 for both low-and high-salience elections. These estimates come from the bivariate probit model and are merely a tabular display of the results shown in Figure 4. 16 Finally, we assume that canvassers are paid $15 an hour and can knock on 25 doors per hour. 17 The cost per vote is shown in columns 4 and 5 of Table 3 and the results are striking. In elections that gener- ate little interest, a campaign fitting our description would spend approximately $93 per vote on average if it targets low-propensity voters, but only a modest $16 per vote if it focuses on high-propensity voters. These results reverse themselves in an election that generates a high degree of interest. For high-turnout elections, high-propensity vot- ers cost over $60 a vote and low-propensity voters are a far more reasonable $25 per vote. Of course, these cost estimates change when the particular campaign context differs from our assumptions. For instance, a nonprofit organization that can rely on volunteer labor may be able to drastically reduce its expected cost per vote. 18 Simi- larly, an organization that can obtain a higher contact rate would also be able to mobilize voters more cheaply. Nev- ertheless, our analysis strongly demonstrates that cam- paigns should not adopt a one-size-fits-all mentality for their GOTV operation. As we will touch on in more detail below, the strategy a campaign chooses is contingent on the interaction between its goals and the election context it confronts.", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Conclusion", "text": "These experimental data offer evidence in support of our contextual model of voter mobilization. Door-to-door canvassing increases turnout mostly by enticing those who are on the cusp of voting. If asked, these fence- sitters are more likely to vote. However, the location of the fence changes depending on the electoral context. In high-profile, competitive races, most registered voters will be above the threshold for voting and mobilization will not be cost-effective. In low-salience, uncompetitive elec- tions, face-to-face conversations about the importance of voting will not be sufficient to bring unlikely voters to the polls. In these cases, a campaign's efforts are best directed towards high-propensity voters, who might be persuaded that voting in \"minor\" elections is a worthwhile endeavor. These findings offer a little encouragement for those who want to engage traditionally low-turnout groups in politics. A cost-effective means of increasing voter par- ticipation among these groups in sleepy local elections is unlikely to present itself. However, such groups can be mobilized in high-profile elections, such as presidential elections. Since voting may be habitual (Gerber, Green, and Shachar 2003), perhaps a culture of voting can be gen- erated and low-propensity voters can gradually be turned into moderate-propensity voters. Our results suggest that the best place to begin the transformation process is high- salience elections (e.g., Republicans targeting 4 million low-turnout Evangelical voters during the 2004 presiden- tial election). Our results also offer support for models that charac- terize the voting decision as an exercise in weighing costs and benefits (e.g., Riker and Ordeshook 1968). Some scholars have argued that because these models do not adequately predict the level of turnout, they are of little analytical use ( Green and Shapiro 1994). Yet, these data solidly support one prediction made by this general ap- proach: it should be easier to mobilize those for whom the costs of voting slightly outweigh the benefits of voting relative to those for whom the benefits are significantly outweighed by the costs. Nevertheless, our theory of contingent mobilization offers a more useful conceptualization of how mobiliza- tion influences turnout than the classic Riker and Ordeshook (1968) framework, which has often been invoked when theorizing about the effects of campaigns on voter turnout (e.g., Rosenstone and Hansen 1993). While our model is consistent with the Riker and Ordeshook model, it replaces abstract terms, such as duty and the costs of vot- ing, with more concrete concepts that can be measured. A major weakness of the Riker and Ordeshook formulation is the practical inability to disentangle its various compo- nents when contemplating how campaign mobilization boosts turnout. A GOTV campaign could be effective via the benefit term by persuading individuals to care about the outcome of the election, through the cost term by lowering the informational costs associated with voting, through the duty term by reminding people about their role as democratic citizens, or even through the infamous p-term by erroneously convincing individuals that their vote could be decisive. Furthermore, it is unclear how the Riker and Or- deshook model explains why voting propensity or the electoral context conditions voting behavior. Do high- propensity voters have a large sense of duty or low costs associated with voting? Do presidential elections, in which there is typically a great deal of general interest, raise turnout by lowering information costs and boosting no- tions of civic duty or by increasing the perceived benefits of electing the preferred candidate? Most relevant aspects of elections and political campaigns contribute to both the costs and the psychological benefits of voting, making the calculus of voting of little practical value in guiding campaigns or predicting individual responses to external stimuli. In contrast, we believe that our model offers a parsimonious, intuitive, and testable explanation of voter mobilization. The propensity to vote (P), salience of the election (G), and effect of contact from the campaign (M) can all be measured, and the model accurately predicts our experimental findings. While our model captures the dynamics of mobiliza- tion from a campaign's perspective, future work should focus on the microfoundations underlying the hetero- geneity in voting propensities that have been taken as exogenous, both here and in the literature. In particular, our model is entirely static and the propensity to vote, P, is viewed as a fixed attribute. As mentioned above, there is solid evidence that voting is habit forming (Gerber, Green, and Shachar 2003), so propensity in one election is a function of mobilization in a past election. That is, P i,t = f (P i,t\u22121 , M i,t\u22121 ). In this manner, the dynamics of mass voter behavior might be modeled appropriately. Our parsimonious model of mobilization offers a useful framework upon which future endeavors can build. There is no reason to believe that the model's utility is restricted to voter turnout. A broad swath of civic engage- ment can be explained by the context-dependent model. For instance, an individual probably has an underlying propensity to donate to a candidate (P). Requests from campaigns to donate money to the candidate make the person more likely to donate (M), but may not push him or her over the threshold to donate. This threshold (G) depends upon the profile of the cause, election, or candi- date. Candidates in close, high-profile elections probably have a lower threshold for donations. In this way, we think the model can be easily adapted to other forms of behavior such as campaign donations, letter writing, vol- unteer work, protests, parental involvement in schools, and any other activity where an individual bears a cost for a collective good. Finally, it is worth noting that these experiments fo- cus on individuals who are already registered to vote. The evidence here has little to say about the effectiveness of first registering and then mobilizing individuals who are not registered to vote. GOTV organizations typically fo- cus mobilization efforts on registered voters since official voting records from which target walk lists are constructed are easy to obtain. Moreover, there have been few, if any, field experiments conducted to date on the effectiveness of registering citizens to vote. Consequently, little is known about the difficulties and costs associated with identi- fying and contacting unregistered citizens. As scholars continue studying ways in which to boost turnout among nonvoters, we believe that the mobilization of unregis- tered citizens is a significant missing puzzle piece and warrants scrutiny.", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Appendix", "text": "In order to be sure that our results are not driven by any particular study or group of studies, we re-estimated the probit model after successively removing each study, or excluding groups of studies, such as the placebo- controlled, low-contact, and high-contact experiments. These estimates, reported in Table A1, show that our find- ings are robust across sample restrictions. We also want to be sure that the probit model, which perfectly captures our theoretical model, is not driving the results by imposing the hypothesized functional form on the data. We do so by adopting the linear probability model and testing for the parabolic function by including a squared term for P and interacting it with M. If, on average, the treatment effect increases across values of P until reaching a tipping point, the coefficient for M * P 2 should be negative. Using two-stage least squares re- gression with random assignment as an instrument for campaign contact (cf. Angrist, Imbens, and Rubin 1996; Gerber and Green 2000), we find this to be the case for the full sample and for a host of sample restrictions. These results are shown in Table A2, column 1. Finally, we want to be sure that the apex of the parabola that represents the treatment effect across voting propensity moves in response to changes in G as hypoth- esized. We can test this in the linear probability frame- work by interacting G with all the variables in the model reported in Table A2. This approach is equivalent to a hierarchal linear model with G conditioning the effect that M and P have on turnout (cf. Steenbergen and Jones 2002). If the apex of the parabola moves as we expect, the coefficient for M * P * G should be negative. The results in Table A2, column 2 show that this is the case across all of the sample restrictions. We graph the results for the full sample model in Figure A1 to demonstrate findings are commensurate with those obtained from the probit models.  ", "title": "Who Is Mobilized to Vote? A Re-Analysis of 11 Field Experiments", "file_name": "Arceneaux and Nickerson - 2009 - Who Is Mobilized to Vote A Re-Analysis of 11 Fiel.pdf"}
{"section": "Abstract", "text": "An increasing fraction of jobs in the U.S. labor market explicitly pay workers for their performance using bonus pay, commissions, or piece-rate contracts. Using data from the Panel Study of Income Dynamics, we show that compensation in performance-pay jobs is more closely tied to both observed and unobserved productive characteristics of workers than compensation in non-performance-pay jobs. We also find that the return to these productive characteristics increased faster over time in performance-pay than in non-performance-pay jobs. We show that this finding is consistent with the view that underlying changes in returns to skill due, for instance, to technological change induce more firms to offer performance-pay contracts and result in more wage inequality among workers who are paid for performance. Thus, performance pay provides a channel through which underlying changes in returns to skill get translated into higher wage inequality. We conclude that this channel accounts for 21% of the growth in the variance of male wages between the late 1970s and the early 1990s and for most of the increase in wage inequality above the eightieth percentile over the same period.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "I. INTRODUCTION", "text": "The standard competitive model of the labor market supposes that wages are equal to marginal products and that the wage structure is determined by the equilibrium of supply and demand. That simple model forms the backbone of most studies of the evo- lution of wage inequality. For example, Katz and Murphy (1992) 2", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "QUARTERLY JOURNAL OF ECONOMICS", "text": "argue that the return to education increased in the 1980s because the rate of increase in the relative supply of more educated labor decelerated, while relative demand steadily increased. Similarly, Juhn, Murphy, and Pierce (1993) argue that the growth in within- group wage inequality throughout the 1970s and 1980s was driven by an increase in the demand for unobserved skills. A main virtue of such studies that use a standard competitive model of the labor market is that they generally provide a straightforward interpre- tation of the evolution of the wage structure in familiar terms of the supply and demand for different types of labor. Despite the appeal of the standard competitive model, it is also well established that it is, at best, only a good approxima- tion for the way wages are actually set in the labor market. In particular, when markets are imperfect and information is costly, wages are not generally equal to the productivity of workers. As a result of those frictions, the distribution of wages does not always accurately represent the distribution of workers' productivity. But as long as the legal, institutional, and contractual arrangements that determine the relationship between wages and productivity remain constant over time, the competitive model will still provide an accurate account of the changes in the distribution of wages. Whether or not this is the case is crucial to our understanding of why wage inequality increased so much over the last thirty years. In this paper, we use data from the Panel Study of Income Dy- namics (PSID) to investigate how one particular form of contrac- tual arrangement, performance-pay schemes, has contributed to changes in wage inequality. Because the intent of these schemes is to more closely align wages with productivity, our empirical strategy builds upon the idea that the wages of performance-pay workers are more closely linked to productive ability than are the wages of non-performance-pay workers. This can result in in- creasing wage inequality as the fraction of workers being paid for performance grows over time, or as the inequality-enhancing ef- fect of performance pay grows because of other underlying changes in the return to productive ability. There are several reasons that studying the link between per- formance pay and wage inequality is particularly appealing. First, there has been a steep growth (in our PSID data and in other data sources) in the fraction of workers who are paid for performance, which suggests that these two phenomena may be closely linked. Second, performance-pay workers tend to be concentrated in the upper end of the wage distribution, which is precisely where wage inequality has grown the most dramatically over time (see, e.g., Piketty and Saez [2003] and Autor, Katz, and Kearney [2006]). Third, it is well known that among executives at the very top end of the wage distribution, performance pay (bonuses, stock options, etc.) accounts for the lion's share of the growth in the level of compensation (see, e.g., Piketty and Saez [2003]) and for much of the dispersion in compensation (Frydman and Saks 2007) in this segment of the labor market. Not surprisingly, performance pay also accounts for most of the growth in inequality among top executives. 1 Equipped with our PSID data, we can investigate whether this phenomenon extends to a broader cross-section of the workforce. Although we would ultimately want to know whether the growth in the use of performance pay is one of the underlying causal factors behind the growth in wage inequality, answering this question raises a number of difficult conceptual and mea- surement challenges. On the conceptual side, the key question is whether the growth in performance pay is driven by a set of exoge- nous factors unrelated to other aspects of the labor market, or is instead a rational response by firms to the same underlying factors responsible for the growth in wage inequality. For sure, measuring and rewarding individual performance is difficult and costly (see Bishop [1987]). One possible view is that performance pay has be- come more prevalent because the cost of collecting and processing information has declined over time with advances in information and communication technologies. Under this interpretation, one could view the growth in performance pay as a causal factor be- hind at least some of the growth in wage inequality. An equally plausible alternative scenario is that as the de- mand for highly productive workers increases, the benefit of implementing a performance-pay system outweighs the cost of introducing new measurement instruments. Under this alterna- tive view, factors such as technological change and globalization that increase the relative demand for highly productive work- ers are the underlying causal factors behind the growth in both wage inequality and the prevalence of performance-pay schemes. growth in wage inequality above the eightieth percentile would have occurred in the absence of performance pay. However, for the reasons discussed above, it would be prema- ture to claim that the growth in performance pay explains 21% of the growth in the variance of wages, and most of the increase in inequality above the eightieth percentile. We can, nonetheless, infer that performance pay is, at a minimum, a very important channel through which other underlying sources of changes in the distribution of worker productivity, such as skill-biased technical change (SBTC), have been translated into higher wage inequality, especially at the top end of the distribution. Absent this channel, inequality would have increased substantially less between the late 1970s and the early 1990s. The paper proceeds as follows. In Section II, we present some background on performance-pay schemes and propose a simple model built upon the insight of Lazear (1986) that the reason per- formance pay is used is that at the time workers are employed one cannot observe their ability. Two important predictions of this model are that more productive workers are those who tend to be paid for performance, and that an increase in the return to ability results in more firms choosing to use performance pay. In Section III, we present our empirical model and the testable impli- cations of the theoretical model. In Section IV, we present the data used for the empirical analysis and illustrate the growth in the in- cidence of performance pay over time. Section V presents the main estimates of the effect of performance pay on the wage structure. We then show in Section VI the connection between performance pay and the growth in wage inequality between the late 1970s and the early 1990s and conclude our discussion in Section VII.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "II. PERFORMANCE PAY", "text": "In the standard competitive model, firms and the rest of the labor market observe the marginal product of workers, while com- petition ensures that the wage is equal to a worker's marginal product. In this setting, modes of payment (fixed wages, perfor- mance pay, etc.) have no empirical content because no matter how workers are paid, they are paid for their marginal product. In practice, firms appear to find the problem of setting wages equal to marginal products difficult if not daunting. 2 Over the past thirty years, the economics literature has explored a number of reasons why firms may not be able to implement pay-for-performance sys- tems, most of which center on monitoring costs. In this section, we provide a brief overview of key empirical findings about the determinants of the incidence of performance pay in the labor market. We then present a simple model that provides the main conceptual framework underlying our empirical analysis.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "II.A. Why Do Firms Use Performance Pay?", "text": "There are a number of reasons why it may be in the interest of firms to introduce performance-pay schemes, even if this entails substantial monitoring and administrative costs. As always, firms will be willing to incur these additional costs provided that they obtain sufficient benefits in return. A commonly mentioned ben- efit of performance pay is that it provides incentives for workers to exert more effort. But even if performance pay has no effect on workers' effort, when workers are heterogeneous in terms of their innate productive abilities, it can be profitable for firms to pay the monitoring cost and then attract more able workers by paying them a wage that better reflects their productivity. In such a set- ting, performance pay plays an important role in sorting workers across different jobs and/or employers. 3 Because the cost of obtaining a good measure of the perfor- mance of workers is likely to be related to job characteristics, the incidence of performance-pay schemes should also vary accord- ing to these characteristics. This prediction holds regardless of whether performance pay is used for incentive or sorting reasons. Using data from the BLS industry wage survey, Brown (1990) explores how the choice between a fixed salary, merit pay, and piece-rate compensation depends on monitoring costs. He finds that firms choose standard rates when monitoring costs are high, as is the case with complex jobs. Merit pay systems are more likely to be used when workers feel that their evaluations are fair. MacLeod and Parent (1999) consider a similar question using a number of panel data sets to control for unobserved worker- specific characteristics. They also extend Brown's analysis to a broader class of compensation systems and differentiate between bonus pay, commission contracts, and piece-rate contracts. They find that commission contracts are widely used in sales jobs, where the level of sales provides a clean measure of performance. When performance measures are more subjective, firms either use bonus pay or pay as a function of hours or days worked, with little explicit pay for performance. In addition to monitoring costs, there are a number of reasons why performance pay may be chosen over other methods of payment in different jobs. Firms that employ high-turnover workers may be more likely to introduce performance-pay schemes than firms with a more stable workforce that can rely upon deferred payments (promotions, pension plans, etc.) to tailor compensation to the characteristics of workers. Indeed, Goldin (1986) shows that around the turn of the twentieth century, piece- rates were more widely used in female-than male-dominated occupations, a phenomenon she attributes to the fact that female workers had a higher rate of turnover. Interestingly, piece-rates were more widely used back then than they are today. As modern management practices were introduced and the fraction of clerical and managerial workers grew steadily over time, long-term employment relationships became more prevalent and firms started relying on promotions and other schemes instead of performance pay to provide incentives to their workers.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "II.B. Performance Pay and Wage Inequality", "text": "As the above discussion makes clear, the decision of firms to introduce performance pay potentially depends on a large num- ber of factors. It is also clear that a decrease in monitoring (or related information processing) costs always increases the proba- bility that firms will use performance pay instead of fixed wages, regardless of the precise reason that firms use performance pay. One would also expect performance pay to increase wage disper- sion relative to a payment system based on fixed wages. This can be trivially seen in the case of a firm that pays all workers the same fixed wage when it does not have any information on the ability or the actual performance of individual workers, whereas differences in productivity are rewarded in a firm that uses per- formance pay. Based on these two predictions, it is tempting to propose a simple explanation for how performance pay has contributed to the recent increase in wage inequality. As is well known, the cost of collecting, processing, and analyzing information has de- clined over time with advances in information and communication technologies. As a result, the cost of introducing performance-pay schemes that require collecting and processing information about workers' performance has presumably declined too, resulting in a growth in the incidence of performance pay. Combining this with the idea that performance pay increases wage inequality, it follows that the growth in performance-pay jobs should have contributed to the rise in wage inequality in the United States. However, there are a number of reasons why this story is overly simplistic. First, even if performance pay increases wage dispersion among workers who are being paid for performance, the overall impact of performance pay also depends on where these workers are in the skill distribution. This is reminiscent of the case of unions and wage inequality, where unions may end up increasing overall inequality by creating a wedge between union and nonunion workers that offsets the equalizing effect of unions within the union sector. 4 Second, there are good reasons to believe that SBTC, or other explanations that have been suggested for the growth in wage inequality, also has an impact on the decision of firms to use per- formance pay. In particular, in the sorting model of performance pay discussed above, as the productivity gap between more and less skilled workers increases, it becomes more and more advanta- geous for firms to introduce performance pay to distinguish highly productive workers from less productive workers. Third, changes in the market for top executives, where perfor- mance pay has always been widespread and has also been growing over time (Frydman and Saks 2007), are hard to reconcile with a simple story based on declining monitoring costs or on related costs of designing sophisticated compensation systems based on stock options, etc. In contrast, the growth in the share of stock op- tions in total compensation is consistent with the market model of Gabaix and Landier (2008), where performance pay is used for selection purposes. Note that these changes are also consistent with \"skimming\" stories where executives use performance pay as a cover for rent extraction (e.g., Bertrand and Mullainathan [2001] and Bebchuk and Fried [2004]).", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "II.C. Model", "text": "We now explore these issues more formally using a model of performance pay presented in detail in Appendix 1 in the 4. For example, unions reduce wage inequality among men but not among women, for whom unionization is concentrated in the upper end of the skill distri- bution (Card, Lemieux, and Riddell 2004). supplemental material to the paper (we refer to the supplemental material as the Web Appendix hereafter). The model builds upon Lazear's (1986) observation that the reason performance pay is used is that at the time a worker is hired the employer cannot observe her ability. This may result in a mismatch between what the worker is capable of doing and what the employer expects. Linking compensation to performance can reduce this mismatch, and thereby increase overall productivity. However, the introduc- tion of an effective performance-pay system is expensive, and thus one faces a trade-off between the cost of introducing such a system and the benefits in terms of improved match quality. Suppose a worker i paid a wage w ij for job j obtains util- ity U ij = w ij \u2212 exp(e ij \u2212 \u03b1 i ), where e ij is effort and where ability is given by the latent variable \u03b1 i \u223c N( \u02c6 \u03b1 i , \u03c3 2 i ). What we call \"ef- fort\" here can be more broadly interpreted as the effective skills supplied by the worker to complete some specific tasks or duties. For example, workers with lower levels of education (lower \u03b1) can supply the same effective skill and perform the same tasks as more educated workers, but doing so is more expensive in utility terms. It is assumed that conditional upon worker characteristics x i , the mean and variance are known and given by\u02c6\u03b1by\u02c6 by\u02c6\u03b1 i = E{\u03b1 | x i } and \u03c3 2 i = var{\u03b1 | x i }. Following a longstanding tradition in labor economics (Jovanovic 1979;Harris and Holmstr\u00f6m 1982), it is as- sumed that information is symmetric; both the worker and the firm learn \u03b1 i at the same time. Output y ij is assumed to be a linear function of effort, y ij = k j + \u03b2\u03b3 j e ij , where k j is the output produced on job j regardless of effort and \u03b3 j is the marginal product of effort on job j. The parameter \u03b2 repre- sents a market return to effort linked, for instance, to the degree of skill bias in technology. Under performance-pay contracts, net output is obtained by subtracting the cost of monitoring effort, M j . Under fixed-wage contracts, workers agree to supply a fixed level of effort \u00af e ij in exchange for a wage w FW ij . Under performance- pay contracts, the firm and the worker agree to a contract linking the wage w PP ij to effort, and the worker sets her effort e ij optimally once her ability \u03b1 i is revealed. As mentioned above, we can think of effort as the tasks or duties performed by a worker on a job. For fixed-wage jobs, the worker and the firm agree on specific duties to be performed in exchange for a fixed wage. For performance-pay jobs, a worker is free to pick the tasks or duties that maximize utility. Firms simply design a contract to make sure the interests of the worker are aligned with those of the firm. Once this is done, there is no need to specify strict duties to be performed, and productivity is improved by letting workers tailor their duties to their own skills and abilities. We show in Appendix 1 that under a fixed-wage contract, the wage is w FW ij = m j + \u03b2\u03b3 j \ud97b\udf59\u02c6\u03b1 \ud97b\udf59\u02c6 \ud97b\udf59\u02c6\u03b1 i \u2212 \u03c3 2 i \ud97b\udf59 ,(1) where m j = k j + \u03b2\u03b3 j log(\u03b2\u03b3 j ). Under a performance-pay contract, the observed wage is given by w PP ij = m j + \u03b2\u03b3 j \u03b1 i \u2212 M j ,(2) while the ex ante expected wage, \u02c6 w PP ij , conditional on observed characteristics x i , is the same as above except that the actual value of ability, \u03b1 i , is replaced by its expected value, \u02c6 \u03b1 i . Proposition 1 in Appendix 1 shows that in a match between worker i and firm j, a performance-pay contract is used if and only if\u02c6wif\u02c6 if\u02c6w PP ij \u2265 w FW ij , or whenever the selection rule \u03b2\u03b3 j \u03c3 2 i \u2265 M j (3) is satisfied. Thus, performance-pay contracts are chosen whenever the efficiency gain of performance pay, \u03b2\u03b3 j \u03c3 2 i , exceeds its cost, M j . The efficiency gain grows with the conditional variance of abil- ity, \u03c3 2 i , because performance-pay jobs more closely tailor workers' abilities to their work efforts. In contrast, mismatch in fixed-wage jobs rises with \u03c3 2 i . This effect is magnified by the extent of the return to effort on the job (\u03b3 j ) or in the overall market (\u03b2). This simple selection rule provides a number of interesting predictions about the conditions under which performance pay is chosen over fixed wage contracts. Obviously, reducing the monitor- ing costs M j increases the likelihood of selecting performance pay. Jobs such as executive positions, where output is more sensitive to effort (high \u03b3 j ), are also more likely to offer performance pay. Similarly, if \u03b2 increases because of SBTC, so will the likelihood of choosing performance pay over fixed wages. Finally, performance- pay contracts are more likely to be selected for workers with a higher conditional variance of ability, \u03c3 2 i . It is well known that the within-group variance of wages grows with education (see, e.g., Lemieux [2006]); hence it is reasonable to assume that \u03c3 2 i is a growing function of expected ability, \u02c6 \u03b1 i . Figures I and II illustrate some basic implications of the model. For the sake of simplicity, we assume that \u03c3 2 i is a lin- ear function of expected ability, \u02c6 \u03b1 i : \u03c3 2 i = \u03b4 \u02c6 \u03b1 i . Substituting into (3), it follows that performance pay is chosen whenever\u02c6\u03b1whenever\u02c6 whenever\u02c6\u03b1 i \u2265 M j /\u03b4\u03b2\u03b3 j . Here we condition on a specific job j; we discuss the case with multiple jobs in the next section. Figures I and II show that performance-pay workers are concentrated at the top end of the ability distribution. As a result, there is also more wage inequal- ity due to higher returns to observed ability among these workers at the top end (performance-pay workers) than among workers at the bottom end (fixed-wage workers) of the distribution. Figure  I then shows what happens when monitoring costs are reduced from M to M \ud97b\udf59 . The fraction of performance-pay workers increases, and so does inequality, because wages at the very top end increase, while wages at the bottom end (fixed wage jobs) remain constant. A very different explanation for the growth in performance pay, illustrated in Figure II, is that an increase in returns to effort (or skill) from \u03b2 to \u03b2 \ud97b\udf59 induces more firms to switch to per- formance pay. Unlike Figure I, Figure II shows that the return to ability increases for fixed-wage jobs and increases even more for performance-pay jobs. This is an important and testable dif- ference between the two scenarios illustrated in Figures I and II that we will examine in detail in Section V. The two scenarios illustrated in Figures I and II have very different implications for the nature of the connection between the growth in performance pay and the growth in wage inequality. In Figure I, the growth in performance pay results in an increase in inequality only to the extent that it moves workers from a less unequal (fixed-wage) to a more unequal (performance-pay) sector. In Figure II, performance pay also interacts with the underlying growth in \u03b2 because a given increase in \u03b2 has a larger impact on the return to ability in performance-pay than in fixed-wage jobs. In that sense, performance pay provides an additional channel through which underlying changes in the relative productivities of different groups of workers (such as SBTC) get translated into higher inequality at the top end of the distribution.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "III. EMPIRICAL MODEL AND TESTABLE IMPLICATIONS", "text": "The two wage equations, (1) and (2), provide a number of interesting testable implications. For the sake of simplicity, we still maintain the above assumption that \u03c3 2 i = \u03b4 \u02c6 \u03b1 i , where \u03b4 > 0. The wage equation under fixed wages becomes w FW ij = m j + \u03b2\u03b3 j (1 \u2212 \u03b4) \u02c6 \u03b1 i ,(4) whereas the wage equation under performance pay can be rewrit- ten as w PP ij = (m j \u2212 M j ) + \u03b2\u03b3 j \u02c6 \u03b1 i + \u03b2\u03b3 j (\u03b1 i \u2212 \u02c6 \u03b1 i ).(5) There are three key differences between these two wage equations, conditional on a job j. First, the intercept is lower for performance-pay than for fixed-wage contracts because of the fixed monitoring cost, M j . Second, the return to expected ability\u02c6\u03b1 ability\u02c6 ability\u02c6\u03b1 i is larger under performance pay than under fixed wages, which explains why high-ability workers sort themselves into perfor- mance pay. Third, there is an error component linked to unob- served ability (\u03b2\u03b3 j (\u03b1 i \u2212 \u02c6 \u03b1 i )) under performance pay, but not under fixed wages. All these implications are obtained conditional on a job j. In Appendix 1 of the web Appendix, we also discuss the market equi- librium in the case where workers with observed characteristics x i have the choice between different jobs j. We show that, in the simplest version of the model, the \"job effects\" on wages linked to either observed industry and occupation or unobserved employer- employee job match characteristics are similar in performance- pay and non-performance-pay jobs. We also show that these job effects should be less important in performance-pay than in non- performance-pay jobs in a more realistic setting where (i) workers partly sort themselves into different jobs on the basis of their unobservable ability (as in Gibbons et al. [2005]), and (ii) search costs prevent firms from exactly tailoring a fixed wage job to the precise characteristics x i of each worker. We now summarize these various predictions using general empirical specifications of the wage equations for the two types of jobs. As a matter of notational convention we use the superscript p for performance-pay jobs, and n for non-performance-pay jobs (i.e., fixed-wage jobs). The wage equation for worker i on job j at time t under performance pay is w p ijt = a p t + x it b p t + z ijt c p t + d p t \u03b8 i + \u03bd p ij + \u03b5 p ijt , whereas the wage for non-performance-pay jobs is w n ijt = a n t + x it b n t + z ijt c n t + d n t \u03b8 i + \u03bd n ij + \u03b5 n ijt , where x it represents standard observable worker characteris- tics such as potential experience and education; \u03b8 i = \u03b1 i \u2212 \u02c6 \u03b1 i is the unobservable ability component; z ijt is a set of observed job characteristics such as occupation or industry; \u03bd p ij and \u03bd n ij are \"firm- specific\" wage components; and \u03b5 p ijt and \u03b5 n ijt are idiosyncratic error terms. The main empirical implications discussed above are summa- rized as follows: 1. The wage intercept is lower in performance-pay than in non-performance-pay jobs: a p t < a n t . 2. The return to observable worker characteristics, x it , is larger in performance-pay jobs than in non-performance- pay jobs: b p t > b n t . 3. The return to observable job characteristics, z ijt , is smaller in performance-pay jobs than in non-performance-pay jobs: c p t < c n t . 4. The return to unobservable ability \u03b8 i is larger in performance-pay jobs than in non-performance-pay jobs: d p t > d n t . Although the model predicts that d n t = 0, the es- timated value of d n t will be positive if the market observes some part of \u03b8 i (e.g., the quality of education, past produc- tivity) that is not reflected in observable characteristics x it , as in Gibbons et al. (2005). 5. The variance of the firm-specific component is smaller in performance-pay than in non-performance-pay jobs: var(\u03bd p ij ) < var(\u03bd n ij ).", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "IV. DATA", "text": "The bulk of our analysis is conducted using data from the PSID. The main advantage of the PSID is that it provides a rep- resentative sample of the workforce for a relatively long time pe- riod, essential for studying the effect of performance pay on wage inequality. One disadvantage of the PSID is that our constructed measures of performance pay are relatively crude, for reasons dis- cussed below. To probe the robustness of the results based on the PSID, we re-estimate some of the key models using the National Longitudinal Survey of Youth (NLSY).  The PSID sample we use consists of male heads of house- holds aged 18 to 65 with average hourly earnings between $1.50 and $100.00 (in 1979 dollars) for the years 1976-1998, where the hourly wage rate is obtained by dividing total labor earnings from all jobs by total hours of work, both reported retrospectively for the previous calendar year. 5,6 Given our focus on performance pay, this wage measure based on total yearly earnings, inclusive of per- formance pay, is preferable to \"point-in-time\" wage measures that would likely miss infrequent payments (e.g., bonuses) of perfor- mance pay. 7 Individuals who are self-employed are excluded from the analysis because our measure of performance pay based on receiv- ing bonuses, commissions, or piece-rates is defined for employed workers only. 8 We also exclude workers from the public sector be- cause it is not clear what it means to pay workers for their produc- tivity in a sector where employment and wage-setting decisions 5. In the PSID, data on hours worked during year t, as well as on total labor earnings, bonuses/commissions/overtime income, and overtime hours, are asked in interview year t + 1. Thus we actually use data covering interview years 1976- 1999. Annual earnings were top-coded at $99,999 until 1982 (and not top-coded since then), but only a handful of individuals were at the top code. We trim very high values of wages (above $100.00 in 1979 dollars) but do not otherwise adjust for top coding.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "IV.A. The Panel Study of Income Dynamics", "text": "6. Our focus on male heads of households stems from the fact that only heads are asked about their income derived from bonuses, commissions, or overtime. In the PSID, males are designated as the head in all husband-wife pairs. The same is true if the female has a boyfriend with whom she has been living for at least a year, even if the female is the person with the most financial responsibility in the family unit. Consequently, the sample of female heads is relatively small. Using the same sample selection criteria as the ones we use for males would leave us with 1,367 females for a total of 8,185 observations. Perhaps more importantly, issues of representativeness would arise, as those female heads are disproportionately nonwhite (24.4%) and are much less likely to be married (9.2%). 7. See Lemieux (2006) and Autor, Katz, and Kearney (2005) for a detailed comparison of these two types of wage measures using March (average hourly earnings) and MORG (point-in-time wage) CPS data. As in the CPS, we find that inequality is lower in the PSID when a point-in-time wage measure is used in- stead of the average hourly earnings measure used throughout the paper. For workers not paid for performance, the difference in standard deviations is 0.015 for salaried workers compared to 0.064 for workers paid by the hour. We also find that the difference between the two measures is larger for salaried workers paid for performance (0.036) than for those not paid for performance (0.015). This con- firms that inequality using point-in-time wage measures may slightly understate inequality as it misses the contribution of performance pay. But because hourly workers are unlikely to be paid for performance (see Table I), performance pay cannot account for much of the difference in inequality between the two wage measures for this large group of workers. Finally, trends in inequality based on the two measures are generally very similar. 8. Self-employed workers can be viewed as being, by definition, paid for per- formance regardless of the mode of payment (earnings, dividends, etc.) they use to remunerate themselves. are not based on profit maximization (we show in Table B.1 of the Web Appendix that including public sector workers has little im- pact on the results). This leaves us with a total sample of 26,146 observations for 3,053 workers. All of the estimates reported in the paper are weighted using the PSID sample weights. Identifying Performance Pay. In the PSID, we construct a performance-pay indicator variable by looking at whether part of a worker's total compensation includes a variable pay compo- nent (bonus, commission, or piece-rate). For interview years 1976- 1992, we are able to determine whether a worker received a bonus or a commission over the previous calendar year through the use of multiple questions. First, workers are asked the amount of money they received from working overtime, from commissions, or from bonuses paid by the employer. 9 Second, we sometimes know only whether workers worked overtime, and if they were working overtime in a given year, not the amount of pay they re- ceived for overtime. Thus, we classify workers as not having had a variable pay component if they worked overtime. Third, workers not paid exclusively by the hour, or not exclusively by salary, are asked how they are paid: they can report being paid commissions, piece-rates, etc., as well as a combination of salaried/hourly pay along with piece-rates or commissions. 10 Through this combina- tion of questions, we are thus able to identify all nonovertime workers who received performance pay in bonus, commission, or piece-rate form. Starting with interview year 1993, there are separate ques- tions about the amounts earned in bonuses, commissions, tips, and overtime for the previous calendar year. Thus, there is no need to back out an estimate of bonuses from an aggregate amount be- cause the question is asked directly. For the sake of comparability with the pre-1993 years, we nevertheless classify as receiving no 9. Note that the question refers specifically to any amounts earned from bonuses, overtime, or commissions in addition to wages and salaries earned. 10. In many survey years workers are not asked whether their compensation package involves a mixture of salary/hourly pay and a variable component. All they are asked is how they are paid if not by the hour or with a salary. Although there is no way to directly verify it, this likely results in understating the incidence of any form of variable pay because workers are not allowed to answer that they are paid, say, a salary, and then report a commission: they have to choose. Our assertion that this response likely understates the extent of variable pay is motivated in part by the fact that workers in the NLSY, to be described below, are not restricted in describing the way they are paid. We find that workers in the NLSY are more likely to report having part of their compensation package contain a performance-pay component. performance pay all workers who report any overtime work. In this way we are able to determine whether a worker's total com- pensation included a performance-pay component for each year of the survey. One obvious drawback is that it is likely that the performance-pay component we construct will be noisy for hourly workers, though not for salaried workers who are not eligible for overtime payments. However, due to our treatment of over- time workers, we conservatively lean on the side of misclassifying workers as receiving no performance pay even when they do. 11 Defining Performance-Pay Jobs. We define performance-pay jobs as employment relationships in which part of the worker's total compensation includes a variable pay component (bonus, commission, piece-rate) at least once during the course of the re- lationship. 12 We use actual payments of bonuses, commissions, or piece rates to identify performance-pay jobs; thus we are likely to misclassify performance-pay jobs as non-performance-pay jobs if some employment relationships are either terminated before per- formance pay is received, or partly unobserved for being out of our sample range. This source of measurement error is problematic because of an \"end-point\" problem in the PSID data. Given our definition of performance-pay jobs, we may mechanically under- state the fraction of workers in such jobs at the beginning of our sample period because most employment relationships observed in 1976 started before 1976, and we do not observe whether or not performance pay was received prior to 1976. Similarly, jobs that started toward the end of the sample period may be performance- pay jobs but are classified otherwise because they have not lasted long enough for performance pay to be observed. The problem is that, conditional on job duration, we tend to observe a given job match fewer times at the two ends of our sample period than in the middle of the sample. Consider, for example, the case of a job that lasts for five years. For jobs that 11. In an earlier version of the paper, we redid the analysis for 1992 to 1998 using the finer measure of performance pay that allows us to identify the performance-pay status of overtime workers. Doing so had little impact on the results. It only increased the fraction of workers on performance-pay jobs (for 1992-1998) by one percentage point, and regression coefficients were essentially unchanged. 12. We use \"jobs,\" \"employment relationship,\" and \"job match\" interchange- ably. Although the PSID does have information on tenure in the position in most of the survey years spanning the sample period, we do not use it. As is well known, simply determining employer tenure in the PSID can be problematic (see Brown and Light [1992]). As a result, what we call a \"job match\" could be called an \"em- ployer match\" instead. We generally use the word \"job\" for the sake of simplicity. last from 1985 to 1989, all five observations on this job match are captured in our PSID sample. For jobs that last from 1973 to 1977, however, only two of the five years of the job match are observed, which mechanically reduces the probability of classifying the job as one with performance pay. Because of this end-point problem, we get an unbalanced dis- tribution of the number of times job matches are observed at dif- ferent points of the sample period. One simple solution to the problem is to \"rebalance\" the sample using regression or other methods. In practice, we adjust measures of the incidence of per- formance pay over time by estimating a linear probability model in which dummies for calendar years and for the number of times the job-match is observed are included as regressors (estimating a logit gave almost identical results). We then compute an adjusted measure of the incidence of performance pay by holding the dis- tribution of the number of times the job-match is observed to its average value for the years 1982 to 1990, which are relatively unaffected by the end-point problem. The end-point problem could also affect the estimates of the effect of performance pay on both the level and the dispersion of wages because the sample of non-performance-pay jobs is being contaminated by observations from performance-pay jobs for which performance-based payments are never observed. We have investigated this issue in detail using a parametric measurement model described in Appendix 2 of the Web Appendix and concluded that, if anything, this measurement problem biases downward the estimated effect of performance pay on the wage structure. For the sake of clarity and simplicity, the wage results we report in the next sections are unadjusted for these measurement issues. Table I compares the mean characteristics of workers on performance-pay and non-performance-pay jobs, respectively. First, notice that 37% of the 26,146 observations are in performance-pay jobs. 13 Workers on performance-pay jobs tend to earn more and have higher levels of education than workers on non-performance-pay jobs. Note that the hourly wage rate in- cludes both regular wage and salary earnings and performance 13. The 37% figure is unadjusted. This fraction jumps to 42% when we adjust for the end-point problem using the procedure discussed above (see the lower right corner of the Appendix table). Notes. The sample consists of male household heads aged 18-65 working in private sector wage and salary jobs. All figures in the table represent sample means. Education, potential experience, and employer tenure are measured in years. Potential experience is defined as age minus education minus 6. Performance-pay jobs are employment relationships in which part of the worker's total compensation includes a variable pay component (bonus, commission, piece rate). Any worker who reports overtime pay is considered to be in a non-performance-pay job. Workers are considered unionized if they are covered by a collective bargaining agreement.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "IV.B. Descriptive Statistics from the PSID", "text": "pay in the case of workers on performance-pay jobs. Annual hours worked and employer tenure also tend to be higher for workers on performance-pay than non-performance-pay jobs. The unionization rate (percent covered by a collective bargaining agreement) is much lower among performance-pay workers. This suggests that, as expected, the pay structure in union firms corresponds more closely to the fixed-wage contracts discussed in Section II. Another important difference is that there is a much larger fraction of workers paid by the hour in non-performance-pay than in performance-pay jobs. Conversely, workers on performance-pay jobs are more likely to be salaried workers than those on non-performance-pay jobs. This is an important point because the growth in wage inequality has been stronger among salaried than hourly workers (Lemieux 2006). Performance pay is thus more likely to affect the very group of workers who have experienced the largest increase in inequality, and who are also least likely to be affected by other institutional factors such as the minimum wage or unionization. With the exception of potential experience, the mean characteristics in FIGURE III Distribution of Log Hourly Earnings: PSID 1976PSID -1998 performance-pay jobs are statistically different from those in non-performance-pay jobs. An important point illustrated at the bottom of the table is that, of the 3,053 workers, 1,271 are observed on a performance- pay job, and 2,616 are observed on a non-performance-pay job. So 834 workers (1,271 + 2,616 \u2212 3,053) are \"switchers\" observed on both types of jobs, which is essential for identifying models with fixed effects presented in Section V. The cross tabulations shown in the Appendix table confirm that performance pay is more prevalent in high-wage occu- pations such as professional, managerial, and sales positions than in other occupations. For example, the fraction of workers on performance-pay jobs ranges from only 30% for craftsmen to 78% for sales workers. Across industries, the incidence of performance pay ranges from a low of 33% in mining and durables to a high of 65% in finance, insurance, and real estate (FIRE). Note that the (one-digit) industry and occupation categories shown in the table are the ones we use to control for industry and occupation effects in the regression models presented later in the paper. Figure III presents kernel density estimates of the distri- bution of wages for performance-pay and non-performance-pay jobs. The figure shows that hourly wages have a higher mean and median and are less evenly distributed among performance-pay than among non-performance-pay jobs. We next turn to the time trends in the prevalence of perfor- mance pay. Figures IVa and IVb show the evolution of the fraction of performance-pay jobs for various subgroups of the workforce. In all cases, we correct for the end-point problem using the procedure described above. Figure IVa shows that the overall incidence of performance-pay jobs has increased from about 38% in the late 1970s to around 45% in the 1990s. The figure also shows the simpler measure based on the fraction of workers actually report- ing performance pay in a given year. This alternative measure clearly understates the incidence of performance-pay jobs because workers on performance-pay jobs will not necessarily receive a performance payment (such as a bonus) in each year on the job. One advantage of this simple measure is that it is not affected by the end-point problem and provides additional evidence of the robustness of the underlying trends in performance pay. Indeed, even this crude measure of performance pay clearly increases over time, especially in the 1980s. Figure IVa also shows the fraction of workers covered by a col- lective bargaining agreement. Interestingly, the decline in union- ization and the growth in performance pay are both concentrated in the same period (the 1980s). This suggests that deunionization may have contributed to the growth in wage inequality by allow- ing firms to offer more variable pay. 14 Figure IVb shows, however, that the growth of performance pay is not simply a spurious con- sequence of the decline in unionization. In particular, the figure shows that the incidence of performance pay has been growing both among union and especially among nonunion workers. Figure IVb also reports another way of looking at the in- crease in the incidence of performance-pay jobs, by breaking it down by how workers are paid. The figure shows that the bulk of the increase in performance pay is driven by salaried workers who are, incidentally, less likely to be unionized. By contrast, per- formance pay is less prevalent and grows more slowly over time among workers paid by the hour. The increase in the incidence of performance-pay jobs among salaried workers is quite remark- able. It increases from less than 45% in the late 1970s to nearly 60% by the end of the sample period. Note that performance pay represents a relatively modest share of total earnings ( Figure B.1 in the Web Appendix shows that the median share is 4.4%). However, this does not mean that performance pay has a limited impact on total compensation, because we expect (and find in Table B.1 of the Web Appendix) the straight wage component to be more sensitive to workers' characteristics on performance-pay than on non-performance-pay 14. See Freeman (1993), Card (1996), and DiNardo, Fortin, and Lemieux (1996) for evidence that deunionization accounts for about a quarter of the growth in male wage inequality during the 1980s. Note. All the adjustments and contributions of characteristics are computed by estimating linear prob- ability models with a full set of dummies for periods (1976-1979, 1980-1984, 1985-1989, 1990-1993, and 1994-1998) and the number of times a job match is observed (1 to 22), as well as dummies for industry, occu- pation, marital status, race, union status, a cubic function in potential experience, and a quadratic function in job tenure. A total of 26,146 observations are used in all columns. jobs. In order to pay for performance, the employer must evaluate the worker, which then affects the straight wage through promo- tions and job assignment. Hence, even though performance pay is a relatively small fraction of compensation for most workers, the fact that it exists is a signal of more careful monitoring.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "IV.C. The Growth in Performance Pay: Some Additional Evidence", "text": "Two important findings reported in Figure IV are that dif- ferent measures of performance pay indicate a clear growth in performance pay and that this growth is not just a spurious con- sequence of deunionization. Table II takes a more general look at these issues by considering a number of additional measures of performance-pay jobs and possible explanations for the growth in performance pay beyond deunionization. In this table and the remainder of the paper, we focus on changes between the late 1970s (1976)(1977)(1978)(1979) and early 1990s (1990)(1991)(1992)(1993). We use 1990-1993 as our end period (instead of data up to 1998) to minimize the end-point problems mentioned above, though using 1994-1998 yields similar results. Although it would arguably be better to use a base period further away from the first observation year (1976) to reduce end-point problems, by doing so we would miss part of the large increase in inequality (and performance pay) that took place in the early 1980s. We argued above that it was too restrictive to just classify a worker-year observation as one where the worker is paid for performance when an actual payment (bonus, commission, or piece-rate) is received in the current year. Instead, our preferred measure is whether or not a worker on a given job receives per- formance pay at any time during the observed employment rela- tionship. One could argue that this alternative definition is too loose. For example, if we have twenty observations on a worker in a given job, but performance pay is only observed once, it is not clear to what extent such a job is really one that pays for perfor- mance. A reasonable alternative is to classify as performance-pay jobs only those for which the frequency of actual performance- based payments exceeds a certain threshold. With this in mind, Table II shows both the incidence and the growth in performance pay under increasingly strict definitions. Column (1) of Table II shows the results for our preferred measure of performance pay based on payments of bonuses, com- missions, or piece-rates in any year of the employment relation- ship. 15 In column (2), we only classify jobs as performance-pay when a payment is observed at least one time out of five. We in- crease the minimum intensity to one time out of two in column (3), and then present the simple measure based on actual payment in the current year in column (4). The most important pattern that emerges from the table is that, regardless of the measure being used, there is always a sub- stantial increase in performance pay between the late 1970s and the early 1990s. In fact, although the incidence of performance pay obviously decreases when stricter measures are considered, the 15. Note that among observations defined as performance pay that way, we observe an actual performance payment in 37% of cases. The average intensity increases to 57%, however, when we average the frequency of payments across jobs, that is, put an equal weight on all jobs irrespective of the number of observations we have for each job. So even under our broadest measure of performance pay, actual performance payments are frequently observed. growth is, if anything, larger in relative terms for these stricter measures. We also show in column (5) that essentially all the growth in performance-pay jobs is driven by the bonus-pay com- ponent, as opposed to commissions or piece-rates. The table also shows the impact of the adjustment for the number of times the job-match is observed. In the case of the broadest measure reported in column (1), the adjustment reduces the growth in performance pay from 12.9 (row (2)) to 7.1 (row (3)) percentage points. The reason the adjustment is quite large is that the base period we chose, 1976-1979, is more directly affected by the end-point problem than the end period of 1990-1993. The second part of the table shows the contribution of other factors to the growth in performance pay. 16 Using these estimates, we perform a simple decomposition to see by how much the inci- dence of performance pay would have changed if the different explanatory factors had remained constant over time. In the case of our main measure of performance pay (column (1)), row (5) of Table II shows that about a third (2.5 percentage points) of the 7.1-percentage-points increase in performance pay can be linked to changes in these explanatory factors. The most important factor is deunionization, which accounts for 1.4 percent- age points of the growth in performance pay, followed by changes in the distribution of industry and occupation that each explain a little more than half of a percentage point. The remaining fac- tors (education, etc.) account for essentially none of the growth in performance pay. The results for other measures of performance pay reported in columns (2) to (5) are very similar to those for our broader measure of performance pay. We conclude from Table II that the growth in performance pay measured in the PSID is very robust to the way performance-pay jobs are defined, and cannot be explained by other factors such as deunionization. One additional source of evidence is the NLSY, which asks more explicitly about performance pay in several years starting in 1988. 17 Using a sample similar to the one used for the PSID, we find that the incidence of performance-pay jobs increases from 26.1% in the late 1980s to 30% in the late 1990s, broadly consistent 16. These contributions are computed by first estimating a linear probability model with a full set of dummies for time periods (1976-1979, 1980-1984, 19851989, 1990-1993, and 1994-1998) (22) dummies, a cubic in potential experience, a quadratic in job tenure, years of completed schooling, calendar year average of the unemployment rate in the county of residence, and dummies for being married, for being nonwhite, and for union status. The \"performance-pay job dummy\" indicates if either a bonus or commission/piece rate earnings were received at any time during the employment relationship; the \"perfor- mance pay received in current year\" dummy indicates if a bonus or commission/piece rates earnings were received in the current year. with the evidence from the PSID. We also looked at another source of information based on a survey of Fortune 1000 corporations conducted between 1987 and 2003 (see Lawler [2003]). The survey asks firms about the fractions of their workers with some forms of performance pay and reports results in categories such as 0% to 9%, 10% to 19%, etc. We compute the implied fraction of workers with performance pay using the midpoints of these intervals. The implied fractions are 20.7 in 1987, 27.1 in 1990, 34.7 in 1996, and 44.5 in 2002. Once again, these trends confirm the growth in performance pay measured (imperfectly) in the PSID.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "V. THE WAGE STRUCTURE IN PERFORMANCE-PAY AND NON-PERFORMANCE-PAY JOBS", "text": "The model of Section II provides a number of testable im- plications about differences in the structure of wages between performance-pay and non-performance-pay jobs. We now present the estimation results and show that they are consistent with the predictions of the model outlined in Section III.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "V.A. Simple Regression Analysis", "text": "Table III reports simple regression estimates of the effect of performance pay on wages (full compensation, including the actual performance-based payment). These regressions are pro- vided as a benchmark before we move to the core predictions of the model about the differences in the returns to measured and un- measured characteristics in the two pay regimes. Because we have repeated observations for the same individual observed in a given job match (the level at which performance-pay jobs are measured), we allow for correlation in the error terms by clustering standard errors at the job-match level in Table III and subsequent tables. The first column of Table III reports the results of an OLS regression of the log hourly wage on a dummy for performance- pay jobs. The regressions reported in Table III also control for standard worker characteristics x it (years of education, a cubic in potential experience, dummies for race and marital status, and the local unemployment rate) and job characteristics z ijt (union status, a quadratic in seniority, and industry and occupation dum- mies), though the estimated coefficients for these variables are not reported in the table. The estimated effect of the performance-pay job dummy is positive (0.087) and statistically significant, though it is much smaller than the raw wage gap reported in Table I (the unad- justed difference in mean log wages is 0.224). The second column shows that the effect of having a performance-pay job declines but remains very significant when a dummy for performance pay received during the year is included. When worker-specific fixed effects are introduced in column (3), the effects of performance- pay jobs and of receiving performance pay in a given year become smaller but remain positive and significant. Both for this table and for the other results reported in the paper, the fixed effect models are precisely estimated due to the large number of workers who switch between the two types of jobs (see Table I). The results are consistent with the positive sorting into per- formance pay predicted by the model of Section II. Note that in- troducing observed covariates reduces the wage gap by 0.139 (raw gap of 0.224 compared to OLS estimate of 0.087), compared to a further 0.047 reduction (column (3) vs. column (1)) when worker- specific fixed effects are added to the wage equation. This implies that most of the sorting happens on observable dimensions of skills. Also note that the estimated effect of receiving a performance- based payment in a given year is around 4%-4.5% in columns (4) and (5), where we further control for worker-job fixed effects (the effects of performance-pay jobs are no longer identified in this specification). This suggests that performance-pay is not merely displacing base pay, but results in increased compensation, even after controlling for individual and job-specific characteristics.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "V.B. Return to Skill in Performance-Pay and Non-Performance-Pay Jobs", "text": "Table IV provides a direct test of some of the implications of the model. Columns (1) and (2) report separate estimates of a stan- dard wage equation for performance-pay and non-performance- pay jobs, respectively. 18 The estimated models include the same variables as those included in Table III. We only report, however, the estimated effect of years of education, potential experience, and job tenure. 19 As expected, both the return to education and the return to ex- perience are substantially larger in performance-pay than in non- performance-pay jobs (e.g., effect of 0.093 vs. 0.067 for education). We next show in column (3) a more parsimonious specification, where we estimate a pooled regression model where education, experience, and tenure (including the full cubic in experience and the quadratic in tenure) are interacted with the performance-pay job dummy, whereas other variables are constrained to have the same effect for both performance-pay and non-performance-pay jobs. Because doing so yields similar results, we keep this specifi- cation for the rest of the table so that all of the differences between performance-pay and non-performance-pay jobs are summarized by the interaction terms reported in the table. The pooled models also provide a simple way of testing whether the returns to charac- teristics are different for performance-pay and non-performance- pay jobs. We first report OLS estimates of the pooled model as a bench- mark in column (3) and then add worker-specific fixed effects in 18. A more sophisticated approach would be to use the technique of Gibbons et al. (2005), where the return to unobserved ability is allowed to differ across job types (as our model predicts), and learning induces endogenous mobility across jobs. For the sake of simplicity, however, we only control for a standard fixed effect because the results suggest that, at least for occupations, doing so corrects for most of the endogeneity bias due to the fact that job choice depends on unobserved ability. 19. To further simplify the table, we only report the effect of twenty years of potential experience and ten years of job tenure. This is obtained by computing the predicted effect from the polynomial specifications (cubic in experience, quadratic in tenure) at twenty (ten) years of experience (tenure). We only report these results because qualitatively similar results were obtained using either five, ten, or twenty years, and the mean values of experience and tenure are close to twenty and ten years, respectively (Table I). Notes. Standard errors (in parentheses) are adjusted for clustering at the job-match level. All specifications also include a full set of industry (10), occupation (8), and year (22) dummies, a cubic in potential experience, a quadratic in job tenure, years of completed schooling, calendar year average of the unemployment rate in the county of residence, and dummies for being married, for race, and for union status. The reported effects of potential experience (at 20 years) and tenure (at 10 years) are the predicted levels computed using the estimated polynomial models. The models in columns (3)- (6) include interactions between the performance-pay dummy and education, a cubic in potential experience, and a quadratic in tenure. The models in columns (5) and (6) include a full set of interactions between period dummies for 1980 -1984, 1985-1989, 1990-1993, and 1994-1998 column (4). In both cases, we find that the return to education is significantly larger in performance-pay than in non-performance- pay jobs. 20 As predicted in Section III, the intercept is also lower in performance-pay jobs when the interactions are included in the specifications, as in columns (3) and (4). Note, however, that the effect of experience is not significantly different for the two types of jobs when fixed effects are included (column (4)). 21 Unlike education and experience, it is not clear a priori whether job tenure is a pure job characteristic linked to adminis- trative pay levels or is in part a worker characteristic linked to spe- cific human capital accumulation. Table IV shows that the effect of job tenure is lower in performance-pay than in non-performance- pay jobs. This supports the view of tenure as a job characteristic. The difference is no longer significant in the pooled regressions with fixed effects, as reported in column (4). Another obvious job characteristic to look at is occupational affiliation. Gibbons et al. (2005) have shown that including worker-specific fixed effects dramatically reduces the magnitude of the occupation effects; thus we estimate occupational wage dif- ferentials for performance-pay and non-performance-pays jobs by interacting the performance-pay job dummy with occupation dum- mies in the fixed effect model reported in column (4) of Table IV. Consistent with the predictions of Section III, the standard devi- ation of the occupation effects is smaller in performance-pay jobs (0.042) than in non-performance-pay jobs (0.044). The last two columns of Table IV allow the return to education to vary over time, in both performance-pay and non-performance- pay jobs. 22 We divide the sample into five periods (1976-1979, 1980-1984, 1985-1989, 1990-1993, and 1994-1998) and interact the period dummies with years of education, performance pay, 20. It is difficult to interpret the main effect of education in the model with worker-specific fixed effects because education is almost time-invariant (for a given person) in our PSID sample. This means that it is difficult to identify the effect of education separately from the fixed effect when running separate models for performance-pay and non-performance-pay jobs. The interaction term between performance-pay and education is still identified, however, because of the \"switch- ers\" who are observed in both performance-pay and non-performance-pay jobs. 21. Although the interaction term between the performance-pay dummy and experience is not significant at the specific level of experience we look at (twenty years), a joint test indicates that the whole experience profile (linear, quadratic, and cubic terms) is significantly different for the two types of jobs in columns (3) and (5). 22. We also looked at the changes in the returns to other characteristics over time, but education was the only variable for which we systematically found a growing effect. and the interaction of these two variables. Given that the growth in the return to education (and wage inequality, more generally) is concentrated in the 1980s, we only report the results for the period 1990-1993 in the table. Note that the main effect of educa- tion (and of the interaction between education and performance pay) now corresponds to the base period (1976)(1977)(1978)(1979). The OLS estimates in column (5) show that, as expected, the return to education increased between 1976-1979 and 1990-1993, and in- creased even faster for performance-pay jobs. The coefficient esti- mates indicate that the return to education increased by 0.0161 for non-performance-pay jobs, and by 0.0351 for performance-pay jobs (0.0161 plus 0.0190). The changes are even more pronounced and highly significant when fixed effects are included in column (6). The fact that the returns to skill are increasing faster in performance-pay than in non-performance-pay jobs is consistent with the case illustrated in Figure II, where an increase in the relative demand for skilled labor may also be the reason for the growth in performance pay.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "V.C. Variance Components Analysis", "text": "Having established that observable worker characteris- tics matter relatively more for performance-pay than for non- performance-pay jobs, whereas the reverse is true for observable job characteristics, we now look at whether this pattern of results also holds in the case of unobservable characteristics. We do so by performing a variance components analysis on the residuals from the wage regressions estimated separately for performance-pay and non-performance-pay jobs (columns (1) and (2) of Table IV). Going back to the wage equations of Section III, the residual for performance-pay jobs, e p ijt , is e p ijt = d p t \u03b8 i + \u03bd p ij + \u03b5 p ijt ,(6) whereas the residual for non-performance-pay jobs, e n ijt , is e n ijt = d n t \u03b8 i + \u03bd n ij + \u03b5 n ijt .(7) The parameters of interest to be estimated are the variances of the six error components in (6) and (7). We estimate the model under the simplifying assumption that the idiosyncratic error terms \u03b5 p ijt and \u03b5 n ijt are uncorrelated over time. Following Parent (2002), we estimate the variance components by fitting regression models to all the cross products of residuals for the same individual. 23 This procedure is similar to the equally weighted minimum distance approach of Abowd and Card (1989), but provides an easy way of dealing with an unbalanced sample such as ours. We first report in Panel A of Table V the results estimated over the whole sample. One potential pitfall of using the whole sample is that some individuals are only observed on performance-pay jobs, whereas others are only observed on non-performance-pay jobs. As a result, the variance of the worker-specific effect \u03b8 i may not be the same in the two subsamples, and differences between the estimated variance components var(d p t \u03b8 i ) and var(d n t \u03b8 i ) may reflect composition effects related to \u03b8 i , as opposed to true differ- ences in the return to unobservables d n t and d p t . To control for this potential problem, we report in Panel B the results for the sub- sample of \"switchers\" who are observed on both performance-pay and non-performance-pay jobs. As a benchmark, we start with simple models in columns (1) and (4) where we do not include the variance component linked to the job match, and also constrain the variance components to be constant over time. We then add the job-match component in columns (2) and (5). In columns (3) and (6), we let the variance of the idiosyncratic terms \u03b5 p ijt and \u03b5 n ijt and the returns to the worker component (the factor loadings) d p t and d n t change over the five subperiods used in Table IV. The results in the two panels of Table V are very similar, but we focus the discussion on Panel B for the reasons mentioned above. Note that because a large num- ber of cross products are available (between 19,597 and 99,554 in the different models reported in Table V), the parameters are precisely estimated and are, unless otherwise indicated, statis- tically different for performance-pay and non-performance-pay jobs. The results show that, as expected, the worker-specific component \u03b8 i accounts for more of the variation of wages in performance-pay than non-performance-pay jobs. When the job-match term is included in columns (2) and (5), the estimated variances of the worker component are 0.102 and 0.053, respec- tively. The ratio of var(d p \u03b8 i ) and var(d n \u03b8 i ) is equal to the square of the relative returns in performance-and non-performance pay 23. See Parent (1999) for a related analysis for the NLSY, comparing piece- rate/commission workers and those receiving bonuses to salaried and hourly paid workers. More details on the identification and estimation of the variance compo- nents models are provided in the Web Appendix (Appendix 4).  476 32,476 32,476 19,597 19,597 19,597 Note. Standard errors in parentheses. Models in columns (3) and (6) allow the variance of the idiosyncratic errors and the factor loadings on the worker component to vary across the 1976-1979, 1980-1984, 1985-1989, 1990-1993, and 1994-1998 periods, whereas models in columns (1), (2), (4), and (5) do not. These equally weighted covariance structure models are fit to the cross products of the residuals of an OLS regression of log wages on the same set of covariates described in Table IV. Note that the factor loadings in columns (3) and (6) are normalized to 1 in the base period (1976)(1977)(1978)(1979), so that the changes in factor loadings can be interpreted as the percentage changes in the return to the worker component. jobs, implying that d p /d n is equal to 1.39. Hence, d p is 39% larger than d n . Also consistent with predictions, the results indicate that the variance of the job-match component is much smaller in performance-pay (0.002) than in non-performance-pay jobs (0.026). In intuitive terms, this suggests that the firm an in- dividual works for explains quite a bit of the wage variation in non-performance-pay jobs, but much less in performance-pay jobs. 24 The variance of the idiosyncratic term that represents year- to-year volatility in wages is slightly smaller in performance-pay than in non-performance-pay jobs, though the model does not have specific predictions in this regard. In columns (3) and (6), where the variance components are allowed to change over time periods, the factor loadings d p t and d n t grow for both performance-pay and non-performance-pay jobs. The variance of the worker component shown on the first row now refers to the variance in the base period (1976)(1977)(1978)(1979). Consis- tent with Baker (1997), the factor loading on the person-specific component increases over time. For both performance-pay and non-performance-pay jobs, the factor loadings in Panels A and B are 17% to 31% higher in 1990-1993 than in 1976-1979. Although the relative growth in the factor loadings d p t and d n t is not statistically different for the two types of jobs, the re- sulting growth in the variance associated with the worker-specific component is larger for performance-pay jobs because the vari- ance (and the corresponding factor loading) is larger in the base period. For performance-pay jobs, the variance grows from 0.067 in 1976-1979 to 0.115 (0.067 times the square of 1.312, the factor loading) in 1990-1993, a 0.049 increase. For non-performance-pay jobs, the variance grows from 0.036 to 0.062, a 0.026 increase.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "V.D. Robustness Checks", "text": "As discussed in Section IV, although our measure of perfor- mance pay is rather crude, the growth in performance pay is ro- bust to the way we measure it (Table II). The results for the wage equations described above are also highly robust to these mea- surement issues, and to a number of other specification choices. We present a detailed analysis of these robustness issues in the Web Appendix (Table B.1) and only summarize the main find- ings here. The focus of the robustness analysis is the difference in the returns to education between performance-pay and non- performance-pay jobs, which is the simplest and clearest way of showing the key difference in the wage structure between the two types of jobs. The first set of alternative specifications reported in the Web Appendix is based on alternative measures of performance pay such as the ones reported in Table II. We also look at what hap- pens when public sector workers are included, when richer sets of interactions are introduced, and when only the base wage (net of performance-based payments) is used as the dependent vari- able. A final estimator is based on a measurement error correction that accounts for the fact that we are more likely to misclassify performance-pay jobs as non-performance-pay jobs when we have only a few observations on a given job match. Using these alter- native specifications has little impact on the results. For instance, the average OLS estimate for the eleven additional specifications in Table B.1 is 0.0388, compared to 0.0365 in Table IV (column (3)). The average fixed effect estimate is also very similar (0.0141) to the estimate reported in Table IV (0.0165 in column (4)). A second piece of evidence in support of our main findings comes from the NLSY data. As in the case of the PSID, we run separate wage regressions for performance-pay and non- performance-pay jobs. We also exploit the fact that the Armed Forces Qualifying Test (AFQT) score, which is available in the NLSY, can be used as a proxy for unobserved productive charac- teristics. The results, reported in Table B.2 of the Web Appendix, show that returns to productive worker characteristics (education, experience, and the AFQT score) are larger in performance-pay than non-performance-pay jobs.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "VI. PERFORMANCE PAY AND INCREASING WAGE INEQUALITY", "text": "We now return to the main question addressed in this paper: what is the relationship between the growth in performance pay and wage inequality? We begin by presenting the results of simple decomposition, or accounting, exercises. We then discuss the in- terpretation of these results in light of the possible explanations of the growth in performance pay presented in Section II.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "VI.A. Reweighting Estimates", "text": "Quantifying the contribution of the change in a wage- determining factor such as performance pay to the wage distribution is a well-known problem in the inequality literature. For example, DiNardo and Lemieux (1997) contrast the observed change in the distribution of wages to the change that would have prevailed in the absence of unions. To do so, they use the reweighting approach of DiNardo, Fortin, and Lemieux (1996) to control for differences in the distribution of worker character- istics in the union and nonunion sectors. Just like a standard regression provides a way of adjusting differences in mean wages between two groups for differences in worker characteristics, the reweighting procedure allows to do so for any feature of the wage distribution, and not just the mean. To fix ideas, let PPJ be a dummy variable indicating whether a worker holds a performance-pay (PPJ = 1) or a non-performance-pay (PPJ = 0) job. Let X now represent all ob- servable characteristics (both the worker and job characteristics discussed earlier). Following DiNardo and Lemieux (1997), the counterfactual distribution of wages that would prevail if all workers were paid like workers in non-performance-pay jobs can be estimated by reweighting all non-performance-pay observa- tions using the reweighting factor \u03c8(X) = Pr(PPJ = 0)/ Pr(PPJ = 0 | X). The idea is very simple. Groups such as sales workers that are very likely to be paid for performance (Pr(PPJ = 0 | X) is low) will be underrepresented among non-performance-pay workers. So this group has to be given a larger weight to get a dis- tribution of non-performance-pay workers that is representative of the whole workforce. This is achieved using the reweighting factor \u03c8(X), which is large for this group because its denominator, Pr(PPJ = 0 | X), is low. It is easy to estimate the conditional probability Pr(PPJ = 0 | X) by running a simple probit or logit model for the probability of being paid for performance as a function of the observable characteristics X. 25 Before presenting the decomposition results, we first report some descriptive information on the trends in wage inequality to be explained. Figure V summarizes the changes in wage inequal- ity in our PSID data by showing the evolution of the standard deviation of wages (three-year moving average) in performance- pay, non-performance-pay, and all jobs between 1977 and 1996. In Figure V and the rest of this section, we follow DiNardo, Fortin, 25. We use a probit model with a specification more flexible than the one used in the wage equations reported in Tables III and IV. Relative to these specifications, we add a set of four education dummies that we also interact with potential experience (linear term), union status, and the race dummy. We also add a cubic in tenure, a dummy for full-time/full-year workers, and an interaction between potential experience and the race dummy.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "FIGURE V", "text": "Wage Inequality: PSID 1976PSID -1998 and Lemieux (1996) and weight observations by the numbers of hours worked during the year to get a representative distribution of wages paid over all hours worked in the labor market. As before, we also weight observations using the PSID sample weight. As expected, the figure indicates a substantial increase in in- equality over time concentrated over the 1980s. For example, the standard deviation of hourly wages for all jobs increased from a little under 0.50 in 1977 to around 0.60 in the early 1990s, before going down a bit in the mid-1990s. These changes are very similar to what has been documented using larger data sets such as the CPS or the U.S. Census. 26 More interestingly, the standard devia- tion in performance-pay jobs generally increases faster than that in non-performance-pay jobs. This suggests that performance-pay jobs are closely linked to the growth of wage inequality because (1) 26. See Katz and Autor (1999) for an overview of the main trends in inequality based on several different data sources. We have also compared the inequality trends in the PSID to those in the March CPS when the same measure of the hourly wage (annual earnings divided by annual hours) and the same sample restrictions (heads only, private sector, age 18-65, not self-employed, and hourly wages between $1.50 and $100.00 in 1979 dollars) are used. In the PSID, the standard deviation of log wages increases from 0.501 in 1976-1979 to 0.593 in 1990-1998 (a 0.092 increase). The corresponding numbers in the March CPS are 0.508 in 1976-1979 and 0.597 in 1990-1998 (a 0.089 increase). The fact that the results are so similar in the two samples gives us great confidence in the representativeness of the PSID data. inequality grew faster in performance-pay jobs and (2) the growing incidence of performance-pay jobs means that an increasingly large fraction of workers are employed in this more unequal sector. The main decomposition results are presented in Table VI and in Figures VI and VII. As before, the results are weighted using the PSID sample weights. Counterfactual distributions are obtained by multiplying the reweighting factor \u03c8(X) by the PSID sample weight. As in Section IV, we also use the number of times a job match is observed to adjust for the end-point problem. 27 Table VI shows that 21% of the increase in the variance of log wages can be accounted for by performance pay. More interest- ingly, the table also shows that most of the impact of performance pay is concentrated at the top end of the wage distribution. In par- ticular, performance pay accounts for only about 10% of the change in inequality at the bottom end of the distribution, as measured by the 50-10 gap (the difference between the 50th and the 10th percentile of log wages). 28 By contrast, performance pay accounts for a large fraction-if not all-of the growth in inequality at the very top end of the distribution (the 99-90 or 99-75 gap). This pattern is illustrated more dramatically in Figure VI, which shows the difference between the actual and counterfac- tual wage distribution at each wage percentile. 29 The striking feature of the figure is that the effect of performance-pay jobs is concentrated at the top end of the wage distribution. It is also clear that the effect becomes larger in the early 1990s than in the late 1970s. Like Table VI, the figure shows that, as predicted by the model of Section II, the impact of performance pay is highly concentrated at the top-end of the distribution. Figure VII then compares the growth in wage inequality that would have prevailed with and without performance-pay jobs, by showing the change in real wages at each percentile in the actual (with performance-pay jobs) and the counterfactual (without performance-pay jobs) wage distribution. Consistent with other studies, the figure shows that inequality grew faster in the top end than in the low end of the 27. We perform this adjustment using yet another reweighting factor to adjust the distribution of the number of job-matches (in both the 1976-1979 and 19901993 periods) so that it is equal to the observed frequency distribution for the 1982-1990 period. 28. The percentiles used to compute the measure of wage dispersion in Table VI (3) and (6) is the difference between columns (1) and (2) and columns (4) and (5)    wage distribution. 30 The figure also shows that a very large frac- tion of the growth in wage inequality above the 80th percentile can be accounted for by performance-pay jobs.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "VI.B. Variance Decomposition", "text": "Although it would be tempting to conclude from these decompositions that the growth in performance pay explains 21% of the growth in the variance of wages and most of the increase in inequality above the eightieth percentile, the conclusion is too strong for several reasons. For instance, the \"effect\" of performance pay documented above depends both on the fraction of workers in performance-pay jobs and on the relative effect of performance pay on the wage structure. The results reported in Table VI can reflect either the impact of changes in the fraction of performance-pay jobs, or simply the increase of the inequality-enhancing effect of performance-pay jobs over time. As discussed in Section II, the two possible channels have very different implications for the role of performance pay in changes in wage inequality. A simple way of clarifying these issues is to compute a variance decomposition of the type that has been used in the literature on unions and wage inequality. Consider a simpli- fied version of the wage equations for performance-pay and non-performance-pay jobs: w p = xb p + e p , and w n = xb n + e n . The overall variance of wages across all workers can be written as var(w) = V 1 + V 2 + V 3 , where V 1 = PPJ \u00b7 var(xb p | PPJ = 1) + (1 \u2212 PPJ ) \u00b7 var(xb n | PPJ = 0), V 2 = PPJ \u00b7 var(e p | PPJ = 1) + (1 \u2212 PPJ ) \u00b7 var(e n | PPJ = 0), V 3 = PPJ \u00b7 (1 \u2212 PPJ ) \u00b7 \ud97b\udf59 2 , and where PPJ is the fraction of workers in performance-pay jobs, whereas \ud97b\udf59 = E[x\u03b2 p + e p | PPJ = 1] \u2212 E[x\u03b2 n + e n | PPJ = 0] 30. Though the difference between the evolution of top-and bottom-end in- equality is particularly striking after the late 1980s (e.g., Autor, Katz, and Kearney [2006]), Table VI shows that the 90-50 gap expanded much more than the 50-10 gap (0.30 vs. 0.14) over our sample period. is the (raw) wage gap between the two types of jobs. The variance component V 1 captures how higher returns to observables among performance-pay workers contribute to wage inequality, whereas the variance component V 2 does the same for unobservables. The between-group component, or \"wage gap\" term, V 3 , captures the fact that a positive wage gap between performance-pay and non- performance-pay jobs also tends to increase wage inequality. Columns (1) and (4) of Table VII show the various components of the overall variance of wages in 1976-1979 and 1990-1993, respectively. 31 Columns (2) and (5) then show the counterfactual variance that would have prevailed if all workers had been paid according to the wage structure observed for non-performance- pay jobs. We do so by replacing the various components of the variance decomposition pertaining to performance-pay workers with the counterfactual components that would have prevailed if these workers had not been paid for performance. 32 Columns (3) and (6) show the \"effect\" of performance pay by just taking the difference between the two other columns. As indicated at the bottom of the table, performance pay ac- counts for 0.0290 of the 0.1361 growth in the variance of wages between 1976-1979 and 1990-1993. Most of the effect is linked to the impact of performance pay on observable determinants of wages. That component (row (3)) increases by 0.0152 (from 0.0093 to 0.0245) over time, which represents over half of the total effect. The wage gap term (row (7)) accounts for most of the remaining effect, whereas differences in the variance of the error terms (row (6)) play a more modest role. These findings are consistent with the results in Table IV and V. Table IV shows that the return to education is higher in performance-pay jobs and that this gap 31. We estimate the variance of the components xb and e by running standard regressions on the same variables used in Table IV, plus the additional interac- tion terms used to estimate the reweighting probits (four dummies in education interacted with experience, etc.). 32. Following DiNardo and Lemieux (1997), we do so using a minor modifi- cation of the reweighting procedure described above. We need to replace var(xb p | PPJ = 1) with var(xb n | PPJ = 1), var(e p | PPJ = 1) with var(e n | PPJ = 1), and E[xb p + e p | PPJ = 1] with E[xb n + e n | PPJ = 1] in the definition of the wage gap component, \ud97b\udf59. To do so, we reweight the non-performance-pay workers us- ing the reweighting factor \u03c8(x)/(1 \u2212 \u03c8(x)) to get the distribution of wages that would have prevailed among performance-pay workers had they been paid like non-performance-pay workers. The counterfactual term E[xb n + e n | PPJ = 1] is the mean of the resulting wage distribution, and var(xb n | PPJ = 1) and var(e n | PPJ = 1) are the explained and unexplained variances in a regression of wages on x in that counterfactual sample. has increased over time. Table V also shows that the return to the worker-component is higher in performance-pay jobs, though this is being offset by a larger idiosyncratic variance and a larger variance of the job-match term for non-performance-pay workers. On balance, performance pay does not have a large impact on the variance of the overall residual component (the sum of the three subcomponents reported in Table V).", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "VI.C. Interpreting the Decomposition Results", "text": "The results reported in Table VII also enable us to answer, at least in part, the question raised in Section II about how the estimated \"effect\" of performance pay on inequality should be interpreted. Under a first scenario illustrated in Figure I, the growth in performance pay is just due to an exogenous decrease in the cost of implementing performance-pay schemes. Under the alternative scenario explored in Figure II, performance pay grows as a result of some underlying SBTC that induces more employers to use performance pay, and also increases the return to skill in performance-pay relative to non-performance-pay jobs. If the first scenario was correct, then the contribution of per- formance pay to the growth in inequality should all be due to the increase in the share of workers paid more unequal wages (perfor- mance pay). This is inconsistent, however, with the fact that the effect of performance pay on the variance of wages of performance- pay workers linked to observables more than doubled from 0.0246 in 1976-1979 to 0.0529 in 1990-1993 (columns (3) and (6) of row (1) of Table VII). So even if the fraction of performance-pay workers had remained at 38% (1976-1979 level) over time, the variance contribution would have still increased from 0.0093 (0.38 times 0.0246) to 0.0201 (0.38 times 0.0529). This represents over two-thirds of the 0.0152 increase in the variance contribution (from 0.0093 to 0.0245) linked to observables shown in row (3). In other words, most of the \"effect\" of performance pay on wage inequality is due to the fact that returns to observable skills increased faster in performance-pay than in non-performance-pay jobs. 33 This is consistent with the scenario of Figure II, where 33. The wage gap effect can also be linked to this phenomena. Performance- pay workers are more skilled than non-performance-pay workers, so an increase in the return to skills results in a larger between-group gap and in more inequality. an increase in returns to skills induces more firms to adopt performance pay, but inconsistent with the simple story based on declining monitoring costs in Figure I. Thus, our preferred interpretation of the results is that performance pay provides a channel through which underlying changes in the relative produc- tivities of different groups of workers get translated into higher inequality. Of course, this interpretation still leaves a very important role for performance pay in recent changes in inequality. Regardless of why performance pay has increased over time, our decomposition results indicate that, absent performance pay, wage inequality would have increased substantially less, and much less in the upper end of the wage distribution. But much of this inequality change would not have happened either absent other underlying changes in the relative demand for skilled labor. It is in this sense that our findings should not be interpreted as the causal effect of the growth in performance pay on wage inequality where all other factors, including the relative demand for skilled labor, are held constant. Although some of the effect of performance pay on wage inequality may be due to exogenous developments linked to a decline in the cost of monitoring and information processing, our evidence suggest that this cannot account for most of the measured effect.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "VII. CONCLUSIONS", "text": "An increasing proportion of jobs in the U.S. labor market in- clude a performance-pay component in addition to regular wages and salaries. In this paper, we look at the connection between the growth of performance pay and wage inequality. The basic premise is that, relative to traditional (fixed-wage) jobs, wages on performance-pay jobs are more sensitive to productive charac- teristics of workers and less sensitive to job characteristics. We develop a simple model to illustrate this point and derive sev- eral testable implications. Consistent with predictions, we show that compensation in performance-pay jobs is more closely tied to both observed (by the econometrician) and unobserved pro- ductive characteristics of workers. As a consequence, wages are less equally distributed among performance-pay than among non- performance-pay workers. Building on these results we show that, in the absence of per- formance pay, the variance of males' wages would have increased by 21% less than it did between 1976-1979 and 1990-1993. In- terestingly, most of the impact of performance pay on the growth in inequality is concentrated at the top end of the distribution. We find that inequality above the eightieth percentile would have increased much more slowly in the absence of performance pay. This is a significant finding because most of the recent growth in wage inequality has been concentrated in this part of the wage distribution (Autor, Katz, and Kearney 2006). Our results also suggest that the growth in performance pay should not be thought of as an exogenous inequality-enhancing change in the labor market that is unrelated to other labor mar- ket developments. In particular, the fact that the returns to skill increased faster in performance-pay than in non-performance-pay jobs suggests that the growth in performance pay is, at least in part, an endogenous response by firms and workers to other un- derlying labor market developments. This is consistent with the view that performance pay provides a channel through which un- derlying changes in the relative productivities of different groups of workers get translated into higher inequality. Going beyond the issue of wage inequality, this paper suggests that performance pay is not merely a way of packaging pay, but is also an integral part of production that can enhance the quality or worker-firm matches. In the absence of performance pay, workers and firms have to engage in costly search before workers with spe- cific talents and abilities eventually get matched to the right job in the right firm. This has important consequences for the func- tioning of labor markets. For example, Shimer (2005) has shown that costly search may explain interindustry wage differentials and why labor markets in the long run may fail to be perfectly competitive. 34 We conjecture that future research will find that performance-pay systems also have a profound effect on wage dy- namics, career concerns, and the overall efficiency of competitive labor markets. 34. Efficiency wage models have often been cited as another potential expla- nation for interindustry wage differences. Even in this case, as MacLeod and Malcomson (1988) show, sorting of workers into ability groups is not instantaneous, but can occur slowly over time.  Table I. The proportions are adjusted by running a linear probability model for the incidence of performance-pay jobs on a full set of dummies for the number of times a job match is observed, and computing the predicted incidence holding the distribution of the number of times the job match is observed at its average value for the period 1982-1990.", "title": "QUARTERLY JOURNAL OF ECONOMICS Issue 1 PERFORMANCE PAY AND WAGE INEQUALITY *", "file_name": "Lemieux et al. - PERFORMANCE PAY AND WAGE INEQUALITY.pdf"}
{"section": "DeTeRmInAnTS oF GeoPolITICAl InFluenCe", "text": "By \"durable geopolitical influence,\" I refer to a state's ability to elicit enduring cooperation from other states, most explicitly through alli- ances, and, more broadly, to shape the inter- state system to its advantage through treaties and other negotiations. In its extreme form, scholars refer to such influence as \"world lead- ership\" ( Modelski and Thompson 1988) or \"hegemony\" (Arrighi 1994;Lachmann 2014).", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Military Accounts", "text": "A classic perspective posits the main source of geopolitical influence as physical force: states exercise influence by means of coer- cion (or the threat thereof ). This position is associated, in political science, with \"hege- monic stability theory\" ( Gilpin 1981Gilpin , 1987 and the \"long-cycle\" approach to world poli- tics (Modelski 1978;Modelski and Thompson 1988;Thompson 1988). It also finds implicit expression in historical and political sociology, as witnessed by accounts of mod- ern state formation as a process of competi- tive selection, whereby militarily successful states became models to which other polities conform at pain of extinction (Ertman 1997:4;Mann 1986:416-517;Tilly 1990). From this perspective, the major mecha- nism by which states exert influence is war- fare (Tilly 1990:181-91). Specifically, scholars seek to identify \"hegemonic\" (Gilpin 1981) or \"global\" (Thompson 1988) wars, the out- comes of which establish new geopolitical leaders (or hegemons). As Thompson (1988:6-7) puts it, \"Global wars . . . are wars fought to decide who will provide systemic leadership, whose rules will govern\" (see also Gilpin 1981:15;Modelski and Thompson 1988:19). These wars are preceded by a period of turmoil in the interstate system, a leadership vacuum in which the decline of a formerly dominant power has become irre- versible but no rival has yet replaced it. They are followed by a peace settlement restructur- ing the rules of the interstate system accord- ing to the dictates of the victorious party (Gilpin 1981:15;Modelski 1978:217;Thompson 1988:46).", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Economic Accounts", "text": "Other accounts identify economic accumula- tion as a critical source of geopolitical influ- ence (Arrighi 1990(Arrighi , 1994Arrighi and Silver 1999;Keohane 1984;Lachmann 2009Lachmann , 2014Wallerstein 1984). Associated with world- systems analysis, this perspective explains states' capacities in world politics with refer- ence to their position in the world economy. In the extreme case, a state that achieves simultaneous productive, commercial, and financial dominance \"can largely impose its rules and wishes,\" not merely in the world economy but also in the \"political, military, diplomatic, and even cultural arenas\" (Wallerstein 1984:38-39). Although military force is implicated in such accounts (Wallerstein 1984:42-44), the key mechanisms of geopo- litical influence are the material capacity to invest in institutions of global governance (Arrighi and Silver 1999:28;Keohane 1984:46) and world-economic structural dependency (Frank 1969). In other words, a state exercises influence to the degree that other states-or their ruling elites-are reli- ant on it for their own accumulation, along with the order and stability that facilitate it. Arrighi (1990,1994) and Lachmann (2009Lachmann ( , 2014 push this model the furthest. Arrighi synthesizes it with both military and cultural understandings of hegemony in a manner revealing an invariant conjunction between economic development and geopo- litical influence. Geopolitical leaders emerge amid a period of interstate turmoil. Against this backdrop of \"systemic chaos,\" the state that manages to reestablish order makes cred- ible its claim to leadership. A state's eco- nomic resources, however, are what enable it to reestablish order in the first place (Arrighi 1994:14-15). Such a state, unlike its competi- tors, has the material ability to satisfy the \"system demand for order\" (Arrighi 1994:31; see also Arrighi 1990:369;Arrighi and Silver 1999:28). As Arrighi (1994:145; emphasis added) explains Dutch influence in seven- teenth-century Europe: \"The more the Dutch succeeded in their endless accumulation of capital . . . the more this accumulation was turned into ever-growing capabilities to shape and manipulate the European political sys- tem.\" For Lachmann (2009,2014), by con- trast, a state's economic resources are less important. What matters instead is the struc- ture of its elites, which determines a state's capacity to mobilize whatever resources it enjoys. Yet economic (and military) mobiliza- tion itself remains the major means of geopo- litical influence (Lachmann 2009(Lachmann :56-57, 2014.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Cultural Accounts", "text": "As military and economic accounts already acknowledge, material capabilities are necessary rather than sufficient conditions of durable geopolitical influence (Arrighi 1990:404;Kennedy 1988:xix-xx;Keohane 1984:34-35). This, Keohane (1984:38-39) suggests, is because military and economic factors explain states' \"ability\" to project power, but they cannot explain states' \"will- ingness\" to do so (see also Arrighi 1990:404;Gilpin 1987:126-27). It is also, says Gilpin (1981:48), because the \"hierarchy of pres- tige\" lags the \"balance of power\" in geopoli- tics. Nevertheless, Gilpin (1981:30) is clear: \"Ultimately . . . the hierarchy of prestige in an international system rests on economic and military power.\" And \"willingness,\" for Keohane (1984:34-35), simply adds a degree of indeterminacy. Recent International Relations (IR) theory, however, suggests that putative matters of will may ultimately conceal additional capac- ities that remain invisible because they are irreducible to military and economic capaci- ties. From this perspective, \"prestige,\" has its own determinations exceeding the lagged effects of material distributions. This approach emphasizes the symbolic aspects of external state power (Adler-Nissen and Pouliot 2014;Ashley 1984;Cox 1983Cox , 1987Keene 2014;Nye 2004;Pouliot 2016;Rupert 1995;Williams 2007Williams , 2013. Converting material (and other) resources into actual influence requires that states conduct themselves in ways that are legitimate-socially recognized by a com- munity of states (Ashley 1984:259;Pouliot 2016:57, 66;Williams 2007:33;cf. Meyer et al. 1997;Suchman 1995). Less clear, however, are the conditions under which states succeed or fail in securing recognition. Nye's (2004) widely used con- cept of \"soft power,\" for instance, rests on the attractiveness of a country's culture and insti- tutions. Nye, however, rarely considers why different cultures and institutions attract dif- ferent external audiences in different socio- historical contexts. 1 Neo-Gramscian theorists, by contrast, identify overly stringent condi- tions: international hegemony \"can be founded only by a country in which [domes- tic] social hegemony has been or is being achieved\" (Cox 1987:149), because a state's internal hegemony makes credible its claim to external leadership (Cox 1983:171;Rupert 1995:2). This account generalizes from the singular experience of U.S. global power in the post-World War II period, the only instance in which a country has ever enjoyed interclass and interstate hegemony on the same social foundations (Lacher and Ger- mann 2012). 2 A recent \"practice turn\" in IR maintains that recognition can never be (fully) explained in advance because it is always (partly) a situ- ational accomplishment of diplomatic agents themselves (Adler-Nissen and Pouliot 2014;Pouliot 2016; for \"practice theory,\" see Adler and Pouliot 2011). As Adler-Nissen and Pouliot (2014:909) put it, \"Power in practice is a performance whose processes often depart from the pre-existing distribution of exoge- nous resources.\" While acknowledging \"dynamics that are common to different sites,\" Pouliot (2016:65) insists that \"ultimately the politics of [diplomatic] competence is very local. The play of practice is eminently contin- gent.\" Practice theorists thus foreground the performative dimension of power: \"the situ- ated effectiveness of acts themselves as mov- ers in the world\" (Reed 2013:207). Without denying the creativity of diplo- mats, this article develops the other side of the theoretical coin: the degree to which prac- tices and their efficacy are themselves struc- turally conditioned. If it is possible to distinguish, by way of speech-act theory, between \"constative\" and \"performative\" social acts (Reed 2013:201-202), then it is also possible, following Bourdieu (1991:73), to ask about the \"conditions of felicity\" of performatives themselves, the conditions under which a performance works. 3 Indeed, although recent statements of practice theory emphasize the emergent power of situated interactions (Adler-Nissen and Pouliot 2014;Pouliot 2016), a key empirical contribution of the practice turn explains the failure of diplo- matic performances in terms of \"hysteresis effects\" (Neumann and Pouliot 2011), point- ing to the acquired histories that shape practice and its perception in the first place (Pouliot 2010). It is this insight that I develop here.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Habitus, Sovereignty, and Geopolitical Influence", "text": "Consistent with recent cultural approaches to geopolitics, influence is never only a product of states' military and economic capacities (plus an increment of will); it is also a product of states' symbolic capacities to elicit recog- nition from their rivals. I argue that states are liable to convert their material capacities into durable influence to the degree that their agents embody a habitus that is congruent- homologous-with the habitus embodied by agents of rival states. Systems of socially acquired, \"durable, transposable dispositions,\" habitus disposes agents to certain ways of acting, perceiving, and classifying (Bourdieu 1990(Bourdieu :53, 57, 2000. Consequently, agents whose habi- tus is congruent with their interlocutors tend to secure the latter's recognition. As Bourdieu (1990:59; emphasis in original) explains, \" [U]ndertakings of collective mobilization cannot succeed without a minimum of con- cordance between the habitus of the mobiliz- ing agents (prophet, leader, etc.) and the dispositions of those who recognize them- selves in their practices or words.\" A precon- dition for \"mobilization,\" such recognition is thus a means of influence: \"[H]abitus is . . . the potential energy, the dormant force, from which symbolic violence, and especially that exercised through performatives, derives its mysterious efficacy. It is also the origin of that particular form of symbolic efficacy, 'influ- ence'\" (Bourdieu 2000:169; emphasis added). In the context of diplomacy, I posit two key dimensions of congruent habitus, each of which promotes recognition (which promotes influence). First, as in any field, agents with a well-adjusted habitus tend to execute perfor- mances perceived as competent. Such social skill (Fligstein 2001) enrolls support and cooperation, which tends to the advantage of the materially dominant state (all else equal). As Adler-Nissen and Pouliot (2014:894; emphasis in original) have shown, \"power involves a socially recognized competence or mastery. . . . Being so recognized typically allows one to wield a form of endogenously generated power often called influence.\" Of course, diplomacy involves agents of a specific kind of principal: a sovereign state (or person). Second, diplomats (and sover- eigns) with a congruent habitus tend to share perceptions about the very nature of sover- eignty. Sharing categories of sovereignty, such agents are predisposed toward a com- mon set of stakes. In short, they enjoy a soli- darity owing to their investment in a common game, a shared illusio (Bourdieu 1990:66-67;cf. Williams 2007:34-36). 4 All else equal, solidarity tends to the advantage of the mate- rially dominant state. Figure 1 presents this model in schematic form. To summarize: states exercise durable geo- political influence to the degree that (1) they enjoy significant military and economic capacities and (2) their agents embody a habi- tus homologous with their foreign counter- parts. 5 Such states thereby influence other states to a degree that neither a materially dominant but \"unrecognized\" state, nor a rec- ognized but materially weak state, would be able to achieve.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "AnAlyTIC STRATeGy", "text": "To assess this model, I pursue a historical analysis of European geopolitics from the late seventeenth to the late eighteenth centuries. In particular, I compare two theoretically rele- vant European polities: France and England (after 1707, Britain). 6 By the standards of both military and economic models, Britain should have exercised preponderant geopolitical influence in eighteenth-century Europe. Yet France was more influential, despite its weaker military and economic capacities. With respect to military and economic models, the eigh- teenth-century European interstate system is an anomalous or \"negative\" case (Emigh 1997). As such, it belongs to a larger class of cases characterized by discrepancies between material capabilities and patterns of influence, which arguably includes world politics today. As with negative case methodology in general (Emigh 1997(Emigh , 2009Riley 2003;Riley and Fern\u00e1ndez 2014;Wilson 2011), my aim is not to \"falsify\" existing theories; it is rather to extend the explanatory range of the literature to account for anomalous cases. The analysis proceeds in two steps. First, I establish the puzzle to be explained-the dis- junction between material dominance and geo- political influence in eighteenth-century Europe. In so doing, I rely on existing economic and diplomatic histories. I also present a network analysis of interstate treaties. Rather than advancing a network-based explanation or oth- erwise making causal inferences at this junc- ture, I use such quantitative data for a descriptive purpose (Spillman 2014): to help specify an outcome that demands subsequent qualitative explanation (for a useful example, see Fishman and Lizardo 2013). Second, I explain this outcome with an intensive historical analysis of causal mecha- nisms (Steinmetz 2004). I use as my primary evidence the correspondence of French and British diplomats with their respective admin- istrations, as preserved in French and British archives and collected in published works. I also conduct a prosopographical analysis of the British diplomatic service between 1660 and 1789 to illuminate the social condition- ings of British diplomats.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "eConomIC AnD mIlITARy PoweR In eIGhTeenTh- CenTuRy euRoPe", "text": "Both France and Britain were well positioned to exercise geopolitical influence in eigh- teenth-century Europe, given their material capabilities. However, Britain was dominant, attaining a position of military and economic primacy with respect to all other European states, France included.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Economic Power", "text": "Economically, England was the first area in Europe to develop fully-fledged capitalist property relations (Allen 1999;Brenner 1985;Lachmann 2000). Consequently, historians maintain that by the eighteenth century, if not earlier, England enjoyed the most efficient and productive agrarian sector in Europe, surpassing even the Netherlands, the eco- nomic hegemon of the seventeenth century (Allen 2000;Coleman 1977:197-200;Crouzet 1990:65-66). Table 1 shows esti- mates of agricultural output per capita in dif- ferent regions of Europe between 1600 and 1750. Allen (2001) calculates that among European countries, real wages (a rough indi- cator of living standards) increased only in England during the early modern period, declining dramatically almost everywhere else. This meant that already by the eigh- teenth century, Britain enjoyed a significantly higher standard of living than most of conti- nental Europe, evidence of its higher agricul- tural (and increasingly manufacturing) productivity (Allen 2001:427-32). With 20 million inhabitants, France was the most populous country in Europe and the wealthiest in aggregate throughout this period (Kennedy 1988:88;Lachmann 2009:51). For this reason, France maintained a larger tax base than any other European state between the mid-seventeenth century and 1789 (Lachmann 2000(Lachmann :168, 2009. However, the French agrarian sector suffered stagnation (if not outright decline) between the mid-seven- teenth and mid-eighteenth centuries (Brenner 1985:318;Crouzet 1990:14, 63;Le Roy Ladurie 1975:395). The result was a recurrent pattern of subsistence crisis, which severely hindered the state's capacity to mobilize its resources (Crouzet 1990:36). Economic accounts of geopolitical influ- ence place particular emphasis on commercial power. In this regard, too, Britain had seized the dominant position in global trade by the early eighteenth century (Adams 2005:17580;Arrighi 1994:210-12;Crouzet 1990:16;Modelski 1978:221), constructing the world's largest colonial empire over the next several decades (Kennedy 1988:138-39;Simms 2009). France remained Britain's major com- petitor in terms of commercial power (and colonial plunder). Taken together, however, the evidence supports the conclusion that Brit- ain \"led the world economy in its eighteenth- century phase\" (Lachmann 2000:169).", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Military Power", "text": "The same pattern holds in the military sphere. The English \"financial revolution\" (the emer- gence of public credit), coupled with the bureaucratization of its fiscal administration, enabled Britain to surpass the military capac- ity of any other state in eighteenth-century Europe (Brewer 1989;Carruthers 1996;Kennedy 1988:79-96). The British navy attained mastery of the seas between 1692 and 1713 (Crouzet 1990:77;Modelski 1978:225). Modelski and Thompson (1988:68-70) have shown that Britain maintained a greater num- ber of warships than any other navy in the world for every year between 1702 and 1799. Figure 2 presents these results as 10-year averages, alongside those for Europe's other major navies, from 1670 to 1799. Because it takes sea power as its primary metric (prior to the advent of aerospace technologies), the \"long-cycle\" approach to global politics thus regards Britain as the undisputed world leader in the eighteenth century (Modelski 1978;Modelski and Thompson 1988;Thompson 1988). But Britain's capacity to put soldiers in the field was unparalleled as well, at least at decisive moments. In 1710 to 1711, during the War of the Spanish Succession, Britain \"supported armies totaling 170,000 troops . . . while France, a nation with almost four times the population, managed to amass an army of about 150,000 troops\" (Carruthers 1996:15). As in the economic sphere, France was Europe's other major military power. On aver- age, France maintained a larger standing army than did Britain or any other European state throughout the later seventeenth and eight- eenth centuries (Kennedy 1988:79). Despite this, British (and British-funded) soldiers repeatedly outmatched their French rivals in major continental and colonial warfare. Take the two largest conflicts to fall within my period of focus. During the War of the Spanish Succession (1702 to 1714), the British-led coalition fared significantly better than its French opponent (B\u00e9ly 1990:35-38;Kennedy 1988:102-106), and the Seven Years' War (1756 to 1763) saw Britain decisively defeat France (Kennedy 1988:114-15;McKay and Scott 1983:199-200;Simms 2009:422-500). In summary, by the eighteenth century, Britain had attained an unparalleled military and economic position in Europe. Providing conclusive support for this trend, Kwon (2011) generated an index that interacts com- mon metrics of economic and military power: it shows that Britain was the most dominant state in the world system, in material terms, from 1708 onward. 7 Especially significant, during the entire period between 1723 and 1777 (and again after 1782), Britain's score on this index is actually twice that of its near- est competitor (see Kwon 2011:603). This is a clear demonstration of material primacy. Judged by its capacity for coercion and accumulation, then, Britain was better suited than France or any other state to exert durable geopolitical influence in eighteenth-century Europe. Nevertheless, France exerted geopo- litical influence more effectively and durably than did Britain.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "GeoPolITICAl InFluenCe In eIGhTeenTh-CenTuRy euRoPe", "text": "Treaties A network analysis of treaties concluded between 1661 and 1785 offers preliminary support for my conclusion. I compiled the network from a comprehensive reference guide to international treaties, published by the Consolidated Treaty Series (Parry 1979(Parry - 1986). 8 Nodes represent contracting parties (states and other actors with the sovereign power to sign treaties). Ties represent treaties. Specifically, a tie is a discrete treaty signing: I cumulate ties to generate networks of all parties that were connected by treaty at any time within successive 25-year intervals. 9 (Different specifications of the intervals yielded similar interpretations.) Typical cate- gories of treaty are alliance, peace, com- merce, marriage, cession or partition of territory, and unknown. 10 Although the treaty network does not directly capture influence, beginning with a global overview of treaties sensitizes us to likely influence patterns that qualitative anal- ysis can then confirm by exploring the con- tents of interstate agreements. For one, the literature on hegemony maintains that hegem- onic states institutionalize their privileged position in treaties (Arrighi 1994:14-15;Gilpin 1981:15;Modelski 1978:217;Thompson 1988:46). Thus, a state occupying an exceptionally central position in a treaty net- work is likely to be exercising hegemonic- style influence. Treaty networks are also a recognized proxy for overall status hierar- chies (e.g., core, semi-periphery, periphery) in both the world-systems and IR literatures ( Keene 2012Keene , 2014Snyder and Kick 1979). Status is obviously not the same as influence, but they are highly correlated by most accounts. Finally, it is worth considering what a treaty actually is: the material presence of international law, \"commonly taken by law- yers as the primary and most reliable source of international rules and obligations\" (Keene 2012:475). Indeed, this was especially true in the eighteenth century, because the lack of intergovernmental organizations (IGOs) meant that virtually all international legal precedent was set by state-to-state agree- ments (see, e.g., Dhondt 2015; Keene 2012). In the eighteenth-century context, a state's treaty-making activity is a good measure of its contribution to the rules and institutions of interstate politics. Specifically, I measure states' degree cen- trality in the treaty network. Degree centrality is defined as \"the number of ties incident upon a node\" (Borgatti 2005:62;cf. Freeman 1979), that is, a simple count of a node's ties. The degree centrality of a node thus shows the extent of its treaty partners in a given interval. As such, it shows the socio-spatial reach of a state's engagement with international law. 11 Table 2 depicts states' centrality scores (num- ber of ties) at each interval. I include every state that ranked in the top five on degree centrality during at least one interval. France enjoyed the most ties at every inter- val except 1686 to 1710. The latter was the period of major coalition wars against Louis XIV (see the case study below). Clearly, France's desirability as a treaty partner suf- fered in consequence. Equally clearly, France fully recovered its treaty-making capacity at war's end. Not only was France the most central actor at every interval from 1711 to 1785; the extent of its lead grew progressively: France and the runners-up were roughly equal in 1711 to 1735, but France had almost 50 percent more ties than the second-ranked actor in 1736 to 1760 and twice as many in 1761 to 1785. To the extent that the wars against Louis XIV aimed to dislodge France's privileged position in the interstate system, the treaty network suggests they failed. Britain, too, had a relatively large number of ties. Significantly, however, its dominance was nowhere near that of France. Britain had more ties than France in only one interval (1686 to 1710), when it was involved in the wars against Louis XIV. Britain also ranked behind Austria during most of the eighteenth century. Although Britain was the second-ranked actor in the last period-its highest rank-it enjoyed merely half the ties of France. Again, the most striking finding is France's lead over all other actors in terms of treaty partners. The treaty network suggests France enjoyed a high level of geopolitical influence in eighteenth-century Europe, likely higher than Britain's. However, it cannot rule out the possibility that Britain rivaled France for influence, as Britain's centrality was by no means negligible. ", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Alliances", "text": "To explore the extent to which French and British treaty-making activity elicited mean- ingful cooperation from other states, I con- sider the principal alliances in which each was involved. France's most consistent allies in the eighteenth century were two major powers: Spain and Austria. France and Spain were allied from 1700 to 1715, intermittently over the next two decades, and continuously from 1733 to 1748 and 1761 to 1793 (L\u00f3pezCord\u00f3n Cortezo 2003;McKay and Scott 1983:146-47, 253-54). France and Austria were allied between 1735 and 1741 and con- tinuously from 1756 to 1792 (Black 1986:3132;Cesa 2010:176-210). France was unquestionably the dominant party in the former case: Spain entered France's wars against Britain in 1761 and 1779 even though it had little to gain from these conflicts ( L\u00f3pez-Cord\u00f3n Cortezo 2003:194-95;McKay and Scott 1983:199, 260). By contrast, Britain's most consistent ally was the geopolitically marginal Portugal, and Anglo-Portuguese cooperation was limited to the commercial sphere (Black 1988:595-96). Britain's second-most consistent ally was the Dutch Republic, with which it was allied between the late seventeenth and mid-eight- eenth centuries (Cesa 2010:85-118). How- ever, Britain's influence over its Dutch ally diminished to the point that in 1739, the Dutch remained neutral in Britain's war with Spain, despite their treaty obligations. Later on, the Dutch were to permanently abandon Britain, remaining neutral in the Seven Years' War (1756 to 1763) and actually fighting against Britain in the early 1780s (Black 1986:72). In effect, Britain had no allies at all in the late 1730s and early 1740s and for the entire period from 1763 to 1787, a fate to which eighteenth-century France was never subject (Black 1986:21-22, 39;McKay and Scott 1983:161, 216-17;Roberts 1970). Signifi- cantly, multiple British ministries tried-and failed-to secure allies during these periods (McKay and Scott 1983:216;Roberts 1970). As Baugh (1998:24) describes the latter period, \"Britain's European diplomacy was an exercise in futility\" (see also Black 1986:90). Thus we cannot attribute Britain's isolation to an intentional strategy of avoiding the con- straints of durable alliances, as the theory of Britain as a geopolitical \"balancer\" might sug- gest (Sheehan 1989:124). France acquired, maintained, and influenced its allies more effectively than did Britain in eighteenth-cen- tury Europe, and France's allies were them- selves more powerful than those of Britain.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Negotiations", "text": "Beyond the character of their alliances, states' geopolitical influence is evidenced by their ability to shape the outcomes of treaty nego- tiations to their advantage. From the later seventeenth century until the Revolution, France's diplomatic negotiators were widely viewed as the most effective in Europe (Black 2010:66;Kugeler 2006:249, 256-58;Roosen 1976;Scott 2007:70-72). This was not a reputation the British shared (Black 1986(Black :109, 2001:53). Indeed, French diplomats, unlike the British, repeatedly won gains from trea- ties that outweighed their military's perfor- mance in war. During the War of the Spanish Succession, for example, Britain defeated France on the battlefield. Yet at the Congress of Utrecht (1712 to 1713), which ended the war, France avoided concessions. In fact, the French Bourbon dynasty acquired the Span- ish crown, precisely what Britain and its allies had waged the war to prevent. The War of the Polish Succession (1733 to 1738) was a victory of even greater scope for French diplomacy. France dictated the terms of peace, primarily through its negotiations with Austria rather than its military victories. As a result, France gained hegemonic influ- ence in the Holy Roman Empire, secured an inheritance claim to the duchy of Lorraine, and acquired Naples and Sicily for another branch of the Bourbons (Black 1986:33;Dhondt 2015:483, 488;McKay and Scott 1983:149-53). To be sure, Britain had the upper hand at the Treaty of Paris (1763), which concluded the Seven Years' War: the peace ceded most of France's North American empire to Britain (McKay and Scott 1983:199-200, 253). Yet British influence was short-lived. The war's end marked the commencement of Britain's diplomatic isolation on the continent, which endured until the 1780s (Roberts 1970). This allowed France to avenge itself in the Ameri- can Revolutionary War (1775 to 1783): that Britain was the one making concessions at the Treaty of Versailles in 1783 owed a lot more to diplomatic isolation than to military weakness (Roberts 1970:39-41;Simms 2009:615-61). Britain's major attempts to act as a third-party mediator also ended unsuccessfully. These events show, again, that Britain's lack of Euro- pean influence was not a product of willful isola- tionism. Britain sought and failed to mediate an end to the War of the Polish Succession in 1735 (Black 1986:21-22). Similarly, Britain's effort to influence constitutional issues in the Holy Roman Empire during the early 1750s alienated both Austria and France, and it may have con- tributed to their long-term realignment against Britain later that decade (Baugh 1988:45;Black 1986:55-56;McKay and Scott 1983:184). By contrast, France successfully mediated an end to an Austro-Russo-Turkish war in 1739, which further elevated its prestige in central Europe (Dhondt 2015:491;Kennedy 1988:108;McKay and Scott 1983:157). As Frederick the Great could claim as late as the 1740s, France was \"the arbiter of Europe\" (quoted in McKay and Scott 1983:151). France thus outperformed Britain in negotiation as well.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Summary", "text": "The evidence suggests France exerted much more durable influence in eighteenth-century European geopolitics than did Britain. Eigh- teenth-century Europe thus constitutes a puz- zling case given the military and economic conditions cited by dominant theories. Although French influence is not itself puz- zling (France was a major military and eco- nomic power), military and economic conditions cannot explain the discrepancy between French and British influence. That is, they cannot tell us why France exerted more influence than Britain, given Britain's superior military and economic capacities. The most compelling way in which mili- tary and economic accounts deal with such discrepancies is to propose that material power determines influence via the prestige mecha- nism. The latter works with a lag because prestige represents agents' perceptions of material power rather than its objective distri- bution, and perceptions change slowly. Hence the need for rising military and economic powers to fight \"hegemonic wars,\" which rea- lign the hierarchy of prestige with the new balance of power (Gilpin 1981:30-34, 48-49). In the case of British hegemony, Gilpin (1981:134-35) identifies the Napoleonic Wars (1801 to 1815) as just such an event. On this account, the eighteenth century was simply an \"interregnum\" in which Britain had not yet fought the necessary war to institutionalize its dominance (Arrighi 1994:164). This account has one major weakness, however. The eighteenth century already wit- nessed what, judged by both its scale and the extent of Britain's military success, is appro- priately classified as a hegemonic war: the War of the Spanish Succession (1702 to 1714). Indeed, on precisely these grounds, the long-cycle approach codes it as a \"global war\" that secured Britain's \"world leader- ship\" (Modelski 1978:221;Modelski and Thompson 1988:16;Thompson 1988:46). But if long-cycle theorists are correct that the Spanish Succession was a global war that Britain won, and yet Arrighi, Gilpin, and vir- tually all other commentators are correct that British hegemony had to wait another cen- tury, then \"prestige\" (as lagged effect) remains an insufficient means to account for influ- ence. Put differently, we must identify sources of prestige that are external to the lagged effects of material distributions (for a similar point, see Keene 2014:661-62).", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "exPlAInInG GeoPolITICAl InFluenCe In eIGhTeenTh- CenTuRy euRoPe", "text": "The remainder of this article advances an alternative explanation of geopolitical influ- ence in eighteenth-century Europe, consistent with my model of symbolic capacities and their dispositional conditions. I identify two dimensions of (in)congruent habitus bearing on French and British influence: (1) the (in)competence of diplomatic performances and (2) the (il)legitimacy of diplomatic stakes. Congruence on each dimension confers recog- nition, which-backed by military and eco- nomic resources-confers influence. Here I use the documentary evidence to establish, first, that French diplomats secured recogni- tion from their counterparts on each of these dimensions, whereas the British did not. Then, I present a case study to show in further detail how recognition mattered for influence.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "CouRTly ComPeTenCe", "text": "In the course of the sixteenth and seventeenth centuries, the hierarchically embedded and territorially overlapping jurisdictions of feu- dal Europe gave way to a system of increas- ingly bounded and centralized (if administratively non-uniform) states (Adams 2005;Anderson 1974;Teschke 2003). Because most of these states were ruled by dynastic monarchies, royal courts became their principal locus of government (Elias 2000). European diplomacy thus became an extension of the court. Whereas it was not uncommon in the sixteenth and early seven- teenth centuries for clerics, minor nobles, and even merchants to fill high diplomatic posts, from the later seventeenth century the diplo- matic services of Austria, Spain, France, Rus- sia, most German, and some Italian principalities became the preserve of the court nobility (B\u00e9ly 1990:291-329;Kugeler 2006:161-62, 194;Scott 2007:72-75). By the eighteenth century, \" [d]iplomacy was tied to a European court culture based on social status and aristocratic manners\" (Kugeler 2006:162). Diplomatic ceremonial was an integral part of court ceremonial, par- ticipation in court the principal means of dip- lomatic training (B\u00e9ly 1990:363, 392;Scott 2007:62). Proper courtly manners were criti- cal to the tasks of diplomatic representation and negotiation (Mori 2010:6). Successful diplomatic missions also depended on diplo- mats' ability to gather information, and this required social contacts at court (B\u00e9ly 1990:391;Black 2001:142). Throughout con- tinental Europe, courtly etiquette was the rec- ognized standard of diplomatic competence.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "France", "text": "France adhered fully to the courtly mold. From the 1660s until 1789, France congre- gated the upper ranks of its nobility in Europe's largest and most prestigious royal court (Duindam 2003;Elias 2000). French diplomats sprang from the court as well. Between the mid-seventeenth and eighteenth centuries, the French diplomatic service was predominantly (and increasingly) recruited from court society, consistent with the Euro- pean trend (Baillou 1984:306;B\u00e9chu 1998;Roosen 1973). The social positions of French diplomats were thus aligned with the courtly field in which eighteenth-century diplomacy was embedded. What of their dispositions? That which goes without saying under nor- mal conditions, the incorporated schemes of habitus are necessarily difficult to textually document. French diplomatic habitus is most visible in circumstances where it was out of place. One such circumstance involved Rather than reversing this judgment, Brit- ish diplomats agreed that their French coun- terparts embodied courtliness, which they enviously associated with diplomatic success. As the earl of Chesterfield observed in the mid-eighteenth century: \"A French [diplomat] . . . has not been six weeks at Court, without having, by a thousand little atten- tions, insinuated himself into some degree of favour with the Prince, his wife, his mistress, his favourite and his minister. He has estab- lished himself upon a familiar and domestic footing in a dozen of the best houses where he has accustomed the people to be not only easy, but unguarded before him\" (quoted in Roosen 1970:318). At the Congress of Aix-la- Chapelle in 1747, the earl of Sandwich feared that the French plenipotentiaries \"may have had skill\" in playing the Spanish against the British, attributing such skill to \"their refine- ment\" (NA State Papers [SP] 84/424:26). That British diplomats appreciated the courtly skill of French diplomats need not contradict their inability to imitate it in practice. On the contrary, they were all the more likely to ver- balize it because it was foreign to them (Mori 2010:18). Continental elites shared this esti- mation of the French. In the 1750s, the secre- tary to the Austrian empress Maria Theresa praised the French ambassador at Vienna for his \"gentleness,\" writing that \"M. d'Hautefort continues each day to gain [supporters] to his side with his good manners [bonnes fa\u00e7ons]\" (Schlitter 1899:28, 33). Indeed, according to Roosen (1976:77) and Kugeler (2006:249, 257-58), French diplomats owed their reputa- tion as skilled negotiators to the fact that French diplomatic etiquette and European courtly etiquette were one and the same. Reinforcing their courtly orientation, the instructions that French diplomats received from their principals stressed the personali- ties, manners, and ceremonies of the courts to which they were accredited, often devoting more attention to these issues than to the offi- cial purpose of their mission (Baillou 1984:224-27, 247-48). Such instructions were penned by the French secretaries of state, themselves prominent courtiers. In a context where interstate diplomacy was largely inter-court diplomacy, the courtly habitus of French diplomats thus allowed them to execute competent performances, the first basis of interstate recognition posited earlier. As suggested by the comments of their British and Austrian rivals, the competence of French diplomats is attested by the fact that they tended to become close confidants of their host sovereigns at the major courts of eighteenth-century Europe. During the 1740s and 1750s, for instance, the marquis de Ch\u00e9- tardie was the favorite of Empress Elizabeth of Russia. The marquis de Valori maintained a personal correspondence with Frederick II of Prussia around the same time. During his 18-year tenure as ambassador (1759 to 1777), the marquis d'Ossun was a favorite at the Spanish court of Charles III. The marquis de Gournay, also ambassador to Madrid (1705 to 1709), even served as the chief adviser to Spain's Philip V. 13 Further evincing the recognition of French diplomatic competence, continental diplo- matic services tended to imitate French prac- tices. Late seventeenth-and eighteenth-century European diplomats overwhelmingly looked to French precedents in matters of protocol ( ", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Britain", "text": "In stark contrast to the continental trend, the early modern English state became progres- sively distanced from the courtly field after the mid-seventeenth century. By the start of the eighteenth century, Parliament was replac- ing the court as the locus of English (then British) government (Brenner 1993:714;Ertman 1997:187-223;Pincus 2009:305-365). Accordingly, the attraction of the court declined for British elites, both because Par- liament's fiscal supremacy restricted courtly splendor and patronage (there were fewer positions available), and because Parliament itself became a more effective lever of state power (Bucholz 1993:11). Diplomatic recruitment followed suit. Again in contrast to the general trend, the English/British diplomatic service became considerably less embedded in court society between the later seventeenth and eighteenth centuries. I coded all diplomatic agents for- mally accredited by the English (after 1707, British) crown between 1660 and 1789 (N = 460) based on whether they belonged to the social milieu of the English/British court (for determining courtly membership, see the methodology in Duindam 2003:45-51). Between 1660 and 1689, 62 percent of all English diplomats belonged to the court. Between 1690 and 1789, a mere 32 percent of British diplomats were members of court society. Table 3 shows these results. To the extent that they lacked social condi- tioning in the world of the court, British dip- lomats lacked a courtly habitus. The earl of Portland, a Dutch native representing a par- ticularly weighty English embassy to France in 1698, described the court of Versailles to his king William III as such: \"I must admit, Sire, that it is impossible for anyone to come suddenly to this Court to find his bearings, and your Majesty says rightly that it is utterly unlike anything I have ever seen, and utterly foreign to my habits and disposition\" (NA SP 8/18:59; emphasis added). British diplomats' courtly unease was compounded by their instructions, which devoted little attention to court customs. As a study of the late seventeenth-century English diplomatic service found, diplomats' instruc- tions \"were businesslike documents. . . . Unlike the French, the English documents told the recipient little of the political lean- ings and personal characteristics of the men he was likely to meet during his mission\" (Lachs 1965:19). This pattern seems to have held during the eighteenth century: British diplomats frequently complained that they were insufficiently informed of their host courts (Black 1986(Black :95, 2001 19; emphasis added). In other words, paying the French princes two visits while receiving only one undermined the equality of rank between ambassadors and princes. Ambassadors could claim equality with princes because they represented the sov- ereignty of the prince who sent them. As this example shows, sovereign representation was a performative accomplishment (cf. Pouliot 2016): by failing to uphold his sovereignty vis-\u00e0-vis princes, Portland was inadvertently setting a new precedent (\"which might be aledged for the Future\"). 14 Understandably, this aroused the ire of his counterparts.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "PATRImonIAl STAKeS", "text": "If the rise of dynastic monarchies inculcated European diplomats into a common courtly culture, it also meant that diplomats (and their principals) converged on categories of practice with regard to sovereignty itself. Eighteenth-century monarchs treated jurisdic- tion as a form of property, typically inherited according to rules of male primogeniture (B\u00e9ly 1999). And they were not alone: a vari- ety of privileged corporate groups enjoyed their own proprietary claims on state power via such mechanisms as venal office-holding, tax farming, chartered monopolies, and the simple persistence of seigneurial rights (Teschke 2003). This applied beyond the domi- nant regime-type of dynastic monarchy. By the second third of the eighteenth century, it even characterized the merchant republic of the Netherlands, where elites increasingly converted their commercial wealth into pro- prietary office (Adams 2005:145-54; De Vries and Van der Woude 1997:561-96). Whether invested in its orthodox (dynastic) or heterodox (republican) form, eighteenth-century European rulers and their agents tended to perceive sovereignty in patrimonial terms: \"public\" power counted as \"private\" (familial and corporate) property in their schemes of perception and classification. Their diplo- macy thus involved a common set of stakes.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "France", "text": "France was an especially orthodox incarna- tion of patrimonial sovereignty. During the seventeenth century, the French state sym- bolically vested sovereignty in the person of the monarch to a greater extent than any- where else in Europe (McClure 2006). Whether or not Louis XIV (reigned 1643 to 1715) ever claimed that he was the state (his apocryphal quip that \"L'\u00c9tat, c'est moi\"), he certainly took for granted that he owned the state (Rowen 1961). At the same time, the crown developed stable rules for venal office- holding and corporate privilege, redistribut- ing state power among broader elites-yet furthering its proprietary character in the process (Beik 1985;Lachmann 2000:118-38;Rowen 1980:75-92). The competing claims to the inheritance of the Spanish crown at the start of the eight- eenth century show the degree to which French agents shared patrimonial categories and stakes with their continental counterparts. The French Bourbon claimant maintained that \"the monarchy of Spain was the property of the Queen his mother, and consequently his own, and, for the sake of the tranquility of Europe, that of his second son, to whom he ceded it with all his heart.\" A pamphleteer of the Austrian Habsburg claimant disputed this assertion, because Spain was an \"entailed estate . . . in such a way that the property . . . remains wholly in perpetuity in the family\" (quoted in Rowen 1980:113-14, 115). Although they emphasized different kinds of property, the claimants fundamentally agreed on the crown's proprietary character. French agents and their continental coun- terparts thus saw each other as legitimate par- ticipants in a common game-the second form of recognition posited earlier. Of course, this was especially pronounced in the realm of intra-dynastic relations, where interests were not merely legitimate but convergent. Eight- eenth-century France enjoyed close dynastic ties with Spain (from 1700) and several Italian states (from the 1730s and 1740s). All of these states occupied branches of the Bourbon dynasty, with France representing the senior branch. Accordingly, the major Franco-Span- ish alliances of the eighteenth century-signed in 1721, 1733, 1743, and 1761-were literally \"family compacts,\" based on private family law, accompanied by marriage treaties, and designed to forward the collective interest of the House of Bourbon (B\u00e9ly 1999:64;Dhondt 2015:236, 462). Many early modern treaties were broken on a whim, but Spain was France's most loyal ally in the eighteenth cen- tury ( L\u00f3pez-Cord\u00f3n Cortezo 2003;Scott 2003). This strongly suggests that familial allegiances carried a moral force of their own. Patrimonial investments gave French agents a sense of solidarity with other dynas- ties as well. In 1756, France intervened in a dispute between the Landgrave of Hesse- Kassel and his son, the former having disin- herited the latter for his conversion from Protestantism to Catholicism. If French actions stemmed from confessional loyalties, their diplomatic instructions made no such mention; instead, they cited a common inter- est in maintaining dynastic heredity. As the French foreign minister explained to his ambassador at Vienna, Louis XV (reigned 1715 to 1774) was motivated by \"the feeling of humanity, by that of paternal tenderness, by the interest that all Princes should take in such grave events in the society of Princes\" (Archives du minist\u00e8re des Affaires \u00e9trang\u00e8res [AAE] Correspondance politique [CP] Autri- che 255:85-86; emphasis added). Because this was a secret instruction from a principal to his agent, it presumably reflected Louis XV's real motives. As a result of these common stakes, the French remained legitimate to their counter- parts even when their interests conflicted. Thus, at the Congress of Utrecht in 1712 to 1713, France avoided ceding territory to Savoy by drawing on shared patrimonial categories. French diplomats marshaled a distinction between conquered lands (unencumbered pri- vate property) and the royal domain (an entailed estate): whereas the former was freely alienable, the latter was not (NA PRO 31/3/200:115; NA SP 105/28:88). That conti- nental elites shared this view is confirmed by the correspondence of a third party, the mar- quis de Monteleon, an Italian representing the Spanish crown. Monteleon wrote approvingly that Louis XIV \"would readily accord a [territorial] barrier to the Dutch on the side of Flan- ders, this being a conquered land [ pays conquis] . . . but it did not seem that His Most Christian Majesty could ever detach anything from any side of the proper Domain of the Kingdom of France\" as compensation for Savoy (NA PRO 31/3/200:110). Tellingly, the British representatives remained skeptical of this distinction. Yet they too accepted it as non- negotiable (NA SP 105/28:88).", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Britain", "text": "In contrast to dynastic rule, seventeenth-cen- tury England saw the emergence of a parlia- mentary regime. After the Glorious Revolution of 1688, Parliament even controlled the royal succession (Brewer 1989:113). To be sure, diplomacy-unlike taxation-remained the formal prerogative of the crown. Contrary to most of continental Europe, however, Parlia- ment dominated British foreign policy in practice. Parliament's fiscal supremacy meant it monopolized the power to raise troops and subsidize allies (Brewer 1989:35;Teschke 2003:256-57). Crown ministers who lacked parliamentary backing tended to lose their positions (McKay and Scott 1983:148, 181). More important, late seventeenth-and eighteenth-century British elites were simply ceasing to invest in property-in-jurisdiction. Office venality was abolished in most admin- istrative departments between the 1670s and the 1690s, as was tax farming (Adams 2005:179;Brewer 1989:77, 92-95). Instead, Britain's aristocracy came to live off the fruits of agrarian capitalism: private property in land and its attendant rents, secured from market-dependent tenants (Brenner 1985(Brenner , 1993Lachmann 2000:173-76, 180-85). 15 Britain's diplomatic service was itself over- whelmingly recruited from this landed elite: between 1690 and 1789, 70 percent of diplo- mats belonged to noble or landed-gentry backgrounds. 16 In short, most British elites (including the diplomatic service) no longer had cause to see sovereignty in patrimonial terms. And those elites who did-principally the crown-were progressively constrained in their scope of action. 17 That non-patrimonial categories of sover- eignty found expression in British diplomacy is evidenced, once again, by the Spanish Suc- cession dispute. Although Britain defended the Habsburg claim, its justification had noth- ing in common with that of the Austrians. According to a pamphlet of the British minis- try, kings \"have no right to dispose\" of their kingdoms, not because of their entailment (the Austrian position), but because \"[e]very- one knows that kingship is an office, an administration, giving kings no proprietary possession\" (quoted in Rowen 1980:117; emphasis added). In fact, no one seemed to know this except for the British. As with diplomatic manners, British diplo- matic stakes inhibited recognition from continental rulers and diplomats because of the ways they clashed with the latter's percep- tual schemes. For one, the British government appeared as untrustworthy because its com- position was liable to change after every par- liamentary election (AAE CP Hollande 234:70, CP Hollande 236:236;Black 1986Black :125-28, 1988NA PRO 31/3/197:68). At a deeper level, Parliament symbolized a threat to the very investments of dynastic rulers (Adams 2005:186-87;B\u00e9ly 1990:518). Recent events had seen Parlia- ment execute an English king (in 1649), depose another (in 1688), temporarily abolish monarchy itself (1649 to 1660), and repeat- edly modify the royal succession, that sup- posedly natural right of heirs (between 1688 and 1701). Foreign diplomats' and rulers' depictions of Britain frequently combined impressions of political mutability and social disorder. In 1713, a French ambassador wrote that one should always expect \"a revolution\" in England, \"a nation that counts variation and license among the rights of society\" (NA PRO 31/3/201:21, 25). As a Swedish diplo- mat explained as late as 1770: \"In a country such as [Britain], where the hasty and change- able feelings of the public and the common people can readily be inflamed, and where they infallibly force the administration to adopt the convictions of the public . . . it is difficult to foresee the unexpected events and convulsions which could arise in the future\" (quoted in Roberts 1970:8). In the 1760s, Frederick the Great justified his refusal to ally with Britain on the grounds of its supposed domestic instability (Roberts 1970:12). Beyond its own illegitimacy, Parliament suppressed the one aspect of British foreign policy that would have otherwise converged with the interests of some continental rulers: the familial allegiances of the British crown. Because of its German royal family, eight- eenth-century Britain actually enjoyed sub- stantial dynastic connections in Europe, which the crown sought to cultivate. How- ever, Parliament consistently refused to pay peacetime subsidies to foreign governments, a standard currency of alliance-formation among continental rulers. As historians have shown, this refusal was a major reason why Britain was unable either to act as mediator in the Holy Roman Empire during the 1750s (McKay and Scott 1983:184) or to forge alli- ances between the 1760s and the 1780s (Baugh 1998:24-25;McKay and Scott 1983:221;Roberts 1970:25-29). Critically, parliamentarians justified such actions with non-patrimonial categories, stressing the interests of their \"country\" as distinct from their monarchy. Consider how the diplomat and career MP, Hans Stanley, praised George III (reigned 1760 to 1820) for privileging Britain over his dynastic patrimony in 1761: \"His Majesty's immutable concern for the honour, and interests of our country in situa- tions so precarious for His Electoral domin- ions [Hanover], has, in my poor opinion, dashed, and confounded his enemies even more than the signal success with which the Patriot virtue of this magnanimous Prince has been crowned\" (NA SP 78/251:230; emphasis added). Whereas France depended on dynas- tic links with Spain and Italy, Parliament's investment in a putative national interest-its deployment of the nation as a category of practice-prevented Britain from enrolling its dynastic connections in Germany. The causal relevance of British elites' mal- adjusted categories and stakes is further con- firmed by considering the one European polity over which Britain did exert consider- able influence: the Dutch Republic between the 1670s and 1740s. As Pincus (2009,2012) has shown, Anglo-Dutch cooperation rested on deep cultural affinities. In particular, the dominant Whig party of Britain shared with the Dutch patriciate a vision of sociopolitical order involving constitutional limitations, partially free trade, a reformed church, and \"national\"-rather than dynastic-interests. In this regard, the Netherlands was the excep- tion proving the rule: Britain's influence was anomalously effective here because the Dutch were anomalously invested in similar catego- ries of sovereignty. Conversely, it is worth noting that Brit- ain's influence over the Dutch Republic waned in the mid-eighteenth century just as Dutch elites were increasingly investing in patrimonial sovereignty (for which, see De Vries and Van der Woude 1997:561-96). Although the Netherlands' neutrality after the 1740s is retrospectively rational in light of its declining world-economic and military posi- tion (Carter 1975), this cannot explain why Britain failed to enroll Dutch support-and it tried-despite the latter's \"interests\" (in the externally imputed sense). For Britain had previously succeeded in doing just this: Dutch support of Britain in earlier decades had actu- ally helped shift world-economic primacy from the former to the latter (B\u00e9ly 1990:46;Crouzet 1990:16;Modelski 1978:221).", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "FoRGInG The PeACe oF uTReChT, 1709 To 1714", "text": "I now turn to a case study: the events that culminated in the Peace of Utrecht, ending the War of the Spanish Succession (1702 to 1714). Utrecht offers a \"hard case\" for my model: the balance of military and economic power especially favored Britain over France during the war itself. Yet Britain failed to convert its military advantage into durable geopolitical influence. By contrast, France's durable influence survived its military set- back. My analysis shows why, despite its characterization as a global war that Britain won (Modelski 1978:221;Modelski and Thompson 1988:16;Thompson 1988:46), the War of the Spanish Succession did not lead to British hegemony in the eighteenth century.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Whig-French Negotiations (1709 to 1710)", "text": "The War of the Spanish Succession saw Brit- ain and its allies defeat France. In response, Louis XIV offered extremely generous peace terms. By 1709, Louis was willing to abandon the French claimant to the Spanish throne (his own grandson Philip of Anjou), cede territory, and even fund his enemies' war on Philip if the latter rejected the peace (B\u00e9ly 1990:38;Torcy 1903:205;Wolf 1968:570-71). That Louis acted in earnest is attested by his for- eign minister and chief adviser, the marquis de Torcy (1903:85), who concluded in his private journal that France was reduced to \"the pure necessity to wish for peace, at what- ever price it was made.\" France's opponents, however, pressed fur- ther: they demanded that Louis XIV commit his own troops to fight Spain (Wolf 1968:573). This Louis could not accept: as a patrimonial sovereign, he feared that making war on his grandson would violate his familial honor (Torcy 1903:157;Wolf 1968:571). But Louis was negotiating with the two European geopo- litical actors who were least oriented to patri- monial categories: the Dutch and (especially) the British Whigs. As a result, they failed to appreciate what was at stake. The British genuinely believed Louis would accept even those demands that contradicted his familial obligations (NA SP 84/233:18, 53, 126, 208209). As a Whig diplomat put it: \"However Advantagious and Glorious these Conditions appear in relation to the Allies, 'tis certainly believ'd, the Present Miseries of France . . . will oblige that Court to a ready Compliance with those Demands\" (NA SP 84:233:64). When Louis refused, the British were at a total loss to comprehend his motives (NA SP 84/235:54-55, 60, 156, 158, 214). Negotia- tions thus broke off in the summer of 1710.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Tory-French Negotiations (1711 to 1713)", "text": "The British squandered their best opportunity to dictate the peace when they rejected Lou- is's offer. Ultimately settled in 1713, the Peace of Utrecht handed wildly improved terms to France. The Bourbon Philip V kept the Spanish crown, conserving both Spain and Spanish America. Far from making terri- torial concessions, France added to its fron- tiers with Germany and Savoy (Osiander 1994:143-44, 146-47). In essence, France achieved everything its opponents had waged the war to prevent. How did France's fate change so dramati- cally? France benefited from a combination of dynastic accident and mechanisms of rec- ognition. In 1711, the Austrian Habsburg claimant to Spain became the new Holy Roman Emperor upon the death of his brother. Because the prospect of a Holy Roman Emperor ruling Spain was as worrisome to European elites as that of a Bourbon, France's bargaining position improved (Wolf 1968: 579). That France saw its improved position recognized, however, owed to a shift in its diplomatic audience-from the Whigs to the Tory party, which had formed a new British government in the meantime. The Tories were relatively more invested in patrimonial cate- gories than were their Whig counterparts: whereas Whigs looked to the Netherlands, Tories took absolutist France as their sociopo- litical model (Dickinson 1970;Pincus 2009Pincus , 2012. Indeed, Tories conferred much greater recognition on French stakes than did their Whig predecessors (or successors). As the Tories told the French: \"England knew per- fectly that the [previous] manner of negotiat- ing did not conform to the honor owed to such a great king\" as Louis XIV (NA PRO 31/3/197:25). Tory recognition of Louis's interests had a direct effect on France's improved diplomatic outcome. In June 1712, Britain unilaterally ceased military operations against France, handing the latter a decisive advantage over the German and Dutch forces with which it was still fighting (B\u00e9ly 1990: 660;Wolf 1968:587-88). Moreover, the Tory secretary of state, Vis- count Bolingbroke, was (for a British agent) unusually receptive to courtly standards of competence. Sent to France in August 1712 to negotiate in person, Bolingbroke displayed a respect for courtly etiquette and rank that genuinely impressed Louis XIV and his advisers (B\u00e9ly 1990:304, 364). Bolingbroke appears taken in by the courtly game: upon returning to Britain, he kept up a private cor- respondence and exchanged gifts with numer- ous French courtiers (NA SP 105/28:80, 84, 88; NA PRO 31/3/199:12, 14). As with patri- monial stakes, Tory recognition of courtly competence directly improved France's dip- lomatic outcome: during his visit to France, Bolingbroke personally conceded the key ter- ritorial gains that France would realize in the Utrecht treaty (Osiander 1994:144-46). By the same token, however, Tories' rec- ognition of courtly-patrimonial forms allowed them to influence the French where the Whigs had failed, because it secured their own rec- ognition from France. This implies that, absent a revolutionary social transformation of multiple major polities, the geopolitical influence of a single revolutionary state (like post-1688 England) depends on its ability to incorporate the existing rules of the interstate game-even at the cost of reproducing built- in advantages for senior players (like France) in the short run. Otherwise there is simply no one to recognize such a state. Thus Boling- broke leveraged his courtly competence to considerable effect, securing colonial advan- tages in exchange for France's dynastic vic- tory (Dickinson 1970:104-105). Had the Tories remained in power, Britain might still have exercised durable influence in the long run, embedding its interests in the post-Utrecht order. As it turned out, the Whigs quickly regained power in 1714, condemned the Utre- cht treaty, and impeached its architects (charg- ing Bolingbroke, who fled to France, with treason) (Dickinson 1970:131-35). Boling- broke became an exception proving the rule: his courtly-patrimonial diplomacy succeeded precisely to the degree that Britons them- selves rejected it as improper.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Summary", "text": "The Utrecht episode offers support for con- gruent habitus as a mechanism of geopolitical influence. Even at its most conciliatory, French diplomacy failed to achieve its goals when its recipients (the Whigs) did not share its patrimonial categories of sovereignty, because such agents could not recognize the legitimacy of French stakes. By contrast, French diplomacy realized more demanding goals when its recipients (the Tories) shared patrimonial categories (and courtly standards of competence). We can thus draw a broader inference from Whig and Tory responses to French diplomacy: France's influence in eighteenth-century Europe hinged on the fact that most European geopolitical actors were socially akin to the Tories, not the Whigs. Conversely, Britain might have secured its \"world leadership\" through the War of the Spanish Succession-as long-cycle theorists suggest-were the Whigs willing to accept Louis XIV's offers in 1709 and 1710, or had the Tories governed at that fortuitous moment. But the Tories did not govern at that moment, and the Whigs were ill-disposed to negotiate with a courtly-patrimonial polity because they did not share its stakes or its standards of competence. Ultimately, it was the Whigs, not the Tories, who set the tone of eighteenth-century British diplomacy. Returning to power in 1714, the Whigs ruled uninterruptedly until 1760 (Williams 1962). Thus Britain contin- ued to diverge from the courtly-patrimonial forms of continental diplomacy-and Brit- ain's geopolitical influence was continuously suppressed as a result.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "ConCluSIonS", "text": "This article explains the relative ability of France and inability of Britain to exert dura- ble geopolitical influence in eighteenth-cen- tury Europe. In so doing, I have expanded theories of geopolitical influence beyond their traditional focus on military and eco- nomic determinants. The discrepancy between French and British influence is surprising for these theories, because it was Britain and not France that enjoyed military and economic primacy in eighteenth-century Europe. Draw- ing on recent developments in IR theory, I argued that while extensive material capacity is a necessary condition, influence also depends on a state's symbolic capacity to secure recognition from competitor states. In so doing, I documented conditions for sym- bolic capacity itself. One key basis of recog- nition, I argued, is a homology between the habitus of state agents. France exerted durable influence because, in addition to its military and economic capacities, the French state produced agents whose habitus was well-adjusted to the field of eighteenth-century European geopolitics. French diplomats and their principals shared courtly standards of competence with most of their European counterparts. And they were invested in stakes that the latter found legiti- mate because they shared patrimonial catego- ries of sovereignty with the French. By contrast, Britain exerted less influence because its agents' habitus diverged from the continental form. The court was foreign to the social conditionings of most British diplo- mats. As a result, they appeared as incompe- tent. British categories of sovereignty were non-patrimonial, which meant British diplo- mats and their principals were invested in stakes that were perceived as illegitimate.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Historical Sociology and International Relations", "text": "This article contributes to an emerging field of inquiry at the intersection of sociology and IR: a \"historical sociology of international relations\" (Hobden and Hobson 2002) or \"global historical sociology\" (Go and Lawson 2017). I make contributions to both the histor- ical-sociological and global-international sides of this equation. Culture and geopolitics in theories of state formation. Historical and political sociologists have long stressed geopolitical factors-especially war and trade-to explain the rise of the \"modern\" (territorial, bureau- cratic, relatively autonomous) state. Interest- ingly, despite increasing attention to the cultural origins of the modern state (Adams 2005;Gorski 2003;Loveman 2005;Steinmetz 1999;Wilson 2011), the standard expla- nation of the latter's institutionalization continues to privilege its military effective- ness and economic efficiency (but see Gorski and Sharma 2017). On this account, the mod- ern state won out over rival polity forms- city-states, empires, all non-bureaucratic structures-because it was better at waging war, raising taxes, or reducing transaction costs in general (Ertman 1997;Mann 1986;Spruyt 1994;Tilly 1990). To borrow the lan- guage of organizational sociology, the rise of the modern state is primarily a case of \"com- petitive isomorphism\" ( DiMaggio and Powell 1983:149-50;cf. Spruyt [1994:257], who draws this connection explicitly). But if the rise of the modern state was a process of \"selection\" by the environment of the interstate system, and yet military and economic power is insufficient to explain influence within that system, then military effectiveness and economic efficiency may not suffice to explain the rise of the modern state either. In fact, it may be better to view the whole process as one of \"institutional isomor- phism\" (DiMaggio and Powell 1983:150-54). This would mean the modern state won out over alternative forms because of its greater legitimacy rather than-or in addition to-its military and economic competitiveness. To take this article's example, the British state was actually much more \"modern\" (bureau- cratic and autonomous) than its rivals in eight- eenth-century Europe, and it outperformed them military and economically as well. Yet such success did not, in itself, induce Euro- pean elites to adopt the British polity form, because it was simply not legitimate to them. In this sense, the British state was uniquely ill-adapted to its broader environment. It would thus take the French Revolution's par- tial delegitimation of the courtly-patrimonial model from within (Sewell 1996) before the British model could become a viable alterna- tive (for a suggestive take on the latter, see Thompson 1965:327-28). Historical sociology of world hegem- ony. This study contributes to the literature on \"world hegemony,\" that is, \"the power of a state to exercise functions of leadership and governance over a system of sovereign states\" (Arrighi 1994:28; see also Arrighi 1990;Cox 1983Cox , 1987Gilpin 1981Gilpin , 1987Keohane 1984;Lachmann 2014;Wallerstein 1984). Although the geopolitical influence of eight- eenth-century France never reached the threshold of hegemony as typically defined, the mechanisms promoting French influence and inhibiting its British alternative offer hypotheses to guide research about hegemony in subsequent centuries. Specifically, the negative case of eight- eenth-century Europe sheds light on analo- gous cases in the world system: cases in which a materially dominant state fails to translate its dominance into hegemony. As most accounts now recognize, the United States already enjoyed globally dominant economic and military capabilities by the end of the First World War, yet it did not exercise world hegemony until 1945 (Kindleberger 1973;Tooze 2014). And dominance without hegemony increasingly characterizes the United States in twenty-first-century world politics as well (Lachmann 2017). Rather than simply attribute dominance without hegemony to a failure of \"will\" (Keohane 1984:34-35), future research on these critical moments might look for a disjunction between the socially acquired dispositions of U.S. for- eign policy elites and those of their counter- parts. 18 For if I am correct, a state's capacity for \"soft power\" never solely inheres in its own institutions; rather, it is the relational product of a contingent fit between sociopo- litical structures. Moreover, aside from the relatively tiny central administration, the vast majority of eighteenth- century Prussian elite households reproduced them- selves-locally-through what can only be seen as property-in-jurisdiction: the institution of serfdom. 16. Results not shown in Table 3 but available upon request. 17. Obviously, direct ownership of extra-economic coercion was a much more prevalent elite strategy in the colonial empire. 18. In this context, we might note that U.S. elites have seemingly rediscovered patrimonial strategies of reproduction in recent decades (Lachmann 2011), something the Trump administration and its openly familial forms of patronage have accelerated-but which they cannot be credited with inventing.", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Acknowledgments", "text": "I would like to thank Dylan Riley, Edwin Ackerman,", "title": NaN, "file_name": "Stuart Brundage - 2018 - The Social Sources of Geopolitical Power French a.pdf"}
{"section": "Abstract", "text": "Pretend play has been claimed to be crucial to children's healthy development. Here we examine evidence for this position versus 2 alternatives: Pretend play is 1 of many routes to positive developments (equifinality), and pretend play is an epiphenomenon of other factors that drive development. Evidence from several domains is considered. For language, narrative, and emotion regulation, the research conducted to date is consistent with all 3 positions but insufficient to draw conclusions. For executive function and social skills, existing research leans against the crucial causal position but is insufficient to differentiate the other 2. For reasoning, equifinality is definitely supported, ruling out a crucially causal position but still leaving open the possibility that pretend play is epiphenomenal. For problem solving, there is no compelling evidence that pretend play helps or is even a correlate. For creativity, intelligence, conservation, and theory of mind, inconsistent correlational results from sound studies and nonreplication with masked experimenters are problematic for a causal position, and some good studies favor an epiphenomenon position in which child, adult, and environment characteristics that go along with play are the true causal agents. We end by considering epiphenomenalism more deeply and discussing implications for preschool settings and further research in this domain. Our takeaway message is that existing evidence does not support strong causal claims about the unique importance of pretend play for development and that much more and better research is essential for clarifying its possible role.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Defining Pretend Play", "text": "A preliminary issue is to define pretend play. Play itself is a notoriously difficult concept to pin down (Burghardt, 2011). For our purposes the four criteria of Krasnor and Pepler (1980) will define play: flexibility, positive affect, nonliterality, and intrinsic motivation (cf. Sutton-Smith & Kelly-Byrne, 1984). Flexibility denotes that play behaviors vary from real ones in form (they might be exaggerated, or truncated) and/or content (one might play at eating with a stick instead of a spoon). Positive affect touches on the idea that people look like they are having fun when they play. Nonliterality refers to the fact that, in play, behaviors lack their usual meaning while paradoxically retaining it; Bateson (1972) famously pointed out that, \"the playful nip denotes the bite, but it does not denote what would be denoted by the bite\" (p. 317). Intrinsic motivation suggests voluntariness: One engages in the activity by choice for its own sake. Pretend play activities are the subset of play activities charac- terized by an \"as-if\" stance ( Garvey, 1990). Beyond being simply nonliteral, in pretend play a \"pretense\" is layered over reality (Austin, 1979); specifically, a pretender knowingly and intention- ally projects some mentally represented alternative on to the pres- ent situation in the spirit of play (Lillard, 1993). Sometimes pretend play is social: A group of children share an alternative reality that they project, perhaps acting like they are different people in another place and time. Other times pretending is a solo activity. Pretend play can involve projecting imaginary objects and properties, or using one object as if it were another (Leslie, 1987). It is most prominent in early childhood, with ages 3 to 5 being declared its \"high season\" (D. G. Singer & Singer, 1992), although it does continue into middle childhood and beyond (E. D. Smith & Lillard, in press). There are several other forms of play besides pretend (see Pellegrini, 2009;P. K. Smith, 2010); in particular there is a small but important literature on physical play (such as hopscotch and rough-and-tumble play), which has been well reviewed elsewhere (Pellegrini & Bohn, 2005;Pellis & Pellis, 2009). Such forms of play assist sustained attention in conventional school situations (Pellegrini & Bohn, 2005); they also (in the case of rough-and- tumble play fighting) assist emotion regulation, social coordina- tion, and normal sexual behavior, at least in some rodents and nonhuman primates (Pellis & Pellis, 2009). Pretend play can overlap with these and other types of play. For example, physical play overlaps with pretend play when children pretend to be fighting warriors. Object play overlaps with pretending when a child animates those objects. The literature is not always clear as to when pretend play specifically, versus play more generally, or some other specific type of play is at issue; this can be seen in the quotes with which we opened (but see footnotes 1-2), and probably arises because young children's play is so often infused with pretense. Our aim here is not to resolve this ambiguity but rather to consider studies used to support claims that play is crucial to positive develop- ments, excluding the physical play literature just mentioned, and retaining focus on pretend play as much as possible. Our main exception to this is in two subdomains of nonsocial cognitive aptitudes, creativity and problem solving, because for those skills several studies concerning manipulative play with small objects (which might or might not involve pretending) are often cited as showing play's cognitive benefits. When a study contrasted pre- tend play with some other form of play (like construction play, as in building with blocks) we focused on the pretend condition. Many studies strain the voluntary aspect of play in that children were told to play or were instructed in acting out a story, but because those studies have been cited as showing play's benefits, they are reviewed here. To locate studies, the first author began with references sup- porting claims of play's benefits in articles like those in the opening paragraph, then back referenced those studies in a snow- ball fashion. Through this process she arrived at the six main topics and six subdomains of nonsocial cognitive skills; the subdomain of mathematics was subsequently eliminated due to an insufficient number of studies. From there a search engine (Google Scholar) was employed, searching by keywords (\"social skills, pretend play\") and the \"referenced by\" and \"related articles\" features, as well as continuing to back reference from within articles. To avoid an unwieldy review, we passed over studies of atypical populations or cultural variation, and largely confined ourselves to published or in press peer-reviewed studies. 3 The first is that pretend play is crucial to optimal development. The second, which Smith supported, is equifinality: Pretending helps some developments, but it is only one possible route. Other activities can work as well or better. The third possibility is that pretending is an epiphenomenon or byproduct of some other selected-for capability, but in and of itself makes no contribution to development; rather, the other activity or condition to which it is sometimes attached is the actual contributor. Two major devel- opmental theorists, Vygotsky and Piaget, align with the first and third views, respectively. For Vygotsky (1978), pretense has a crucial developmental role, because it is the activity by which children learn to separate referent from object. In play, children first understand that actions (and objects on which one might act) can be separated from reality and can be based on the meaning of a given situation rather than on the physical properties of objects (Vygotsky, 1967). In this way, for example, a banana could become a phone in a pretense situa- tion and the child could act on it as if it were a phone, inhibiting how he or she would act on it if it were a banana. The upshot of this is that children develop abstract thought through pretend play (Vygotsky, 1967). In addition, because reality must be inhibited, children also develop inhibitory control through pretending (Bodrova & Leong, 1996). Because of these features, \"in play, it is as though [the child] were a head taller than himself\" (Vygotsky, 1978, p. 102); play takes a child to the upper end of his or her \"zone of proximal development\" (p. 86). In contrast, for Piaget (1962), pretend play is more an index than a promoter of development. 4 Its appearance around 18 months indicates the development of the semiotic function, which also allows for deferred imitation and language. The semiotic function separates an idea from its referent, a memory from its context, and an object from its label, allowing one to entertain and elaborate on mental content that is separate from the physical, present reality. Here we consider which of these views is best supported by the evidence. Each view is compatible with a particular pattern of evidence from correlational and experimental (short-term and training) studies, shown in Table 1 (cf. P. K. Smith, 2010, Table  9.2, p. 187). These patterns assume methodologically sound stud- ies including sufficient duration and sample sizes. First, if pretend play does crucially cause positive developments (Vygotsky's po- sition), then strong positive correlations between pretend play and those developments should consistently be found; if a child pre- tends more, whether naturally (in a correlational study) or due to an experimental manipulation, the development should increase. If pretend play causes creativity, then children who pretend more will generally be more creative. Additional predictors, like intelligence, are also possible, but if pretending is truly the important causal factor, the unique and important relationship to pretend play should hold even when those other predictors are partialled out. Conversely, if Smith's equifinality position is correct, then one would generally expect positive relationships between play and the outcome but also correlations with other predictors that engender the outcome. For example, if social pretend play develops theory of mind and so does adult talk about mental states, then correla- tions should be found for both variables. Interventions increasing mental state talk and pretend play might have an additive effect when combined, which could lead to even larger effects (but not if there was a ceiling on development for that age). There could be cases when although equifinality is the best model, pretend play fails to evince a significant effect. This might occur, for example, when there is substantial multicollinearity, or when an alternate predictor's effect is much larger, masking pretend play's effect. Thus equifinality does not insist on 100% consistent results, but it generally expects them. The third, or epiphenomenon, position is supported if pretend play coincides with some other causal circumstance; in such cases pretend play might mistakenly be considered causal. For example, if social pretend play is related to theory of mind because adults who engage in a lot of mental state talk also happen to encourage pretend play, then perhaps what is actually leading to the increased theory of mind is not the pretend play, but the mental state talk; social pretend play is secondary or epiphenomenal to the mental state talk-theory of mind relationship. If pretend play is an epiphe- nomenon then one might find inconsistent correlations with outcomes (because pretend play does not always go along with the real predic- tors) but consistent correlations between real predictors and outcomes. Because different studies measure different possible predictors, the true predictors might not always be evident. Here we evaluate the patterns of evidence with an eye to each of these positions. Before beginning to do so, it is useful to note some recurring problems in this literature (see also Cheyne, 1982).", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Common Methodological Problems", "text": "Several problems recur in the literature on whether pretend play helps development. Sometimes these problems occur because the 4 Others have claimed Piaget gives pretense a stronger role in develop- ment; for example, Singer and colleagues, citing Piaget (1962), claimed he \"concluded that play was a vital component to children's normal intellec- tual and social development\" (D. G. Singer et al., 2009, p. 285). In our reading the closest Piaget (1962) comes to this is when he says it is undoubtedly \"a preparation for imaginative aptitudes\" (p. 155), where imagination (as in pretend play) is the assimilative pole of thought (in contrast to accommodation), and creative imagination arises only when one integrates the two. This is essentially the position taken by Harris (2000) and D. G. Singer and Singer (1992): Pretending assists imagination. But whereas for these modern authors this is a reason to centralize pretend play, our reading of Piaget's text on play suggests that this role in imaginative development was a minor concern; pretend play was primarily an offshoot of the symbolic function. Perhaps confusion has arisen because elsewhere Piaget assigns manipulative activities (Piaget, 1929) and peer interaction (Piaget, 1932) as important to development, and pretend play often in- volves these other activities. But in considering manipulative activity, Piaget refers more to what is now referred to as embodied cognition (\"manual work is essential to the child's mental development\"; Piaget, 1929, p. 383), and regarding peer involvement Piaget's own focus on pretend play was particularly as a solitary activity. Piaget (1962) did think pretending served an egoistic function in that it allowed the child to fulfill wishes that he or she could not fulfill in reality. A child who wants to be a mother can simply pretend to be one. But Piaget was concerned with cognitive development, not personality development, and pretend play was pre-operational because it indicated what the child lacked. For Piaget, children outgrow pretending as they develop the ability to accommodate reality. Here he followed some major figures of his time in child psychol- ogy, such as Freud (1955, as discussed by Harris, 2000) and Montessori (1989). Aligning with our own reading, Sutton-Smith (1966) colorfully summarized Piaget's view of pretending as \"a buttress to an inadequate intelligence\" (p. 108). For further discussion, see P. K. Smith (2010, pp. 31-37). research was conducted when experimental standards were not as high as they are today, pointing to the need to modernize the evidence base. In more recent studies, perhaps scholars did not apply more rigor because of a deep belief in the power of play (Elkind, 2007), what P. K. Smith (1988) dubbed the \"play ethos\" and Sutton-Smith (1995, p. 279) the rhetoric of \"play as progress.\" Here we strive to overcome the tendency to favor pretend play by holding all studies to a high standard. One common problem in discussions of the impact of play on development is that correlational findings are often discussed as if they were causal. When children who play more do better on some other measure, of course it does not mean that the play definitely caused the outcome. Positive correlations between pretend play and a development are only a necessary precondition to pretend play being causal. Likewise prominent authors have described elaborate pretend worlds they constructed as children, and one might see the earlier behavior as causing their subsequent literary genius (Root-Bernstein, in press), but it is as plausible that their creativity led to conjuring up elaborate imaginary worlds at both time points. A second recurring problem is failure to replicate. For example, one study shows increases in empathy associated with pretense training (Saltz & Johnson, 1974) and another does not (Iannotti, 1978), and typically only the positive finding is cited. If other key experimental factors are essentially equal, either the reported pos- itive result reflects a Type I error or the failure reflects a Type II error. Inconsistent findings in correlational studies contradict the causal view but would be expected with either the equifinality or epiphenomenalism. For equifinality, nonreplications would occur when the alternate route was stronger in one study, and including it masked the effect of pretend play; for the epiphenomenon position, nonreplications would occur because the underlying cause sometimes accompanied pretend play and sometimes did not. In the literature extolling play's benefits, failures to replicate are often ignored. A third problem concerns experimenter bias. Every under- graduate research methods course should impart the importance of experimenters being \"masked\" insofar as possible: that is, unaware of (a) the hypotheses being tested and (b) participants' conditions. Yet cognitive development research rarely uses masked experimenters. This might usually be fine: Child de- velopment researchers and the kinds of tests they give might not be vulnerable to experimenter bias under the usual circum- stances. For example, we know of no research suggesting that false belief or conservation errors occur at certain ages only when experimenters are unmasked. However, for research on the benefits of pretend play there are several cases where results obtained with knowledgeable experimenters went away when masked ones were employed (Christie, 1983;Guthrie & Hudson, 1979;Pepler & Ross, 1981;Simon & Smith, 1983P. K. Smith, Simon, & Emberton, 1985;P. K. Smith & Whitney, 1987). Nonreplications with masked experimenters make a strong case for being cautious about pretend play results ob- tained with knowledgeable experimenters. Besides correlational data, nonreplication, and unmasked ex- perimenters, other recurrent problems are very small sample sizes, nonrandom assignment, confounding implementer with intervention (particularly concerning when there is only one implementer per condition and interventions last for several months), control conditions that differ beyond pretend play, confounding content with pretend play, and unsound statistical practices like using subsets of data and one-tailed tests without prior rationale. Methodological problems are so prevalent in this literature that meta-analysis is precluded. E. P. Fisher (1992) did a meta-analysis of the impact of play (generally) on development, despite aware- ness of these limitations (see \"Shortcomings of the Studies,\" pp. 164 -168), but he also did not have a consistent even-handed approach to which statistics he included, and further, he used some wrong statistics that inflated his result. As a particularly egregious illustration of this, from Christie (1983)  Inconsistent, but consistent with other variables that are causal. For example, if presence of certain toys increases pretending in children who are more creative, but other objects have no impact, then correlations between pretending and creativity will be seen only in environments with those toys. Experimental (short-term and training) studies Strong, unique, and consistent. Strong and consistent but not unique, so other conditions could also affect development. For example, skills training and pretend play training could both increase the development. Effects found only if the crucial underlying factor(s) is (are) influenced by the intervention. For example, suppose pretend play only assists development when intensive adult interaction is part of that training; when children pretend but there is no intensive adult interaction, the pretend play does not increase the development; in addition, another condition might show that intensive adult interaction alone increases the development, even in the absence of play. have been used. Careful reading reveals many more problems, yet this article is often cited (126 times, Google Scholar, as of May 28, 2012) as evidence that play helps development (e.g., Bergen, 2009;Ginsburg et al., 2007;Wyver & Spence, 1999). Because so many studies in this area are methodologically unsound, the current literature base is best suited to a descriptive review, on which we now embark. In each section, we begin with theoretical and construct issues, then review studies. A series of 10 tables compiles the studies pertinent to each domain or subdomain of development. After reviewing the studies, each section concludes by dis- cussing the evidence with respect to the three views (summa- rized in Table 12). In these discussions, we sometimes rely on the absence of evidence to support a position. We do this with caution, since one can never prove that a relationship does not exist (Altman & Bland, 1995). However, inconsistent correla- tion patterns across studies with similar samples and methods and reliable coders are against a causal view. Likewise, when sound experimental methods yield null effects or even effects showing play is less positive than the alternative, this is also relevant. Finally, doubt is also cast on a causal view when masking experimenters or equalizing other aspects of interven- tions nullifies previous findings. A final note before treading into the evidence concerns the \"straw person\" element of the crucial causal view. When put on the stand, perhaps few would endorse the position that pretend play is crucial (in the sense of essential or vital) for various aspects of development. Yet the quotes with which we opened and additional references throughout this review show that this stance is taken in the literature, so we consider it here.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Nonsocial Cognitive Aptitudes", "text": "As was seen in our opening paragraph, many scholars have asserted that pretend play produces cognitive benefits. One way pretend play could help cognition is by predisposing children to a generally playful attitude (Dansky & Silverman, 1973) that could lead to production of unusual ideas, creative problem solving (Vandenberg, 1980), and then to other cognitive aptitudes. This view is compatible with Fredrickson's (2001) broaden-and-build theory of positive emotions, with play eliciting joy, which in turn leads to a broadening of individuals' thought-action repertoires. Vygotsky's ideas on symbolic and abstract thought, just reviewed, also suggest how pretend play could assist cognitive abilities. Here we discuss evidence that pretend play assists development in five subdomains: creativity, problem solving, intelligence, conserva- tion, and reasoning.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Creativity", "text": "Although creativity has been operationalized in a number of ways, in the studies on play it has typically been defined as the ability to produce original content relevant to a particular task (Wallach & Kogan, 1965). The most commonly used measure of creativity in this literature is the alternate uses task (R. C. Wilson, Guilford, & Christensen, 1953), in which participants give possible uses for common objects, like a paper towel or a paperclip (Dansky, 1980a). Task responses are typically coded for fluency (num- ber of uses named) and originality (number of uses not given by any other participant). Another common task is the Torrance Thinking Creatively in Action and Movement Test or TCAM (Torrance, 1981), which includes several subtests, including hav- ing children move like trees in the wind, and also alternate uses. Correlational studies. Several studies have addressed the claim that pretending makes children more creative ( Ginsburg et al., 2007;J. L. Singer, 1973) by looking for correlations between naturalistic play and creativity, since they should exist if more frequent pretenders have become more creative via their pretend activity. Of course correlations are not evidence of causation, but if causation exists, correlations should be consistently found. Naturalistic classroom play has been categorized differently in different studies, but a combination of Smilansky (1968) and Parten's (1932) schemes has been used most often (Rubin, 2001). Smilansky's scheme (derived from Piaget) divides play into cog- nitive categories (functional play when a child repeats motor actions on objects, construction play when a child builds things, dramatic or symbolic play when the child substitutes an imagined world for reality, and games-with-rules like Checkers). Parten's scheme is focused on the social aspect of children's play: coop- erative when children are actively interacting in a common group endeavor; associative when they interact but not toward a single common endeavor; parallel when they play similarly but side by side, with little interaction; solitary independent when they play alone at their own games; onlooker when they watch others play; and unoccupied. We found eight studies correlating pretend play and creativity (see Table 2). Seven concerned preschoolers. Typically children's play in preschool was coded for 1-5 min per day for a period of 20 days or more, and then alternate uses with two to four objects (or in some cases the TCAM or another test) was administered. Results were inconsistent. Johnson (1976) found that, controlling for IQ, amount of social but not solitary fantasy play was related to fluency. This would suggest that something about the social element, rather than pre- tending in and of itself, was related to creativity. However, Johnson (1978) later failed to replicate this finding in a very similar study, showing no relationship between pretend play (social or solo) and alternate uses. A different study with the same age range also found no relationship between creativity and social pretend play (L. Dunn & Herwig, 1992) but found a negative relationship between originality of responses and solo pretend play that disap- peared when IQ was partialled out. Pellegrini and Gustafson (2005) also did not find children's frequency of pretend play to be related to creativity (r \ud97b\udf59 -.18) yet did find frequency of construc- tion play was related. On the other hand, a different study found all pretend play was significantly related to creativity in a sample of 15 high-IQ preschoolers (Moran, Sawyers, Fu, & Milgram, 1984). Wyver and Spence (1999) found that particularly fantastical pre- tend play was related to semantic creativity (naming all the objects one could think of) but not figural creativity (making objects from shapes); two other categories of pretend play that they coded were not related to either. The remaining preschool study, Lloyd and Howe (2003), was not useful regarding pretend play because they combined it with functional play, which was twice as common as pretend play, but it is worth noting that in contrast to Pellegrini and Gustafson (2005) they found construction play (which was coded separately) was not related to creativity. In the eighth and final correlational study, looking at somewhat older children, Russ and Grossman-McKee (1990) examined play with Russ's Affect in Play Scale (APS), which uses 10 min of play with puppets and blocks coding generativity. Alternate uses also taps generativity, so it is not surprising that positive correlations were found, both concurrently and over time (Russ, Robins, & Christiano, 1999). The eight correlational studies show an inconsistent pattern of relationships that does not support the causal model. The other models seek alternate routes or third variables that could underlie relationships when they are found. With this particular set of studies, no obvious other variable emerges. IQ is sometimes re- lated to creativity (Johnson, 1976;Wyver & Spence, 1999) but sometimes not (L. Dunn & Herwig, 1992;Lloyd & Howe, 2003;Moran et al., 1984;Wyver & Spence, 1999); Johnson (1978) did not test IQ. Johnson worked with low-socioeconomic status chil- dren, whereas others involved middle-class ones, but this cannot explain the discrepant results across Johnson's own studies. An- other third variable that fits either view is environment (i.e., types of toys supplied), which can drive the types of play children engage in (McLoyd, 1983). Because environment was not mea- sured, we cannot evaluate if this could underlie the inconsistencies. Regardless, distinguishing the second and third views requires experimental studies. Experimental studies. Four experimental studies found higher associative fluency when children first played with objects for which they later named uses (Dansky, 1980b;Dansky & Silverman, 1973Li, 1978). This fit with Sutton-Smith's (1968) quasi-experimental study in which boys came up with more alternate uses for traditional boy toys than did girls (although for girl toys, they were equal). To check whether experience with the objects, rather than playing with them per se, was important, Dansky and Silverman (1973) included an imitation group, in which children gained experience by imitating the experimenter handling the objects. The imitators gave no more uses than the controls, suggesting that experience with the objects did not ex- plain the first results (see also Hughes, 1981, as cited in Hutt, Tyler, Hutt, & Christopherson, 1989. Pretend play's effect on creativity could be limited to the objects at hand. To test this, Dansky and Silverman (1975) used a different set of objects in the test phase, and here again the play group produced more uses, suggesting play's hypothesized effect on creativity generalizes. In the Dansky studies just described, although the theoretical rationale concerned pretend play, it is unclear whether children pretended with the objects or just manipulated them. Li (1978) tested whether pretend play might improve creativity above and beyond play generally. In the pretend play condition, the experi- menter told a short fantasy story, then showed children the stim- ulus objects and said, \"Let's make-believe or imagine that these Note. Type of study: C \ud97b\udf59 correlational; E \ud97b\udf59 experimental; T \ud97b\udf59 training. Type of play: Play \ud97b\udf59 pretense status unspecified; Const. \ud97b\udf59 construction play (e.g., blocks); Solo \ud97b\udf59 pretend play alone; SPP \ud97b\udf59 social pretend play; PP \ud97b\udf59 pretend play (social unspecified). \ud97b\udf59: positive relationship to play; \ud97b\udf59: no correlation or play \ud97b\udf59 nonplay condition; \ud97b\udf59 \ud97b\udf59 negative relationship to play. Masking: Intervention (Int) or posttest experimenters (Exp). If masking status was not specified, we assume experimenters were not masked, since that is the unmarked case. Masking for correlational studies is omitted because it is rarely mentioned, even when it is likely (because play observations occurred several years earlier than testing, for example). Irrel. \ud97b\udf59 irrelevant. objects could become anything you want them to be. Play with all of these things\" (p. 33). Free play, imitation, and control condi- tions were similar to those used in Dansky's studies. After 10 min, the same experimenter administered the alternative uses test with the three objects used in the intervention and a new fourth object. Significant differences were found for one of three old objects (a paperclip), for the make-believe group and the free play group, and for the new object (a screwdriver) only for the make-believe group versus the control. Overall, Li's children came up with far fewer uses than children had in the previous studies, perhaps reflecting population variance. Taking a different tack, Dansky (1980b) examined whether children who naturally engage in more pretend play would benefit more from a play intervention. Children were classified as pre- tenders if they engaged in pretense more than 25% of the time, or nonpretenders if they engaged in it less than 5%. They were then assigned to free play or control conditions. Suggesting the class- room classifications had validity, in the free play condition 88% of the pretenders but only 6% of the nonpretenders pretended with the objects. The alternate uses test was given with a different set of objects, and only the pretenders in the free play condition had higher fluency. Dansky concluded that play induces creativity only for those who are predisposed to pretend. These experimental studies suggest that play might have a causal effect on creativity, at least for children who frequently pretend (see also Sutton-Smith, 1967). However, in these studies the experimenters administering the creativity test knew which condition each child was in, and perhaps their expectations influ- enced children's responses. With alternate uses, the experimenter elicits answers until they think a child has run out of possibilities. More coaxing might inadvertently occur when children are ex- pected to produce more uses. In an attempted replication of Dan- sky and Silverman (1973), P. K. Smith and Whitney (1987) used different experimenters for the intervention phase and the posttest, with the latter masked to condition. Results showed no significant differences-in fact the control condition obtained the highest mean score. This finding is consistent with another study using a masked experimenter: The number of uses given was not signifi- cantly different for children in a play condition (Pepler & Ross, 1981, Study 2). 5 A less direct test of possible experimenter effects occurs when a different hypothesis is being tested. Two studies tested the hypothesis that focused questioning would elicit more uses than would playing with objects (Pellegrini, 1981;Pellegrini & Greene, 1980). The hypothesis was upheld: Focused questioning led to more uses, and free play was not different from the control. Another way to check for experimenter biasing is to videotape task delivery, which could encourage standardization; Russ and Kaugars (2001) did so, and children who engaged in pretend play generated no more uses than children who did puzzles. All this suggests that the alternate uses task might be particularly vulner- able to experimenter bias. A solution is to use a different creativity task, like making a collage, the creativity of which is assessed by masked judges (Howard-Jones, Taylor, & Sutton, 2002). Unfortunately the study that did this did not use a good control condition. Six-and 7-year-olds played with salt dough for 25 min or copied words from the board (and if they finished early, were told to start over). Afterward, they made collages, and the play group's were more creative. However, perhaps forced copying had a deleterious effect on creativity, rather than play having a positive one. Further research with a more neutral control condition is needed. Training studies. Experimental studies (as defined here) examine short-term change; perhaps play does influence creativity but requires longer incubation periods. We located seven longitu- dinal play training studies. The first three produced effects that were in the expected direction, but four others controlled for adult contact and found play training itself had no effect. Dansky (1980a) compared 36 low-income preschoolers in so- ciodramatic play, free play, and object exploration interventions over 3 weeks, with three 30-min sessions per week. The sociodra- matic play training involved enacting pretend play themes like going on a picnic. Children in the free play group could play as they wished, and they rarely engaged in pretend play. The explo- ration training group explored and discussed objects. All experi- menters were masked. The sociodramatic play group outperformed both other groups on alternate uses. In a similar study, Feitelson and Ross (1973) compared 24 kindergarteners in play tutoring, music tutoring, toy play without tutoring, and control groups, with ten 30-min interventions over 5 weeks. Creativity was measured with the picture completion sub- test of Thinking Creatively With Pictures (Torrance, 1966), in which children complete up to 10 pictures in a way that \"no one else will think of\" and give each picture a title, and Dog and Bone (Banta, 1970), in which children make up different routes from a dog to a bone. The play tutoring group's scores increased the most on the number of unusual titles given (but not number of pictures or picture elements) and trended toward better performance on Dog and Bone. Wyver and Spence (1999) trained 38 children in three types of play based on Parten's social categories and play type (associative fantastical, cooperative fantastical, or cooperative constructive/ everyday sociodramatic) for 4 hr over 4 weeks, with pre-and posttests of figural and semantic creativity; they included a no- intervention control. The former two groups increased their fan- tastical pretend play, and the third did not increase in any type of pretend play. However, only the third group showed increases in both semantic and figural creativity. Associative fantastical play training improved semantic but not figural creativity, and cooper- ative fantastical play did not improve in either. These results are quite mild then for the hypothesis that pretend play increases creativity, since it did not reliably do so for the fantasy groups and the sociodramatic group also engaged in construction play. Other experiments in this study tested the reverse direction by training children in creativity and then observing play; results suggested complex and bidirectional relationships. The results of these studies suggest that the play training can increase children's creativity. However, it is unclear whether ex-perimenter involvement was greater in the play training group. Christie (1983) controlled for adult contact (and used a blind experimenter, which Dansky did but the others did not) when comparing preschoolers in play tutoring with those in skills tutor- ing, using nine weekly 20-min sessions. With adult interaction controlled, both groups improved on the fluency factor of the TCAM, with no advantage for play over skills tutoring. There were no changes for either group on the originality or the imagination factors of the TCAM. On the other hand, in this study there was no increase in pretend play in the play training group. One could argue that this should not be necessary-that pretending within the intervention should be all that is needed; the sociodramatic/ construction play children in the Wyver and Spence study im- proved in creativity without increasing their pretend play. Yet they had a 4-hr intervention over 4 weeks, whereas Christie's had 3 hr over 9 weeks. Perhaps pretend play has an effect but only after a more intensive play intervention schedule. Yet P. K. Smith and Syddall (1978) and P. K. Smith, Dalgleish, and Herzmark (1981) did increase pretend play with their more lengthy intervention and also controlled for adult contact, compar- ing fantasy play and skills tutoring groups. Although the sample size was very small in the first study (14), 65 children participated in the second study. They again found no group differences in a creativity outcome, this time on the Dog and Bone task. One final study involved slightly older children who partici- pated in five 30-min training sessions over 3 to 5 weeks (Moore & Russ, 2008). Experimental children were asked to play out partic- ular stories, emphasizing affect or fantasy and story coherence. A control group did puzzles, and experimenter contact was standard- ized to involve similar levels of encouragement in each group. Although the play groups did play more following the intervention, the control group actually increased significantly more in fluency on the alternate uses test. Thus although Christie (1983) is incon- clusive because the intervention was perhaps insufficient, these other three studies suggest that increased adult interaction might drive increases in children's creativity, rather than pretend play itself. Summary. The evidence that pretend play enhances creativ- ity is not convincing. Correlational studies are inconsistent, with some showing relationships only to social pretend play, pretend play, or construction play, and other studies failing to show rela- tionships to those same constructs. Inconsistent correlations are against a causal model and unfriendly but not fatal to equifinality. In the experimental studies with strong control conditions, evi- dence that play increases associative fluency disappeared when experimenters were masked. This shows that the play ethos is a considerable problem in this domain, and it is fatal to both the causal and equifinality views. Furthermore, effective training stud- ies with controlled adult contact found that skills training increased creativity just as much as pretend play training. Although this could support equifinality and epiphenomenalism, equifinality was eliminated by the experimental studies with masked experiment- ers. Unless one argues that the length of the play interventions was insufficient (training ranged from 3 to about 26 hr, with no linear relationship to results) or that the creativity tests were invalid, the pattern of results from the methodologically sound studies (masked experimenters, control conditions that equalize important nonfocal factors, effective interventions) best supports that pretend play is an epiphenomenon of some other relationship to creativity, coincident with adult interaction. If pretend play is epiphenomenal, one must explain why corre- lations are found between pretend play, or social pretend play, or construction play, in some studies but not others. Most of the correlational studies took place on school playgrounds, hence in different and unique settings. Perhaps some feature of the settings, like varying styles of adult interaction or different toys those adults provide, could explain the inconsistent patterns of correlations. In closing, note that the studies here were limited in many ways. Ns were often very small, training schedules were often paltry, and the operationalization of creativity was limited. Just one study examined artistic creativity, and although it needs follow-up, it was promising. There is not a compelling case that pretend play improves creativity as it has been measured, and there is a strong need for more high quality research. Intelligence Vygotsky (1967Vygotsky ( , 1978 is the preeminent proponent of the position that pretend play increases intelligence. His claim was that repeated experiences separating object from referent develop ab- stract reasoning. The definition and measurement of intelligence are controversial ( Neisser et al., 1996), but abstract reasoning is clearly central. In the literature concerning play, intelligence has been operationalized by a variety of measures, such as Raven's Progressive Matrices, the Stanford-Binet and the Peabody Picture Vocabulary Test (PPVT), which are correlated (Sattler, 1992), and the Wechsler Preschool and Primary Scale of Intelligence (WPPSI). Below we review correlational and training studies. Correlational studies. Several studies have examined rela- tionships between naturally occurring play and intelligence in preschoolers, and results have been inconsistent (see Table 3). Three studies were previously discussed regarding creativity. In the first of these (Johnson, 1976), social pretend play was related to the PPVT and the Picture Completion subset of the WPPSI, but nonsocial pretend play bore no relation. This suggests that being able to incorporate others into one's play is related to intelligence, but failing to do so is unrelated. The other two studies also discussed for creativity found nega- tive relationships between intelligence and lower levels of play. Lloyd and Howe (2003) used the same intelligence tests as John- son (1976) but did not code social play; a negative relationship was found with a variable combining Parten's lowest categories (on- looker and unoccupied). L. Dunn and Herwig (1992) coded social play but found no correlation with IQ; instead they found a negative correlation to solitary play, such that children who en- gaged in more solitary play had lower IQ scores. An earlier study also found a negative relationship (r \ud97b\udf59 -.19) between IQ and solitary functional play for a sample of 122 older 4-year-olds in preschool settings (Rubin, 1982); it also found a negative relationship for onlooker play and a positive one for construction play, but no relationship to pretend play. Parten (1932), using a much larger age range (1-4 years) and smaller sample (n \ud97b\udf59 42), also found a negative relationship between IQ and solitary play (r \ud97b\udf59 -.20, an equal effect size that was nonsig- nificant with the sample size) and found larger positive relation- ships with both cooperative and parallel play. Taking these studies together, a consistent picture that emerges is that in preschool 8 settings, where other children are available to play with, choosing to play alone or not play at all might be indicative of lower intelligence. Three other studies could be seen as going against this picture, but methodical variations can explain the difference. Peisach and Hardeman (1985) scored fantasy play either via observation (4-year-olds) or interview (5-to 7-year-olds) and administered a test similar to the Raven's; they did not obtain significant positive correlations. However, the small number of 4-year-olds (10) and the fact that older children were inter- viewed rather than observed in school settings could be why. Cole and Lavoie (1985) found no relationship between solitary or social pretend play and PPVT scores in children ages 2 to 6, but in this study dyads were randomly paired, which might have affected the nature of their play. Johnson, Ershler, and Lawton (1982) found that 4-year-olds who engaged in more functional play at preschool scored lower on the PPVT and the Raven's, and those who engaged in more construction play scored higher. Although this is consistent with the picture painted thus far, its third finding is not: Contra Johnson (1976), sociodramatic play frequency was not related to intelligence (cf. Henniger, 1991). Perhaps the preschool setting was influential here: It was a cognitive-based program, and almost three times as much con- struction as pretend play was coded. Whereas in some settings pretend play is most frequent (Parten, 1933), here it was the least common type of play. It might be the case that the program offered abundant construction materials and few toys inspiring sociodramatic play, restricting its range and preventing stronger correlation. The picture that emerges is that less intelligent children engage in lower levels of play in most of the preschool settings studied. By the Vygotskian model, these children failed to become more intelligent because they did not engage in higher levels of pretend play. Although the consistent correlations observed support this possibility, level of intelligence might induce level of play, rather than the reverse. Training studies can resolve this. Training studies. We found six training studies that exam- ined intelligence. Just one reported a solid and unique pretend play training effect (Saltz, Dixon, & Johnson, 1977). Three cohorts of children were given, over 6 months, a 15-min intervention 3 times per week enacting thematic fantasies (like The Three Billy Goats Gruff), discussing those same stories, or acting out typical expe- riences like going to the doctor. A control group engaged in other preschool activities. The sociodramatic and thematic fantasy groups had higher posttraining intelligence scores than the other groups. However, the researchers who did the training also admin- istered the posttests, which could be problematic for studies of pretend play. Among the other five studies, three of which used masked posttesters, pretend play tutoring showed no more benefit than skills/classification tutoring (Christie, 1983;Saltz & Johnson, 1974;P. K. Smith et al., 1981;P. K. Smith & Syddall, 1978) and less benefit than music (especially voice) training (Schellenberg, 2004); the only hint of a difference favoring play was in one study (P. K. Smith et al., 1981), on one of eight tests (geometric design), in one of two schools, which is not better than would be expected by chance. Thus when more sound methods were used, pretend play training did not increase intelligence more than the compar- ison condition. Regarding Schellenberg (2004), however, we have two concerns: differential dropout (17% in the keyboard condition) and combining the keyboard and voice groups for analysis when the IQ gain of the drama group was only one point less than the gain of the keyboard group. Summary. Relationships are found in natural settings be- tween levels of play and intelligence, but the direction of effects is uncertain. Training studies with solid methods suggest that pretend play is no better than other adult interventions in raising intelli- gence scores, and music interventions raised them more (cf. Note. Type of study: C \ud97b\udf59 correlational; T \ud97b\udf59 training. Type of play: Const. \ud97b\udf59 construction play (e.g., blocks); Solo \ud97b\udf59 pretend play alone; SPP \ud97b\udf59 social pretend play; F \ud97b\udf59 functional play; PP \ud97b\udf59 pretend play (social unspecified); D \ud97b\udf59 enacting stories with dolls or other children. \ud97b\udf59: positive relationship to play; \ud97b\udf59: no correlation or play \ud97b\udf59 nonplay condition; \ud97b\udf59 \ud97b\udf59 negative relationship to play. Masking: Intervention (Int) or posttest experimenters (Exp). If masking status was not specified, we assume experimenters were not masked, since that is the unmarked case. Masking for correlational studies is omitted because it is rarely mentioned, even when it is likely (because play observations occurred several years earlier than testing, for example). Wenner, 2009). Perhaps some feature of adult interaction or some aspect of music training drives group differences, which is more consistent with epiphenomenalism. Ideally in future research one could remove these key features, or isolate them for singular presentation in a third condition.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Problem Solving", "text": "Problem solving involves inventing strategies to overcome an obstacle and reach a goal. Theorists have speculated that playing with objects enables children to think about them in myriad ways and thus employ them in new ways to solve problems. Studies of problem solving and play have typically used Kohler's lure- retrieval paradigm, in which one must join two sticks together with a clamp or block in order to reach something. With this paradigm, it is not always clear if children are pretending: mapping an alternate reality on to the objects. But in discussions of the studies, pretend play is often considered key (Garvey, 1990). See Table 4. Correlational studies. Four correlational studies of play and problem solving found that engaging in construction but not pre- tend play in preschool was positively related to performance on the lure-retrieval problem (Cheyne & Rubin, 1983;Pellegrini & Gustafson, 2005;Rubin, 1982), although for one study that rela- tionship held only for boys (Gredlein & Bjorklund, 2005). Con- struction play could be related to performance on the lure-retrieval problem due to a third variable. For example, if some children are particularly drawn to constructing things, this could lead to high scores on both measures. Experimental studies are needed to examine causality and to more specifically address the issue of pretend play. Experimental studies. In the original study to use the lure- retrieval paradigm with children, Sylva , summary published in Sylva, Bruner, & Genova, 1976 gave preschoolers a 1-min demonstration of how to put a clamp on a single stick. Then one group was given 10 sticks and seven clamps and allowed to play for 9 more min, an observe group was given an additional 1-min demonstration in which the adult joined two sticks with a clamp, and a control group went straight to the lure-retrieval problem. The problem involved removing a chalk from a plexiglass box by clamping two sticks together, using them to dislodge the chalk, and then raking it toward oneself. When children did not engage with the tools during the problem time they received up to five hints, culminating in direct instruction. The play and observe groups both outperformed the controls, with about 40% of each obtaining the chalk with no hints, and there were also no differences between these groups in latency to solve the problem. Although overall children in the play group solved the problem with fewer hints by a McNemar's test, exam- ination of Table 6 in Sylva's thesis shows the play group received 74 hints, the observe group 77, and the control group 120, so the difference between the first two groups was utterly trivial. We conclude that playing with a set of objects for 9 min, having had a 1-min demonstration of how an object works, is as good as a 2-min demonstration of how that object works. Another study removed demonstration altogether and found the effects of 10 min of play versus a control were task dependent, appearing for a lure-retrieval but not a dislodging problem (Vandenberg, 1981). A further study using the lure-retrieval paradigm addressed the differential exposure time across Sylva et al.'s conditions by extending the observe condition so it would also take 10 min (P. K. Smith & Dutton, 1979). To further test problem solving, they added a second test, in which the child had to put three sticks together to reach the reward. Performance across the observe and play groups was equal on the first problem by all three measures- percentage solving, solution latency, and number of hints. How- ever, the play group solved the second problem more quickly than the observe group, thus they obtained a play-favored result for one of six measures. Yet Smith and his colleagues were leery that experimenters might have introduced bias into the procedure, so they reran the study with a masked experimenter, and condition differences were eliminated (Simon & Smith, 1983). They con- cluded that experimenter bias in delivering hints in the prior study led to its positive results, a hypothesis confirmed by a later study (P. K. Smith et al., 1985; see also . Null results for play in a further study led to the conclusion that there should be \"serious questions about the merits of play for enhancing problem solving\" (Vandenberg, 1990, p. 271). Finally, there is also controversy regarding the fantasy versus construction play element in all of these studies. Sylva (1977) reexamined her results and found children who pretended with the sticks were most apt to solve the problem, yet in an explicit study of this (Hughes, 1981, as described in Hutt et al., 1989, children who pretended with the sticks needed more hints to reach a solution than children who had engaged in exploratory/ constructive play (see DeLoache, 2000, for another case in which play impedes cognition). From this focused study, construction but not pretend play appears to be the important factor. Some recent experiments have shown that being allowed to freely interact (\"play\" in the sense of explore, an action with which play is often contrasted; see Berlyne, 1960;Hutt et al., 1989, p. 11) with specially designed puzzle toys known to do interesting things can lead to discovering solutions that are more efficient than a single inefficient solution that was taught (Bonawitz et al., 2011;Buchsbaum, Gopnik, Griffiths, & Shafto, 2011). In these cases, the specific problem-getting a specially constructed toy to make music-is probably important; there is no indication that explora- tion improved problem solving generally, although such a study would be welcome. Reviews and meta-analyses of the \"discovery learning\" literature have concluded that children tend not to learn what was intended in open discovery learning paradigms (Alfieri, Brooks, Aldrich, & Tenenbaum, 2011;Mayer, 2004). Rather, children tend to learn best when problems are structured such that children are likely to find the solution ( Klahr & Nigam, 2004) as they are in Montessori education. This is in line with a \"playful learning\" (Hirsh-Pasek et al., 2009) but not an open free play approach (see Vandenberg, 1990). Summary. Correlational studies show that construction but not pretend play is correlated with solving problems that involve construction. Does the construction play cause the problem solv- ing, or does a common trait like a propensity toward constructing underlie both (see Pellegrini, 2009)? Experimental studies could shed light on this, but they have had inconsistent and mild results that have not replicated with more tightly controlled procedures or masked experimenters; in those cases performance has been equal across play and observe conditions. This pattern of results could support epiphenomenalism or equifinality as regards construction play. But the fact that construction and not pretend play is the consistent correlate to solving the lure-retrieval problem, and that pretend play even interferes with using objects as tools, suggests that pretend play does not help problem solving, at least not the types used in research thus far. Research does show that exploring specially constructed puzzle toys leads to figuring out their solu- tions, and further research should examine if such \"play\" helps problem solving generally.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Reasoning", "text": "Another cognitive skill sometimes discussed with reference to the purported benefits of pretend play (D. G. Singer & Singer, 1992, p. 237) is solving logical syllogisms, in which one must reason from false premises like, \"Dogs live in trees. Rex is a dog. Does Rex live in a tree?\" The logically correct answer is that he does, but the problem is difficult since one must set aside one's real knowledge. Several studies (see Table 5) have found children do better on such problems when they are embedded in fantasy (Dias & Harris, 1988Hawkins, Pea, Glick, & Scribner, 1984;Kuczaj, 1981;Richards & Sanderson, 1999), for example by using exaggerated pretend intonation, or saying, \"Let's pretend that everything in the stories is true\" (Dias & Harris, 1988, p. 210). However, Harris and Leevers (2000) thought this might be because fantasy manipulations got children to consider the premises more carefully. A series of experiments showed that using any cue that \"clarified the experimenter's intention that the children should accept the premises as a basis for reasoning\" (p. 77) helped. This supports the equifinality view: Pretend play, as operationalized in these studies, is one of many means to enhance children's ability to solve logical syllogisms. 6 It makes sense that pretend play might help children reason about false premises, since they are defini- tional to pretend play: One acts as if something false were true. Further research should use correlational and training paradigms to explore whether pretending affects logical reasoning more gener- ally. We note that this reasoning research is also consistent with epiphenomenalism; research separating pretend play from the cue to consider the premises is needed to show whether pretend play alone is effective.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Conservation", "text": "Conservation, or understanding that objects retain certain core properties after superficial transformations, is structurally like pretending: In both cases children must keep one reality in mind while simultaneously focusing on a represented alternative (Fink, 1976). One must think about the water in a short glass while looking at that same water in a tall glass, or about the telephone while holding the banana. However, studies do not support a natural relationship between pretend play and conservation (see Table 6), since three correlational studies failed to detect one (Aisenson, 1978;Doyle, Ceschin, Tessier, & Doehring, 1991;Johnson et al., 1982). These are not discussed, but they are prob- lematic for a causal account. Training studies have had mixed results, with positive ones eventually explained by other factors. The first training study had partially positive results. Fink (1976) encouraged one group of 12 to pretend, gave another group extra free play time but did not interact, and also had a no- intervention control group. Interventions lasted 25 min, twice a week for 4 weeks; masked experimenters administered the post- tests. The pretend play group showed a trend toward improvement in quantity and number conservation and significant improvement in social role conservation (understanding that a father was also a doctor). However, adult involvement was very different across the conditions and might be the source of the effects. In the next training study on play and conservation, Golomb and Cornelius (1977) assigned 4-year-olds to symbolic play training or a construction play control condition. The play training involved engaging children in pretense and then asking how the play objects could have both a real and a pretend identity. The play children subsequently did better on standard conservation tasks. Two meth- odological problems hold for this and two further studies from Golomb's laboratory (Golomb & Bonen, 1981;Golomb, Gowing, & Friedman, 1982). First, the trainers were the posttesters; Guthrie and Hudson (1979) failed to replicate the result with masked experimenters. Second, better performance might have been due to the extensive and pointed questioning about how play objects could simultaneously have two identities. Golomb et al. (1982) in fact cited an unpublished study (Golomb & Adams, 1978) as showing that verbal inquiry was key (see also Hughes, 1981, as cited in Hutt et al., 1989). Further, these later Golomb studies showed that conservation training was as or more effective than pretend play training. In sum, there is no compelling evidence that pretend play leads to conservation. The crucial causal view requires consistent cor- relations that do not exist. Equifinality suffers from this lack of  consistency but is supported by Golomb's studies showing con- servation training was as effective. However, the results were attributed by the lead author to adult questioning in the play condition, not pretend play alone.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Conclusion: Cognitive Aptitudes", "text": "The literature reviewed here does not support the view that pretend play is crucial for children's cognitive development (cf. Isenberg & Quisenberry, 1988). Correlations have been examined for all domains here except reasoning and are inconsistent for creativity, null for conservation, and for problem solving are to construction play, not pretend play. For intelligence, the direction of effects could be the reverse, thus from intelligence to level of play. Equifinality is supported only for reasoning, for which epiphe- nomenalism is also possible. For creativity, intelligence, and con- servation, epiphenomenalism is best supported, because of the combination of inconsistent/null findings from correlational stud- ies and the elimination of training results when experimenters were masked or interactions made more equal. For problem solving, again, the relationship appears to concern construction and explo- ration but not pretend play. Taken together and examined closely, these studies present a dim view for the oft-made claim that pretend play importantly enhances cognitive development. Perhaps further and better research will show otherwise, but existing evi- dence is not supportive.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Theory of Mind", "text": "Next we examine studies addressing whether pretend play might assist social cognition or a theory of mind (Premack & Woodruff, 1978). Theory of Mind (ToM) refers to the coherent network of interrelated concepts people use to explain and predict behavior (Wellman, 1990). Theorists have long speculated that ToM could be promoted through pretend play (see chapters by Flavell, Leslie, and others in Astington, Harris, & Olson, 1988). Leslie (1987) claimed that the cognitive architecture that supports understanding that someone can have a false belief is the same architecture needed to understand pretense, so pretending facilitates use of those cognitive structures (Flavell, 1988;Forguson & Gopnik, 1988;Moses & Chandler, 1992). By this view all pretending should be related to ToM. Simulation theory suggests that children come to perceive others' mental states by analogy to the self, imagining oneself in another's shoes (Harris, 1995). Role play provides children with practice at such simulations, so for simu- lation theorists sociodramatic play should particularly assist ToM. We focus first on solitary and then social pretend play (see Table 7). Since the early 1980s, ToM has been tested with a well-known set of tasks, including the false belief task, in which someone thinks something that is not true, and children are asked to contrast the belief with reality (Wimmer & Perner, 1983); the appearance- reality task (Flavell, Flavell, & Green, 1983), in which children state the real and apparent identities of a fake object (like a candle that looks like an apple); and perspective taking tasks, in which children contrast points of view (Flavell, 1978). Earlier studies used tasks like Borke's (1971) affective and Piaget's \"3 Moun- tains\" spatial perspective taking tasks.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Solitary Pretense", "text": "We found seven studies examining correlations between ToM and different measures of solitary pretend play. These measures concerned the sophistication and frequency of children's object substitutions. Four studies used the imaginary object task (Overton & Jackson, 1973), which asks children to pretend various actions, like to brush their teeth (self-directed) or cut paper (object- directed). Using an imaginary object (shaping one's hand as if holding a toothbrush) receives a higher score than using a body part (like one's finger) to perform the act, because older children tend to use imaginary objects more (Taylor & Carlson, 1997). Two studies found a positive relationship between ToM and this task (Nielsen & Dissanayake, 2000;Suddendorf, Fletcher-Flinn, & Johnston, 1999), one found it for self-but not object-directed acts (Taylor & Carlson, 1997), and the fourth (which used only self- directed acts) did not find it (Lillard, 2001b). Thus using an imaginary object as opposed to a body part-considered a higher level of pretending-is inconsistently related to ToM scores. The methods used in these studies were very similar, although Lillard and Taylor and Carlson had test session gaps of at least 1 week. In contrast, for the others much of the testing occurred in the same session, so masking status of the experimenter could be a concern here. Object substitution with external objects has also been used as a measure of solitary pretend play. It has been coded by having children play freely with blocks for 3 min and then describe what they built; higher scores reflect more elaborate descriptions. Taylor and Carlson (1997) found a significant correlation (r \ud97b\udf59 .23) between ToM and free block scores; Lillard (2001b), using a much smaller sample, found a trend (r \ud97b\udf59 .27, thus a similar effect size). However, the free block variance-like that of the false belief task-is partially accounted for by verbal skills-Lillard (2001b) found free block correlated .21 with PPVT-so language could account for much of the ToM relationship. Another study coded object substitution only from observation and found no significant relationship to ToM (Schwebel, Rosen, & Singer, 1999). On the other hand, teacher endorsement of the descriptor \"engages in simple make-believe play alone\" (which plausibly stems from observing object substitutions) was related (r \ud97b\udf59 .37) to ToM score (Lalonde & Chandler, 1995). Thus object substitution in free play is also inconsistently related to ToM, with three positive results- some of which could be carried by language-and a null one. We found only one experimental study of solitary pretend play and ToM, and it is unsatisfactory (Matthews, Beebe, & Bopp, 1980): Prior play might have influenced performance on one of three spatial perspective taking tasks, but no statistical analyses were done, and results are inconsistent regardless. The inconsistent relationships between solitary pretending and ToM could support equifinality or epiphenomenalism; they do not suggest a crucial causal role.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Social Pretense", "text": "Correlational studies. Four studies correlating social pretend play with early measures of ToM had inconsistent results. For spatial perspective taking, one study showed a positive relationship (Rubin & Maioni, 1975), 7 but two did not (Cole & LaVoie, 1985;Peisach & Hardeman, 1985); yet the latter found a positive rela- tionship to moral perspective taking. For affective perspective taking, one had a positive relationship to social pretend play complexity but not amount (Connolly & Doyle, 1984), but two found no relationship (Cole & LaVoie, 1985;Rubin & Maioni, 1975). Connolly and Doyle (1984) found no relationship to cog- nitive perspective taking (matching gifts to recipients, from Fla- vell, Botkin, Fry, Wright, & Jarvis, 1968). Thus earlier studies of this issue were inconsistent, with three positive relationships and six null ones. One possible reason for some inconsistency is that Borke's affective perspective-taking task could rely on vocabulary more than deep understanding, reflecting a language-pretend play relationship. Spatial perspective taking is influenced by familiarity with task materials (Borke, 1975), and perhaps this led to incon- sistent results for it. Studies relating social cognition tasks of this earlier era to those of today would be useful. More recent studies have operationalized social pretend play in two ways, broadly speaking: (a) impersonation, including role enactment and having an imaginary companion, and (b) joint proposals and role assignments, in which a child states what they are pretending. We located 12 studies relating these behaviors to ToM (Astington & Jenkins, 1995;Doyle & Connolly, 1989;J. Dunn & Cutting, 1999;Goldstein & Winner, 2010; Hughes & Note. Type of study: C \ud97b\udf59 correlational; CL \ud97b\udf59 cross-lag correlation; E \ud97b\udf59 experimental; T \ud97b\udf59 training. Type of play: Const. \ud97b\udf59 construction play (e.g., blocks); Solo \ud97b\udf59 pretend play alone; SPP \ud97b\udf59 social pretend play (including imaginary companion play in Taylor studies); PP \ud97b\udf59 pretend play (social unspecified); D \ud97b\udf59 enacting stories with dolls or other children. \ud97b\udf59: positive relationship to play; \ud97b\udf59: no correlation or play \ud97b\udf59 nonplay condition; \ud97b\udf59 \ud97b\udf59 negative relationship to play. Masking: Intervention (Int) or posttest experimenters (Exp). If masking status was not specified, we assume experimenters were not masked, since that is the unmarked case. Masking for correlational studies is omitted because it is rarely mentioned, even when it is likely (because play observations occurred several years earlier than testing, for example). AR \ud97b\udf59 appearance-reality; PT \ud97b\udf59 perspective taking; IC \ud97b\udf59 imaginary companion; ToM \ud97b\udf59 theory of mind. Dunn, 1997;Lalonde & Chandler, 1995;Lindsey & Colwell, 2003;Nielsen & Dissanayake, 2000;Schwebel et al., 1999;Taylor & Carlson, 1997;Taylor, Carlson, Maring, Gerow, & Charley, 2004;Youngblade & Dunn, 1995), and findings were inconsistent. For example, some studies found relationships only with specific tasks, like with appearance reality but not false belief ( Schwebel et al., 1999), or with role enactment but not explicit role assignment (Youngblade & Dunn, 1995), or with joint proposals and role assignments but not social pretend play generally (Astington & Jenkins, 1995). Several studies used a large age range but did not partial out age, when both social pretend play and ToM scores increase with age (Taylor & Carlson, 1997). With age partialled out, in some studies the association became a trend (Doyle & Connolly, 1989) or disappeared entirely (Hughes & Dunn, 1997). Other studies seem less solid because maternal education was very strongly related to both social pretense and ToM, and socialization practices could undergird both outcomes independently (J. Dunn & Cutting, 1999). However, Youngblade and Dunn (1995) used a limited age range and partialled out verbal ability, and Astington and Jenkins (1995) partialled out both age and verbal ability; both studies still found relationships between aspects of social pretense and ToM; Schwebel et al. (1999) partialled both factors and still found a relationship but only to appearance-reality. Although it is incon- sistent, the relationship appears often enough that it is frequently remarked upon in the literature (Harris, 2000;Kavanaugh, 2006;Lillard, 2001a). It is important to consider direction of effects. Longitudinal studies, of which we found two, can shed light here. Youngblade and Dunn (1995) assessed pretending at 33 months and ToM at 40 months and found that role enactment (but not total amount of social pretending, diversity of pretend themes, or explicit verbal role assignment) predicted later ToM. The role enactment finding is consistent with simulation theory and a pretend-to-ToM causal view. In a study cited far less often (Google Scholar, March 19, 2012), Jenkins and Astington (2000) tested children ages 34 -45 months three times-at an initial point, then 3.5 and 7 months later-for ToM, language, and the proportion of their social pretend play turns dedicated to making joint proposals in pretense and explicitly assigning roles. Here, earlier ToM pre- dicted later social pretend play, but not the reverse. The different directions of effects seen in these two studies can be explained in at least four ways: (a) The first study did not allow for finding the ToM-to-pretend direction, since it did not include ToM tests at earlier time points; (b) the first study used verbal and behavioral indicators of pretending, whereas the second used only verbal ones; (c) only role enactment is related, and the second study did not measure it; (d) the first study used younger children. More longitudinal studies examining a range of variables are needed to more definitively determine the direction of effects between social pretend play and ToM. Yet still a third variable could underlie results found with correlational methods; training studies provide the best test. Training studies. Many pretend play-social cognition train- ing studies were conducted in the 1970s (see summary in Rubin, 1980), inspired by Smilansky (1968). Noting that middle-class children play more than lower class ones and show many of the advantages pretense might be expected to confer, Smilansky trained children of lower class immigrants to Israel to engage in sociodramatic play for 90 min/day. Nine weeks of training in- creased their pretending. As an afterthought she also looked at their verbal skills. She did not test for any of the social cognitive outcomes that she theorized pretense might influence (cf. Burns & Brainerd, 1979), but many others have done so, making social cognition the most studied potential outcome of pretend play training. Yet all but one study had a serious methodological shortcoming preventing clear conclusions: The experimenters were not masked, or the level of adult contact across conditions was very different, or the training explicitly \"taught to the test.\" Some studies have additional problems as well. The most solid study had null results. The first six studies we discuss lacked masked experimenters, which we now know is problematic in studies of the effects of play. Saltz and Johnson (1974) found improvement in affective perspective taking following thematic fantasy training (acting out stories) but not following training in identifying object dimensions or no training at all. However, Saltz's second study ( Saltz et al., 1977) showed no main effect of thematic fantasy or sociodramatic play, yet they went on to compare thematic fantasy only with the other three groups combined (sociodramatic play, fantasy discus- sion, and control), and only for the first two years of the 3-year study-not accepted statistical practice. Burns and Brainerd (1979) engaged children in sociodramatic play or construction play, preceded and followed by a variety of perspective taking tasks. A summed perspective taking score re- vealed a significant effect of sociodramatic relative to construction play, and each improved more than the no treatment group. How- ever, effects were inconsistent across tasks. A doctoral student in Brainerd's laboratory went on to do a more rigorous and larger replication and extension of this study (Scheffman, 1981, described in Brainerd, 1982. As summarized by Brainerd, \"when it comes to the question of how powerful dramatic (and constructive) play is as an enhancer of conceptual knowledge, the results were extremely disappointing. For all intents and purposes, there were no learning effects\" (p. 125). Two more recent studies (Goldstein & Winner, 2010, 2012) examined whether acting classes (compared to other arts classes) improve ToM and empathy; they lacked random assignment as well. Children who want to take acting classes might differ from other children in ways that influence ToM development, and there were in fact group differences in some key measures prior to training. In addition, although positive results were found on some tests, results were inconcordant across the experiments. A sixth unmasked training study also had unequal adult contact across conditions (Dockett, 1998). The experimental group visited a pizza restaurant and then a pizza play area was set up in their classroom. Over the next month, an experimenter encouraged their play toward more complex levels. Teachers added props and resources to promote pizza play, posted a photographic record of the play on the walls, and devoted large blocks of time to it. From pre-to post-and even on a delayed posttest, the play group improved more on ToM. Some 1970s-era training studies also had unequal adult contact across experimental and control groups but did use masked exper- imenters. Fink (1976) found that sociodramatic play led to in- creases in social role but not spatial perspective taking, and Ros- en's (1974) sociodramatic play group improved significantly more on perceptual and semantic role taking. In this latter study the trained group had four times more adult contact. P. K. Smith and Syddall (1978) explicitly equalized experi- menter contact for two tutoring groups-skills (jigsaw puzzles, games) and sociodramatic play-although experimenters were not masked. Cognitive perspective taking improved significantly more in the play group. Smith and Syddall then questioned the tasks they (and Rosen, 1974) had used. One was the gift assignment task and the other was putting objects with the appropriate person, for example a stethoscope with a doctor. Such tasks require simple matching paralleled in the sociodramatic play training, when props are assigned to roles. Three other early studies examining the influence of role play training on social cognition, although quite sound in most ways, also seem to \"teach to the test\" (Chandler, 1973;Chandler, Greenspan, & Barenboim, 1974;Iannotti, 1978). Because of his concern about this issue, Smith conducted a follow-up study with a more diverse array of social cognitive tasks and this time also employed a masked posttester and equalized contact (P. K. Smith, et al., 1981). Using these very sound meth- ods, they found no improvement in role taking in the play tutoring group or the skills training group. The authors concluded that some aspect of adult contact, rather than pretense, was responsible for prior findings. One might fault this study in that one of two settings showed no posttraining increase in fantasy play, but it still ought to have obtained results at the setting where fantasy play did increase. Thus the one study in the group without a significant methodological problem showed no significant increases on ToM measures for either pretend or skills tutoring.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Summary", "text": "Logically, it seems like pretend play could well assist ToM, but solid evidence that it does so is lacking. Correlational results have been inconsistent, and one study has suggested a reverse direction of effects: A more developed ToM enables sociodramatic play. Many training studies have been conducted, but most had at least one serious methodological shortcoming, and there are also im- portant failures to replicate. The most solid study showed no improvement in ToM from either skills or pretend play training. The inconsistent correlational results and hints of a reverse direc- tion of effects lead us to see the body of evidence as being more supportive of epiphenomenalism than equifinality. Children who have more advanced ToM skills often also engage in more ad- vanced pretend play, but possibly because of some third variable, like having parents (or experimenters) who interact with them in ways that encourage mental state reasoning and pretend play.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Social Skills", "text": "Next we examine evidence that pretend play improves chil- dren's social skills. Social skills are distinguished from ToM in that they involve enactment but not necessarily knowledge. Al- though social competence is empirically associated with ToM (Bosacki & Astington, 1999;Watson, Nixon, Wilson, & Capage, 1999), knowledge can be inert; alternatively, one could conceiv- ably act in a socially skilled manner without underlying ToM knowledge. Two theories guide research in this area. Both suggest that pretend play causes social skills; one claims that even solitary pretending does so (Stagnitti & Unsworth, 2000) because children often pretend about emotional and difficult issues; working through such issues in pretense would enhance social skills gen- erally (Bretherton, 1989;Fein, 1989;Fein & Kinney, 1994). The second theory highlights social pretend play because it requires negotiation (Harris, 2000;Lillard, Pinkham, & Smith, 2011). Stud- ies concerned with social skills tend to measure children's behav- ior in real-life settings with peer sociometric ratings and teacher or parent reports.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Solitary Pretense", "text": "Addressing the first theory are five studies correlating social skills and solitary pretense (see Table 8). The first two were playground studies using peer sociometric ratings; one found no relationship (Rubin, 1982), and the other found a negative one (Rubin & Daniels-Beirness, 1983). However, as has been noted, on a school playground the norm is social pretend play, and children who instead play alone might already have poor social skills. Thus, these results do not really speak to whether solitary pretend play might develop social skills. The other three studies provide a better test. Two found a positive correlation between children's solitary pretend play in a laboratory setting and a teacher measure of social competence (McAloney & Stagnitti, 2009;Uren & Stagnitti, 2009), but the third (Swindells & Stagnitti, 2006) found no relationship using a parent measure. Close examination of the measures used in the first two experiments reveals that social compliance, tapped by both the teacher report and pretend play measures, might undergird the relationships. Further research should examine this possibility.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Social Pretense", "text": "If social pretend play helps develop social skills, one would expect that (a) one would see more socially competent behavior within social pretend play than outside of it, (b) more advanced social pretend play, whether occurring naturally or after training, would predict more advanced social skills, and (c) training in social pretend play would improve social skills. Does social pretend play entail more advanced social behav- ior? We found five studies examining this. Two found that social pretend play involved greater social skills than social non- pretend play. In a study comparing social pretend and literal activities, pretense interactions lasted longer, involved larger groups of children, and had more positive and less negative affect (Connolly & Doyle, 1984). The second study replicated this and also found more reciprocity/complexity in social pretend play (Connolly, Doyle, & Reznik, 1988). The third study occurred in the laboratory and only involved girl dyads; it had mixed results. Social pretend play involved more verbal exchanges and shared focus than social literal play, but it also involved longer, less often resolved conflicts ( de Lorimier, Doyle, & Tessier, 1995). The fact that girl dyads were more able to solve their conflicts outside of the pretense setting might suggest that they were more socially skilled in literal settings. Against the idea that social pretend play entails more social skills, Howes and Matheson (1992) found preschoolers' level of social competence was equal across social and nonsocial pretense contexts, and Doyle, Doehring, Tessier, de Lorimier, and Shapiro (1992) found the complexity and positive affect (which could index prosocial interaction) of peer exchanges to be equal in pretense and nonpretense contexts, although the latency to com-plex interaction was shorter in pretense ( Doyle et al., 1992). A strength of this latter study is that it controlled for whether children were frequent or infrequent pretenders. Dyads in which both children were classified in another setting as frequent pretenders engaged in more complex social interaction than dyads in which both children were classified as infrequent pretenders, which could explain the results of studies that found more complex interactions in pretend play: It could be that more socially competent children are more apt to engage peers in social pretend play, in which case the direction of effects is opposite to that speculated. This leads to the next issue. Does more advanced social pretend play predict greater social skills? The findings on this are also mixed, with two studies finding it to be the case, two finding it not to be, and two revealing different results by gender. Rubin and Maioni (1975) found a positive correlation between sociometric ratings and social pretend play in 3-and 4-year-olds. Connolly and Doyle (1984) replicated this and also found a positive relationship with teacher reports of social competence with peers, but not with the social skill of following teachers' rules. In contrast, Rubin and Daniels- Beirness (1983) did not find a positive relationship between so- ciometric status and social pretend play, and Galyer and Evans (2001) failed to find a positive relationship to teacher-reported social skills. Finally, Lindsey and Colwell (2003) and Colwell and Lindsey (2005) had gender-specific findings, with more mixed-sex play predicting higher same-sex sociometric and teacher-reported social competence ratings for girls but lower ones for boys. This result is also against the view that pretense leads to better social skills, since boys who pretend with girls should gain social skills just as well as boys who pretend with boys. Does social pretend play training improve social skills? We located only one study that trained children in social pretend play (drama lessons) and then examined social skills (Schellenberg, 2004). The purpose of the study (mentioned earlier) was actually to see if music lessons enhance IQ, and parent assessments of children's social competence (adaptability, social skills, and leadership) were administered for exploratory reasons. Six-year- olds were randomly assigned to 36 weeks of keyboard lessons, voice lessons, drama lessons, or no lessons. Consistent with the hypothesis, children who received keyboard and voice lessons increased most in IQ. However, the drama group unexpectedly increased most in social competence. It is not clear whether this result stems from playing out roles or from the social interaction inherent in doing so versus doing keyboard or voice lessons. Still the findings are very interesting, and future research should clearly explore them with control conditions matched for social contact.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Summary", "text": "Results from studies examining correlations between social skills and pretend play are inconsistent, which is against the crucial causal position. The serendipitous finding in a training study suggests a potential causal relationship from drama training to social skills, which would (along with inconsistent correlations) be in keeping with equifinality, but alternate epiphenomenal expla- nations must be ruled out (e.g., perhaps level of interaction was different in the drama training). Pretending is perhaps a route to social skills, but without more convincing evidence it is equally feasible that social pretending and social skills emerge from a latent factor like sociability or some aspect of interaction.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Symbolic Understanding: Early Language", "text": "In pretend play children use one object to stand for another, hence, a symbol. Language is also symbolic (Piaget, 1962;Werner & Kaplan, 1963). Aligning with the causal position, some theorists Note. Type of study: C \ud97b\udf59 correlational; O \ud97b\udf59 observational studies comparing means rather than correlating; E \ud97b\udf59 experimental; T \ud97b\udf59 training. Type of play: Solo \ud97b\udf59 pretend play alone; SPP \ud97b\udf59 social pretend play; PP \ud97b\udf59 pretend play (social unspecified). \ud97b\udf59: positive relationship to play; \ud97b\udf59: no correlation or play \ud97b\udf59 nonplay condition; \ud97b\udf59 \ud97b\udf59 negative relationship to play. Masking: Intervention (Int) or posttest experimenters (Exp). If masking status was not specified, we assume experimenters were not masked, since that is the unmarked case. Masking for correlational studies is omitted because it is rarely mentioned, even when it is likely (because play observations occurred several years earlier than testing, for example). have claimed that repeated practice using symbols in pretend play \"contributes greatly to language development\" (Miller & Almon, 2009, p. 63;see also Ervin-Tripp, 1991). Yet perhaps pretend play is just one of many behaviors that can improve children's language or is epiphenomenal to some other factors that give rise to lan- guage. In this section we examine evidence for these views with regard to early language, namely, acquiring first words and syntax. We end with a brief note about the research on how pretend play affects written language or literacy. Findings are summarized in Table 9.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Correlational Concurrent Studies", "text": "Many studies have revealed concurrent associations between language development and pretend play (Bates, 1979;Bates, Bretherton, Snyder, Shore, & Volterra, 1980;Casby & Corte, 1987;Corrigan, 1982;Elias & Berk, 2002;Jurkovic, 1978;Lewis, Boucher, Lupton, & Watson, 2000;Lyytinen, Poikkeus, & Laakso, 1997;McCune, 1995;Shore, 1986), with particularly strong cor- relations for children under 4 years of age (Doswell, Lewis, Sylva, & Boucher, 1994). 8 They have looked at different aspects of language (vocabulary size in comprehension and/or production, syntax) measured in different ways (checklist, free speech, elicited speech), and different aspects of pretend play (object substitutions, doll-directed acts, length of play sequences) measured in both free and elicited play situations. The evidence that pretend play and language are related early in development is compelling. Is there evidence for causality?", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Longitudinal Studies", "text": "Longitudinal studies have attempted to determine whether pre- tend play predicts symbolic understanding. One study of 10 chil- dren tested monthly, from 8 to 24 months, 9 was conducted along- side a cross-sectional study of 102 children, six of each age from 8 to 24 months (McCune, 1995). The sample size restricted ana- lytic techniques, but McNemar's tests indicated that new levels of pretend play emerged roughly 2 months prior to what was consid- ered the analogous level of language skill. 10 A similar longitudinal study was carried out with four Japanese children, with similar results (Ogura, 1991). McCune (1995), like most authors in this domain, did not assume a causal relationship from play to lan- guage, but instead believed an underlying third mechanism was responsible for the relationship (thus taking the epiphenomenon view). McCune posited that language emerges later than play because it relies on later-maturing vocal control. Larger longitudinal samples have permitted cross-lagged corre- lational and regression analyses. Four studies tested children at the beginning and end of the second year; only one (Bornstein, Vibbert, Tal, & O'Donnell, 1992) did not find cross-lagged correla- tions, and it used more limited measures. Two found relationships between other-directed play early in the second year and measures of comprehension, production, and syntax later in that year (Lyytinen, Laakso, Poikkeus, & Rita, 1999;Ungerer & Sigman, 1984). Ungerer and Sigman (1984) suggested that other-directed play indicated an interest in others and communication that is then transferred to language. The fourth study found no association from play to productive vocabulary or mean length of utterance (MLU) but did find a relationship to semantic diversity, or the number of categories of speech represented (Tamis-LeMonda & Bornstein, 1994). Two studies also found the reverse direction of effects, from language to play (Tamis-LeMonda & Bornstein, 1994;Ungerer & Sigman, 1984), although for the former the finding disappeared when mother behaviors were partialled out. Note that there is some methodological bias against finding rela- tionships from language to play: One study did not test play at the older ages ( Lyytinen et al., 1999), and Ungerer and Sigman used just one language measure at the earlier age versus five later (although they did find a positive relationship for that one mea- sure). Finally, a fifth longitudinal study tested slightly older chil- dren-20 and 28 months-and did not find that play predicted language (Shore, O'Connell, & Bates, 1984). In sum, most research shows that children who are more ad- vanced in their play around 1 year of age are more advanced in one or more aspects of their language around 2. Cross-lagged studies across the second (but not into the third) year are mostly consistent with the possibility that play could be crucially important to language, but the reverse direction of effects is also possible, as is a third underlying variable. Thus the correlational evidence cannot distinguish the three accounts; intervention studies are needed.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Play-Language Interventions", "text": "We found four studies that increased play and monitored lan- guage, three with preschool-aged children. The best control con- dition was in Smilansky's (1968) study training children of immi- grants to Israel in pretend play or exposing them to other \"meaningful experiences.\" Although the play group increased im- pressively in some aspects of language, statistical analyses were not conducted. For the other three studies, language increases were seen, but it is not clear what aspect of the intervention contributed. Lovinger (1974) engaged 20 low-income 4-year-olds in a daily hour of pretend play for 25 weeks; a control group had no intervention. The experimental group's language increased, but possibly simply due to increased adult contact and conversation. Levy, Schaefer, and Phelps (1986) used a pre-posttest design with no control group. They expanded a university preschool classroom's pretend play area and trained the children in pretend play; in addition, these children were exposed to more songs, stories, field trips, and classroom visitors. Boys' but not girls' PPVT scores increased. 8 These studies are not listed in the table because the list is long and they are not discussed in any detail. 9 This is an approximation: For each child, testing was stopped when the highest levels were achieved. 10 Determining what is analogous across the domains of language and pretense is a challenge. McCune (1995) sequenced play into five levels and matched them with verbal ability. Level 1, presymbolic schemes (putting a cup to one's lips), had no lexical match, but Level 2, self-directed pretense (carrying out the full drinking behavior, perhaps with sound effects or exaggeration) and Level 3, other-directed pretense (having a doll drink), were equated with lexical onset (5\ud97b\udf59 words). Level 4, pretend combinations (pouring before having the doll drink), was equated with multiword onset (producing 3\ud97b\udf59 word combinations), and Level 5, hierarchical pretense (pretend acts in the absence of perceptual support, e.g., finding a cup for the doll to drink from), was equated with rule-governed combinations or syntax. The most recent study distributed sets of blocks with instructions about how to play with them to parents of toddlers (Christakis, Zimmerman, & Garrison, 2007). Low-income children whose fam- ilies received this block set, compared with controls who did not, had more advanced language postintervention, but (a) we do not know if children pretended with the blocks, and (b) the interven- tion might have increased parent interaction, which then improved language. More controlled intervention studies are needed to show whether pretend play specifically might cause language develop- ment.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Literacy", "text": "Ample research has shown that exposure to literacy play mate- rials (like plastic letters or a model post office) increases literacy (Neuman & Roskos, 1992;Roskos & Neuman, 1998). This is similar to findings with board games and math (Ramani & Siegler, 2008). Although it is useful to know that one can influence what children play with, and through those play materials can influence their skills, it is particular to specific pretend play content and not pretend play generally. Increased exposure to that same content outside of pretend contexts might be equally effective. What could be important here with regard to pretend play is motivation. If pretend play is a context that especially motivates engagement with literacy materials, this is significant.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Summary", "text": "Children's levels of pretend play and their early language de- velopment do appear to be related, with pretending preceding language. Researchers in this area tend to think that the domains are related due to an underlying symbolic function-an epiphe- nomenal reason. A causal account is possible, although a reverse direction of effects is as well, and better intervention studies are needed to determine which of the three models is best supported in this domain.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Narrative", "text": "Another development that pretend play is claimed to assist is narrative, the ability to tell and comprehend stories. Some have claimed that pretend play and narrative are different ends of a continuum: \"Play . . . [is] story in action, just as storytelling is play put into narrative form\" (Paley, 1990, p. 4). If this is the case, then practice or training with either skill should improve the other. Other theories focus on specific pretense skills that might benefit narrative skills. For example, pretending could foster metalinguis- tic skills necessary for storytelling, because during social pretend play communication often occurs at the meta level (Garvey & Kramer, 1989;Giffin, 1984). In addition, pretense role play re- quires a child to imagine and track the perspective of another character, a skill also used when following and telling a story (O'Neill & Shultis, 2007;Ziegler, Mitchell, & Currie, 2005). Finally, embodied cognition ( Lillard, 2005, Chapter 2;Scott, Harris, & Rothe, 2001) would predict children would remember sto- ries better after acting them out. We located 14 studies of whether pretend play leads to better narrative skills, operationalized as storytelling, memory, and comprehension (see Table 10).", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Correlational Studies", "text": "Three studies examined whether children who naturally en- gage in more pretend play have better narrative skills. In the initial one, Johnson (1976) coded the free play of 3-to 5-year- olds as nonfantasy, social fantasy, or nonsocial fantasy and later gave children a story completion task. Scores were significantly correlated with social but not solitary pretend play. Trionfi and Reese (2009) compared 5-year-olds with and without imaginary companions (ICs), and children with ICs scored higher on narrative quality (but not memory) when retelling a story and telling personal narratives. Both of these studies suggest a possible causal relationship from pretend play to narrative. A longitudinal correlational study would provide better evidence. Note. Type of study: C \ud97b\udf59 correlational; CL \ud97b\udf59 cross-lag correlation; E \ud97b\udf59 experimental; T \ud97b\udf59 training. Type of play: SPP \ud97b\udf59 social pretend play; PP \ud97b\udf59 pretend play (social unspecified). \ud97b\udf59: positive relationship to play; \ud97b\udf59: no correlation or play \ud97b\udf59 nonplay condition; \ud97b\udf59 \ud97b\udf59 negative relationship to play. Masking: Intervention (Int) or posttest experimenters (Exp). If masking status was not specified, we assume experimenters were not masked, since that is the unmarked case. Masking for correlational studies is omitted because it is rarely mentioned, even when it is likely (because play observations occurred several years earlier than testing, for example). MLU \ud97b\udf59 mean length of utterance. a Numerous other studies show this; 10 are cited but not described in the section. In the third correlational study researchers coded the social pretend play talk of 48 Head Start children and their mothers during a free play session in both kindergarten and fourth grade (Tenenbaum, Snow, Roach, & Kurland, 2005), and when the children were in sixth grade, told them a novel story and then tested for comprehension. Controlling for maternal education and age 3 verbal ability, neither child's nor mother's pretend play talk in kindergarten or fourth grade predicted sixth grade story comprehension. Thus narrative production but not com- prehension or memory might be associated with pretend play. One possibility here is that having a high fantasy predisposition results in (a) pretending a lot with one's peers, (b) having an IC, and (c) telling elaborate stories; this is an epiphenomenal ex- planation. However, experimental studies are the best source of evidence for whether and how pretend play affects narrative skills.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Experimental Studies", "text": "Three studies have addressed whether children tell better stories when they are pretending, with pretending operational- ized as being provided with small model toys. Ilgaz and Aksu- Ko\u00e7 (2005) and Benson (1993) contrasted the stories children told with and without toys. The former study had no external supports for the control condition, whereas Benson provided the control group with drawings of characters. Ilgaz and Aksu-Ko\u00e7 found no condition differences on a first pass. They then excluded the youngest few children in each age group (3-, 4-, and 5-year-olds, original n \ud97b\udf59 10 per age, within-subject design) and conducted another analysis of variance with 20 children. With this approach, they obtained a near-significant result ( p \ud97b\udf59 .05) of more complex stories with toy props for the older 4-year-olds only. Especially with the decision to exclude certain subjects being post hoc, this is properly considered a null result. Benson found that children in the control condition actually told more complex stories. The third study involved retelling a story that had been read to them rather than generating an original story (Kim, 1999), as well as answering questions about it (considered below). Children given toy animals produced more complex and complete narrative retellings than children who were given pictures of the same animals. Results are thus inconsistent as to whether pretense-supporting props lead chil- dren to tell better stories; perhaps pictures (as in Benson's control condition) help children more with making up new stories, and props are more helpful when the task is to retell a previously heard story. Props might assist story retellings via embodied cognition. Supporting this possibility are three other studies looking at the impact of pretense reenactment on story memory. Pellegrini and Galda (1982) compared three groups of 5-to 7-year-olds, each of which had two training sessions followed by a test session. In all three sessions a story was read, and then a pretend play group reenacted the story, a discussion group answered questions about it, and a drawing group drew pictures about it. Children in the reenactment group scored better both at retelling the story and answering questions about it. Pellegrini (1984) also found that peer-directed play was as effective for story memory as adult- directed play. At issue is whether the pretend play was the source of this effect versus embodied cognition (moving one's body in ways that represent the story). Testing this, Marbach and Yawkey (1980) compared 5-year-olds' recall of a story in two pretend conditions: reenactment with one's whole body or with a puppet, thus only with one's hands. Although both experimental conditions could be considered pretend play, children who reenacted the story with their whole body showed significantly better recall than children who did so with a puppet. Kim (1999) also supported this: Chil- Note. Type of study: C \ud97b\udf59 correlational; CL \ud97b\udf59 cross-lag correlation; E \ud97b\udf59 experimental; T \ud97b\udf59 training. Type of play: Solo \ud97b\udf59 pretend play alone; SPP \ud97b\udf59 social pretend play; D \ud97b\udf59 enacting stories with dolls or other children; IC \ud97b\udf59 imaginary companion. \ud97b\udf59: positive relationship to play; \ud97b\udf59: no correlation or play \ud97b\udf59 nonplay condition; \ud97b\udf59 \ud97b\udf59 negative relationship to play. Masking: Intervention (Int) or posttest experimenters (Exp). If masking status was not specified, we assume experimenters were not masked, since that is the unmarked case. Masking for correlational studies is omitted because it is rarely mentioned, even when it is likely (because play observations occurred several years earlier than testing, for example). Tr \ud97b\udf59 teacher. dren who retold the story with toy props were no better at answer- ing memory questions about the story than were children who had pictures of the animals. In sum, research suggests that pretend role play, but not toy props/puppets, leads to better memory for stories, due to embodied cognition; also when retelling but not when making up new stories, having toy props leads to more elaborate narratives. Thus certain types of pretend play influence certain aspects of narrative devel- opment, but the effects are limited.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Training Studies", "text": "We located six studies that trained children in pretend play and then examined narrative skills. Saltz and Johnson (1974) had preschoolers act out stories (thematic fantasy play) or label and classify objects. A narrative memory test involved arranging six pictures in proper sequence while retelling a story, and a story generation task involved making up new stories with sets of five pictures. Thematic fantasy play led to significantly higher scores on the story memory task and longer stories with more inferences and connectives in the story generation task. Although initially compelling, this study has several problems, such as lack of masked experimenters and teaching to the test. On the latter point, fantasy training involved going over stories repeatedly including answering questions about them (\"Why did the billy goats cross over the bridge?\"), so one cannot know if it was the acting out or simply the deeper consideration of stories that helped. Saltz et al. (1977) addressed teaching to the test by including a fantasy story discussion group along with fantasy play training; the other two groups were sociodramatic play training and an arts and crafts control. On two narrative tasks similar to the ones used earlier, all children did poorly, and there were no condition dif- ferences. However, for the story generation task and the high IQ subset of children, a significant difference was found between the two pretend play and two nonpretend play conditions. Yet even for this subset, scores were between 0 (naming pictures but with no relation to others) and 1 (minimal story, no elaboration). Thus an unplanned analysis led to a small result on one of several measures with unmasked experimenters for a subset of children. In a more tightly controlled study that took the precaution of using masked experimenters, Baumer, Ferholt, and Lecusay (2005) tested the effects of a kindergarten pretend play intervention im- plemented for 2 hr/week for 14 weeks. In one experimental and one control classroom, the intervention began with hearing and discussing The Lion, the Witch, and the Wardrobe. Then in the pretend play classroom, four experimenters reenacted passages from the book, and eventually the teacher and then the children joined in. In the control classroom, the teacher continued to read and discuss the story, and the same experimenters drew pictures and wrote stories with the children. Results showed that children in the pretend play classroom had significantly higher posttest scores for story comprehension and story length and coherence (but not complexity). Three concerns with this study are small samples (12 control, 17 experimental), classrooms with very different ethnic compositions that could influence narrative trajectories, and the use of one full-time classroom teacher per condition-the teachers might have differed in narrative skill teaching effectiveness out- side the intervention. Although the results are intriguing, replica- tion is needed. Another study used many more teachers-13 in all (Silvern, 1986). Children from kindergarten to third grade (n \ud97b\udf59 505) acted out or were simply read stories, preceded and followed by 10 multiple-choice questions. Teachers administered all aspects of the procedure and served as their own controls by administering the experimental condition to their own class and the control condition to another one. Although on face this seems like a good idea, teachers have a personal stake in their own class (the experimental one) doing well; having multiple disinterested outsiders as exper- imenters would be better. Pretend training did result in signifi- cantly more gain for teachers' own classrooms. Further analysis showed this was only for children with poor narrative skills (Williamson & Silvern, 1990). Although narrative skill and IQ might be orthogonal, the contrast with the Saltz et al. (1977) finding that only higher IQ children were affected by training is worth noting. Except for the sociodramatic play condition in Saltz et al. (1977), the intervention studies reviewed so far have involved reenacting stories. Dansky (1980a) used more everyday pretend play, with low-income preschoolers in three 30-min sessions per week for 3 weeks. In the sociodramatic play group, 12 children were encouraged to play out everyday themes. A free play group received no direction in their play, and an exploration training group discussed objects in a manner that appeared to equalize adult contact. The intervention increased the amount of pretend play in the first group, whereas the other groups-like the first group preintervention-showed little pretend play. Masked experiment- ers administered three narrative posttests of story memory and quality, and children in the sociodramatic training group scored higher on nine of 10 measures. This study is promising for the hypothesis that pretend play causes narrative skills, especially because the methods were sound (the free play group had less adult contact, but contact appears similar for the exploration group) and children were not acting out stories but rather were merely encour- aged to engage in the types of everyday pretend they might engage in on their own. Replication with a larger sample size would be helpful.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Summary", "text": "The research reviewed here suggests that providing toys does not enhance new stories but does help with story retelling. Story memory is helped by role play, probably due to embodied cogni- tion. Children who pretend more also appear to tell more elaborate stories, although when older they were not better at story compre- hension. Thus experimental and correlational studies had qualified results for the hypothesis that pretend play causes narrative devel- opment. Although most of the training studies have methodolog- ical shortcomings, the strongest one suggests that social pretend play positively influences narrative development. This would make sense, since social pretend play does involve creating and acting out narrative. Still, this evidence does not make a solid case that pretend play is crucial for narrative development. Further research should examine additional alternate routes, like story reading without enactment. Although Baumer et al. did this, the confounding of intervention and classroom teacher is problematic. In addition, researchers in this area should be careful to distinguish the different aspects of narrative development under consideration: telling stories, story memory, and story comprehension.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Pretend Play and Self-Regulation", "text": "Pretend play is sometimes claimed to improve children's self- regulation (Bergen, 2002;Bredekamp, 2004), which involves the top-down cognitive processes called the executive functions, and (possibly overlapping) processes that regulate one's level of emo- tional arousal (Blair & Raver, 2012). We focus on each in turn.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Executive Function", "text": "Executive function (EF) is an umbrella term for a suite of related skills including inhibitory control, working memory, and attention (Blair & Razza, 2007;Garon, Bryson, & Smith, 2008;Miyake, Friedman, Emerson, Witzki, & Howerter, 2000;Rueda, Posner, & Rothbart, 2005). Vygotsky (1978) claimed pretending helps such skills based on two aspects of pretend play. First, it involves behaviors that are internally guided as opposed to stimulus bound. Pretending involves operating at two levels, the real and the pretend (Lillard, 1993); the pretend level is largely internally guided, while the real (external world) level must in many ways be inhibited. In this interpretation all pretending should be related to EF. Second, Vygotsky noted that pretend role play involves be- having according to norms that often differ from one's own ev- eryday behaviors. Studies of pretend play and EF (see Table 11) have used teacher or parent ratings of self-regulation and direct tests of EF. Some have coded naturalistic pretend play, and others have used laboratory measures. Correlational and experimental studies. Several studies have reported that children with better self-regulation are more likely to engage in positive peer play generally (Fantuzzo, Sekino, & Cohen, 2004;Mendez & Fogle, 2002;Mikami, 2010). When it comes to pretend play specifically, however, results concerning the relationship to EF are inconsistent. Two older studies used the ability to stand still as an index of EF. J. L. Singer (1961) rated 6-to 9-year-olds as high or low in fantasy, based on fantastical toy and game preferences and having an imaginary companion, then asked them to remain as still as possible for up to 15 min. The cover story justifying this request was that the researchers were conducting \"a study of persons suitable for space flight with its long periods of solitary confine- ment in a narrow space [to find] space men of the future'\" (p. 404). High fantasy children remained still twice as long as low fantasy children. Children's high imagination appeared to help: Some of the children who waited the longest told the experimenter, in posttest interviews, that they had been engaged in fantasy while waiting (e.g., \"playing a rocket game,\" p. 408). Manuilenko (1975) examined conditions under which 3-to 6-year-olds could stand still the longest. Two conditions embedded the instruction in a pretense game, Factory and Guards, in which the guards were to stand motionless while other children did the factory work (packing boxes). Four-year-olds appeared to be most successful (no statistical analyses were done) when standing guard over two to four children packing boxes. They were less successful when standing guard outside the classroom, out of sight of the other players or for other, nonpretense reasons. Younger and older children were not as affected by the pretend scenario. Taken together, these two older studies suggest that specific pretend play content can motivate and strengthen voluntary behav- ior for some children. The limitations (high fantasy children, one of four age groups, social context) are problematic for the idea that pretending causes EF generally. This position would be better Note. Type of study: C \ud97b\udf59 correlational; E \ud97b\udf59 experimental; T \ud97b\udf59 training. Type of play: PP \ud97b\udf59 pretend play (social unspecified). \ud97b\udf59: positive relationship to play; \ud97b\udf59: no correlation or play \ud97b\udf59 nonplay condition; \ud97b\udf59 \ud97b\udf59 negative relationship to play. Masking: Intervention (Int) or posttest experimenters (Exp). If masking status was not specified, we assume experimenters were not masked, since that is the unmarked case. Masking for correlational studies is omitted because it is rarely mentioned, even when it is likely (because play observations occurred several years earlier than testing, for example). EF \ud97b\udf59 executive function. supported if children who pretend a lot show higher EF outside of the play situation. Cemore and Herwig (2005) tested this with a standard delay of gratification test (with no cover story) and by assessing play in three interviews (mother, teacher, child) as well as via direct preschool observation. Only the child interview was related to delay time, but this interview occurred just after the delay task with the same experimenter, leaving open the possibility of experimenter biasing. None of the other measures of play was significantly related to EF. Three other studies found a relationship to laboratory tests of knowledge about pretend play (Albertson & Shore, 2009;Carlson, White, & Davis-Unger, 2012;Kelly, Hammond, Dissanayake, & Ihsen, 2011); however, the knowledge tests themselves appear to require EF. What would be most telling is if engaging in pretend play were itself related to EF. The latter two studies measured spontaneous pretend play and found no relationship to it. Perhaps one can see a relationship from pretend play to EF only over time. In a cross-lagged correlational study, Elias and Berk (2002) examined fifty-one 3-to 4-year-olds in four classrooms at two times points 4 months apart, recording play behaviors at the first time point and self-regulatory behaviors (attentiveness at circle time and behavior at cleanup time) at both time points. Pretend play was coded as solitary, social, and complex (e.g., adopting a role, talking in that role, and using substitute objects). Partialing out age, verbal ability, and Time 1 cleanup or circle time attention, complex social pretend play predicted Time 2 cleanup, but not Time 2 circle time attention. Further analyses showed the cleanup relationship existed only for children rated high in impul- sivity; pretend play made no difference for low-impulsive children. The authors' explanation for the lack of relationship to the circle time measure is that the teacher more closely supervised circle time. However, children were not at ceiling on this measure at Time 2-they scored an average of 104 out of a possible 120, with a standard deviation of 16.8 -so it is not clear that closer teacher supervision explains the lack of association. Most concerning about this study, however, is failure to repli- cate the result with low-income children (Harris & Berk, 2003, as cited in Berk, Mann, & Ogan, 2006). In a similarly designed study, Head Start children's Time 1 sociodramatic pretend play was actually negatively related to their Time 2 cleanup behavior (r \ud97b\udf59 -.25). If social pretend play helps self-regulation, one would expect it to do so for low-income children as well-indeed the Fantuzzo et al. (2004) positive correlation with social play was found for Head Start children. Berk and colleagues explained the discrepant results as being due to the themes of children's pre- tending, which they said were often aggressive in the Head Start group. If that is the case, then one cannot say that pretend play helps self-regulation generally (and as the Elias & Berk, 2002, study is often cited as showing, e.g., Bredekamp, 2004;Whitebread, Bingham, Grau, Pino Pasternak, & Sangster, 2007); rather, complex social pretend play with particular content (Elias & Berk stated the main play theme was \"housekeeping\") might help more impulsive children's self-regulation as assessed by a cleanup mea- sure but not an attention measure. In sum, strong evidence that pretend play helps EF is lacking. Two older studies showed that when pretending a role where standing still is important, pretend play helps some children under some circumstances. Relationships to spontaneous pretend play have rarely been shown, and even then appeared only for one of two measures, and only with high-impulsive middle-class children, not low impulsive or lower class ones. This pattern of results does not support that pretense is key to developing EF. But as in other domains, training studies provide the best test. Training studies. We found three studies examining the effect of training pretend play on EF. In a study discussed in several earlier sections, Saltz et al. (1977) used thematic fantasy play, sociodramatic play, fantasy discussion, and controls. Recall that this study used unmasked experimenters and subsets of data without prior rationale. Here there is rationale, because although the study involved three cohorts, EF tasks were given only to the first and third. First year children were \"guardians of the toy\": They had to sit by an attractive toy and if they did not touch it during the waiting period, they were later rewarded with another toy. Two thirds of the children were also given instructions about how to make waiting easier (Mischel, Shoda, & Rodriguez, 1989): \"think about your favorite story\" or \"read this book\" (p. 371). Children in the play training conditions (combined) waited longer than children in the other two conditions; although an interaction with instructions was not significant, the authors stated that \"the data clearly display an interaction\" (p. 376), limiting the result to the subset of children instructed in how to make waiting easier. This result did not replicate in the third year, however, when the task was sitting in a chair for 5 min pretending to be in a spaceship. Two other studies are cited as showing that pretend play assists EF (Hirsh-Pasek et al., 2009;Nisbett, 2009), although we question whether pretend play is the source of the effect. Tools of the Mind is a Vygotsky-inspired preschool program with many \"tools\" to assist self-regulation. For example, one tool is a freeze game, in which children run around to music, and when the music stops all children must freeze until it starts again, directly challenging their self-regulation. Another is \"reading buddies,\" in which one child holds a symbol of an ear while another holds a symbol of a mouth, to help them remember if it is their turn to listen or to speak. Regarding pretending, the program has an unusual requirement: Before they begin play, children must draw or dictate to the teacher a play plan; once play commences, they are not allowed to deviate from that plan (Diamond & Lee, 2011). Of five studies that have looked at the EF outcomes of this program, one had strong results (Diamond, Barnett, Thomas, & Munro, 2007), one had milder ones (W. Barnett et al., 2008), and three others saw no impact (Clements, Sarama, Unlu, & Layzer, 2012;Lonigan & Phillips, 2012;S. J. Wilson & Farran, 2012). Even were results consistently positive, one cannot separate pretend play from other aspects of the program, preventing a clear conclusion that pretend play influ- ences EF. Other aspects of this multifaceted program might be the engines. In sum, training studies of pretending and EF are entirely inconclusive, with failures to replicate and uncertain causes. Cou- pled with very limited correlational findings, evidence that pretend play assists EF is sparse at best.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Emotion Regulation", "text": "Others have focused on pretense assisting emotion regulation. Freud (1955) saw all play as releasing tension, and Erikson (1950) believed children could master disturbing events through play. Fein (1989) and Bretherton (1989) went so far as to claim emotion regulation is the primary function of pretend play. This possible function of play relates closely to pretend play therapy. For reasons of space we do not review this literature here but note that a current review from within the field, which considered two recent meta- analyses, concluded that evidence for play therapy's efficacy is \"largely inadequate\" (p. 13) and that the strongest effects are seen in studies of children who play out theme-specific issues concern- ing medical treatment (Phillips, 2010). Thus, pretending about specific content might help children cope with that content in real life, but in general we lack good evidence for the efficacy of pretend play therapy. We found four studies focused on pretend play and emotion regulation. In a correlational study, parents of 4-year-olds filled out a measure of emotion regulation (Shields & Cicchetti, 1997) and reported on their child's play at home (Galyer & Evans, 2001), and they found that children who were rated as higher in emotion regulation were also reported to pretend more at home. Then a laboratory measure was given to determine if children who have higher emotion regulation are better pretenders. Children were engaged in disrupted pretend play: In the middle of a nice episode, a toy crocodile suddenly threatened to eat all the other toys. The parent emotion regulation scores were not significantly related to how long the children spent in pretend play with the researcher or how effectively the disruption was handled, but they were more likely to continue the play. This reflected only a 2-point difference in emotion regulation score (26 vs. 28 of 60) for children who did and did not continue the play; further, the emotion regulation measure was not validated on 4-year-olds. The small difference might not be of practical importance, although the naturalistic correlational result is interesting. In a study in which specific pretense content appears to be key, L. A. Barnett and Storm (1981) showed forty 3-to 5-year-old children a traumatic episode of the television show Lassie, and half the children (the \"unresolved\" group) did not see the episode's happy resolution. Afterward, those in the unresolved group chose to play significantly more with a toy Lassie, and their play more frequently involved themes from the episode (although they did not reenact it). Their anxiety (Palmar Sweat Index) also was significantly reduced after the play, and their self-reported happi- ness was increased. Since there was no control that saw the unresolved episode but did not play, it is not clear how much the passage of time versus the play itself is responsible, but the finding that children chose to play with Lassie and then their anxiety was reduced is interesting. A later experimental study examined pretending's impact on stress following mother's departure on the first day of preschool (L. A. Barnett, 1984). Seventy-four children (mean age 3.3 years) were divided into eight conditions. Roughly half of the children in each group were not very anxious at their mother's departure and half were (Palmar Sweat Index). Within each of these groups, half were told to sit at a table and to hear a story about local vegetation, and half were brought to a room full of toys to play freely. Within each of these two conditions, half of the children were alone, and half were with five other children. Thus the conditions were social/alone nested in story/play nested in high/low anxious. For the high anxious children, playing with toys alleviated anxiety more than listening to the story. In addition, playing alone allevi- ated anxiety more than social play. Low anxious children showed little effect of condition. These two studies suggest that solitary pretend play might reduce anxiety. But in the first study, the passage of time could be the crucial factor, and in the second study, the reduction could be only relative to a rather restrictive control activity. Further research is needed to examine these possibilities. A training study, mentioned earlier regarding creativity, found no immediate nor long-term effect of pretend play on teacher-rated emotion regulation (Moore & Russ, 2008). Forty-five first and second grade children were trained in pretend play and 2 to 8 months later were tested. Although their pretending had increased relative to that of controls, and so had their positive affect expres- sion, the trained children did not improve in emotion regulation as measured by the teacher. Perhaps positive affect expression re- flected an improvement in underlying emotion regulation that the teachers had as yet failed to detect, a speculation that requires further research. Taken together these four studies leave open the case as to whether pretend play assists emotion regulation. Although parents rate children similarly on both, experimental paradigms have im- portant alternate explanations that need to be ruled out, and the single training study we found failed to find that pretend play training increased emotion regulation as measured by the teacher.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Summary", "text": "Some scholars have claimed that pretend play improves, and even is crucial to the development of, self-regulation (Bergen, 2002;Bredekamp, 2004;Hirsh-Pasek et al., 2009;Tomlinson, 2009). This review shows little support for such claims. The inconsistency in correlational studies is against a general causal account. Without further research, there is no basis to determine if equifinality is supported; the direct path might not exist, and no other paths have been examined. Epiphenomenalism arises when there are inconsistent correlations, but in this literature correlations are so sparse there might be nothing to explain.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Summary and Conclusions", "text": "This review has examined evidence cited to support claims that pretend play is a crucial engine of child development. In addition to the causal Vygotskian model inherent in such claims, we con- sidered two alternative possibilities: that pretending is one of several possible routes to development, or that pretending is merely an epiphenomenon, something that often goes along with important developments, but does not cause them. The overriding conclusion from this review is that there is currently not evidence to support the first position and that more and better research is needed to clarify pretend play's possible role in children's devel- opment. Table 12 summarizes findings from this review as regards the three possible models, which will now be discussed in turn. The causal position is that pretend play has a unique and important role in promoting healthy development. This might seem like a straw-person view, but the claim is repeatedly made in the literature, as we have shown throughout this article. If this position were supported, then for any development pretend play causes, strong, consistent, and unique correlations should be seen between pretend play and the development. We concluded that the causal account is possible, based on existing research, for four of 11 developments reviewed here: reasoning, language, narrative, and emotion regulation. Of these developments, the causal account is most plausible for language, as pretend play is quite consistently related to it. However, correlation is not causation, and reverse causality (from language to play) is shown in some studies. An underlying variable like adult interaction could also be important, with pretend play possibly being epiphenomenal to intensive de- velopmentally oriented adult interaction, explaining results from training studies. For narrative development, conclusions were sim- ilar, although the database is not as dense and the domain is complicated by the different aspects of narrative showing some- what different results (unlike language, where results were fairly consistent regardless of aspect of language measured). Emotion regulation and reasoning are more difficult to evaluate given the scarcity of solid research. For all other areas, the causal account is not supported by available research: Correlations to pretend play were inconsistent for no clear reason (creativity, theory of mind, social skills) or did not exist (conservation, problem solving), or pretend play had no unique role since other training worked as well (intelligence), or results were limited to subsets of children or measures (executive function). Equifinality was supported for the domain of reasoning, since studies have clearly shown that pretend play is one way to get children to focus on the premises and perform better on logical syllogism tasks. However, one might also argue that this supports epiphenomenalism: The pretend play aspect of the experimental condition was epiphenomenal to the true underlying cause of focusing on the premises. Equifinality is also possible for some other domains, although few other routes have been explicitly compared-this is a topic for further research on the development of social skills (music training was not as good), language, and narrative (free play was not as good, story discussion route requires further study). For theory of mind, in the most solid study neither pretend play training nor skills training led to significant improvements; other studies had at least one serious methodological problem. For creativity and con- servation, equifinality was not supported because masked experi- menters eliminated the causal result. For intelligence, equifinality was not supported because music lessons led to greater gains than drama lessons. For the two aspects of self-regulation, there are not clear findings showing that pretend play is a possible route; exist- ing findings are too sparse, limited, or attributable to other factors. Epiphenomenalism is less likely for narrative because of the one small but solid training study (Dansky, 1980a), but it could explain some findings. For example, high fantasy children might produce more complex narratives and also be apt to pretend more and have ICs. For self-regulation, there are too few findings showing a relationship. Epiphenomenalism seemed possible for language, social skills, reasoning, and problem solving; and for creativity, intelligence, conservation, and theory of mind, we believe it is the best supported position. Two truths are abundantly clear from this review. One is that we do not have a good basis of evidence from which to claim that pretend play is crucial to development. The second is that much of the evidence on pretend play suffers from serious methodological problems that must be addressed in further research to allow for a solid assessment of whether pretend play causes any important development. But as a thought experiment, we next take seriously the third position, asking of what pretend play might at least sometimes be an epiphenomenon.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Three Epiphenomenal Reasons for Some Findings", "text": "If pretend play is at least in some domains epiphenomenal, then of what might it be an epiphenomenon? It is not likely to be just one thing, but we hypothesized in several cases that correlations between pretend play and positive developmental outcomes could be due to the same adults influencing children's development in both domains. Adults who hear the child development rhetoric noted at the beginning of this article and have time and interest to devote to their children's psychological development would en- courage pretending, along with other development-enhancing ac- tivities. When adults encourage pretending, children pretend more (Lillard, 2011). But where pretending is not considered so impor- tant, children learn from adults in other contexts (Berk et al., 2006), and then correlations between pretend play and the outcome do not exist. To examine this hypothesis will require more careful study of the relation between pretense and other developments in the context of adult-child interactions. A second possible epiphenomenal reason for positive results concerns features of children that would also go along with higher pretend play scores in the studies. In some cases, we noted that children who are more socially compliant and/or intelligent would do better on pretend play and the other tasks, since the researcher was asking them to pretend and asking them to engage in some other task. For language, a plausible alternative child factor is a \"symbolic function\" supporting both language and pretend play. Indeed, research has shown that children's performance on sym- bolic tasks that are not primarily concerned with language or pretend play (DeLoache, 2000) is associated with both (Kavanaugh & Lillard, 2012;Walker & Murachver, 2012). A third underlying factor that could explain some positive experimental results is the content about which children are asked to pretend. In studies of problem solving, theory of mind, execu- tive function, and narrative, teaching to the test, or using content in the pretend training or manipulation that directly involves ele- ments that would then help on the outcome test, could plausibly cause results. If children's pretending involves assigning props to roles, they do better on ToM tasks that involve assigning objects to people, and if they pretend by focusing on the narrative structures of stories, their own stories come to have better narrative structure. This is important in that children are naturally motivated to play, and if we can embed learning in play materials such that we positively influence development, this could be good. It is the same rationale underlying many technology toys, and even Sesame Street. But just as watching television generally does not help development even though watching particular content can, the evidence reviewed here suggests that pretend play might not generally help development on its own but that playing with particular content can. It is notable that in one study, the more that \"hard to manage\" children pretended, the worse off they were developmentally-but their pretend content was often violent (J. Dunn & Hughes, 2001). In sum, features of the adults with whom children interact, features of the children themselves, and the content with which children pretend are potential epiphenomenal reasons for some findings relating pretend play to positive devel- opmental outcomes.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Implications for Educational Settings", "text": "Despite the poor state of the evidence on pretend play's benefits, research does not advocate what is often offered as the only alterna- tive to a playful approach in educational settings: adult-centered instruction. Research in U.S. schools has clearly shown that adult- centered learning environments are less positive for young children than more active, child-centered approaches (Stipek, Feiler, Daniels, & Milburn, 1995) dubbed \"playful learning\" (D. G. Singer, Golinkoff, & Hirsh-Pasek, 2006), like Reggio Emilia, Montessori, and Tools of the Mind. Developmental science does not support young children sitting in desks while teachers lecture at them. What else about child-centered classrooms leads to more posi- tive developmental outcomes if it turns out not to be the pretend play? Child-centered classrooms differ from teacher-centered ones in several qualities. Like pretend play, child-centered classrooms often provide free choice, interesting hands-on activities for which the child is intrinsically motivated, and peer interactions. Unlike pretend play, these elements have been shown in independent research to be consistently associated with more positive outcomes (see summaries of the literatures in Lillard, 2005). Compared with free play programs, more structured classrooms with carefully designed, challenging, hands-on activities that confer learning appear to help children's development the most ( Chien et al., 2010;Lillard, 2012;Lillard & Else-Quest, 2006). Do these findings regarding pretend play mean children need no time to play (pretend or otherwise)? First, there is good research showing that recess restores attention in conventional school set- tings where the basic instructional method involves children sitting at desks and listening to teachers (Pellegrini & Smith, 1993). In addition, exercise, be it from athletics or recess, improves cogni- tive function (Lillard & Erisir, 2011;Ratey, 2008). A perfectly sufficient reason for play time might simply be that it is fun (Power, 2000). Pretend play is also relaxing, associated with more heart rate variability (Hutt et al., 1989, p. 12). Finally, the research reviewed here often suggested that adult interaction might be the real underlying cause of positive effects from various interven-tions. Pretend play might be useful because it is a setting that can facilitate positive adult-child interaction (Paley, 2005;P. K. Smith et al., 1981).", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Implications for Methods", "text": "The literature on the possible benefits of pretend play for de- velopment showcases an array of methodological problems. Fur- ther research in this area must avoid these problems to elucidate if and how pretend play might help development. Here we highlight a few major problems that must be addressed (see also Cheyne, 1982;Christie & Johnsen, 1983). Experimenters. In the literature on pretend play, experiment- ers have rarely been masked, but when they were, results often went away, suggesting experimenter bias created the original re- sult. It is possible that masked experimenters sometimes result in null effects because children are not familiar with the second experimenter. If this is the case, then the positive influence of pretending is so tenuous that being tested by an unfamiliar posttest experimenter dwarfs it. We think this unlikely (Lillard & Peterson, 2011). More likely it seems that experimenters who are swayed by the cultural view of play and knowledgeable about condition subtly and probably unconsciously influence the children's perfor- mance. The biasing does not always happen, perhaps because of stricter experimenters, procedures that are less vulnerable to bias, or experimenters with less of the play ethos, but it happens enough to taint the picture. Masked experimenters are rare in cognitive development research, but in this domain, not using them has been shown to be a problem. Thus, a clear recommendation from this review is that in pretend play research, masked experimenters should be used unless measures are absolutely impervious to bias. Those who administer outcome measures should not know how much children have pretended in observations or whether children were in the pretend play condition. Ideally intervention adminis- trators are masked to hypotheses, alternate conditions, and what tests will be given. Samples and conditions. In experimental research, random assignment to condition is necessary. If children choose their condition (acting classes) then unmeasured preexisting differences could account for results. Further, care must be taken to develop control conditions that are equal except for the pretend element: The degree of adult interaction, the content, and the context must be the same save the pretend play. Finally, researchers must avoid potential confounds with other stable variables that might influ- ence children's development, for example, by providing several implementers per condition, or using implementers as their own control but without them having a personal stake in the perfor- mance of children in one condition but not another. Measures. Researchers should use uniform measures to fa- cilitate comparisons across studies. For younger children, McCu- ne's hierarchy (derived from Piaget; see footnote 10) is a useful standard. For examining natural play in the classroom, a service- able and widely used scoring system nests Parten's (1932) in Smilansky's (1968) scheme, as described in the Creativity section (Rubin, 2001). We would also add to this combined scheme a tally of the number of transformations children make. If pretending assists children because it gets them thinking in unusual ways, then tracking the extent to which children do so is important. When not coding naturally occurring play, using a standard measure is im- portant. The Test of Pretend Play (Lewis & Boucher, 1997) ad- dresses levels of play derived from observational studies. Analyses. A recurrent problem with the existing research is how results were analyzed and reported. Brainerd (1982) pointed out that even when positive effects of pretend play were obtained, they were tiny. Experimenters in this area have not always been rigorous in their analyses, perhaps because of the play ethos (P. K. Smith, 1988). In some instances, no result was found on an omnibus test but follow-up tests were done. One-tailed tests were used without strong rationale. Only subsamples were included, for no compelling reason. Unfavorable results were ignored. The existing meta-analysis (E. P. Fisher, 1992) used erroneous and cherry-picked statistics. Solid truths stand up to rigorous analyses, and unrigorous techniques only muddy the waters. Other recommendations. Although our own preference is experimental methods, other researchers favor correlational de- signs with natural settings. Modern statistical techniques allow causal inferences from such designs with sufficiently large sam- ples and numerous measures. Since pretend play might have ef- fects only over a long time course, longitudinal efforts should be encouraged. Finally, we would recommend that experimenters open them- selves to other potential benefits of pretend play. Well-being and a sense of personal agency are two possibilities. When children are pretending they appear to feel in control. This might be mitigated in many experiments, when children are instructed to pretend, but experimental conditions might be designed that minimize the sense of external control. Another issue to consider is the reverse of what was investigated here: whether absence of play is harmful, as has been shown in some animals (Pellis & Pellis, 2009). With more experiments incorporating rigorous methodology, one day researchers will have a more solid answer regarding whether pretending helps specific aspects of children's develop- ment, is just one route among many, or simply often goes along with other circumstances that lead to positive developments.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Conclusion", "text": "Despite over 40 years of research examining how pretend play might help development, there is little evidence that it has a crucial role; equifinality and epiphenomenalism have as much if not more support. With equifinality, pretend play would be just one of many routes to a positive developmental outcome. With epiphenomenal- ism, pretend play would often go along with a positive develop- mental outcome, but for extraneous reasons; it would not itself serve any causal role in that outcome. Because the literature is riddled with weak methods (correlational studies, lack of masked experimenters, poor control conditions) and unrigorous statistical approaches, we cannot definitively state which of these models is most supported. In many areas the current research base is clearly inconsistent with the causal model, but leaves open the other two. The methodological problems must be remedied with sound experiments and longitudinal studies before we can know whether and how pretend play helps development. Meanwhile, the lack of existing evidence that pretend play helps development should not be taken as an allowance for school programs to employ tradi- tional teacher-centered instructional approaches that research has clearly shown are inferior for young children. The hands-on, child- driven educational methods sometimes referred to as \"playful learn-ing\" (Hirsh-Pasek et al., 2009) are the most positive means yet known to help young children's development.", "title": "The impact of pretend play on children's development: A review of the evidence.", "file_name": "Lillard et al. - 2013 - The impact of pretend play on children's developme.pdf"}
{"section": "Abstract", "text": "Gilbert et al. conclude that evidence from the Open Science Collaboration's Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted. A cross multiple indicators of reproducibil-ity, the Open Science Collaboration (1) (OSC2015) observed that the original result was replicated in ~40 of 100 studies sampled from three journals. Gilbert et al. (2) conclude that the reproducibility rate is, in fact, as high as could be expected, given the study methodology. We agree with them that both methodological differences between original and replication studies and statistical power affect reproducibility, but their very optimistic assessment is based on statistical misconceptions and selective interpretation of correlational data. Gilbert et al. focused on a variation of one of OSC2015's five measures of reproducibility: how often the confidence interval (CI) of the original study contains the effect size estimate of the rep-lication study. They misstated that the expected replication rate assuming only sampling error is 95%, which is true only if both studies estimate the same population effect size and the replication has infinite sample size (3, 4). OSC2015 replications did not have infinite sample size. In fact, the expected replication rate was 78.5% using OSC2015's CI measure (see OSC2015's supplementary information , pp. 56 and 76; https://osf.io/k9rnd). By this measure, the actual replication rate was only 47.4%, suggesting the influence of factors other than sampling error alone. Within another large replication study, \"Many Labs\" (5) (ML2014), Gilbert et al. found that 65.5% of ML2014 studies would be within the CIs of other ML2014 studies of the same phenomenon and concluded that this reflects the maximum reproducibility rate for OSC2015. Their analysis using ML2014 is misleading and does not apply to estimating reproducibility with OSC2015's data for a number of reasons. First, Gilbert et al.'s estimates are based on pairwise comparisons between all of the repli-cations within ML2014. As such, for roughly half of their failures to replicate, \"replications\" had larger effect sizes than \"original studies,\" whereas just 5% of OSC2015 replications had replication CIs exceeding the original study effect sizes. Second, Gilbert et al. apply the by-site variability in ML2014 to OSC2015's findings, thereby arriving at higher estimates of reproducibility. However, ML2014's primary finding was that by-site variability was highest for the largest (replicable) effects and lowest for the smallest (nonreplicable) effects. If ML2014's primary finding is generalizable, then Gilbert et al.'s analysis may leverage by-site variability in ML2014's larger effects to exaggerate the effect of by-site variability on OSC2015's nonreproduced smaller effects, thus overestimating reproducibility. Third, Gilbert et al. use ML2014's 85% repli-cation rate (after aggregating across all 6344 participants) to argue that reproducibility is high when extremely high power is used. This interpretation is based on ML2014's small, ad hoc sample of classic and new findings, as opposed to OSC2015's effort to examine a more representative sample of studies in high-impact journals. Had Gilbert et al. selected the similar Many Labs 3 study (6) instead of ML2014, they would have arrived at a more pessimistic conclusion: a 30% overall replication success rate with a multisite, very high-powered design. That said, Gilbert et al.'s analysis demonstrates that differences between laboratories and sample populations reduce reproducibility according to the CI measure. Also, some true effects may exist even among nonsignificant replications (our additional analysis finding evidence for these effects is available at https://osf.io/smjge). True effects can fail to be detected because power calculations for replication studies are based on effect sizes in original studies. As OSC2015 demonstrates , original study effect sizes are likely inflated due to publication bias. Unfortunately, Gilbert et al.'s focus on the CI measure of re-producibility neither addresses nor can account for the facts that the OSC2015 replication effect sizes were about half the size of the original studies on average, and 83% of replications elicited smaller effect sizes than the original studies. The combined results of OSC2015's five indicators of reproducibility suggest that, even if true, most effects are likely to be smaller than the original results suggest. Gilbert et al. attribute some of the failures to replicate to \"low-fidelity protocols\" with meth-odological differences relative to the original, for which they provide six examples. In fact, the original authors recommended or endorsed three of the six methodological differences discussed by Gilbert et al., and a fourth (the racial bias study from America replicated in Italy) was replicated successfully. Gilbert et al. also supposed that non-endorsement of protocols by the original authors was evidence of critical methodological differences. Then they showed that replications that were endorsed by the original authors were more likely to be replicated than those not endorsed (nonendorsed studies included 18 original authors not responding and 11 voicing concerns). In fact, OSC2015 tested whether rated similarity of the replication and original study was correlated with replication success and observed weak relationships across reproducibility indicators (e.g., r = 0.015 with P < 0.05 criterion; supplementary information, p. 67; https://osf.io/k9rnd). Further, there is an alternative explanation for the correlation between endorsement and repli-cation success; authors who were less confident of their study's robustness may have been less likely to endorse the replications. Consistent with the alternative account, prediction markets administered on OSC2015 studies showed that it is possible to predict replication failure in advance based on a brief description of the original finding (7). Finally, Gilbert et al. ignored correlational evidence in OSC2015 countering their interpretation , such as evidence that surprising or more underpowered research designs (e.g., interaction tests) were less likely to be replicated. In sum, Gilbert et al. made a causal interpretation for OSC2015's reproducibility with selective interpretation of correlational data. A constructive step forward would be revising the previously non-endorsed protocols to see if they can achieve endorsement and then conducting replications with the updated protocols to see if reproducibility rates improve. More generally, there is no such thing as exact replication (8-10). All replications differ in innumerable ways from original studies. They are conducted in different facilities, in different weather, with different experimenters, with different computers and displays, in different languages, at different points in history, and so on. What counts as a replication involves theoretical assessments of the many differences expected to moderate a phenomenon. OSC2015 defined (direct) replica-tion as \"the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding.\" When results differ, it offers an opportunity for hypothesis generation and then testing to determine why. When results do not differ, it offers some evidence that the finding is generalizable. OSC2015 provides initial, not definitive , evidence-just like the original studies it replicated.", "title": "Response to Comment on \"Estimating the reproducibility of psychological science\"", "file_name": "Anderson et al. - 2016 - Response to Comment on Estimating the reproducibi.pdf"}
{"section": "ACKNOWLEDGMENTS", "text": "Preparation of this response was supported by grants from the Laura and John Arnold Foundation and the John Templeton Foundation.", "title": "Response to Comment on \"Estimating the reproducibility of psychological science\"", "file_name": "Anderson et al. - 2016 - Response to Comment on Estimating the reproducibi.pdf"}
{"section": "Chapter 1 Introduction", "text": "What does it mean to do empirical social science? Asking good questions. Digging up novel data. Designing statistical analysis. Writing up results. For many of us, most of the time, what it means is writing and debugging code. We write code to clean data, to transform data, to scrape data, and to merge data. We write code to execute statistical analyses, to simulate models, to format results, to produce plots. We stare at, puzzle over, fight with, and curse at code that isn't working the way we expect it to. We dig through old code trying to figure out what we were thinking when we wrote it, or why we're getting a different result from the one we got the week before. Even researchers lucky enough to have graduate students or research assistants who write code for them still spend a significant amount of time reviewing code, instructing on coding style, or fixing broken code. Though we all write code for a living, few of the economists, political scientists, psychologists, sociologists, or other empirical researchers we know have any formal training in computer science. Most of them picked up the basics of programming without much effort, and have never given it much thought since. Saying they should spend more time thinking about the way they write code would be like telling a novelist that she should spend more time thinking about how best to use Microsoft Word. Sure, there are people who take whole courses in how to change fonts or do mail merge, but anyone moderately clever just opens the thing up and figures out how it works along the way. This manual began with a growing sense that our own version of this self-taught seat-of-the-pants approach to computing was hitting its limits. Again and again, we encountered situations like: \u2022 In trying to replicate the estimates from an early draft of a paper, we discover that the code that produced the estimates no longer works because it calls files that have since been moved. When we finally track down the files and get the code running, the results are different from the earlier ones. \u2022 In the middle of a project we realize that the number of observations in one of our regressions is surprisingly low. After much sleuthing, we find that many observations were dropped in a merge because they had missing values for the county identifier we were merging on. When we correct the mistake and include the dropped observations, the results change dramatically. \u2022 A referee suggests changing our sample definition. The code that defines the sample has been copied and pasted throughout our project directory, and making the change requires updating dozens of files. In doing this, we realize that we were actually using different definitions in different places, so some of our results are based on inconsistent samples. \u2022 We are keen to build on work a research assistant did over the summer. We open her directory and discover hundreds of code and data files. Despite the fact that the code is full of long, detailed comments, just figuring out which files to run in which order to reproduce the data and results takes days of work. Updating the code to extend the analysis proves all but impossible. In the end, we give up and rewrite all of the code from scratch. \u2022 We and our two research assistants all write code that refers to a common set of data files stored on a shared drive. Our work is constantly interrupted because changes one of us makes to the data files causes the others' code to break. At first, we thought of these kinds of problems as more or less inevitable. Any large scale endeavor has a messy underbelly, we figured, and good researchers just keep calm, fight through the frustra- tions, and make sure the final results are right. But as the projects grew bigger, the problems grew nastier, and our piecemeal efforts at improving matters-writing handbooks and protocols for our RAs, producing larger and larger quantities of comments, notes, and documentation-proved ever more ineffective, we had a growing sense that there must be a way to do better. In the course of a project involving a really big dataset, we had the chance to work with a computer scientist who had, for many years, taught the course on databases at the University of Chicago. He showed us how we could organize our really big dataset so that it didn't become impossible to work with. Neat, we thought, and went home. Around that time we were in the middle of assembling a small (but to us, very important) dataset of our own. We spent hours debating details of how to organize the files. A few weeks in we realized something. We were solving the same problem the computer scientist had shown us how to solve. Only we were solving it blind, without the advantage of decades of thought about database design. Here is a good rule of thumb: If you are trying to solve a problem, and there are multi-billion dollar firms whose entire business model depends on solving the same problem, and there are whole courses at your university devoted to how to solve that problem, you might want to figure out what the experts do and see if you can't learn something from it. This handbook is about translating insights from experts in code and data into practical terms for empirical social scientists. We are not ourselves software engineers, database managers, or computer scientists, and we don't presume to contribute anything to those disciplines. If this handbook accomplishes something, we hope it will be to help other social scientists realize that there are better ways to work. Much of the time, when you are solving problems with code and data, you are solving problems that have been solved before, better, and on a larger scale. Recognizing that will let you spend less time wrestling with your RA's messy code, and more time on the research problems that got you interested in the first place.", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Chapter 2 Automation", "text": "Rules (A) Automate everything that can be automated. (B) Write a single script that executes all code from beginning to end. Let's start with a simple research project. We wish to test the hypothesis that the introduction of television to the US increased sales of potato chips. We receive an Excel file by e-mail with two worksheets: (i) \"tv,\" which contains for each county in the US the year that television was first introduced; and (ii) \"chips,\" which contains total sales of potato chips by county by year from 1940 to 1970. We wish to run a panel regression of log chip sales on a dummy variable for television being available with county and year fixed effects. Here is one way we might proceed: Open the file in Excel and use \"Save As\" to save the work- sheets as text files. Open up a statistical program like Stata, and issue the appropriate commands to load, reshape, and merge these text files. Define a new variable to hold logged chip sales, and issue the command to run the regression. Open a new MS Word file, copy the output from the results window of the statistical program into a table, write up an exciting discussion of the findings, and save. Submit to a journal. Just about everybody learns early in graduate school, if not before, that this \"interactive\" mode of research is bad. They learn that the data building and statistical analysis should be stored in scripts-.do files in Stata, .m files in Matlab, .r files in R, and so forth. It is worth pausing to remember why we don't like the interactive mode. There are many reasons, but two big ones. The first is replicability. If the next day, or the next year, we want to reproduce our regression of chip sales on TV, we might dig up tv.csv and chips.csv, load them back into Stata, and set to work reshaping, merging, defining variables, and so forth. Perhaps we will get lucky, since this analysis is so simple, and get back the same coefficient when we run the regression. Or perhaps not. Even in this simple example, there are innumerable things that could go wrong: since writing the paper we have received an updated version of tv.csv and we inadvertently use the new one rather than the old one; we forget that we dropped several counties whose chip sales were implausibly large; we compute regular standard errors whereas before we computed robust standard errors; and so on. On a deeper level, because there is no record of the precise steps that were taken, there is no authoritative definition of what the numbers in our paper actually are. If someone later asks why the number of observations reported in our table is different from the number of observations in the raw data, or how we computed our standard errors, or what we did with county-years with missing chip sales, and we ran the analysis interactively, we will have no way to say for sure. The second reason is efficiency. If we decide to run a different regression, say using the level rather than the log of chip sales, we will have to go back and repeat all of the steps of building and cleaning the data. We can avoid this by saving the combined dataset before running any regressions, but if we later wish to change which observations we keep and which we drop, we will be back to square one. In a real project, there might be a thousand steps from raw data to final results. For each of these, there could be several alternatives, detours, and experiments that were tried and discarded. Each step is typically run hundreds of times as the analysis is developed and refined. Trying to run and re-run all these steps interactively would be completely untenable. For this reason, most researchers learn to script key steps, especially data manipulation and statistical analysis. Here is what the project directory for the paper above might look like after we switched to writing .do files, expanded our analysis a bit, and switched to L A T E X for word processing: We suspect that the experience of trying to reverse-engineer the build steps for a directory like this will feel familiar to many readers who have tried to make sense of directories their RAs or coauthors produced, or even directories that they produced themselves a few months in the past. In this toy example, the problems are probably surmountable and, assuming that we didn't do anything silly like modify and rerun regressions.do after the PDF was produced, we could probably reproduce the paper in a reasonable amount of time. But as most of us know from painful experience, the reverse-engineering process for a moderately complex project can easily become days or weeks of frustrating work, and the probability of those \"silly\" mistakes that render CHAPTER 2. AUTOMATION 9 replication all but impossible is remarkably high. To make the output of our directory replicable, we need to automate more steps. And we need a way to store the information about the order in which the steps are run. The rundirectory.bat script works like a roadmap, telling the operating system how to run the directory. Importantly, the rundirectory script also tells a human reader how the directory works. But unlike a readme file with notes on the steps of the analysis, rundirectory.bat cannot be incomplete, ambiguous, or out of date. The proof in the pudding is that we can now delete all of the output files in the directory - the .csv files, the .log and .eps files, tables.tex, the .pdf -and reproduce them by running rundirectory.bat. This is the precise sense in which the output is now replicable. Writing a shell script like rundirectory.bat is easy. 1 You may need a few tweaks, such as adding Stata to your system path, but many of these will be useful anyway. You could write all these steps into a Stata script (rundirectory.do), but a system shell provides a more natural interface for calling commands from multiple software packages, and for operating system commands like moving or renaming files. Of course, rundirectory.bat does not automate everything. We could (and, admittedly, are tempted to) write a little Python script to submit the paper to a journal, but that seems like overkill even to us. On the other hand, we have consistently found that pushing the boundaries of automation pays big dividends. The costs tend to be lower than they appear, and the benefits bigger. A rule of research is that you will end up running every step more times than you think. And the costs of repeated manual steps quickly accumulate beyond the costs of investing once in a reusable tool. We used to routinely export files from Excel to CSV by hand. It worked ok until we had a project that required exporting 200 separate text files from an Excel spreadsheet. We followed our usual practice and did the export manually. Some time later, the provider sent us a new Excel file reflecting some updates to the data. We had learned our lesson.", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Chapter 3", "text": "Version Control Rules (A) Store code and data under version control. (B) Run the whole directory before checking it back in. In the last chapter, we showed what the project directory for our seminal TV and potato chips project might look like. After we work on the directory for a while, the key files might look like The goal is admirable, but the method is wrong. There are two main reasons why. First, it is a pain. The researcher needs to decide when to \"spawn\" a new version and when to continue to edit the old one (hence 022113a). The researcher needs to tag authorship and date every file. Failing to do that will result in confusion: Why is the date on the file name February 21 when the operating system says this was last edited in March? And confusion is the second, and by far the more important, reason why this \"date and initial\" method is poor. Look at the file names above and answer the following questions: Which is the log file produced by regressions_022713_mg.do? Did the author (darn you, Matt!) fail to change the output file name in the code, overwriting regressions_022413.log? Did he simply not output a log? Which version of cleandata.do produces the data file used by regressions_022413.do? Is it the one labeled 022113a-the last one before February 24? Or was regressions_022413 cre- ated on February 24 but edited later, raising the possibility that it needs output from cleandata_022613.do to run correctly? Unfortunately, we failed to tag tvdata.dta with a date and initial-probably because changing the file name in three different places with each new version is an enormous hassle. Given a few minutes to look at the system dates and file contents, you could probably work out which inputs are needed by which scripts. And, having learned your lesson, next time you will harangue your coauthor and RAs to remember to date and initial every script, LOG file, and intermediate data file, so hopefully there's no more confusion. This is too much work just to keep track of multiple versions of files. And it creates a se- rious risk that, later, you won't be able to sort out which file goes with which, and hence you won't be able to replicate your results. Fortunately, your computer can take care of this for you, automatically, using free software that you can set up in a few minutes. Before we tell you how, we will start with a fact. (This is, after all, a handbook for empirical researchers.) Not one piece of commercial software you have on your PC, your phone, your tablet, your car, or any other modern computing device was written with the \"date and initial\" method. Instead, software engineers use a tool called version control software to track successive ver- sions of a given piece of code. Version control works like this. You set up a \"repository\" on your PC (or, even better, on a remote server). Every time you want to modify a directory, you \"check it out\" of the repository. After you are done changing it, you check it back in. That's it. You don't change file names, add dates, or anything. Instead, the software remembers every version that was ever checked in. What happens if you change your mind about something? You ask the software for a history of changes to the directory and, if you want to go back to an old version of the directory or even of a single file, the operation just takes a click. And what about your sneaky coauthor's decision to change the main regression model spec- ification without telling you? The version control software automatically records who authored every change. And if you want to see what the changes were, most modern packages will show you a color-coded side-by-side comparison illustrating which lines of code changed and how. 1 The main thing about this approach that is great, and the reason real software engineers must use a tool like this, is that it maintains a single, authoritative version of the directory at all times. In rare cases where two people try to make simultaneous and conflicting changes to the same file, the software will warn them and help them reconcile the conflicts. A major ancillary benefit is therefore that you can edit without fear. If you make a mistake, or if you start in a new direction but later change your mind, you can always roll back all or part of your changes with ease. This requires no keeping track of dates and initials. All file names can remain just as nature intended. The software handles the versioning for you, so you can focus on writing the code and making it right. You didn't spend six years in grad school so you could type in today's date all over the place. To visualize how much better your life would be with your code and data under version control, recall (if, gasp, you are old enough) what word processing was like before the invention of the \"undo\" command. A bad keystroke might spell doom. Version control is like an undo command for everything. So our first rule is to keep everything-code and data-under version control. The way to fix this problem and ensure it never happens again is just to execute rundirectory.bat, from start to finish, and check for errors before checking in the directory. If every version you check in has been run successfully via rundirectory.bat, then you know that, barring changes in the software itself, the next time you check it out, you will get back the output in regressions.log exactly. Note that this is not a problem with the version control software. The \"date and initial\" method creates the same potential for this type of within-directory conflict, arguably more so, since a lot of effort is required to keep track of which input files are required for which scripts. Rather, version control, coupled with the rule of checking in complete runs of a directory, provides a comprehensive solution that guarantees both replicability and undo-ability with minimal effort. OK, you're convinced. Now what? A step-by-step guide to setting up and using version control software is a bit outside our scope here like the one above can be run anywhere with network access. The fixed revision bit means that if someone modifies the structure of the data and checks in a new revision, the analysis code will continue to work, as it still points to the old revision. (Of course at some point someone will probably want to redirect it to the new revision, but the user gets to decide when to do this, rather than having her code break unexpectedly.) We have only outlined a few of the advantages of using modular, functional directories to orga- nize code. There are many others. For example, the output of C:/build is now easily accessible by any directory, which makes it easier to have multiple projects that use the same data file without creating multiple, redundant copies. And, separating scripts into functional groups makes debug- ging easier and faster when something goes wrong. 1 It is easy to create a \"symbolic link\" to a file in another directory; see your operating system's documentation for details. An alternative would be to add code to rundirectory.bat that copies tvdata.dta into /input from C:/build/output. This would also allow your code to use local file references, but at the cost of duplicating the storage of tvdata.dta. Data stored in the form we have outlined is considered normalized. Storing normalized data means your data will be easier to understand and it will be harder to make costly mistakes. Most statistical software won't run a regression on a relational database. To perform our anal- ysis we are going to need to merge (or join, in database-speak) the tables together to produce a single rectangular array. Plus, we might need to calculate some variables that aren't in our source data, such as the log of population. To get from the data you downloaded, entered, or bought from an original source to the matrix on which you will perform estimation, we recommend proceeding in three steps. First, store your raw data in normalized files that preserve the information in the original data source and follows the rules above. Don't worry about how you plan to use the data. Rather, imagine that you are preparing the data for release to a broad group of users with differing needs. Do this because you, yourself, are likely to want to use the data in ways you do not currently anticipate. Second, construct a second set of normalized files that includes the transformations of the original variables that you will need for your analysis. For example, you might add to the county Following the steps above means you can keep your data in a normalized form until the last possible step in the code (rule B). Note the errors. In the first \"copy-and-paste\" operation, we failed to replace an instance of state with metroarea. In the second, we propagated the first error, plus we failed to replace one use of the per-capita potato variable with the per-household analogue. The code will run, but everything after the first code block will be totally wrong. Consider an alternative to the copy-and-paste approach, which is to write a general-purpose Now the amount of copying and pasting is minimized: each input is changed only once as we go from line to line. And because we wrote the leaveout_mean function to be totally general, we can use it for other projects as well as this one. 1 We will never again have to write code from scratch to compute a leave-out mean. Key to achieving these goals is recognizing that all three code blocks were just specific in- stances of the same abstract idea: compute the mean of a variable across observations in a group, excepting the given observation. In programming, turning the specific instances of something into a general-purpose tool is known as abstraction. Abstraction is essential to writing good code for at least two reasons. First, as we saw above, it eliminates redundancy, which reduces the scope for error and increases the value you can get from the code you write. Second, just as importantly, it makes code more readable. 2 A reader scanning one of the three code blocks above might easily miss their purpose. By contrast, a call to a function called leaveout_mean is hard to misunderstand. Abstraction can be taken too far. If an operation only needs to be performed once, and the code that performs it is easy to read, we would not advise abstraction. Abstracting without a purpose can lead you to spend a lot of time dealing with cases that will never come up in your work. When you do have a function you plan to use often, you should take the time to implement it carefully. One thing we have found helpful is the software engineering practice of \"unit test- ing.\" This means writing a script that tests out the behavior of the function you've written to make sure it works as intended. For example, we might make some fake data and verify that the leaveout_mean calculates the leave-out mean correctly. An advantage of unit testing is that it al- lows you to safely change your function without fear that you will introduce errors that will break your code down the line. It also provides a convenient way to document how the function works: what inputs it requires, what inputs it will not accept, etc. Abstraction is not just about code. It is relevant anywhere you find yourself repeating an operation. The principles in this chapter, for example, explain why word processing packages come with templates for standard document types like memos or reports. And these principles are the reason why, rather than just repeatedly telling our RAs how we thought code should be written, we decided to write this handbook!", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Chapter 7", "text": "Documentation Rules (A) Don't write documentation you will not maintain. (B) Code should be self-documenting. We have estimated the effect of television on potato chip consumption. To illustrate the per- nicious consequences for society we wish to perform a welfare analysis, for which we will need to compute an elasticity. Fortunately, Jesse's dissertation studied the effect of a tax increase on demand for potatoes, from which we can back out the elasticity of demand.  Table 2A. compute_welfare_loss, elasticity (2) Notice the helpful comments that provide a roadmap to the reader. Many researchers we know spend a lot of energy haranguing themselves, their coauthors, and their research assistants to write more comments like the above, and in general, to carefully docu-ment the organization of their code and data outside of the scripts and data files themselves. You might expect us to say the same. After all, we love organizing things. But in this chapter we will try to convince you to document less, not more. To see why, we continue our story. A few months after writing the first version of our script we return to our code to revise the analysis. We find the following:  Table 2A. * compute_welfare_loss, elasticity (3) Notice the conflict in red. Maybe someone noticed a typo in the original calculation, or decided to use the estimates from Table 2B instead of Table 2A of Jesse's dissertation. Whatever its origin, the problem is clear: the comments contradict the code, and it is now unclear which (if either) is correct. Someone will have to go back to the source to figure out what number we should be using. Readers will notice that this is an instance of a more general problem: anytime you have more than one representation of the same information (in this case, an elasticity), you run the risk that the two will someday come in conflict. In the best case scenario, you will need to do some work to untangle the mess. In the worst case scenario, your results will be wrong or internally inconsistent. The problem of internal inconsistency is especially severe when it comes to documentation- comments, notes, readmes, etc.-because you don't have to keep them up to date for your code to work or for your results to be quantitatively right. It is therefore tempting to make improvements to the code without making parallel improvements to the comments, only to find later that your comments are confusing or misleading. In the case above, the practice of letting comments go stale resulted in code that is probably less clear than it would have been if we had not had so much documentation in the first place. To avoid such confusion, you will need to keep your comments up to date, meaning just as up to date as your code. If it's not worth maintaining a piece of documentation up to that standard, it probably isn't worth writing it in the first place (rule A). That raises the important question of how to make the code clear without extensive comments. Imagine the selection above with no comments at all. How would a reader know why the elasticity is 2 and not 3? To solve that problem we turn to the code itself. Much of the content of the comments above can be readily incorporated into the code: * See Shapiro (2005), The Economics of Potato Chips, * Harvard University Mimeo, Table 2A. change the percent change in quantity without also changing the elasticity, and you can't get a different elasticity number with these percent changes. When possible, then, you should write your code to be self-documenting (rule B). Use the naming of variables and the structure of the code to help guide a reader through your operations. That's a good idea anyway, because even the best comments can't untangle a coding mess. To boot, writing such code will mean you don't have to write comments and other notes only to find that they have later lost their grip on what the code is really doing. These principles apply far beyond code, and indeed they underlie many of the other chapters in this handbook. Organizing your data files so that their structure makes their meaning clear lets you avoid pairing every dataset you make with extensive documentation (chapter 5). Naming files, directories, and other objects intelligently means their names declare their function (chapter 4). A cleverly drawn figure or table will often say so much that notes are present only to confirm the obvious or clarify minor details. And so on. Documentation does have its place. In the example above, if we don't include the citation to Jesse's stellar thesis, how will a reader know where 0.4 comes from? There is no (practical) way to script the link back to the original paper, so a comment is appropriate. Documentation can be used to make clear that something is right when it at first may seem wrong. Suppose, for example, we have a variable y distributed lognormal with location \u00b5 and scale \u03c3 . If we wish to compute the log of the variable's expectation, it might be wise to write * Log of the expectation is not the expectation of the log * See http://en.wikipedia.org/wiki/Log-normal_distribution log_expected_y = `mu' + 0.5*(`sigma'^2) so that a reader isn't surprised that the expression is not simply log (E (y)) = \u00b5. Of course, what to document is in the eye of the beholder: if you and your collaborators are not likely to forget the expression for the expectation of a lognormal, then the comment above is probably superfluous. Documentation can also be used to prevent unintended behavior. Suppose you write a com- mand to estimate a regression model via maximum likelihood. If two or more variables are collinear, your solver will iterate forever. So, you may wish to put a warning in the code: \"Don't try to estimate an unidentified model.\" But be careful. As we note above, nothing documents code quite like code. Writing a function to test whether your (X X) matrix has full rank will provide just as much documentation, will not require the user to be conscientious enough to read the comments, and will likely lead to a faster resolution of the problem. Which brings us to a related point. In Jesse's house there is a furnace room with two switches. One controls a light. The other turns off the hot water for the whole house. When he first moved in, people (let's not name names) conducting innocent business would occasionally shut off the hot water while fumbling for the light switch. He tried having a sign: \"Do not touch this switch.\" But in the dark, in a hurry, a sign is worthless. So he put a piece of tape over the switch. If there are some inputs you really, really want to prevent, comments that say \"don't ever do X\" are not the way to go. Write your code so it will not let those inputs in the door in the first place. Chapter 8", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Management", "text": "Rules (A) Manage tasks with a task management system. (B) E-mail is not a task management system. I am writing the section on dipping sauces and wanted to mention it.", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "-Jesse", "text": "From Matthew Gentzkow To: Jesse Shapiro Sorry, I thought you were doing that because it's similar to that other thing you were doing with con- trolling for salsa sales. Let me know if you want to do it or if you want me to take over. What's wrong with this picture? Mainly, it's ambiguity. Mike thought his task was done, when Jesse thought it was not. Matt thought Jesse was working on the ranch dressing task, but Jesse thought Matt was doing it. In fact, a careful reader will notice that even after all that e-mail, it's still not clear who is going to do the ranch dressing robustness check! It's worse than that. If we come back to the salsa task in two weeks, where will we look to find out its status? This thread? The one Mike mentions from 8/14? And how will we reference our discussion? By date? By forwarding this whole thread, including all the extraneous exchanges about ranch dressing? If you work alone, these problems are small. You probably have a legal pad or a Word document or a spot on your whiteboard where you keep track of what you need to do. Every now and again you might forget what you were planning to do or where you jotted something down, but if you are organized you probably get by ok. The minute two people need to work together, however, the problems exemplified in the thread above are big. And although we haven't proved this formally, we think they grow more than arithmetically with the number of people (coauthors, RAs, etc.) involved in a project. Software firms handle project and task management systematically. Microsoft does not just say, \"Hey Matt, when you get a chance, can you add in-line spell-checking to Word?\" Rather, enterprises engaged in collaborative work use project and task management systems that enforce organized communication and reporting about tasks. In the old days, those often involved handing physical reports up the chain of command. Now, they increasingly involve the use of browser-based task-management portals. In one of these portals, Mike's salsa task would have looked like this: Task: Salsa Robustness Check", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Assigned To: Michael Sinkinson", "text": "Assigned By: Jesse Shapiro", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Subscribed to Comments: Matthew Gentzkow", "text": "Status: Completed.", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Description:", "text": "Run main specifications adding a control for per capita salsa consumption. Add a line to our robustness table reflecting the results. ", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Completed By: Michael Sinkinson", "text": "Notice that now there is no ambiguity about whose responsibility the task is or what the goals are. Anyone looking at the task header will know that Mike is expected to do it, and no explicit communication is needed to figure out who is doing what. There is also a natural place to store communication about the task. Everyone expects that questions and answers will be posted to the appropriate task. And, weeks, months, or years later, there will still be a task-specific record of who did what and why. There are lots of good online systems for task management available at the moment that look something like the example above. Many are free or at least have a free no-frills option. Most have apps for mobile devices and offer some kind of e-mail integration so, for example, Mike's comments above would be e-mailed to Jesse so he knows there's something he needs to look at. These systems are changing all the time and which one you want is a matter of taste, style, budget, and the like, so we won't review them all here. Good free options as of this writing include Asana (www.asana.com), Wrike (www.wrike.com) and Flow (www.getflow.com). We use a program called JIRA, which is not free and requires a little more work to install. While we're on the subject of useful tools, you should probably get yourself set up with some kind of collaborative note-taking environment. That way, you're not bound by the limitations of your task management system in what you can share or record. It's helpful to have a place to jot down thoughts or display results that are less structured than the code that produces your final paper, but more permanent than an e-mail or conversation. The best system is one that lets you easily organize notes by project and share them with other users. It's great if you can add rich attachments so you can show your collaborators a graph, a code snippet, a table, etc. There are a bunch of options, and again, many are free. Evernote (www.evernote.com) has a free basic option and is available across lots of platforms and interfaces. Another option for Windows users is OneNote, which is included with Microsoft Office.", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Appendix: Code Style", "text": "Every piece of code you write has multiple audiences. The most important audience is the computer: if the code does not deliver unambiguous and correct instructions, the result will not be what the author intends. But code has other audiences as well. Somewhere down the line, you, your coauthor, your RA, or someone wishing to replicate your findings will need to look at the code in order to understand or modify it. Good code is written with all of these audiences in mind. Below, we collect some of the most important principles that we have learned about writing good code, mostly with examples using Stata or Matlab syntax. A lot has been written about good code style and we don't intend this as a replacement for a good book or more formal training or industry experience. But we have found the notes below useful in reminding ourselves and our collaborators of some important elements of best practice. Keep it short and purposeful. No line of code should be more than 100 or so characters long. Long scripts should be factored into smaller functions. Individual functions should not normally be more than 80 or so lines long. Scripts should not normally be longer than a few hundred lines. If you are finding it hard to make a long script short and purposeful, this is a sign that you need to step back and think about the logical structure of the directory as a whole. Every script and function should have a clear, intuitive purpose. Make your functions shy. A reader should know exactly which variables a function uses as inputs and which variables it can potentially change. Most functions should explicitly declare their inputs and outputs and should only operate on local variables. Make the set of inputs and outputs as small as possible; the functions should be reluctant to touch any more data than they need to. For example, if a function only depends on the parameter beta, pass it only beta and not the entire parameter vector. Use global variables rarely if ever. Order your functions for linear reading. A reader should be able to read your code from top to bottom without skipping around. Subfunc- tions should therefore appear immediately after the higher level functions that call them. Us descriptive names. Good names replace comments and make code self-documenting. By default, names for variables, functions, files, etc. should consist of complete words. Only use abbreviations where you are confident that a reader not familiar with your code would under- stand them and that there is no ambiguity. Most economists would understand that \"income_percap\" means income per capita, so there is no need to write out income_percapita. But income_pc could mean a lot of different things depending on the context. Abbreviations like st, cnty, and hhld are fine if they are used consistently throughout a body of code. But using blk_income to represent the income in a census block could be confusing. Avoid having multiple objects whose names do not make clear how they are different: e.g., scripts called \"state_level_analysis.do\" and \"state_level_analysisb.do\" or variables called x and xx. Names can be shorter or more abbreviated when the objects they represent are used frequently and/or very close to where they are defined. E.g., it is sometimes useful to define short names to use in algebraic calculations. This is hard to read: The second test is better because it is logically identical to what we want to check, whereas the first test relies on the fact that all entries of x are greater than or equal to 0. Be consistent. Typically, this will be enough information to alert the user to the fact that the code failed because the user attempted to square a string. Adding additional error-checking to check that x is not a string will add one or more lines of code with little gain in functionality. However, there are some circumstances in which error-checking should be added to code. Error-checking should be added for robustness. For example, if the wrong argument will cause your script to become stuck in an infinite loop, or call so much memory that it crashes your com- puter, or erase your hard drive, you should include code to ensure that the arguments satisfy suffi- cient conditions so that those outcomes will not occur. Error-checking should be added to avoid unintentional behavior. For example, suppose func- tion multiplybytwo() multiplies a number by 2 but is only written to handle positive reals. For negative reals it produces an incoherent value. Because a user might expect the function to work on any real, it would be a good idea to throw an error if the user supplies a negative real argument. (Of course, it would be even better not to write a function with such confusing behavior.) Error-checking should be added to improve clarity of error messages. For example, suppose that function norm() requires a vector input. Suppose the default error message that gets returned in the event you pass norm() it a scalar is \"Input to '*' cannot be an empty array.\" A user could spend a lot of time trying to understand the source of the error. If you suspect that people may be tempted to pass the function a scalar, it is probably worth checking the input and returning a more informative error message like \"Input to norm() must be a vector.\" Chances are that this will save a few more 30-minute debugging sessions in the future and be worth the time. Note that there is an intrinsic tradeoff between time spent coding error-handling and time spent debugging. It is not efficient to code explicit handling of all conceivable errors. For example, it is probably not worth adding a special warning in the case where the user passes a string to norm(), because the user is unlikely to make that mistake in the first place. Error checking code should be written so it is easy to read. It should be clearly separated from other code, either in a block at the top of a script or in a separate function. It should be automated whenever possible. If you find yourself writing a comment of the form % Note that x must be a vector ask yourself whether you can replace this with code that throws an error when isvector(x) is false. Code is more precise than comments, and it lets the language do the work. Write tests. Real programmers write \"unit tests\" for just about every piece of code they write. These scripts check that the piece of code does everything it is expected to do. For a demand function that returns quantity given price, for example, the unit test might confirm that several specific prices return the expected values, that the demand curve slopes down, and that the function properly handles zero, negative, or very large prices. For a program to compile, it must pass all the unit tests. Many bugs are thus caught automatically. A large program will often have as much testing code as program code. Many people advocate writing unit tests before writing the associated program code. Economists typically do not write unit tests, but they test their code anyway. They just do it manually. An economist who wrote a demand function would give it several trial values interac- tively to make sure it performed as expected. This is inefficient because writing the test would take no more time than testing manually, and it would eliminate the need to repeat the manual tests every time the code is updated. This is just a special case of the more general principle that any manual step that can be turned into code should be. We therefore advocate writing unit tests wherever possible. Profile slow code relentlessly. Languages like Matlab and R provide sophisticated profiling tools. For any script for which com- putation time is an issue (typically, anything that takes more than a minute or so to run), you should profile frequently. The profiler often reveals simple changes that can dramatically increase the speed of the code. Profiling code in Stata or similar statistical packages is more difficult. Often, the sequential nature of the code means it is easy to see where it is spending time. When this is not the case, insert timing functions into the code to clarify which steps are slow. Speed can occasionally be a valid justification for violating the other coding principles we articulate above. Sometimes we are calling a function so many times that the tiny overhead cost of good, readable code structure imposes a big burden in terms of run-time. But these exceptions are rare and occur only in cases where computational costs are significant. Store \"too much\" output from slow code. There is an intrinsic tradeoff between storage and CPU time. We could store no intermediate data or results, and rerun the entire code pipeline for a project back to the raw data stage each time we change one of the tables. This would save space but would require a tremendous amount of computation time. Or, we could break up a project's code into hundreds of directories, each of which does one small thing, and store all the intermediate output along the way. This would let us make changes with little computation time but would use a lot of storage (and would likely make the pipeline harder to follow). Usually, we compromise, aggregating code into directories for conceptual reasons and to efficiently manage storage. It is important to keep this tradeoff in mind when writing slow code. Within reason, err on the side of storing too much output when code takes a long time to run. For example, suppose you write a directory to estimate several specifications of a model. Estimation takes one hour. At the time you write the directory, you expect to need only one parameter from the model. Outputting only that parameter is a mistake. It will (likely) be trivial to store estimates of all the model parameters, and the benefits will be large in compute time if, later, you decide it would be better to report results on two or three of the model's parameters. If estimation is instantaneous, re-estimating the model later to change output format will not be costly in terms of compute time. In such cases, concerns about clarity and conceptual boundaries of directories should take priority over concerns about CPU time. Separate slow code from fast code. Slow code that one plans to change rarely, such as code that estimates models, runs simulations, etc., should ideally be separated from fast code that one expects to change often, such as code that computes summary statistics, outputs tables, etc. This makes it easy to modify the presentation of the output without having to rerun the slow steps over and over. Consider again a directory that estimates several specifications that take, together, an hour to run. The code that produces tables from those specifications will likely run in seconds. Therefore, it should be stored in a separate directory. We are likely to want to make many small changes to how we format the output, none of which will affect which specifications we want to run. It will not be efficient to have to repeatedly re-estimate the same model in order to change, say, the order of presentation of the parameters in the table. Again, if instead estimation were very fast, it might be reasonable to include the script that produces tables inside the same directory as the script that estimates models. In such a case, the decision should be based on clarity, robustness, and the other principles articulated above, rather than on economizing CPU time.", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Acknowledgments", "text": "We benefited greatly from the input of coauthors and colleagues on the methods described in this handbook. Ben Skrainka's Institute for Computational Economics lecture slides showed us that we were not alone. Most importantly, we acknowledge the tireless work of the research assistants who suffered through our obsessions and wrong turns.", "title": "Code and Data for the Social Sciences: A Practitioner's Guide", "file_name": "CodeAndData.pdf"}
{"section": "Abstract", "text": "Proxy advisors are information intermediaries that enable shareholders to exercise their voting rights. While proxy advisors' influence is documented in market-based corporate governance systems, we know little about the corporate governance role of proxy advice in relationship-based governance systems. Drawing on agency theory and the comparative corporate gover-nance literature, we theorize that shareholders are sensitive to the costs and benefits of monitoring by considering internal monitoring capabilities. We also theorize that relative to market-based corporate governance systems, proxy advice is both less influential and has lower predictive quality in relationship-based governance systems. We test our multilevel model using 13,497 voting results from 613 firms in 16 Western European countries and generally find support for our predictions.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Theory and Hypotheses", "text": "According to agency theory, the main aim of corporate governance is to minimize the sum of all agency costs that result from the separation of ownership and control, including moni- toring costs, bonding costs, and residual losses (Jensen & Meckling, 1976). Residual losses result from ineffective corporate governance, such as when firms are shielded from the mar- ket for corporate control (Dalton et al., 2007). Since our aim is to explain the differential reliance of shareholders on proxy advisors across firms and countries, residual losses are not our main focus. Nor do we focus on bonding costs, which have been the main explanatory focus of the executive compensation literature (Hoskisson, Castleton, & Withers, 2009). 1 Instead, we focus on the costs and benefits of relying on internal versus external monitoring, as shareholders face high monitoring costs in exercising their control rights effectively (Goranova & Ryan, 2014). For two reasons, shareholder voting is potentially the most powerful control right that shareholders can use to secure their interests in the firm (Easterbrook & Fischel, 1983). First, shareholders have mandatory consent rights in significant corporate decisions, such as merg- ers, reincorporations in other jurisdictions, and charter amendments, meaning that they can veto management proposals that seek to change the firm-level \"rules of the game\" (Bebchuk, 2005). Second, by voting against management, shareholders can publicly challenge the legit- imacy of management (Hillman et al., 2011;Sauerwald, van Oosterhout, et al., 2016). As such, the expression of shareholder dissent has been found to lead to changes in the leader- ship and governance of firms (Cai et al., 2009;Fischer et al., 2009) and is therefore a poten- tially useful corporate governance mechanism in addressing agency costs (Goranova & Ryan, 2014;Yermack, 2010). Empowering shareholders in areas such as shareholder voting has become increasingly important (Bebchuk, 2005;Campbell, Campbell, Sirmon, Bierman, & Tuggle, 2012;Iliev et al., 2015;Yermack, 2010). Yet, empowering shareholders to resolve agency problems is problematic for firms that are predominantly owned by uninvolved outsiders, such as institu- tional investors ). Compared with corporate insiders, outside shareholders are at an informational disadvantage that makes it difficult to identify and address agency costs (Jensen & Meckling, 1976). Shareholders may circumvent agency costs by investing only in firms with proven and effective corporate governance mechanisms (Bushee, Carter, & Gerakos, 2013;Gompers, Ishii, & Metrick, 2003). Yet, many sharehold- ers, such as increasingly popular index funds, are unable to pick investments (Appel, Gormley, & Keim, 2016) and will therefore need to address agency problems by finding ways to resolve their information problems. Shareholders may resolve information problems by accumulating or retaining large equity stakes (Shleifer & Vishny, 1986). Large equity stakes provide incentives to overcome free- rider issues and allow shareholders to engage in private behind-the-scenes negotiations with management (Becht, Franks, Mayer, & Rossi, 2009). Alternatively, shareholders may under- take various activities around the highly public event of a firm's shareholder meeting (Ferri, 2012). Such activities include asking questions, submitting proposals, and voting against proposals supported by management (Sauerwald, van Oosterhout, et al., 2016). Even when poorly equipped to vote in an informed way, shareholders may overcome their informational disadvantage by basing their voting on a simplifying agency-theoretical logic (Hillman et al., 2011). While shareholders have been found to use governance logics to guide their voting (Sauerwald, van Oosterhout, et al., 2016), they may also wish to become informed about the proposals put to the vote at the shareholder meeting (Yermack, 2010). For three reasons, the monitoring costs resulting from collecting and processing information may make it difficult for shareholders to vote. First, shareholders often lack the incentives to collect firm-specific information, because they would bear all the information collection and processing costs but capture only a fraction of the benefits (Shleifer & Vishny, 1986). Second, shareholders may not have the ability to analyze and process sophisticated information released in proxy and accounting statements (Malmendier & Shanthikumar, 2007). Third, shareholders' ability to coordinate their voting with other shareholders may also be limited because of the high coor- dination costs associated with forming shareholder coalitions (Crespi & Renneboog, 2010). While some shareholders, such as retail investors, may respond to these informational issues with \"rational ignorance\" and abstain from voting altogether (Downs, 1957), other shareholders, such as institutional investors, may have a fiduciary duty to vote in an informed way and therefore need to resolve these information problems in a different way (Coffee, 1991;David, Yoshikawa, Chari, & Rasheed, 2006). In such cases, shareholders may use the services of information intermediaries.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Proxy Advisors and Shareholder Dissent", "text": "Information intermediaries are agents that provide information to reduce information asymmetries between investors and firms (Healy & Palepu, 2001). Examples include audi- tors (Sikka, 2009), credit-rating agencies (Partnoy, 2009), and investment analysts (Westphal & Clement, 2008). Proxy advisors are a relatively new type of information intermediary that offers to resolve voting difficulties in publicly listed firms (Larcker et al., 2015). Their core business is the provision of fee-based voting recommendations on proposals put to the vote to enable shareholders of public firms to exercise their voting rights. These voting recom- mendations can reduce information asymmetries between firms and their shareholders, because proxy advisors collect, process, and analyze firm-specific information and issue voting recommendations to shareholders. Most importantly, proxy advisors may issue nega- tive voting recommendations to advise shareholders to vote against management proposals (Choi, Fisch, & Kahan, 2010). Agency theory posits that \"any improvement in the principal's information position ought to yield positive results\" (Jacobides & Croson, 2001, p. 204). To the extent that proxy advisors are able to provide cost-efficient information to guide share- holder voting, shareholders may value proxy advice since it lowers their monitoring costs compared to conducting research and voting on their own. Proxy advisors are able to provide cost-efficient information because they can recover the costs of information processing by selling their services to many investors simultaneously. Because the economies of scale involved in producing these services create significant barri- ers to entry, ISS has become the globally dominant proxy advisor. In 2012, ISS provided voting recommendations to shareholders at 40,000 shareholder meetings (Larcker, McCall, & Tayan, 2013), affecting more than $26 trillion in assets globally (Copland, 2012). Four reasons may explain shareholders' reliance on ISS's voting recommendations. First, it is a cost-effective way of acquiring the information needed for informed voting. Although ISS's services are available only on a subscription basis, many shareholders using the service are able to spread its costs over a large number of portfolio firms (Dharwadkar, Goranova, Brandes, & Khan, 2008). Second, shareholders may use ISS's negative recommendations to identify crucial propos- als that warrant special attention at shareholder meetings ( Choi et al., 2010). Given that some shareholders, such as mutual and index funds, vote on thousands of proposals in the rela- tively short time windows when shareholder meetings are held, ISS's recommendations allow shareholders to focus their attention on the most important proposals ( Ocasio, 1997). Third, shareholders may rely on ISS's negative voting recommendations because they are widely followed by other shareholders. Given that the efficacy of shareholder voting criti- cally depends on how other shareholders vote, shareholders may strategically focus on pro- posals with a negative ISS voting recommendation because these proposals have a much higher chance of accumulating significant shareholder dissent (Maug & Rydqvist, 2009). Finally, the negative voting recommendations provided by ISS may also help to address the agency problems that institutional investors face with their own investors, as these rec- ommendations may legitimize voting decisions (Anabtawi & Stout, 2008;Pollock & Rindova, 2003). Shareholders in the United States (Hillman et al., 2011) and Europe (Sauerwald, van Oosterhout, et al., 2016) tend to follow an agency-theoretical logic in their voting, meaning that they are more likely to follow management recommendations if firms espouse governance mechanisms suggested by agency theory. ISS is a close ally of share- holders and incorporates a wide array of shareholder concerns in annual policy reviews (Choi et al., 2010). Similar to how firms espouse an agency-theoretical logic to gain legitimacy among their shareholders (Fiss & Zajac, 2004), ISS espouses an agency-theoretical logic to appeal to commonly held views of both its subscribers and the investment community more generally (Ferri, 2012). In sum, Hypothesis 1: Negative voting recommendations by a proxy advisor, such as ISS, are positively related to shareholder dissent.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Firm-Level Contingencies: Ownership Conditions and the Influence of Proxy Advice", "text": "While shareholders may rely on proxy advice due to its cost efficiency, we propose that they also consider the quality of the information involved and the channels through which they may influence management. Specifically, shareholders may substitute external proxy advice for the internal monitoring capabilities of large blockholders (Connelly, Tihanyi, Certo, & Hitt, 2010), because large blockholders can overcome information problems by acquiring private information and by developing private influence channels. These internal monitoring capabilities allow shareholders to reduce their dependence on proxy advisors, who rely on public information (which may be less informative) as well as shareholder meet- ings to exercise dissent (which may be less effective). Moreover, shareholders may comple- ment institutional investor ownership of the firm with an increased reliance on proxy advice, because institutional investors as outside shareholders typically lack internal monitoring capabilities that large blockholders can rely on. Ownership concentration. Shareholders in dispersedly owned firms face collective- action problems and high coordination costs when voting (Easterbrook & Fischel, 1983). In such firms, internal monitoring capabilities are largely absent. Managers may therefore have leeway to put proposals on the ballot that are potentially value decreasing. Managers may also bundle potentially value-decreasing proposals with value-enhancing proposals (Bethel & Gillan, 2002) so that shareholders cannot reject these proposals independently. Because dispersed shareholders face severe difficulties screening out value-decreasing proposals before the shareholder meeting, they would need to rely more on external proxy advice and the expression of shareholder dissent at shareholder meetings to influence management (Bebchuk, 2005). When ownership is more concentrated in the hands of blockholders, however, sharehold- ers can more easily overcome the monitoring and collective-action problems that burden dispersed shareholders (Holderness, 2003). This may be the case in spite of possible agency conflicts between large blockholders and minority shareholders-known as principal-princi- pal (PP) agency problems (Young, Peng, Ahlstrom, Bruton, & Jiang, 2008). While these PP agency problems may be a cost that shareholders take into account when deciding to rely on blockholder monitoring, the severity of these costs is often contingent on firm-specific fac- tors. Specifically, PP agency problems are likely to be more severe when large shareholders are entrenched (Thomsen, Pedersen, & Kvist, 2006), when conflicts among blockholders are present (Zellweger & Kammerlander, 2015), and when blockholders leverage their control rights over and above their cash flow rights (Claessens, Djankov, Fan, & Lang, 2002). If we account for the firm-specific risk of PP agency problems arising, shareholders may still be more willing to rely on blockholder monitoring over proxy advice because blockholders have both incentives and abilities to collect and process firm-specific information needed for effective monitoring. Given that blockholders are often able to engage privately with man- agement ( Becht et al., 2009), they tend to be better informed than dispersed shareholders about controversial management proposals. This ability enables blockholders to screen out value-decreasing proposals from the ballot. Shareholders may thus delegate substantial dis- cretion to blockholders and depend less on external proxy advice to exercise their voting rights (Aghion & Tirole, 1997). Hence, Hypothesis 2: Ownership concentration in the firm will moderate the positive relationship between negative voting recommendations and shareholder dissent such that the relationship is less posi- tive when ownership concentration in the firm is high. Institutional investors. Although the degree of ownership concentration in a firm cre- ates conditions conducive to the development of internal monitoring capabilities that may be a superior substitute for external proxy advice, not all shareholders are equally effective monitors (Thomsen & Pedersen, 2000). In Europe, different types of shareholders, such as families, governments, and institutional investors, hold sizable ownership stakes (Faccio & Lang, 2002). These different types of blockholders differ in their incentives and abilities to monitor managers ( Thomsen et al., 2006;Yoshikawa, Phan, & David, 2005). At the most basic level, we can distinguish inside blockholders who often control and may even actively manage the firm (van Essen et al., 2013) from outside shareholders that only provide equity capital to the firm at arm's length (Connelly et al., 2010). In contrast to inside blockholders, who are typically the ultimate owners of the firm, outside shareholders, such as institutional investors, pool funds on behalf of other investors (called fund investors). As institutional investors thereby owe fiduciary duties first and foremost to their fund investors (Johnson, Schnatterly, Johnson, & Chiu, 2010), institutional investors tend to avoid insider positions (such as board directorships) that would allow them to acquire private information and behind-the-scenes access to management. This is not only because insider positions gen- erate conflicts of interests between their portfolio firms and their fund investors (Bainbridge, 2003) but also because insider positions may compromise the liquidity of their investments through the blackout periods during which insiders cannot trade their shares (Bettis, Coles, & Lemmon, 2000). While some institutional investors may be able to generate useful firm- specific information from their large ownership position alone (Schnatterly, Shaw, & Jennings, 2008), most institutional investors are too diversified to effectively monitor any single firm but instead monitor the performance of their investment portfolios as a whole ( Dharwadkar et al., 2008;Goranova, Dharwadkar, & Brandes, 2010). As a result, institu- tional investors lack the behind-the-scenes access to management that inside blockholders typically possess (Becht et al., 2009). Hence, the larger the ratio of institutional investors in a firm, the less likely that the firm will have strong internal monitoring capabilities and the more likely that its shareholders will complement the dearth of internal monitoring capabili- ties by increasing their reliance on external proxy advisors. Thus, Hypothesis 3: Institutional investor ownership in the firm will moderate the positive relationship between negative voting recommendations and shareholder dissent such that the relationship is more positive when institutional investor ownership in the firm is high.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Country-Level Contingencies: Corporate Governance Systems and the Influence and Quality of Proxy Advice", "text": "Research in comparative corporate governance is premised on the idea that corporate governance systems reflect institutions that are static and require country-specific analysis (Aguilera & Jackson, 2003;David et al., 2006;Fiss & Zajac, 2004). Yet, proxy advisors may not be appropriately incentivized to provide country-specific analyses and voting recommen- dations (Veldkamp, 2006). ISS may face difficulties tailoring information to firms in a spe- cific country (Daines, Gow, & Larcker, 2010;Larcker et al., 2015). Moreover, financial intermediaries may apply governance standards from their home country to other countries, which may clash with local governance standards (Desender, Aguilera, Lopez-Puertas Lamy, & Crespi, 2016). Explaining the effects of institutions in a comparative research design is theoretically complex and methodologically complicated. Institutional explanations are often causally complex (Acemoglu, Johnson, & Robinson, 2001). Not only do they involve many different institutional features (Jackson & Deeg, 2008), but these features may also combine in many different and complex ways (Fiss, 2007). In addition, institutions may affect governance practices indirectly (Aguilera & Jackson, 2003) and lead to theoretically unexpected out- comes (Gilson, 2006). Finally, limited variation at the country level often frustrates efforts to appropriately measure and model the effects of discrete institutional features (Fiss, 2007). To cope with this complexity, we take an approach that compares whole corporate gover- nance systems as a multifaceted institutional context in which corporate governance is embedded (Whitley, 1994). Specifically, we revert to a theoretically robust and empirically corroborated distinction between market-based corporate governance systems and relation- ship-based systems (Peng, 2003), which captures the underlying institutional differences on a single dimension by looking at some of the theoretically most relevant country-level out- comes. This approach is particularly useful to answer our research questions because the costs and benefits of proxy advice depend on the institutional context in which proxy advi- sors operate ( Aguilera et al., 2015;Peng, Sun, Pinkham, & Chen, 2009). The influence of proxy advice across corporate governance systems. Market-based cor- porate governance systems facilitate impersonal exchange between equity market partici- pants and enable shareholders to take up an arm's-length position toward the firms they own. Market-based governance systems enable impersonal arm's-length exchange in at least three ways. First, a market-based governance system must provide sufficient liquidity for share- holders to enter and exit firms at little cost. This is typically the case when equity markets are sufficiently deep and open to foreign investors (Rydqvist, Spizman, & Strebulaev, 2014). Second, a market-based governance system must provide sufficient safeguards against expropriation of minority shareholders by controlling shareholders, such that controlling shareholders cannot use their private, behind-the-scenes influence to achieve private benefits of control ( Young et al., 2008). Third, impersonal arm's-length exchange between equity market participants can take place only when sufficient information is publicly available (Bushman & Smith, 2001). In such a context, inside blockholders have fewer advantages over outside shareholders because all equity market participants have access to extensive and reliable public information. Shareholders can therefore more safely rely on external proxy advisors to exercise their voting rights, because proxy advisors can cost-effectively analyze the large amounts of public information on behalf of their clients. In relationship-based governance systems, relational contracting and social networks shape interactions between firms and their environments. The flow of information through these systems may also be significant, but it likely flows in smaller circles and through less public channels (Peng, 2003). For instance, insiders may expropriate minority shareholders through tunneling, propping, and related party transactions (Friedman, Johnson, & Mitton, 2003), all of which are difficult to observe and to prove in courts due to a lack of public information (Dyck & Zingales, 2004). Insiders may also reduce the influence of outside directors, making it difficult for proxy advisors to evaluate director effectiveness and allow- ing insiders to gain private benefits (Peng, 2004;Young et al., 2008). Such conditions make it riskier for shareholders to rely on external proxy advisors, who use public information and the public shareholder meeting to exercise control. We hence expect proxy advice to be more influential when the country's governance system is more market based. Specifically, Hypothesis 4: The country's governance system will moderate the positive relationship between negative voting recommendations and shareholder dissent such that the relationship is more positive when the country's governance system is more market based. The predictive quality of proxy advice across corporate governance systems. A partic- ularly interesting reason why shareholders' reliance on external monitoring by proxy advi- sors differs cross-nationally is that the predictive quality of proxy advice may differ between governance systems ( Daines et al., 2010). The main issue at stake is whether proxy advisors can successfully identify residual loss and predict future financial performance shortfalls from their monitoring of firms and the proposals put to the vote in these firms (Core, Holthausen, & Larcker, 1999). Proxy advisors issue a negative voting recommendation when they expect the proposal at stake to increase agency costs in the firm. Negative voting recommendations successfully identify agency costs when these recommendations accurately predict negative future outcomes, such as reduced financial performance (Larcker, McCall, & Tayan, 2013). Proxy advisors generally argue that their negative recommendations are able to accurately predict future financial performance shortfalls in all country settings. However, proxy advi- sors often rely on best practices and face potential difficulties tailoring recommendations to specific firm conditions ( Daines et al., 2010;Larcker et al., 2015), which may also affect the quality of negative recommendations across countries. We suggest that proxy advisors' nega- tive recommendations are more suitable for market-based corporate governance systems than for relationship-based systems, because proxy advisory services were specifically developed to address the needs of shareholders in market-based systems (Choi, Fisch, & Kahan, 2009). Three reasons stand out. First, a U.S.-based proxy advisor, such as ISS, may face substan- tial challenges in building necessary country-specific monitoring capabilities. This is because proxy advisors may be unable to appropriately consider the unique country conditions out- side of their home country and may fail to tailor their recommendations to more relationship- based governance systems (Kostova, 1999). While this problem may reduce over time as foreign entrants learn (Petersen & Pedersen, 2002), the incentives for proxy advisors to learn about specific conditions in foreign markets are modest, as their profitability depends on generalizable recommendations that can be sold to many customers across many countries simultaneously (Veldkamp, 2006). Second, proxy advisors generally rely on firm-specific information that is available from public sources (Ferri, 2012). Yet, private information resulting from behind-the-scenes access to managers tends to be more important to identify governance issues in more relationship-based governance systems (Zaheer & Venkatraman, 1995). Moreover, firm-specific information is harder to gather in relationship-based governance systems, because the quality of firm disclo- sures differs across countries and is generally considered to be superior in market-based systems (Bushman, Piotroski, & Smith, 2004;Peng, 2003). Proxy advisors may therefore find it difficult to elicit useful public information from firms in relationship-based governance systems. Third, proxy advisors' business model of financing their monitoring activities by selling services to many investors across many countries motivates them to develop standardized voting policies that may not be readily applicable in relationship-based governance systems ( Daines et al., 2010;Wellstein & Kieser, 2011). As a result, we expect ISS negative voting recommendations to better predict agency costs and the resulting future firm performance shortfalls in market-based governance systems. Hence, Hypothesis 5: The country's governance system will moderate the negative relationship between negative voting recommendations and future firm performance such that the relationship is more negative when the country's governance system is more market based.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Data and Sample", "text": "Following calls for more multilevel research in management Hitt, Beamish, Jackson, & Mathieu, 2007), we compiled a comprehensive data set with three levels of analysis: (a) proposal level, (b) firm level, and (c) country level. This data set allows us to account for the multilevel nature of the relationships that we investigate and also enables us to control for the attributes of the proposals put to the vote ( Cai et al., 2009). After matching the 18,553 proposal-level voting results from ISS with firm-level and country-level data, missing values for firm-level variables reduced our final sample to 13,497 proposal-level observations (Level 1) nested in 613 publicly listed firms (Level 2) and 16 Western European countries (Level 3). Table 1 presents an overview of key country-level differences and sample observations per country.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Dependent Variables", "text": "Shareholder dissent.  individual proposal. Management typically recommends voting \"for\" management-spon- sored proposals and \"against\" shareholder-sponsored proposals. All votes not following management's recommendation are classified as shareholder dissent. In addition to voting for and against, shareholders may also abstain. In calculating share- holder dissent, we included abstain votes because these votes are cast at the shareholder meeting and hence contribute to fulfilling quorum requirements. Abstain votes also indicate shareholders' skepticism vis-\u00e0-vis management (Conyon & Sadler, 2010). Importantly, abstain votes differ from shares \"not voted\" because shareholders who do not vote are not counted in the voting outcomes. We log-transformed shareholder dissent since the vote dis- tribution is skewed. Future firm performance. Two measures of firm performance were used: return on assets (ROA) and Tobin's Q. ROA was measured as total net income divided by total assets. Tobin's Q was measured as the ratio of the year-end market value of the firm's outstanding equity divided by the firm's book value. Both variables were measured with a 1-year forward lag for each firm-year.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Model Variables", "text": "ISS negative recommendation. ISS negative recommendation is a binary variable coded as 1 if ISS advises shareholders to vote against management recommendations and 0 to sup- port management. For Hypothesis 5, we aggregated the number of negative recommenda- tions to the firm-year level by counting the number of negative ISS recommendations during the firm-year divided by the total number of proposals in the firm-year. A larger number of ISS negative recommendations indicates more agency problems identified by ISS in a par- ticular firm-year. Ownership concentration. We measured ownership concentration as the ratio of shares held by large shareholders to total shares outstanding. It was calculated by subtracting the number of shares owned by shareholders with less than 5% ownership (also known as \"free float\") from all outstanding shares at year-end, divided by all outstanding shares at year-end. While some studies have operationalized ownership concentration with the ownership of the largest shareholder (Holderness, 2003;Thomsen & Pedersen, 2000), we took a finer-grained approach and also considered the ownership of other shareholders (Thomsen et al., 2006). The data were hand-collected from the most recent company filings prior to the shareholder meeting. Institutional investors. We measured institutional investors as the ratio of shares owned by all investors that pool funds on behalf of other investors to total shares outstanding. Institutional investors in our sample are primarily investment firms, such as BNP Paribas of France, Allianz of Germany, and UBS of Switzerland. While these investment firms also offer other financial services, such as investment banking (e.g., UBS), insurance (e.g., Allianz), and consumer banking (e.g., BNP Paribas), investment management is a sizable share of their business. Ownership information was collected from firm proxy statements and annual reports. Market-based governance index. To test Hypotheses 4 and 5, we created an index based on three country-level variables that each capture an aspect of the national corporate gov- ernance system. We developed this construct using three variables. First, private benefits of control was obtained from Dyck and Zingales (2004). 2 This variable measures the extent to which controlling shareholders appropriate financial returns from outside shareholders. High private benefits of control are typical for more relationship-based governance systems because stock markets are underdeveloped (La Porta, Lopez-de-Silanes, Shleifer, & Vishny, 1997) and many corporate transactions are carried out between trusted partners (Peng, 2003). Second, foreign investor ownership was measured as the country-level percentage of the total share ownership of foreign investors in the focal stock market. The data came from a 2007 survey of the FESE (2008). Foreign shareholders typically condition their invest- ments on developed stock markets (La Porta et al., 1997) and free-market governmental policies ( Rydqvist et al., 2014). Thus, foreign shareholders are typically more prevalent in market-based governance systems than in relationship-based systems. Third, stock market capitalization was collected from the World Bank's World Development Indicators. It was calculated as the percentage of market capitalization of listed companies divided by the gross domestic product (GDP) for the current year. High stock market capitalization is a critical determinant of a well-functioning stock market (Morck, Yeung, & Yu, 2000). We combined these three variables into an equally weighted index using principal com- ponent analysis (PCA; Jackson, 1991). PCA is a useful data reduction technique if no theo- retical reason exists to rank order the components of an index (Boyd, Gove, & Hitt, 2005). We retained one component, which had an eigenvalue of 1.74. Following Larcker, Richardson, and Tuna (2007), we retained this component because all country-level vari- ables had a component loading factor of at least 0.4. The index ranges on a continuum from relationship-based governance (low index values) to market-based governance (high index values).", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Control Variables", "text": "We controlled for the country-level variable institutional ownership/GDP, which reflects the degree of institutional ownership in a country. This was included because ISS may focus its efforts on countries with high institutional ownership. The data came from the Organisation for Economic Co-operation and Development and was scaled by GDP (Gonnard, Kim, & Ynesta, 2008). Following Iliev et al. (2015), we controlled for country- level variables that capture shareholders' ability to cast meaningful dissenting votes against the actions of powerful insiders. First, shareholders may be more likely to cast dissenting votes when they fear expropriation by corporate insiders. We included the anti-self-dealing index, which captures the protection of outside shareholders against expropriation by con- trolling shareholders. The data were provided by Djankov, La Porta, Lopez-de-Silanes, and Shleifer (2008). Second, effective rule of law may increase the effectiveness of dissenting shareholder votes by increasing the threat of follow-up litigation. We derived rule of law from Freedom House (2010). Third, we controlled for corporate transparency to measure the public availability of firm-specific information. Shareholders may be more inclined to cast dissenting votes in countries that have better disclosures. These data came from Bushman et al. (2004). Routine proposal. Following Mallin (1996), proposals to approve the annual report, director elections, profit distributions, share repurchases, discharge of directors, and auditor elections were classified as routine proposals because they are decided at the shareholder meeting annually. Proposals regarding issuance of shares and debt, amendment of company articles, mergers and acquisitions, and share cancellations were considered nonroutine. Extraordinary meeting. We included a dummy variable that has the value of 1 if the firm had an extraordinary shareholder meeting in a given year and 0 if the firm only had an ordi- nary annual general shareholder meeting. Firm size. Larger firms are exposed to higher and more diverse stakeholder expectations, which may result in socially motivated shareholder dissent (Rowley & Moldoveanu, 2003). We controlled for firm size (the natural logarithm of total assets) as derived from Datastream. Firm performance. Shareholder dissent may be affected by poor firm performance (Krause, Whitler, & Semadeni, 2014). We measured firm performance with Tobin's Q (as defined earlier). This variable was measured before the shareholder meeting took place. Financial leverage. Shareholder activists may voice their dissent to increase the firms' financial leverage (Klein & Zur, 2009). We measured financial leverage as total debt divided by the book value of total assets derived from Thomson Datastream. Board size. Because larger boards may be viewed as less effective and attract shareholder dissent (Hillman et al., 2011), we controlled for board size, measured as the number of direc- tors. CEO duality. CEOs who also chair the board may weaken board monitoring and hence increase shareholder dissent ( Dalton et al., 1998). We included a binary variable taking the value 1 if CEO and board chairman position was held by the same individual and 0 otherwise. CEO tenure. Longer tenured CEOs may be more entrenched and attract shareholder dis- sent ( Dalton et al., 1998). We measured CEO tenure by counting the number of years since the CEO took office. Two-tier board. Some European countries legislate a two-tier board system consisting of a management board and a supervisory board. We included a binary variable set to 1 if a separate supervisory board exists and 0 otherwise. Board independence. We measured this variable as the number of nonexecutive directors who have no relationship with the firm divided by the number of all directors. Largest shareholder ratio. Having several blockholders in the firm may result in mutual monitoring and reduce the opportunities for PP conflicts (Bennedsen & Wolfenzon, 2000). We included largest shareholder ratio, defined as the ownership of the largest shareholder divided by the ownership of the five largest shareholders combined. This variable ranges from 0.2 when the largest shareholder has 1/5 of the ownership of the top 5 shareholders and approaches 1.0 when the largest shareholder is considerably larger than the next four largest shareholders. Dual-class shares. Dual-class stock separates cash-flow rights from voting rights, which may give insiders opportunities to expropriate outside shareholders (Villalonga & Amit, 2009). We collected this variable from Factset, taking the value 0 if the firm issued only one type of stock and 1 if the firm issued at least two types of stock with differing voting rights.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Estimation Strategy", "text": "To analyze the influence of proxy advisory services on shareholder dissent as theorized in Hypotheses 1 through 4, we use multilevel modeling (MLM) to account for the hierarchical nature of our data (Hillman et al., 2011;Hitt et al., 2007). While ordinary least squares (OLS) estimation assumes independence between observations, this assumption is violated because higher-level effects constrain the influence of proxy advice across firms (i.e., Level 2; Aguilera et al., 2008) and countries (i.e., Level 3;van Essen et al., 2013). MLM overcomes this limita- tion by estimating a random intercept for each level and a random coefficient for ISS negative recommendation. The random intercept reflects a change in scale (i.e., shareholder dissent is higher or lower in different firms or countries), while the random coefficient reflects a change in magnitude (i.e., ISS negative recommendation has a stronger or weaker effect across lev- els). The estimations were performed using the xtmixed command in Stata 14.1. It is important to determine the appropriate number of random intercepts in MLM models, which can be accomplished by examining the variance in the dependent variable (Raudenbush & Bryk, 2002). This is important since we have two dependent variables, which may lead to different levels for each dependent variable. For the dependent variable shareholder dissent, significant variance exists on the country and firm-year level. Proposals are clustered within firm-years because shareholders vote on specific proposals, resulting in variation in the dependent variable shareholder dissent at the proposal level. We do not model the firm level as separate random intercept due to the short time dimension of only 2 years in our sample, which may overestimate firm-level variance (Clarke, 2008). We thus specify a three-level model for shareholder dissent, while controlling for firm-level fixed effects. For the dependent variable future firm performance, however, there is no variation on the proposal level because firm performance is measured at the firm-year level. While some meth- odological innovations exist to estimate higher-level outcomes with lower-level predictors (Croon & van Veldhoven, 2007), MLM techniques generally require that the dependent vari- able is measured at the lowest level (Hofmann, Griffin, & Gavin, 2000;Preacher, Zyphur, & Zhang, 2010). We therefore specified a two-level model for the dependent variable future firm performance, while again controlling for firm-level fixed effects. Additionally, the independent variable ISS negative recommendation was aggregated to the firm-year level by counting the number of times ISS negatively recommended against a proposal divided by the total number of proposals in the firm-year. While aggregating data comes with limitations, such as reduced sample size, this aggregation was theoretically mandated (Snijders & Bosker, 1999). First, we are theoretically interested in the degree of agency costs identified by ISS for each firm-year, not in the proposal source of agency costs. To test Hypothesis 5, we therefore took the number of negative recommendations in each firm-year, scaled by the total number of proposals voted in that firm-year, as a proxy for the degree of agency prob- lems identified by ISS in each firm-year. Second, ecological inference bias (i.e., generaliz- ing findings to an inappropriate lower level) was not a concern as we did not generalize our findings to the proposal level. Rather, our focus was to examine if the level of agency costs identified by ISS was able to predict future performance shortfalls that derive from these agency costs. As we did not theoretically consider the variation in the individual proposals, aggregation is needed. We group-mean centered ISS negative recommendation at the firm-year level ( Hofmann et al., 2000). Group-mean centering was needed to estimate the relationship between the low- est level variables more precisely (Enders & Tofighi, 2007). It is also required for testing cross-level hypotheses because group-mean centering reduces the between-firm variance in the Level 1 variable, which may lead to spurious cross-level interactions (Enders & Tofighi, 2007;Hofmann & Gavin, 1998). Last, we grand-mean centered the higher-level variables to facilitate interpretation and avoid multicollinearity (Raudenbush & Bryk, 2002, p. 31). We used maximum-likelihood estimation so we could conduct deviance tests to exam- ine the improvement in model fit (Snijders & Bosker, 1999). Deviance statistics (defined as \u22122 times the log likelihood) account for the multilevel nature of errors (Raudenbush & Bryk, 2002). Similar to examining R 2 changes in OLS, a statistically significant reduction in deviance upon the stepwise addition of predictor variables indicates model improvement.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Endogeneity Procedure", "text": "ISS negative voting recommendations may be influenced by factors that also influence shareholder dissent, which may raise endogeneity concerns. Specifically, firm performance and corporate governance characteristics, such as board size and board independence, may influence shareholder dissent (Hillman et al., 2011;Krause et al., 2014). These same factors may also influence ISS negative recommendation, because ISS recommends against propos- als if the firm's governance characteristics are indicative of agency problems (Cai et al., 2009) and when firm performance is consistently weak (Ertimur, Ferri, & Oesch, 2013). This raises the question whether negative voting recommendations have an effect on shareholder dissent independent of firm performance and corporate governance characteristics (Cai et al., 2009;Fischer et al., 2009). Following Cai et al. (2009), we used a two-stage approach in which we created a measure of ISS negative recommendation that is uncorrelated with firm performance and corporate governance characteristics, such as board size and board independence. The residuals derived from the first-stage model are uncorrelated with these factors and will be included in the second-stage MLM models. While this two-stage endogeneity procedure has been used in previous studies to address reverse causation concerns (Cai et al., 2009;Wiersema & Zhang, 2011), it does not control for omitted-variable bias. We therefore follow Choi et al. (2010) and also control for an extensive list of covariates to account for variables that are correlated with both ISS negative recommendation and shareholder dissent. Since ISS negative recommendation is a binary endogenous variable, we used a probit regres- sion (see Table 2). To estimate the residuals in nonlinear models, we followed Edmans, Goldstein, and Jiang (2012) by adopting the \"generalized residual\" for discrete response models. Model 2 includes ISS negative recommendation to test Hypothesis 1. We find that a nega- tive voting recommendation from ISS increases shareholder dissent (\u03b2 = 0.91, p < .001), supporting Hypothesis 1. In practical terms, a negative ISS recommendation increases share- holder dissent by 148%. Given the average level of shareholder dissent of 3.78%, a negative recommendation from ISS would boost the average level to 9.37%, resulting in an average effect size of 5.59%. This effect size is similar to studies estimating a causal effect of ISS on shareholder dissent-for example, Choi et al. (2010) report an effect size of about 6%. 3 A deviance test indicates significant model fit improvement compared to Model 1, \u03c7 2 (1) = 811, p < .001, thus justifying the inclusion of ISS negative recommendation.  The cross-level interactions are tested in Model 3. Hypothesis 2 predicts that ISS negative recommendation is less influential when ownership is more concentrated. The coefficient of the interaction term is negative and significant (\u03b2 = \u22120.63, p < .001). In terms of practical   significance, increasing ownership concentration from one standard deviation below to one standard deviation above mean ownership concentration decreases the influence of a nega- tive ISS recommendation by around 7.5%. A graphical inspection of this effect supports Hypothesis 2 (Figure 1, Panel A). This figure shows not only the substantial positive main effect of ISS negative recommendation but also that the slope of the regression line is steeper for low ownership concentration. When ownership concentration rises to one standard devia- tion above the mean, the slope becomes less steep. Thus, Hypothesis 2 is supported. Hypothesis 3 predicts that when institutional investor ownership is high, ISS negative recommendation will become more influential. The coefficient of the interaction term is significant and positive (\u03b2 = 0.30, p = .003). Practically, an increase in institutional investor ownership from one standard deviation below to one standard deviation above its mean increases the influence of ISS by 1.7%. While this increase is less than the moderating influ- ence of ownership concentration, it is still sizable given the average level of shareholder dissent of only 3.78%. Figure 1 (Panel B) supports this prediction because the slope is steeper when institutional investor ownership is relatively high in a firm. Therefore, Hypothesis 3 is supported.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Results", "text": "Hypothesis 4 predicts that the influence of ISS increases in market-based governance sys- tems. Consistent with this prediction, we find a significant and positive interaction term (\u03b2 = 0.63, p < .001). Practically, an increase in the market-based governance index from one stan- dard deviation below to one standard deviation above its mean increases ISS's influence by 4.9%. This finding supports Hypothesis 4. Figure 1 (Panel C) also supports this hypothesis because the slope of ISS negative recommendation is steeper in more market-based governance systems, such as the United Kingdom, than in less market-based systems, such as Austria. Table 5 tests Hypothesis 5, predicting that ISS negative recommendations are more nega- tively related to future firm performance in more market-based governance systems. We find a negative and significant interaction effect of ISS negative recommendations and the mar- ket-based governance index predicting ROA in Model 2 (\u03b2 = \u221210.80, p = .019) and predicting Tobin's Q in Model 4 (\u03b2 = \u22120.58, p = .009). Figure 1 (Panels D and E) provides further sup- port. ISS negative recommendations in more market-based governance systems have a more negative slope than ISS negative recommendations in less market-based governance sys- tems. Overall, Hypothesis 5 is supported.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Robustness Checks", "text": "We have performed five robustness checks. 4 First, our primary measure of shareholder dissent includes not only votes against management but also votes abstained (Conyon & Sadler, 2010;Hillman et al., 2011). To ensure reliability, we exclude abstentions from share- holder dissent. Our results remain similar. Second, the largest institutional investor in the firm may already have sufficient incentives to actively collect and process information to monitor the firm ( Schnatterly et al., 2008). We therefore subtract the largest institutional investor from the overall measure of institutional investors (while separately controlling for the largest institutional investor in the MLM mod- els). The coefficient of the interaction term remains positive and significant. Third, our MLM models include a dummy variable to identify routine proposals that are voted with a high likelihood at every shareholder meeting. Director elections are typically considered routine proposals, but they also receive much attention from shareholders (Hillman et al., 2011), which may affect the level of shareholder dissent. Our results do not change when we classify director elections as nonroutine proposals. Fourth, some countries in our sample have few firm-level observations but still provide considerable information on the proposal level (e.g., Luxembourg has six firms and 128 pro- posals). It is generally advised to keep as many observations as possible on the higher levels in MLM models (Snijders & Bosker, 1999), even if the group size is small (Snijders, 2005(Snijders, , p. 1570. However, our results involving the Level 3 (country-level) variables may be affected due to data sparseness on the firm level (Bell, Morgan, Kromrey, & Ferron, 2010). Excluding  observations from the two countries with fewer than 10 firms (Luxembourg and Sweden) does not change our results. Finally, we test Hypotheses 4 and 5 with three alternative country-level measures for market-based governance systems in unreported analyses. We replace our market-based gov- ernance index with the anti-self-dealing index (i.e., shareholder protection against expropria- tion by controlling shareholders; Djankov et al., 2008) as well as the revised anti-director rights index (i.e., shareholder protection against expropriation by managers and directors; Spamann, 2010). Higher values on these indexes reflect stronger institutional constraints on corporate insiders, which facilitates market-based corporate governance. We also replace our market-based governance index with the coordination index developed by Hall and Gingerich (2009). Higher values on this index reflect institutional conditions resembling relationship- based governance systems, while lower values reflect market-based systems. Overall, we find similar results using these three alternative country-level variables.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Discussion", "text": "This study started with the question whether the corporate governance role of proxy advice differs between firms and across countries. Leveraging a cross-country sample, we find that the influence of proxy advice on shareholder dissent is conditioned by the owner- ship composition of firms as well as by the prevailing corporate governance system in a country. At the firm level, we document that shareholders seem to substitute external proxy advice for the internal monitoring capabilities of blockholders that can rely on private infor- mation and private access to management, while proxy advice seems complementary to own- ership by institutional investors, who typically lack such monitoring capabilities. At the country level, we find that shareholders rely more on external proxy advice in market-based governance systems than in relationship-based systems and that proxy advice appears to be better able to predict future financial performance shortfalls in market-based governance systems than in relationship-based systems. Overall, three contributions to the comparative corporate governance literature emerge.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Substitution Between Internal and External Monitoring", "text": "First, our findings are consistent with the view that shareholders are sensitive to the costs and benefits of internal versus external monitoring and that they prefer to rely on internal monitoring capabilities over external proxy advice. Prior studies have documented similar substitution effects between monitoring and incentive mechanisms (Beatty & Zajac, 1994;Misangyi & Acharya, 2014), between blockholder monitoring and board monitoring (Desender, Aguilera, Crespi-Cladera, & Garc\u00eda-Cestona, 2013;Rediker & Seth, 1995), and between external monitoring by the market for corporate control and internal board monitor- ing (Sundaramurthy, Mahoney, & Mahoney, 1997). Contributing to this stream, we provide evidence that substitution effects also exist between external monitoring by proxy advisors and internal monitoring by blockholders. This finding is noteworthy, because blockholders do not always act in the best interest of minority shareholders. In fact, agency conflicts between controlling and minority sharehold- ers make up the most pressing corporate governance problem in countries where insider ownership is the norm ( Young et al., 2008). In spite of possible PP agency costs, however, our results suggest that shareholders expect to obtain more value from blockholders' internal monitoring capabilities than they fear losing through expropriation. This conclusion is consistent with our finding that the presence of a dual-class share struc- ture, which aggravates PP agency problems in blockholder-owned firms (Villalonga & Amit, 2009), does not significantly affect shareholder dissent. This suggests that at the firm level, shareholders do not see PP agency problems as an inhibiting cost. Yet, our country-level find- ings show that reliance on proxy advice is lower in relationship-based governance systems where PP agency costs tend to be relatively high (Thomsen et al., 2006), suggesting that PP agency problems may be a bigger cost to shareholders in some countries than in others.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Complementarity Between Proxy Advice and Institutional Investor Ownership", "text": "Our second contribution centers on the complementarity of external proxy advice and institutional investor ownership. We have argued that institutional investors typically lack internal monitoring capabilities because conflicts of interests and liquidity concerns keep them from taking up insider positions, while their often high degree of portfolio diversifica- tion stands in the way of developing effective internal monitoring capabilities. Previous stud- ies have suggested that institutional investors have incentives to build internal monitoring capabilities to increase their fund performance (Holland & Doran, 1998). Yet, this may not always be the case. For instance, only institutional investors with large ownership stakes may have sufficient incentives to develop meaningful information advantages ( Schnatterly et al., 2008). Moreover, instead of developing internal monitoring capabilities, institutional inves- tors may rely on generic capabilities, such as stock picking (Nain & Yao, 2013). Our findings are consistent with this finer-grained research stream. We present arguments and evidence that institutional investor ownership does not decrease the dependence on information inter- mediaries (as would be expected when institutional investors are able to rely on internal monitoring capabilities similar to large blockholders) but increases the dependence on infor- mation intermediaries instead.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Complementarity Between Market-Based Governance Systems and Proxy Advice", "text": "Third, our findings show that proxy advice is more effective in market-based governance systems than in relationship-based systems. These findings echo the central premise of the comparative corporate governance literature that the effectiveness of any individual corpo- rate governance mechanism may depend on the country-level institutional context in which it is deployed (Aguilera & Jackson, 2010). Specifically, complementarities exist between the corporate governance role of proxy advice and the country-level context in which it operates (Jackson & Deeg, 2008). Our results suggest that the corporate governance role of proxy advisory services is to an important degree specific to the market-based governance systems in which these services originated. Exporting these services to more relationship-based sys- tems thus poses challenges. Our study also suggests an important reason as to why the corporate governance role of proxy advice differs between countries. Our findings show that over all sample countries com- bined, ISS negative voting recommendations are unable to unequivocally identify residual loss in firms and predict the resulting future performance shortfalls. This changes once we account for country differences in corporate governance systems. In this finer-grained analysis, proxy advisors are better able to predict future performance shortfalls in more market-based gover- nance systems than in more relationship-based systems. Shareholders may therefore be less concerned with the generic usefulness of proxy advice as a corporate governance mechanism but instead assess its usefulness in specific country contexts (Bebchuk, Cohen, & Wang, 2013).", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Practical Implications", "text": "Both our study as a whole and our finding that the predictive quality of proxy advice dif- fers across contexts suggest that general \"best practices\" in corporate governance hardly exist ( Daines et al., 2010;Haxhi & van Ees, 2010) and that the quest for globally applicable corporate governance standards may be misguided (Bebchuk & Hamdani, 2008;Black, de Carvalho, Khanna, Kim, & Yurtoglu, 2014). Instead, our study suggests that it makes more sense to adjust corporate governance prescriptions to the local conditions in which they are applied (Desender et al., 2016;Fiss & Zajac, 2004). Doing so may have three advantages. First, academic research may benefit from the development of corporate governance constructs that capture local conditions by allowing a better identification of causal relationships and a stronger prediction of the degree to which corporate governance mechanisms affect firm value (Black et al., 2014). Second, proxy advi- sors may benefit from evidence-based adjustments of their voting recommendations to local conditions, which not only will strengthen their corporate governance role across borders but may also secure future business. Finally, managers of firms monitored by proxy advisors may be able to engage more effectively with their shareholders and proxy advisors about the proposals put to the vote and the voting recommendations on these proposals.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Limitations and Future Research", "text": "Our study offers several opportunities for future research. First, our firm-level measures for PP agency costs capture only part of the possible PP agency costs in blockholder-owned firms. While we also control for PP agency costs at the country level as part of the market- based governance index, future research may further unpack how the costs of blockholder monitoring weigh up to its benefits at the level of the firm. Second, by operationalizing institutional investors as shareholders that invest in firms on behalf of other shareholders, we have tested only whether conflicts of interests and liquidity concerns prevent institutions from building internal monitoring capabilities but not whether particular types of institutional investors (such as investors with high degree of portfolio diversification or short investment horizons) drive our results (Bushee, 1998;Connelly et al., 2010). As we are unable to measure different degrees of portfolio diversification and differ- ent investment horizons for the institutional investors across the 16 countries in our sample due to data availability issues, we must leave the task of further unpacking the effects of institutional investors with different investment strategies on the influence of proxy advice to future research. Third, while the multilevel design adopted in this study allowed us to empirically tease out firm-level from country-level effects on the influence of proxy advice, we were unable to control for possible causal effects between country-level institutions and firm-level ownership conditions that are theoretically plausible in the longer run. As a result, we cannot rule out that some of our findings may be alternatively explained by proxy advisors simply following the gradual change of ownership conditions in our sample countries, for example. Although con- trolling for the institutional ownership in a country does not suggest that this is the case, we urge future researchers to use more sophisticated causal identification techniques to account for the internationalization patters of proxy advisors.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Conclusion", "text": "Proxy advisors have emerged as useful information intermediaries that facilitate share- holders to exercise their voting rights. But how effective they are in different firm and coun- try contexts has remained unclear to date. In one of the first studies investigating the corporate governance role of proxy advisors across countries, we find that their effectiveness in facili- tating shareholder dissent is conditioned by firms' ownership composition as well as the prevailing corporate governance system in a given country. Although proxy advisors are therefore useful aids facilitating shareholder dissent, their effectiveness varies with context.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Notes", "text": "1. In its original agency-theoretical formulation, bonding costs are borne by the agent because agents will need to assure their principals that they will loyally serve their interests (Jensen & Meckling, 1976). However, the empiri- cal corporate governance literature on bonding has focused mostly on assessing the functioning of executive com- pensation, and stock-or option-based incentive plans more specifically, as a bonding instrument (Morris, 1987). We thank an anonymous reviewer for calling our attention to this discrepancy. 2. Four countries in our database (Belgium, Greece, Ireland, and Luxembourg) were not considered by Dyck and Zingales (2004), leading us to manually collect the average private benefits from control block purchases from the SDC database following Dyck and Zingales. 3. Malenko and Shen (in press) estimate a causal effect of Institutional Shareholder Services on shareholder dis- sent of about 25% but only do so for highly controversial and thus dissent-prone \"say-on-pay\" proposals. 4. Results are available upon request.", "title": "Proxy Advisors and Shareholder Dissent: A Cross-Country Comparative Study", "file_name": "Sauerwald et al. - 2018 - Proxy Advisors and Shareholder Dissent A Cross-Co.pdf"}
{"section": "Abstract", "text": "Ethnic diversity is considered detrimental to national unity, especially if ethnicity is politically mobilized: Ethnic parties in electoral competition in particular are thought to increase the salience of ethnic differences and, with it, ethnic tensions. Yet the individual links of this psychological chain have only been examined cross-sectionally, and never together. This article employs original longitudinal survey data to simultaneously assess changes in ingroup identification, outgroup aversion, and national identification over the election period within one diverse society: Romania. While ingroup identification does increase, ethnic relations do not worsen. On the contrary, outgroup aversion decreases while national identification increases, for minority and majority Romanian citizens alike. I explain these findings with the common ingroup identity model from social psychology: Elections in ethnically diverse societies may not only increase the salience of ethnic groups but also that of the superordinate, national identity. The findings question the often assumed automaticity of intergroup threat.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Intergroup Threat", "text": "Studies on elections in ethnically diverse societies are pre- dominantly based on theories of intergroup threat (see also Fearon and Laitin 2000). According to intergroup threat theory, relationships across ethnic group bound- aries tend to be hostile rather than harmonious: We tend to show opposition to the outgroup while favoring our in- group. This ingroup bias increases with the perception of threat to the ingroup by the outgroup, that is, especially in times of danger or contention (see Stephan, Ybarra, and Rios Morrison 2009). In electoral competition for political power, group members may perceive a threat through the respective outgroup, particularly when the ethnic boundary is emphasized by political actors like ethnic parties. Elections in ethnically diverse societies may therefore polarize ethnic relations by politicizing them (see also Michelitch 2015). Figure 1 depicts the hypothesized links of the chain in this polarization framework. First, ethnic politicization affects attitudes predominantly by making ethnicity and ethnic differences more salient, such that thought processes-and behavior-are more strongly in- fluenced by ingroup-outgroup considerations (link A). Eifert and colleagues (2010) show that, across 22 survey rounds in 10 sub-Saharan African countries, respondents were more likely to emphasize their ethnic identity over other identities the closer the survey was conducted to an election. In consequence, the theory goes, ingroup identifica- tion increases (link B). Not only does electoral competi- tion make people more aware of the existence of ethnic differences, but also their ethnic identity becomes more important to them as a result. The mechanism may be either cognitive (Valentino, Hutchings, and White 2002) or rational (Eifert, Miguel, and Posner 2010): People may increasingly recognize a \"utility of sticking together\" dur- ing election periods. Others imply that the mechanism is emotional, that is, that people's ingroup identifica- tion increases in reaction to fear of the now more salient outgroup (Higashijima and Nakai 2016). This brings us to intergroup attitudes more generally. The salience of ethnicity-and specifically of ethnic differences-also in- creases outgroup aversion (link C). People are more aware", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "FIGURE 1 Hypothesized Changes in Intergroup Relations during Elections", "text": "Note: The figure, including both full and dashed arrows, depicts how the polarization framework expects intergroup relations to change over the electoral process due to the increased salience of ethnicity. The dashed arrows in addition signify the mechanisms that the integration framework expects to be blocked during the electoral process despite the increased salience of ethnicity. of how different \"the other\" is and, as this happens in the context of competition, that this difference is negative. Finally, studies on elections in ethnically diverse soci- eties also use intergroup threat theory to infer how group members relate to the polity as a whole, the nation. 2 For members of the dominant group, particularly so if it is a titular group, identification with the nation is expected to increase (link D). For members of the nondominant group, however, national identification is expected to de- crease as their attitudes toward the group nominally rep- resenting the nation worsen and their feeling of belong- ing to that nation decreases (link E). Overall, then, the polarization framework understands ethnic relations to be antagonistic in the longer term, with the antagonism being spurred in the shorter term by intense periods of politicization, such as elections (see also Weber, Hiers, and Flesken 2016). However, some studies suggest that the links between ethnic group salience and attitudes toward the outgroup and the superordinate, national group theorized in inter- group threat theory may be blocked under certain con- ditions. In Figure 1, these blocked pathways are depicted with dashed arrows. According to this integration frame- work, the political salience of ethnicity may not mani- fest itself in outgroup aversion (link C) and low national identification (link E) among members of the nondom- 2 In this article, the nation as an object to which attitudes or feelings are extended is understood to be the political community (see Easton 1965). inant group if the political system provides them with access to group representation. Large-scale cross-country survey analyses find that the minority-majority gap in feelings of national pride is significantly smaller in coun- tries with greater political equality ( B\u00fchlmann and H\u00e4nni 2012;Ray 2018;Staerkl\u00e9 et al. 2010). 3 Examining ethnic group behavior, Birnir (2007) shows that the incidence of political violence is significantly lower where minorities have access to power through institutional means such as proportional representation. Among members of the dominant group, the political salience of ethnicity does not lead to increased outgroup aversion (link C) if they learn, over one or more election cycles, that minority representation does not endanger their own interests. Hajnal (2001) shows that white Amer- icans' attitudes toward blacks improve after experiencing black mayoral leadership. Similarly, Barack Obama's elec- tion as U.S. president reduced whites' prejudice toward blacks (Goldman 2012;Welch and Sigelman 2011). Yet in this integration framework, both the attitudi- nal changes among minority and majority group mem- bers are the outcome of either longer-term processes or first-time experiences, in which members learn that they do not need to feel threatened by the respective out- group and update their views on that group. But for an election in a political system with long-standing ethnic minority representation, as is the case in Romania, we would expect that this learning process has already taken place; as the links are blocked, the short-term electoral process itself should not have a discernible effect on inter- group relations, even if it temporally increases the salience of ethnicity (Figure 1, link A) and, with it, ingroup iden- tification (link B).", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Common Ingroup Identity", "text": "The field's strong focus on antagonistic intergroup re- lations results in an oversight of alternative theories to intergroup threat. Moreover, its focus on ethnic relations ignores one of the key constructivist insights that people have a multiplicity of identities of varying salience (see Chandra 2012); ethnic identity may not always be the most salient identity, even during electoral competition. The assumed automaticity of intergroup threat-barring the interference of contextual roadblocks-reduces con- structivism to a form of \"as-if primordialism\": Although ethnic groups may be constructed, once they are con- structed, they are seen to behave as if they were primordial (see also Kalyvas 2008). I argue that elections in ethnically diverse societies do not necessarily worsen intergroup re- lations but may even improve them, as the electoral pro- cess increases not only the salience of ethnic groups but also that of the common superordinate, national identity. The argument is motivated by findings from social and political psychology that question the automaticity of intergroup threat, as understood in the literature on eth- nic politics. First, increased ingroup identification does not necessarily lead to increased outgroup antagonism (Brewer 2001;Reicher 2004). Davis and Brown (2002) show that African Americans with a strong black social identity do not automatically harbor greater antipathy to- ward whites than those with weaker black social identity. Gibson's (2006) study on South Africa finds no evidence that strong ingroup identities lead to political or racial intolerance. Moreover, any ingroup bias is largely the re- sult of enhancing the ingroup rather than of devaluing the outgroup (Brewer 2001)-and intergroup contact can re- duce the ingroup-outgroup differential (Pettigrew 1998). This intergroup contact theory is the main competitor to the intergroup threat theory in social psychology. A second set of studies questioning the automaticity of intergroup threat finds that increased ingroup identification does not necessarily lead to decreased national identification among minorities (Sidanius and Petrocik 2001). In the United States, de la Garza, Falcon, and Garcia (1996) find that Mexican American citizens are equally or even more patriotic than Anglo American citizens, whereas Citrin, Wong, and Duff (2001) con- clude that ethnic and national identities are generally complementary rather than competing. In multicountry studies, Dowley and Silver (2000) only get mixed results, suggesting that exogenous factors affect the relationship, and Elkins and Sides (2007) find that minorities can feel attached to both the ethnic group and the nation at once. This simultaneity of nested group identities is taken into account in the common ingroup identity model in social psychology, aligning it with constructivist theory on ethnic politics. The model argues that a common ingroup identity such as national identity can extend or redirect \"the cognitive and motivational processes that produce positive feelings toward ingroup members to former out- group members\" (Gaertner, Dovidio, and Bachman 1996, 271; see also Gaertner and Dovidio 2009). That is, when the common ingroup is made salient and people see each other as fellow members of a shared group, antagonism between the subordinate groups and, with it, any ingroup bias decreases. In social psychology, the model is supported by var- ious laboratory experiments, survey studies, and field experiments (see Gaertner and Dovidio 2009). In po- litical science, Transue (2007) shows that making the na- tional identity salient to U.S. study participants improves intergroup relations by reducing the social distance to citizens of different racial backgrounds. Similarly, Levendusky (2017) shows that highlighting the common Amer- ican national identity decreases affective polarization be- tween Democrat and Republican partisans. Beyond the U.S. context, increasing the salience of shared national identity has been found to increase pro-social behavior between Hindus and Muslims in India (Charnysh, Lucas, and Singh 2015), as well as trust across ethnic boundaries in Malawi (Robinson 2016). In these studies, national identity was primed in ex- perimental conditions, except for one study in Levendusky (2017), in which he examines the priming effect of Independence Day celebrations. Yet it is reasonable that general elections may also serve as a primer of broader national identity, particularly in its civic aspect of a politi- cal community. 4 First, ethnic parties are rarely concerned with issues pertaining to ethnic group interests alone but also campaign on general interest issues like taxation or foreign affairs. Second, ethnic parties are rarely the only, let alone the biggest, parties in any election. Often polit- ical parties portray their policy positions as affecting the country and its future as a whole, and the mobilization efforts of campaigns build political efficacy and social capital among the citizenry ( Banducci and Stevens 2015;Rahn, Brehm, and Carlson 1999). Citizens may therefore also have a heightened awareness of their membership in a wider political community. The common ingroup framework as well as the po- larization and integration frameworks can be put to the test by observing changes in intergroup attitudes during an election period. Table 1 summarizes the changes in ingroup identification, outgroup attitudes, and national identification we would expect according to each frame- work for citizens from three different constellations of ethnic group relations: those in the overall majority and those in the overall minority, as well as those citizens of the majority group living in areas dominated by the minority. The common ingroup framework would lead us to expect an increase in both ingroup and national identifi- cation for each group, and-as a consequence of perceiv- ing the respective ethnic outgroup as part of the shared national ingroup-also more positive attitudes toward that outgroup (Table 1c). In both the polarization frame- work and the integration framework (Table 1a and Ta- ble 1b, respectively), ingroup identification also increases for all groups, though in the polarization framework the increase is likely stronger among the two minority pop- ulations, as for these groups the intergroup threat is felt more keenly. Accordingly, the change in outgroup atti- tudes and national identification is also stronger for these two groups than for the majority population, with the overall minority population experiencing a marked de- cline in national identification. In the integration frame- work, on the other hand, we would expect no change in outgroup aversion or national identification in any group over the short term once a learning process has taken place over the longer term.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Ethnicity in Romanian Politics", "text": "According to the 2011 census (Institutul Na\u021bional de Statistic\u02d8 a 2011), Romania's slightly over 20 million peo- ple are divided into over 20 recognized ethnic groups, including ethnic Romanians, who comprise 83.5% of the population. The largest ethnic minority is that of Hun- garians, with 6.1% of the population, followed by Roma at 3.1%, and Ukrainians, Germans, and Turks at 0.2% each. While ethnic minorities are scattered across Roma- nia, they are most numerous in the historical region of Transylvania in the center and northwest of the country, where much of the Hungarian population is concentrated. In the counties of Harghita and Covasna, Hungarians are even in the numerical majority, with 71.6 and 82.9% of the population, respectively. 5 Up until the beginning of the twentieth century, Hun- garians were the socially, economically, and politically dominant population in Transylvania. This changed in the interwar period and particularly in the communist era, when the influence of Hungarians and Hungarian language and culture were curtailed. Following the fall of communism, the 1990s saw minority mobilization for community rights as well as political backlash among the majority population, with violent manifestations (Birnir 2007;Stroschein 2012). Nowadays, ethnicity still plays a role in Romanian elections. A number of minority or- ganizations compete for reserved seats, one assigned to each recognized minority (see King and Marian 2012). The relatively large size of the Hungarian minority, how- ever, allows Hungarian politicians to clear the 5% vote threshold, and they hence compete not for a reserved seat but for proportional representation on the general ballot. 6 The largest Hungarian party is the Democratic Alliance of Hungarians in Romania (DAHR; Uniunea Democrat\u02d8 a Maghiar\u02d8 a din Rom\u00e2nia / Rom\u00e1niai Magyar Demokrata Sz\u00f6vets\u00e9g). Since the 1990s, the DAHR has consistently obtained at least 80% of the Hungarian vote and thus parliamentary representation ( Kiss and Sz\u00e9kely 2016). While the DAHR is ideologically flexible enough to serve as coalition partner to Romanian parties from both the left and right in parliamentary politics, Hungarian minority rights are high on its official agenda and in its contact with its constituency ( Andriescu and Gherghina 2012;Kiss and Sz\u00e9kely 2016). According to the Ethnona- tionalism in Party Competition data set, which summa- rizes country experts' coding of the importance of eth- nic issue positions to various parties in Eastern Europe (Sz\u00f6csik and Zuber 2015), ethnonationalism, cultural au- tonomy, education in and of Hungarian, the use of Hun- garian language, and territorial autonomy are highly im- portant to the party (coded as at least 9 on a 0-10 scale). According to a 2013 survey, the Hungarian electorate is confident that the DAHR effectively represents their community's interests in national government, more so than any other Hungarian party ( Kiss and Sz\u00e9kely 2016). Ethnic issues are not just a fringe concern of the minority parties in the reserved seats or of the DAHR: When examining the same issues for mainstream Roma- nian parties, issue importance ranges from 3.4 for the use of the Hungarian language to 6 on ethnonationalism, to which they are largely opposed. Adding the issue impor- tance of the nationalist Greater Romania Party increases the numbers further (Sz\u00f6csik and Zuber 2015). That is, while possibly not as central to party programs as, for example, the economy or foreign policy, ethnic concerns do play a role for Romanian political parties too (see also Andriescu and Gherghina 2012). The December 2016 general elections were no differ- ent. While the DAHR campaign covered general topics such as the economy, education, infrastructure, and rural development (DAHR 2016b) and advocated peaceful in- terethnic coexistence (DAHR 2016e), Hungarian minor- ity representation was at the heart of the campaign. The party advocated the extension of existing minority rights (e.g., DAHR 2016i; DAHR 2016l) but also warned of their rollback, evidenced, it argued, among others by \"an attack on the entire Hungarian community\" in the form of the court-mandated closure of a Hungarian-language school in T\u00e2rgu Mure\u0219 (e.g., DAHR 2016a). To clear the 5% threshold and thus be able to pre- serve Hungarian rights, the DAHR increasingly called on Hungarians to vote. The message was reinforced by Hun- garian Prime Minister Viktor Orban (DAHR 2016g) as well as by entrepreneurs, who likened DAHR representa- tion to the \"survival of Hungarians\" (DAHR 2016j), and by church leaders, who equated the act of voting to a \"re- ligious duty\" (DAHR 2016f) to protect the community (DAHR 2016d). On Election Day, the DAHR reported on mass text messages sent by Romanian nationalist politicians to encourage Romanians to vote, as this could prevent Hungarian representation in parliament (DAHR 2016k). This report followed warnings of increased levels of anti-Hungarianism in Romanian society more gener- ally (DAHR 2016c). The DAHR campaign hence clearly politicized the ethnic boundary between Hungarians and Romanians and increased its salience. In the end, the party cleared the electoral threshold with 6.2% of votes, result- ing in 21 seats in the lower chamber and nine seats in the senate. It does not form part of the government, but is in a confidence-and-supply agreement with the governing coalition of the Social Democracy Party (Partidul Social Democrat) and the Alliance of Liberals and Democrats (Alian\u021ba Liberalilor \u0219i Democra\u021bilor).", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "The Romanian Election Panel Study", "text": "The election campaign thus had the potential to polarize ethnic relations, particularly so in the counties in which the Hungarian population is concentrated and in which Romanians are in the minority. To assess the dynamics of ethnic relations, I conducted a panel survey surrounding the general elections on December 11, 2016. The first wave was conducted 4-6 weeks before Election Day, before the official campaign start on November 11 (OSCE/ODIHR 2016); the second wave was 4 weeks after, in mid-January 2017. 7 The survey included questions concerning respon- dents' political interest; party affiliations, voting behavior, and perceptions of electoral fairness; satisfaction with the economy, government, and democracy; social trust; na- tional identity; group identification and relations; and demographic information. In this article, I examine three key sets of questions covering ethnic relations: belong- ing to one's own ethnic group, evaluation of the respec- tive outgroup, and belonging to the national political community. To assess the effects of elections on ethnic relations, I surveyed representative samples of voting-age respon- dents living in three different constellations of ethnic group relations: (a) Romanians living in the majoritar- ian Hungarian counties of Harghita and Covasna (n = 401 in both waves); (b) Romanians in majoritarian Ro- manian counties (n = 417; in the following, \"national Romanians\"); and (c) Hungarians in counties in which they make up a substantial share (n = 423). 8 The initial response rate was 65.8%, in line with other large-scale representative surveys, and the attrition rate was com- paratively low at 8.3%, with no apparent bias in attrition across samples. 9 With the three samples, it is possible to examine all links in the theoretical chains described in Figure 1 and Table 1, by observing changes in the majority population and the minority population, as well as the majority pop- ulation living in minority-dominant counties. Among the latter two samples, any election effect should be particu- larly strong because it is here that the DAHR focuses its electoral campaign, whereas mainstream parties present themselves as \"defenders of the 'Romanian minority'\" in Harghita and Covasna and unite against the Hungarian vote in more balanced counties (Kiss and Sz\u00e9kely 2016, 604). The panel survey design has several advantages over other research designs. In contrast to cross-sectional sur- veys, it allows identification of not only aggregate but also individual change over time, and thus also for more con- fident identification of causality (Lynn 2009). In contrast to many experimental designs, the panel survey design lends external, and specifically ecological, validity to the results, as it covers attitude change among representative samples and in real-world contexts rather than in simu- lations (Holbrook 2011). 8 The Hungarian survey was conducted in Bihor, Cluj, Covasna, Harghita, Mures, and Satu Mare (see Figure A1). 9 See Table A1. Initial response is unlikely to be biased along in- terethnic attitudes, as the survey was introduced as an election study, without mention of a focus on ethnic relations. Neither is there an apparent bias by media attention or political interest (see Table A2). Data on demographic representativeness are presented in Table A3.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Results", "text": "I examine the data for their fit with the expectations de- rived from the polarization, integration, and common identity framework (compare Table 1). I first examine differences between samples and changes within samples for the three outcomes of interest-ingroup identifica- tion, outgroup attitudes, and national identification- followed by additional tests of the common ingroup iden- tity framework and of alternative explanations. The vari- ables and methods used are described in the respective sections.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Ingroup Identification", "text": "To test the first set of hypotheses concerning ingroup identification, I use two questions: 10 \"On a scale of 0-10, how important is being [Romanian/Hungarian] for you personally?\" \"I am going to ask you to use a scale like a thermometer to express your evaluation of members of this group. 100 degrees means you typically evaluate them to be extremely favorable, 50 degrees means neither favorable nor unfavorable, and 0 degrees means extremely unfavorable.\" Responses to the importance and feeling thermometer questions are strongly associated with each other ( 2 w1 = 513.49, 2 w2 = 862.06, p < .001), and after rescaling the latter to range from 0 to 10, I create an index out of their mean. The results for ingroup identification do point to changes associated with the election, as hypothesized in all three frameworks (Figure 2a). 11 In Wave 1, identification is relatively high for all three samples, with mean values 8.76 to 8.90 on a 0-10 scale; the means do not differ significantly between samples. The values increase even further in Wave 2, by an average of 0.22 points among national Romanians and 0.45 among Hungarians. But the strongest increase occurs among Romanians living in the Hungarian-majority counties of Harghita and Covasna, with a mean increase of almost 1 point. All changes are significantly different from 0 (p w2-w1 .004). although the changes may appear relatively small, they amount to an increase of up to 11 percentage points, and that despite a likely ceiling effect. The results are in accordance with intergroup threat theory, and hence the polarization and integration frameworks, in that they suggest that the election more strongly affected the majority population in counties in which they are not dominant, perhaps as a backlash against Hungarian election rhetoric. However, they do not inform us whether perceived threat is indeed the mechanism behind increased ingroup identification.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Outgroup Attitudes", "text": "For the second set of hypotheses concerning outgroup attitudes, I use the same thermometer question as above, but for the respective outgroup. Figure 2b describes between-and within-sample differences in outgroup attitudes. Note that values below 5 signify outgroup aversion, 5 neutrality, and those above 5 affinity. In Wave 1, all samples report outgroup affinity. Among national Romanians, the extent of affinity is limited at 5.34, but among Romanians in Harghita and Covasna and among Hungarians, affinity is relatively high-and significantly higher than among national Romanians-at values above 7. This pre-election result alone suggests that intergroup contact rather than threat theory is applica- ble in the Romanian context: In counties with a higher probability of intergroup contact, between-group affinity is stronger than in counties with a lower probability of intergroup contact. Just as for ingroup identification, outgroup affinity increases between waves for all three samples. Among na- tional Romanians and Hungarians, the increase is about 1 point, while among Romanians in Harghita and Cov- asna, the increase is almost twice that with 1.84 points. All increases are statistically significant (p w2-w1 < .001). The election thus did not have a polarizing effect on ethnic relations: On the contrary, outgroup affinity increased among all samples, and most among Romanians who would be most strongly affected by the minority's in- crease in power. Hungarians, too, report increased affin- ity toward Romanians, so neither can it be said that the minority is being incited against the majority.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "National Identification", "text": "To test the third set of hypotheses on national identifica- tion, I also create an index of the mean of the responses to two questions: \"To what extent are you proud of being a citizen of Romania?\" and \"On a scale of 0 to 10, to what extent do you agree with the following statement? It is important to me to promote a positive image of Romania in contact with foreigners or when I go abroad\" ( 2 w1 = 1,240.2, 2 w2 = 838.06, p < .001). Figure 2c describes between-and within-group dif- ferences in national identification. In Wave 1, the mean is, not surprisingly, lowest among Hungarians, but still relatively high at 6.61 on a 0-10 scale. And even though it is higher among Romanians in Harghita and Covasna, at 8.09, this value is still significantly smaller than that of national Romanians, which lies at 8.64. The latter value does not substantively change in Wave 2, possibly due to the already high value in Wave 1. However, national identification increases among Romanians in Harghita and Covasna by more than 1 point to 9.16, and among Hungarians by nearly 2 points to 8.51. The changes differ significantly from each other as well as from zero. The data therefore indicate that in the time between the survey panels, national identification among national Romanians was relatively stable and that that among Ro- manians in Harghita and Covasna and among Hungar- ians increased substantially. Moreover, national identi- fication among Romanians in Harghita and Covasna is now significantly higher than among national Romani- ans. Both this and the stark increase among Hungarians again strongly suggests that the election campaign did not have a polarizing effect, as otherwise national identifica- tion should have decreased or, at the very least, remained stable among Hungarians.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Individual-and Subsample-Level Changes", "text": "The aggregate analysis so far has shown that ingroup identification, outgroup affinity, and national identifica- tion all increased in all three samples, pointing away from both the polarization and integration frameworks and toward the common ingroup framework. To avoid eco- logical fallacies, I also examine whether this association holds when disaggregating the data. First, the national Romanian sample includes respon- dents from counties in Transylvania, other than Harghita and Covasna, and the rest of the country. As intergroup tensions have historically been highest in Transylvania, it is necessary to examine whether the threat-based frame- works apply to this region, if not outside of it. However, looking at Romanian respondents living in Transylva- nia (specifically, in Bihor and Cluj) supports the argu- ment set out so far: Although ingroup identification in- creases significantly, again suggesting higher salience of intergroup relations in these counties, national identifica- tion also increases and outgroup attitudes do not worsen in Transylvania either (see Figure A2 in the supporting information). Second, the Hungarian sample includes respondents from counties where they are in the clear majority (Harghita and Covasna) and from counties where they are in the minority. Threat and contact dynamics may work differently in these contexts; for example, majority Hun- garians may be more resistant to the idea of a common ingroup identity than minority Hungarians, who have more opportunity for outgroup contact. However, ma- jority Hungarians show a significantly and substantially higher increase in ingroup identification, outgroup affin- ity, and national identification than minority Hungari- ans ( Figure A3 in the supporting information), suggest- ing that the election period fostered a common ingroup Note: Numbers represent correlation coefficients (r) between changes in the respective attitudes between the pre-and post- election survey wave at the individual level. H&C = Harghita and Covasna identity particularly among those for whom it may not have been as prevalent before. Finally, I examine whether the simultaneity of in- creases in ingroup identification, outgroup affinity, and national identification holds not only at the aggregate level but also at the individual level. A series of simple correlations shows that the change scores of all three out- comes are positively correlated with each other in all three samples (Table 2). Among national Romanians, the cor- relations between change in ingroup identification and outgroup attitudes are not particularly large, but posi- tive (r = .21). The largest correlation in this sample is that between the change scores in ingroup and national identification, which indicates that the ingroup identity of Romanians, as the titular group, is strongly associated with national identity. Among Romanians in Harghita and Covasna, as well as among Hungarians, change in ingroup identification and outgroup attitudes is more strongly positively correlated (r = .46 and .55, respec- tively). That is, among respondents in the samples with the highest rate of intergroup contact, we see a stronger increase in outgroup affinity where there is also an in- crease in ingroup identity. To reiterate, this result aligns more strongly with the general intergroup contact theory on which the common ingroup framework is based than with the intergroup threat theory explicitly or implicitly underlying much of the literature on ethnic conflict in political science.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Common Ingroup Identity", "text": "The stark increase in national identification through- out the population suggests that the common identity has indeed been primed over the election period in be- tween the two survey waves. The data may lend additional", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "TABLE 3 Between-and Within-Sample Differences in Mean Values of Ingroup Bias", "text": "Within-Wave, Between-Sample Differences Between-Wave, Within-Sample Differences Sample n mean w1 p w1 mean w2 p w2 mean w2-w1 p w2-w1 Romanian (national Note: n = valid observations; w1 = pre-election wave; w2 = post-election wave; p w1 and p w2 = significance values of two-tailed t-tests of mean differences between the Romanian (national) sample and the other two samples in the respective wave; p w2-w1 = significance values of one-tailed t-tests testing whether the change in within-sample means is negative; H&C = Harghita and Covasna. evidence for the common ingroup framework. First, if the common ingroup framework is correct and an emphasis on the superordinate identity refocuses ingroup bias from the ethnic group to the national ingroup, we should also see a decrease in social distance between groups (see also Transue 2007). I examine social distance with the dif- ferences between the ingroup and outgroup feeling ther- mometers. Table 3 reports the values for ingroup bias per sample in Wave 1 and Wave 2, and their differences. It is evident that national Romanians generally have a significantly and substantively higher ingroup bias than the other two samples, with feelings toward the ingroup nearly 3.5 points higher than feelings toward the outgroup. This again points toward the intergroup contact theory, with those respondents in areas with higher intergroup contact (specifically, Harghita and Covasna) having lower biases. But among national Romanians, just as for Romanians in Harghita and Covasna, the ingroup bias decreases signifi- cantly in Wave 2, showing that the election period indeed coincides with an improvement in interethnic relations. Among Hungarians it decreases too, and although it is only significant at the .1 level, this is so despite the already very low level of ingroup bias among Hungarians before the elections: 79.3% of all Hungarians show an ingroup bias of 0 or 1 in Wave 1, whereas only 65.7% of Harghita and Covasna Romanians do so. Second, the common identity framework may be fur- ther tested by examining attitudes toward pro-social be- havior. Field experimental studies on the nation as com- mon ingroup identity have shown that salience of national identity triggers greater support for redistribution and pro-social behavior across ethnic boundaries (Charnysh, Lucas, and Singh 2015;Robinson 2016;Transue 2007). De Cremer and van Vugt (1999) explain such changes with increased value being assigned to the public good. Therefore, if the superordinate, national identity and, in particular, its political aspect have been effectively primed, we would also expect an increase in respondents' support  w1 mean w2 mean w2-w1 p w2 Note: n = valid observations; w1 = pre-election wave; w2 = post- election wave; p w2-w1 = significance values of one-tailed t-tests testing whether the change in within-sample means is positive; H&C = Harghita and Covasna. for the public good, here examined with their willing- ness to support the state by paying their taxes in full. As Table 4 shows, this is indeed the case: Respondents in all three samples are significantly more willing to pay their taxes in full after the elections than before. Both ingroup bias and tax attitudes hence provide further evidence for the common ingroup framework.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Discussion", "text": "The positive association between ingroup identification, outgroup attitudes, and national identification over the election period rejects the assumptions of both the polar- ization and integration frameworks and instead suggests that national elections serve as a primer of a common ingroup identity. Here, I discuss potential alternative ex- planations. First, might the changes have been caused by factors other than the elections? Commemorative national hol- idays and sporting contests, terrorist attacks and natural disasters, or other short-term events putting a spotlight on the nation, such as a census, may evoke a sense of national identification (e.g., Bonikowski 2016). Yet, of these, only a few international games and a national hol- iday occurred between the two survey waves. The games were nothing out of the ordinary, and the Romanian teams usually found themselves on the losing side. The Romanian national holiday on December 1 is for many merely another day to go out and celebrate, without heed- ing the national meaning, which makes it unlikely that its effects are still felt in January. Moreover, the day is not perceived as a holiday by Hungarians, as it signifies the unification of Transylvania and Romania in 1918 and to them represents a loss of power; rather than a day of cele- bration, it is a \"day of mourning\" (Fox 2006, 223). Neither the sporting contests nor the holiday celebrations could thus have had the observed effects. Second, might the timing of the observations not tell the whole story? For example, although the of- ficial campaign period began only after completion of Wave 1, it may be that parties began campaign- ing earlier and that therefore the attitudes observed in Wave 1 did not represent \"base attitudes\" but those al- ready affected by campaign rhetoric. However, media use for campaigning is strictly regulated (OSCE/ODIHR 2016), and if the DAHR already campaigned informally beforehand, it is unlikely to have reached a large enough number of citizens, particularly Romanians, to have had a substantial impact. And if it did, there are two pos- sibilities: The attitudes observed in Wave 2 show either (a) a continuation or (b) a reversal of this impact. In the first case, the argument advocated so far that the elec- tions improved intergroup attitudes holds, but because the true base attitudes are not known, it is not possible to pinpoint the extent of the improvement. In the second case, the observations would indeed suggest a polarizing campaign effect, but one that does not last and is already forgotten a few weeks later; the substantive effect of eth- nic party rhetoric would not be as strong as feared. The same holds for the argument that polarization should have been strongest shortly before or on Election Day, which remained unobserved. Which case is correct can only be resolved with a follow-up study with more waves over a longer time period. But in either case, we still ob- serve that changes in ingroup, outgroup, and supergroup attitudes are positively associated with each other, rather than negatively as predicted by the intergroup threat the- ory, underlining this article's arguments that it is neces- sary to look beyond that theory and to examine the dif- ferent attitudes separately, rather than making inferences from one to the other. Third, might the findings be the result of the ques- tionnaire design? Surveys are prone to social desirability bias, in which respondents do not report their true atti- tudes but those they perceive to align with social norms. If social norms portray ethnic tolerance and multicul- turalism as desirable, respondents may not be willing to express the true extent of their ingroup bias. However, the question most explicitly inquiring about attitudes to- ward other groups, the feeling thermometer, was phrased identically to that inquiring about ingroup attitudes, and the setup was identical in both waves. Social desirability bias hence cannot explain the change in attitudes be- tween waves. Instead, attitude change may be explained by learning effects, or panel conditioning, with respon- dents updating their response patterns after answering the same questions earlier. However, updating responses requires remembering the previous responses, which is unlikely over the more than 2-month gap between the survey waves. Moreover, it would be unclear what trig- gered the learning effect, save perhaps that the election period acted as a reminder of any social norms on in- tergroup relations. 12 Even if that were the case, it would qualify the common ingroup framework only to the ex- tent that it is not indeed the common ingroup itself but the idea of a common ingroup that effected the change. Finally, might the changes not have been driven by considerations of identity, but by those of material ben- efit? Romanian residents of Harghita and Covasna may recognize that there are benefits of being represented by the DAHR, as the indivisible spoils of government partici- pation are distributed in the region as a whole: Romanians there, too, will benefit from investments in infrastructure, housing, and the job market. In this case, we would expect an increase in affinity not only toward the Hungarian out- group but also toward the DAHR. However, whereas in the pre-election wave, party affinity for the DAHR is higher among Romanians in Harghita and Covasna than in other regions, it decreases in the post-election wave, such that party affinity is the same across the country. 13 That is, DAHR support cannot explain why minority Romanians report increased affinity for Hungarians. 14 Moreover, any such consideration would still not explain the changes among Hungarians and Romanians elsewhere.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Conclusion", "text": "Ethnic parties are often seen to undermine national unity, as they are thought to politicize ethnic differences and 12 Research on attitudinal panel conditioning is in its infancy (Sturgis, Allum, and Brunton-Smith 2009). 13 See Table A6 in the supporting information. 14 Neither is the decrease in party affinity specific to the DAHR but affects all parties. raise their salience, and in doing so increase ingroup co- hesion and outgroup rivalry. The longitudinal analysis conducted in this article does not support this polar- ization framework. Although ingroup identification did increase, neither outgroup relations nor national identi- fication worsened. Neither does the analysis support the integration framework, according to which intergroup threat is blocked by contextual factors such as power relations and the history of intergroup conflict: One may argue that the elections did not worsen intergroup relations because Hungarians make up only 6.1% of the popula- tion and are regionally concentrated in Transylvania. As such, Romanians outside of Transylvania might not have been exposed to much of the campaign, and Romanians may feel that even when Hungarians gain representation, their power is relatively easily contained. DAHR's junior partner position in a variety of government coalitions may reassure them. That is, the uneven power relations together with the absence of evidence of a Hungarian threat may explain the lack of a polarization effect among Romanians. The same factors may also explain the lack of a polarization effect among Hungarians, as they have learned that, despite the uneven power relations, propor- tional representation provided them with the opportunity to enter parliament and even government. However, the data do not evidence attitudinal stability but an increase in outgroup affinity and national identity throughout the electoral process. That is, the findings at least qual- ify the mechanisms suggested to underlie the integration framework, in that the demographic and institutional context not only block more or less automatic threat dy- namics engendered by ethnic politicization, but also allow identities other than ethnicity to become more relevant. The results invite consideration of alternative hy- potheses on intergroup relations. Here, I explained the observed simultaneous increase in all three attitudes with the main contestant of intergroup threat theory under- lying the polarization and integration frameworks: in- tergroup contact theory and its offspring, the common ingroup identity model. The national nature of elections raises the salience of a common ingroup identity and in- spires national identification, and, as a result, antagonism between the subordinate groups decreases-despite eth- nic party rhetoric. The effect of ethnic parties may hence be negligible for interethnic attitudes, whereas national elections can have a positive effect. As the discussion of the integration framework shows, the Romanian context may only allow general- ization to similarly stable political regimes with similarly low levels of ethnic tensions; we may obtain a different picture if we repeated the analysis in societies marred by systemic discrimination and ethnic violence. Even so, the findings generalize to a large number of countries with ethnic diversity and ethnic parties, such as Belgium, Czech Republic, Finland, Mauritius, Namibia, New Zealand, or Peru (Lublin 2014): There, too, ethnic party rhetoric may not necessarily lead to worsened ethnic tensions. And in less stable countries, it is necessary to identify how ethnic party rhetoric may work together with other factors in fueling ethnic tensions. The findings presented here question the assumed automaticity of intergroup threat and invite future stud- ies to make one's assumptions about the psychological processes at work explicit, and then to empirically assess each of its component parts, rather than to make infer- ences from one part about the others. The field's tendency to equate ingroup identification with outgroup aversion or to assume that minority groups and national identifi- cation are mutually exclusive has resulted in dispropor- tionate emphasis on intergroup threat to explain ethnic relations. This also implies that we may not easily make inferences from attitudes to behavior, and vice versa: For example, while voting behavior may follow ethnic lines, this does not necessarily indicate nationalistic tendencies or outgroup aversion on the part of either the minority or the majority. 15 Future research needs to more clearly disaggregate interethnic attitudes and behavior in theory and practice, and to explore these in both international and intranational comparison.", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Supporting Information", "text": "Additional supporting information may be found online in the Supporting Information section at the end of the article. Online appendix Ethnic parties, ethnic tensions? Results of an original elec- tion panel study Figure A1: Percentages of ethnic Romanians and Hun- garians in Romania by county Figure A2: Mean values per survey wave for Romanians (nat.) in counties with a Hungarian population above and below national average of 6.1 percent Figure A3: Mean values per survey wave for Hungarians in majority and minority Hungarian counties Table A1: Sample sizes and response and attrition rates ", "title": "Ethnic Parties, Ethnic Tensions? Results of an Original Election Panel Study", "file_name": "Flesken - 2018 - Ethnic Parties, Ethnic Tensions Results of an Ori.pdf"}
{"section": "Efficacy Beliefs in Third-Person Effects", "text": "Extensive research on the third-person effect has shown that people support censorship when they perceive others as being more influenced by \"harmful\" media than they are themselves. This perception is related to the belief that others are relatively more exposed to such media and, although this perception occurs for many kinds of media content, it is most pronounced for content that may have undesirable influences. This combination of exposure and influence is strikingly similar to conceptualizations of threat in models of risk perception (Fischhoff, Slovic, Lichtenstein, Read, & Combs, 1978;Neuwirth, Dunwoody, & Griffin, 2000;Rogers, 1975;Witte, 1994). Another key concept in models of risk perception relates to control, where the uncontrollability of a noxious event amplifies perceived risk. Third-person effects research has provided a limited account of control beliefs that may influence support for censorship. Nonetheless, at least two control beliefs may factor into the equation: (1) the belief that, upon exposure to noxious media content, audiences lack the cognitive wherewithal to directly mitigate negative influence and (2) the belief that censorship can effectively reduce exposure. The former belief relates to the efficacy of self-regulation, while the latter belief relates to the efficacy of censorship as a systemic remedy. Not only may these efficacy beliefs motivate support for censorship, but they may also exhibit the classic self-other perceptual asymmetry that defines third-person perception. Several cognitive biases are helpful to explain why people tend to think that others are more influenced by media than they are themselves. These same biases may produce self-other asymmetries in beliefs about efficacy of self-regulation and of censorship. Toward a better understanding of the kinds of self-other asymmetries that underlie public support for censorship, the current study evaluates Singaporeans' third-person perceptions of sexual content in films and the effects of those perceptions on support for censorship. In particular, this study seeks to understand the nature and extent of self-other asymmetries of efficacy beliefs and how those perceptions may directly and indirectly influence support for censorship.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "The Need for Censorship", "text": "In order to understand why the public may or may not support censorship and why efficacy beliefs may influence these attitudes, it is useful to explore briefly some of the rationale for media content regulation. Content regulation has many forms, including self- regulation, parental mediation, and government censorship ( Frau-Meigs, 2003). The need for each different form of restriction can depend on characteristics of the offending media content, as well as on community standards. For example, Singapore's Media Development Authority recently loosened restrictions on television broadcasts that contain nudity or explicit violence (AFP, 2010). When Lui Tuck Yew, then Minister for Information, Communications and the Arts, explained the policy shift, he emphasized the roles of self-regulation and parental mediation: \"We decided that we ought to be governed by the principle that you make [content] available in a way where the adult, and especially the parent, will be in a position to exercise greatest control\" (AFP, 2010). In this case, the reassessment of community standards promoted a regulatory scheme that gave individuals more control over their media use. Furthermore, the 2010 report of the Censorship Review Committee specifically recommended new tools to enhance parental mediation, such as Internet filters and simplified content rating schemes, as well as new education programs to improve public media literacy (Goh, 2010). These and other recommendations formed the basis of the policy update. To the extent that self-regulation and parental mediation are effective at protecting vulnerable segments of the population from harmful effects of media, the shift in regulatory policy should satisfy public concerns about exposure to sex and violence in television content. This observation has implications for media literacy education, which can buttress effective self-regulation and parental mediation. A report by the Free Expression Policy Project describes media literacy as an organic alternative to censorship: Rather than resorting to censorship or ratings schemes in response to the presumed influence of violent or otherwise troublesome messages in popular culture, policymakers should commit to making media literacy an essential part of every young person's education. [\u2026] Media literacy is far better than censorship, not only for those concerned about troublesome media messages but for everyone committed to modern education, intellectual freedom, or the healthy development of youth (Heins & Cho, 2003, p. 38). The construct of media literacy has many facets that relate to the media environment, social factors, and individual differences. However, a central feature of media literacy is that it affords individuals control over their media experiences. This assertion frames the overarching research goal of the current study. The third-person effects perspective has informed an important area of research focusing on public concerns about harmful media content and public support for censorship. The following sections describe this perspective, explicate concepts of control and efficacy in the context of media use, and integrate these concepts into a model of third-person effects.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Third-Person Effects Perspective", "text": "Scholarly reviews of the third-person effect often describe it in terms of two components (e.g., Xu & Gonzenbach, 2008). The first component is related to a self-other perceptual asymmetry, which bears on certain cognitive biases, and is the crux of the third-person effect. The second component is related to attitudinal and behavioral outcomes of the self-other perceptual asymmetry. The following sections review literature on third-person perception, relevant cognitive biases that help explain why third- person perception arises, and some of the attitudinal and behavioral consequences of third- person perception.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Third-person perception", "text": "Davison (1983,1996) described the perceptual component of the third-person effect as people's belief that, given exposure to a persuasive message via mass media, its effect on them would be smaller than its effect on other people. A review of the first decade of research on this effect found that this perceptual bias occurs most consistently when the object of a persuasive message is perceived to be undesirable and when the issue is personally important (Perloff, 1993). More recent research has affirmed these findings, and has sought to identify the psychological origin of the third-person effect. Gunther and Storey (2003) describe as a negative influence corollary the tendency of third-person perception to occur when the object of persuasion is undesirable. Other studies have extended the negative influence corollary beyond persuasive messages to include generally \"harmful\" media content, including violent video games (Boyle, McLeod, & Rojas, 2008), idealized body image (Chia, 2009), depictions of homosexuality (Ho, Detenber, Malik, & Neo, 2012), alcohol product placement (Shin & Kim, 2011), reality television shows , and \"sexting\" (Wei & Lo, 2013). By the same corollary, people may believe they are more influenced than others by messages when the content or object of persuasion is desirable. Some studies have documented such an inverse third-person perception of public service announcements (Hoorens & Ruiter, 1996;Innes & Zeitz, 1988;White & Dillon, 2000) and emotional advertisements (Gunther & Thorson, 1992). Observations of third-person perception and its inversion (also called first- person perception) highlight the psychological mechanism that underlies the effect (Andsager & White, 2007). Some additional factors that influence the magnitude of third-person perception include perceived social distance, audience vulnerability, and likelihood of exposure, (Boyle, Schmierbach, & McLeod, 2013;Eveland, Nathanson, Detenber, & McLeod, 1999;. Notably, Eveland et al. (1999) found that perceived others' exposure is an important factor related to perceived influence on others, which suggests that people invoke a \"magic bullet\" theory of communication effects when assessing others' media experiences, and further highlights the psychological basis of the third-person effect.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Sources of perceptual bias", "text": "Researchers have related the self-other asymmetry of third-and first-person perceptions to a number of psychological mechanisms, and commonly point to an optimism bias (Gunther & Storey, 2003;Hoorens & Ruiter, 1996). According optimism bias, people view themselves as less likely to be affected by negative events and more likely to be affected by positive events than are other people, and this effect increases with the magnitude of the negative or positive events (Weinstein, 1980). Other researchers have described third-person perception as a consequence of the fundamental attribution error (e.g., D. M. McLeod, Detenber, & Eveland, 2001), in which people attribute an undesirable outcome to situational factors when it occurs to them and to individual characteristics when it occurs to others (E. E. Jones & Harris, 1967;Ross, 1977). In general, such motivational biases that lead to third- person perceptions are self-serving; people tend to think they are better than average, and denying media influence on the self helps support this positive view of the self (Perloff, 2009). A supplementary argument suggests that the effect arises out of more fundamental cognitive processes. Indeed, people express self-other asymmetries partly because they lack direct access to other's introspections, while having unfettered access to their own. Consequently, people use lay psychology to explain others' thoughts and behaviors, while exempting their own thoughts and behaviors from the same analysis. This psychological process is related to the introspection illusion, in which people view their own introspection as a highly authentic source of information for self-assessment (Pronin, 2008). Furthermore, as na\u00efve realists, people tend to view their own perceptions as corresponding directly to an observable reality, while others' divergent responses to a common stimulus imply others' distorted perception (Gibbon & Durkin, 1995;Pronin, Gilovich, & Ross, 2004). The combined influence of the introspection illusion and na\u00efve realism help explain the cognitive basis of self-other asymmetries in general and, for current purposes, third-person perceptions in particular. Such cognitions may also help explain self-other asymmetries of efficacy beliefs in the context of media effects. The current study assumes that the third-person effect simultaneously reflects both motivational and cognitive orientations; thus, subsequent arguments refer to both motivational and cognitive processes that may precipitate the third- person effect in relation to efficacy beliefs.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Behavioral consequences", "text": "In addition to describing the nature and psychological bases of self-other perceptual asymmetries, scholars have considered their attitudinal and behavioral outcomes (Perloff, 1999;Rojas, 2010;Xu & Gonzenbach, 2008).  describe three categories of behavioral outcomes: promotional behaviors directed at messages with desirable social influences, corrective or educational behaviors directed at messages with ambiguous influences, and restrictive behaviors directed at messages with undesirable social influences. On the subject of restrictive behaviors, a large body of research has found that support for censorship is related to third-person perception of influence of video games (Schmierbach, Boyle, Xu, & McLeod, 2011), television violence (Rojas, Shah, & Faber, 1996), pornography (Gunther, 1995;B. K. Lee & Tamborini, 2005;Rojas et al., 1996;Zhao & Cai, 2008), controversial product advertising (Shah, Faber, & Youn, 1999;Shin & Kim, 2011), and social media (Paradise & Sullivan, 2012), among others. Furthermore, recent research supports the causal relationship between perceived influence and support for censorship, suggesting that the former causes the latter and not the reverse (Tal-Or, Cohen, Tsfati, & Gunther, 2010). The linkage between third-person perceptions and support for censorship resonates with models of sociotropic influence, which address beliefs about social collectives (J. M. McLeod, Sotirovic, & Holbert, 1998). Because information about social conditions is generally overt, at least in perception, while personal experiences are often compartmentalized, \"sociotropic judgments transfer quite easily to political preferences, while personal experiences do not\" (Mutz, 1998, p. 108). Consequently, when people perceive collective problems (e.g., harmful effects of mass media), they are more likely to hold social institutions accountable for resolution (e.g., censorship) than when they perceive these problems to affect them personally.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Control over perceived risk", "text": "In this sociotropic view of censorship, offending media content may constitute a collective risk, as its influence can promote antisocial beliefs and behaviors. Such an evaluation of media influence represents a threat appraisal consistent with protection motivation theory ( Shah et al., 1999). Another key concept that appears in protection motivation theory and also in other models of risk perception-e.g., the extended parallel process model and the psychometric paradigm of risk perception-relates to beliefs about control and efficacy ( Fischhoff et al., 1978;Neuwirth et al., 2000;Rogers, 1975;Witte, 1994). In the context of perceived risk, \"efficacy pertains to the effectiveness, feasibility, and ease with which a recommended response impedes or averts a threat\" (Witte, 1994, p. 114). Conversely, inefficacy corresponds with inability to mitigate a threat, and thus conveys some of the risk associated with the threat. Indeed, people tend to view uncontrollable hazards as posing greater risk (Slovic, 1987). In the context of the current study, the belief that audiences are unable to control how the media influence them-i.e., that they have self- regulatory inefficacy-should prompt the belief that exposure translates to influence. When individuals lack the ability to mitigate risk on their own-for example, when the risk is too large or dispersed-they may seek relief through institutional remedies. Support for such remedies relates to another efficacy belief: institutional efficacy. For example, studies of risks related to crime (Perdomo, 2010), food safety ( Chou & Liou, 2010), and the environment (N. Jones, Clark, & Tripidaki, 2012) suggest that authorities' failure to effectively control a social risk amplifies perceived risk. Toward mitigating perceived social risks, the public should prefer and support efficacious institutional remedies. Similarly, we suggest that exposure to harmful media content may pose a social risk that government institutions can seek to mitigate and whose efficacy in that regard can alleviate relevant public concerns. Thus, to the extent that people view exposure to media as posing a social risk and to the extent that they positively evaluate censorship efficacy for mitigating the risk, they should tend to support censorship.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Self-other asymmetries in efficacy beliefs", "text": "Na\u00efve realists believe that they see the media for what they really are and are able to interpret media content accurately and without bias. Consequently, they believe they are able to assimilate \"positive\" information and filter out \"negative\" information, while others are less capable in that regard. Their self-assessment depends on having access to their own subjective introspections, while other-assessment relies more on intuitive theories of media effects. Na\u00efve realists who assume a \"magic bullet\" theory of media effects are prone not only to believe that others' media exposure is tantamount to others being influenced, but also that others being influenced implies others' inability to mitigate influence. This belief has clear implications for perceived self-regulatory inefficacy, and may precipitate a self-other asymmetry. Shen, Pan, and Sun (2010) demonstrated aspects of this asymmetry, finding that people perceive others to be relatively more susceptible to media influence and less critical in their media use. Susceptibility to influence and uncritical media use imply a lack of cognitive control that may emerge in the form of self-regulatory inefficacy: H1. There is a self-other asymmetry of perceived self-regulatory inefficacy such that others are less efficacious than self. A defining feature of efficacy beliefs is the perception of having personal autonomy over behavioral decisions (Ryan & Connell, 1989). When people believe that their intentions primarily determine their behaviors, they have an internal perceived locus of causality; when people view their behaviors as largely the result of external pressures, they have an external perceived locus of causality (Deci & Ryan, 1985). Consistent with the fundamental attribution error and self-serving bias, people may exaggerate in their own behaviors an internal locus of causality (e.g., that they can control their own media experiences) and downplay an external locus of causality (e.g., that censorship controls their media experiences), while inverting these attributions to explain others' behaviors (see Jellison & Green, 1981). Such biased processing further supports the self-other asymmetry of perceived self-regulatory inefficacy and has additional implications for perceived censorship efficacy. H2. There is a self-other asymmetry of perceived censorship efficacy such that censorship more effectively restricts exposure to harmful media content for others than for self. This argument leads, as well, to the following hypothesis: H3. There is a positive relationship between self-other asymmetry of self-regulatory inefficacy and self-other asymmetry of censorship efficacy.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Self-efficacy in third-person effects", "text": "Earlier, we suggested that efficacy beliefs should be related to support for censorship, but did not make specific claims regarding self-other asymmetries. However, the aim of this study is to examine the extent to which self-other asymmetry of efficacy beliefs may augment a model of third-person effects. At least two studies help guide such examination. In one study, Haridakis and Rubin (2003) were interested in how self-other asymmetries of exposure to news about terrorism and of ability to ignore media bias, among other factors, influence support for stricter measures to combat terrorism. They found that neither self-other asymmetry predicted the outcome variable, but their study suggests the feasibility of incorporating into a third-person effects model self-other asymmetry of efficacy beliefs. In another study, Lee and Tamborini (2005) found that Internet self-efficacy was marginally- significantly related to third-person perception of Internet pornography. As their measure of Internet self-efficacy did not account for other-perception, their finding does not reveal an effect of self-other asymmetries of efficacy beliefs. Nonetheless, their finding suggests that Internet self-efficacy is related to the belief that self is less influenced than others, and may imply that others' Internet inefficacy is related to the belief that others are more influenced than self. The need to test this assertion motivates a third hypothesis: H4. Self-other asymmetry of perceived self-regulatory inefficacy is (a) positively related to self-other asymmetry of perceived influence and (b) indirectly via this path, positively related to support for censorship. A similar prediction applies to perceived censorship efficacy, which highlights the \"social risk\" element that may amplify the need for an effective institutional remedy. That is, beliefs about the capacity of censorship to reduce exposure may have the greatest influence on support for censorship when those beliefs concern others, rather than the self. H5. Self-other asymmetry of perceived censorship efficacy is positively related to support for censorship. Finally, we consider potential relationships between exposure and efficacy. Intuitively, self-regulatory inefficacy should be positively related to exposure, as people who cannot control how media influence them probably also lack self-control to avoid exposure in the first place. That is, others' self-regulatory inefficacy indicates a more general self- inefficacy, which may be related to an inability to limit exposure; thus, the relationship between others' self-regulatory inefficacy and exposure is positive and spurious. However, it may also be that having more exposure is akin to having more practice thinking about media messages. Engagement with certain kinds of media is related to better message assimilation Respondents ranged in age from 21 to 82 (M = 39.76, SD = 14.13; Mdn. = 40). The gender split was roughly equal, with 51.7% of the sample being female. The majority of the sample was Chinese (75.5%), followed by Malay (10.3 %), Indian (9.4%), other (3.5%), and Eurasian (1.2%). Median educational attainment was \"Diploma\" (roughly equivalent to an associate's degree in the U.S.), and median income was in the range of S$4,501 to S$5,500. This demographic profile closely matches official census data. 1 ", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Independent variables", "text": "We measured perceptions of exposure, influence, self-regulatory inefficacy, and censorship efficacy using six items, which had identical wording except for the referent person and referent content. Half of the items referred to \"you\" (i.e., the respondent), and half referred to \"the average Singaporean.\" For each referent person, three items referenced different kinds of sexual content: nudity in movies, portrayals of premarital sex in movies, and portrayals of extramarital sex in movies. We computed each self-and other-perception as the average of the three items. We adapted items from Ho et al. (2012) to measure perceived exposure and influence, and developed measures of self-regulatory efficacy and censorship efficacy in a pilot study (see the appendix for details). ", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Dependent variable", "text": "We measured support for censorship with responses to \"Do you think restrictions on [target content] should be [1 = a lot more liberal to 5 = a lot more strict]\" (M = 3.47, SD = 0.96). The three-item measure, which we adapted from Gunther and Ang (1996) and Ho et al. (2012), had good reliability (\u03b1 = .84).", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Control variables", "text": "Regression analysis showed that three demographic variables-sex, age, and income-were consistently and strongly related to the variables of interest, and especially to support for censorship. We controlled for these variables in our analyses.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Imputation of Missing Values", "text": "There was high missingness on income (24.4%) and others' exposure (three items ranging from 18.9% to 20.2%), which was weakly correlated with being female. In addition, missingness on others' exposure was weakly correlated with age. Otherwise, missingness did not exceed 6.9% on any items, and overall missingness was 5.9%. Little's missing completely at random (MCAR) test was significant (p < .001), which suggests data are not MCAR; thus, we assumed data are missing at random (MAR). We imputed missing values in Mplus using full information maximum likelihood estimation (FIML). This approach is consistent with recommendations of Buhi, Goodson, and Neilands (2008), who report that FIML imputation of MAR data with 25% missingness only slightly biases estimates in regression models and performs as well as multiple imputation.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Testing for self-other asymmetries", "text": "A series of Wald tests in Mplus evaluated the self-other asymmetries that hypotheses 1 and 2 propose. Specifically, the analyses tested the null hypothesis that other-perception minus self-perception is equal to zero, which a significant finding would reject.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Analysis of self-other asymmetries", "text": "The diamond method isolated self-other asymmetries for analysis (Schmierbach, Boyle, & McLeod, 2008;. For each pair of other-and self-perceptions (O and S, respectively), the diamond method calls for three computed variables: Variable 1: For all cases, O + S. Variable 2: For O > S, O -S, else 0. Variable 3: For S > O, S -O, else 0. The first variable (hereafter, \"O + S\") is an additive index that corresponds with perceived total influence, the second variable (hereafter, \"O -S\") is a subtractive index that corresponds with third-person perception, and the third variable (hereafter, \"S -O\") is a subtractive index that corresponds with first-person perception. By controlling for total influence in a regression model, this method better differentiates effects of O -S and S -O asymmetries and is more theoretically consistent with the third-person effects model that other computational approaches ). The diamond method computed three variables for each of the four self-other asymmetries for a total of 12 new variables. Table 1 provides descriptive statistics for and measures of association among these variables, the three demographic control variables, and support for censorship.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Estimating path coefficients", "text": "Endogenous variables included support for censorship and O -S asymmetries for exposure, influence, self-regulatory inefficacy, and censorship efficacy. Exogenous variables included S -O asymmetries and the additive indexes for exposure, influence, self-regulatory inefficacy, and censorship efficacy; and the three demographic control variables. The model estimated covariance among exogenous variables freely. Figure 1 gives an example of how the model controlled for exogenous variables. For the sake of visual simplicity, subsequent figures do not depict control variables. The analysis used 10,000 bootstrap samples for determining significance levels and confidence intervals of indirect effects.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Self-other asymmetries", "text": "Consistent with prior research, respondents perceived that others are more exposed to sexual content in films (M = 2.50, SD = 0.75) than they are (M = 1.94, SD = 0.72; Wald \u03a7 2 = 165.04, p < .001), and that such content has more negative influence on others (M = 3.56, SD = 0.93) than it has on them (M = 3.26, SD = 0.72; Wald \u03a7 2 = 234.68, p < .001). In addition, respondents perceived that others have greater self-regulatory inefficacy (M = 2.81, SD = 1.08) than they do (M = 2.11, SD = 1.14; Wald \u03a7 2 = 191.88, p < .001) and that censorship efficacy is greater for others (M = 3.82, SD = 1.18) than for themselves (M = 3.51, SD = 1.32; Wald \u03a7 2 = 33.21p < .001). These latter two findings support H1 and H2. Table 2 summarizes the evaluations of self-other asymmetries.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Path coefficients", "text": "In a baseline third-person effects model, O -S exposure predicts O -S influence, which predicts support for censorship (Eveland et al., 1999). Current analyses added to this model. O -S self-regulatory inefficacy as an additional predictor of O -S influence and O - S censorship efficacy as an additional predictor of support for censorship. The overall model Results show a positive relationship between O -S self-regulatory inefficacy and O - S censorship efficacy (r = .15, p < .001), which supports H3. Furthermore, O -S self- regulatory inefficacy was positively related to O -S influence (\u03b2 = .13, p < .001) and, indirectly via this path, support for censorship. The indirect path, which is the product of the two direct paths, was significant (\u03b2 = .013, p = .019; 90% CI: .004 .028). These results support H4a and H4b. Finally, censorship efficacy was positively related to support for censorship (\u03b2 = .07, p = .04), which supports H5. For additional reference, Table 3 shows the complete regression models predicting O -S influence and censorship, including the effects of control variables.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Relationship between efficacy and exposure", "text": "Two significant correlations address the research questions: O -S exposure was positively related both to O -S self-regulatory inefficacy (r = .13, p = .002) and O -S censorship efficacy (r = .19, p < .001).", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Discussion", "text": "This study extended research on the third-person effect to include self-other asymmetries of efficacy beliefs. Although the concept of efficacy has appeared in prior third- person effects research, the current study is the first to examine perceived self-other asymmetries of such beliefs and their influence on a third-person effect. First, this study found that respondents rated other Singaporeans as being more exposed to and negatively influenced by sexual content in films than they are themselves. Similar findings appear throughout third-person effects research (e.g., Ho et al., 2012;B. K. Lee & Tamborini, 2005;Zhao & Cai, 2008). Going beyond the typical pattern of third-person perceptions and effects, the current study found that respondents perceive others as having relatively less control over how sexual content in films affects them, that censorship more strongly inhibits others' exposure to such content than it does their own, and that these two perceptions are correlated. These findings are consistent with theorizations of cognitive bias, and supported novel hypotheses regarding self-other asymmetries of efficacy beliefs. Furthermore, the finding that the two asymmetric efficacy beliefs are correlated suggests a common psychological mechanism, which can involve both cognitive and motivational processes. As a consequence of the introspection illusion (Pronin, 2008), people may conclude that their own exposure and responses to \"harmful\" media content are largely the result of intention, while other people fit into a generic model of powerful media effects. Such conclusions would bear on largely cognitive processes. In particular, the relatively large self-other asymmetry of self-regulatory inefficacy may be related to perceptions of others relatively greater gullibility (see ). Yet, it also enhances the ego for people to feel self-determined in their thoughts and actions (Hodgins, Yacko, & Gottlieb, 2006;Muraven, Gagne, & Rosman, 2008). Such a motivational process would incline people to assert their own internal locus of causality. Both cognitive and motivational processes may hinge on beliefs about cognitive complexity and perhaps on beliefs about media literacy, which further research could explore. Second, this study found that self-other asymmetries of efficacy beliefs contributed novel elements to the traditional third-person effects model. Findings suggest that the perceptual component of the third-person effect is related to beliefs that others are relatively more exposed to sexual content in films and less able to control how such content influences them. In other words, other's relative inability to control how media influence them is a significant source of influence regardless of the level of exposure. Findings suggest also that other's relatively greater self-regulatory inefficacy is indirectly related to support for censorship. Regarding censorship efficacy, theory suggests and results support a direct linkage with support for censorship. This finding is rather intuitive, as the belief that censorship is effective is analogous to holding a positive attitude toward censorship. The association between positive attitude and positive preference is one of the best established linkages in psychological research (e.g, Ajzen, 1985Ajzen, , 1991. Thus, the association between censorship efficacy and support for censorship is not theoretically novel; however, it is theoretically useful as a counterpoint to the indirect influence of self-regulatory inefficacy. Considerations of internal locus of causality (in this case, lack thereof) and external locus of causality are related but unique sources of information about the desirability of media content regulation. In order to further evaluate the effects of asymmetric efficacy beliefs, future research should consider how they may influence support for media literacy education, especially as it may reduce others' relative self-regulatory inefficacy. People who have high media literacy have greater efficacy to identify credibility, bias, believability, and similar characteristics in media messages (Claussen, 2004), and by identifying these characteristics, they can assert greater control over attitudinal and behavioral outcomes of their media use (B. K. Lee & Tamborini, 2005). Thus, media literacy education seems a clear alternative to censorship, and support for such education may be informative to study as a third-person effect. Finally, two research questions considered linkages between perceived exposure and efficacy beliefs. Regarding the first research question, findings show a positive relationship between self-other asymmetries of self-regulatory inefficacy and exposure. This finding suggests that self-regulatory inefficacy and exposure are related to a fundamental self- inefficacy, which is greater for others than for self. Also, this finding rejects the alternative explanation that people who have more exposure to media also have more practice, and are thus more able to control how it affects them. This alternative explanation may be valid in the context of certain kinds of informational media content, but it fails to account for perceptions of undesirable media content or, at least, sexual content in films. Regarding the second research question, findings show a positive relationship between self-other asymmetries of censorship efficacy and exposure. This finding is counterintuitive on first glance, as more effective censorship should result in less exposure. We can explain this finding if we consider audience intentions: exposure to sexual content in films is often intentional and, importantly, not incidental. Film rating schemes give audiences information about the nature of a film's content. Thus, when people have a high level of exposure to sexual content in films, it is likely because they are actively seeking it out. Whereas, people who have low exposure are likely avoiding such content intentionally. For the latter group of people, censorship has little effect, as their exposure is already low. Thus, the effect of censorship on exposure emerges only with respect to the former group of people. Examination of the current operationalization of censorship efficacy further supports this explanation. We measured censorship efficacy as respondents' agreement with the statement, \"Without censorship, the average Singaporean would see more [sexual content in films].\" Thus, censorship efficacy reflects the belief that other people who have the greatest exposure would seek even more sexual content in films if it were not for censorship. This explanation suggests that the relationship between self-other asymmetries of censorship efficacy and exposure is circular: while effective censorship should reduce exposure to undesirable media content, there must first be exposure for censorship to be effective. A future longitudinal study could clarify this relationship.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Limitations and Conclusion", "text": "We should acknowledge two limitations. First, the relatively small proportion of explained variance in support for censorship limits statistical inference. Although the current model explains nearly one-fourth of the variance in support for censorship, much of the explanatory power was linked to control variables and not variables of interest. Further, the large residual variance suggests room for model adjustment or expansion. For example, this study did not account for trust in social institutions (e.g., government), which may have improved the model's explanatory power and helped clarify for whom perceived censorship efficacy matters. Indeed, Singaporeans place considerable trust in the government to act in their best interest, and their support for censorship may hinge significantly on this trust. Additional explained variance might emerge in cross-national studies that make comparisons among different political systems, media environments, and regulatory schemes. Second, the current study is somewhat limited by the type of media content and behavioral outcomes it considered. Even in the complete absence of government regulation of sexual content in films, community standards might remain as a powerful, albeit informal, regulatory framework. In Singapore, public screenings of new films would likely continue to reflect conservative content preferences. To the extent that the public implicitly understands the role of community standards in local media programming, they may increasingly view government regulation as unnecessary. Media contexts that are less a part of the shared public experience-for example, Internet pornography-may prove to be more fertile grounds for testing the effects of third-person perception of censorship efficacy and its interaction with other third-person perceptual constructs. Despite these limitations, the current study elaborates the cognitive processes that underlie public support for censorship. As prior research of third-person effects has shown, support for censorship is related to the perception that the average person has a relatively high risk of suffering adverse effects of \"harmful\" media. However, such an appraisal of threat will only partly motivate support for censorship. Beliefs about efficacy may further undergird support for censorship, which the current study has documented. The social context of this study has additional theoretical and practical implications: in Singapore censorship is the norm, which Singapore public opinion generally favors (Gunther & Ang, 1996;Ho et al., 2012). Public support for censorship is likely related to favorable opinion of the government, which further asserts the need to study institutional trust in future research. At the same time, there appears to be a gradual shift in government policy to put content control in the hands of capable media users. This shift reflects careful attention to public opinion of offensive content, and accompanies a call for new measures to promote self-regulation and parental mediation (Goh, 2010). The success of these measures may diminish the perceived need for censorship and engender user-focused content regulation that the public supports. Thus, this study not only contributes to the fields of media psychology and public opinion research, but it may also help distinguish the relative utilities of self-regulation and government censorship in maintaining reasonable local standards for media content. Note. The diagonal (shaded) contains variances. Covariances are above the diagonal (shaded) and correlations are below the diagonal. For correlation coefficients in bold, p < .05.   Figure 1. Two independent variables (IV1, IV2) predict a dependent variable (DV), controlling for a covariate (COV). The relationships of interest are related to O -S asymmetries. Path \u03b2 1 controls for, in addition to the covariate, the additive indices and S -O asymmetries of IV1. Path \u03b2 2 controls for, in addition to the covariate, the additive index and S -O asymmetry. Finally, this model isolates the O -S asymmetry of IV1 and IV2 by controlling for the covariate and their respective additive indices and S -O asymmetries. This isolation allows correlation of IV1 and IV2.  Figure 2. Support for censorship is related to O -S exposure, O -S influence, O -S self- regulatory inefficacy and O -S censorship efficacy and support for censorship both directly and indirectly. This model does not depict control variables, which convey a large portion of explained variance. *p < .05. **p < .01. ***p < .001.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Appendix", "text": "We developed six-item indexes for self-regulatory inefficacy and perceived censorship efficacy in a pilot study, which we administered to a convenience sample of 44 undergraduate students. Responses to open-ended questions suggested that respondents understood that the notion of \"control over effects\" refers to cognitive processes that can mitigate the effects of media on thoughts, feelings, and behaviors, and that mechanisms other than censorship-such as self-regulation-can effectively limit exposure The latter finding is important because, given sufficiently effective alternatives, censorship is less impactful. Exploratory factor analysis with maximum likelihood extraction and oblique rotation resulted in two factors that explained 75% of the variance in the 12 items and had good simple structure (i.e., each item had a strong factor loading [\u03bb > .6] on exactly one factor). Common factor analysis within each six-item index failed to converge; however, principal components analysis revealed simple structure that distinguished between perceptions of self and others. We submitted the six-item indices from the main study-each split into two sets of three items for perceptions of self and others-to confirmatory factor analysis in Mplus, using the default maximum likelihood estimator. The four-factor model had good fit per Hu and Bentler's (1999) recommendations [\u03c7 2 (48) = 195.36, p < .001; CFI = .98; RMSEA = .06 (90% CI: .05, .06); SRMR = .02] , and standardized factor loadings all exceeded .85.", "title": "Title Efficacy beliefs in third-person effects", "file_name": "Rosenthal et al. - 2018 - Efficacy Beliefs in Third-Person Effects.pdf"}
{"section": "Abstract", "text": "Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of thirteen classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, ten effects replicated consistently. One effect-imagined contact reducing prejudice-showed weak support for replicability. And two effects-flag priming influencing conservatism and currency priming influencing system justification-did not replicate. We compared whether the conditions such as lab versus online or U.S. versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect. Word Count = 121 words Many Labs 3 Investigating variation in replicability: A \"Many Labs\" Replication Project Replication is a central tenet of science; its purpose is to confirm the accuracy of empirical findings, clarify the conditions under which an effect can be observed, and estimate the true effect size (Brandt et al., 2013; Open Science Collaboration, 2012, 2013). Successful replication of an experiment requires the recreation of the essential conditions of the initial experiment. This is often easier said than done. There may be an enormous number of variables influencing experimental results, and yet only a few tested. In the behavioral sciences, many effects have been observed in one cultural context, but not observed in others. Likewise, individuals within the same society, or even the same individual at different times (Bodenhausen, 1990), may differ in ways that moderate any particular result. Direct replication is infrequent, resulting in a published literature that sustains spurious findings (Ioannidis, 2005) and a lack of identification of the eliciting conditions for an effect. While there are good epistemological reasons for assuming that observed phenomena generalize across individuals and contexts in the absence of contrary evidence, the failure to directly replicate findings is problematic for theoretical and practical reasons. Failure to identify moderators and boundary conditions of an effect may result in overly broad generalizations of true effects across situations (Cesario, 2013) or across individuals (Henrich, Heine, & Norenzayan, 2010). Similarly, overgeneralization may lead observations made under laboratory observations to be inappropriately extended to ecological contexts that differ in important ways (Henry, MacLeod, Phillips, & Crawford, 2004). Practically, attempts to closely replicate research findings can reveal important differences in what is considered a direct replication (Schimdt, 2009), thus leading to refinements of the initial theory (e.g., Aronson, 1992, Greenwald et al., 1986). Close replication can also lead to Many Labs 4 the clarification of tacit methodological knowledge that is necessary to elicit the effect of interest (Collins, 1974). Overview of the Present Research Little attempt has been made to assess the variation in replicability of findings across samples and research contexts. This project examines the variation in replicability of thirteen classic and contemporary psychological effects across 36 samples and settings. Some of the selected effects are known to be highly replicable; for others, replicability is unknown. Some may depend on social context or participant sample, others may not. We bundled the selected studies together into a brief, easy-to-administer experiment that was delivered to each participating sample through a single infrastructure (http://projectimplicit.net/). There are many factors that can influence the replicability of an effect such as sample, setting, statistical power, and procedural variations. The present design standardizes procedural characteristics and ensures appropriate statistical power in order to examine the effects of sample and setting on replicability. At one extreme, sample and situational characteristics might have little effect on the tested effects-variation in effect magnitudes may not exceed expected random error. At the other extreme, effects might be highly contextualized-for example, replicating only with sample and situational characteristics that are highly consistent with the original circumstances. The primary contribution of this investigation is to establish a paradigm for testing replicability across samples and settings and provide a rich data set that allows the determinants of replicability to be explored. A secondary purpose is to demonstrate support for replicability for the thirteen chosen effects. Ideally, the results will stimulate theoretical developments about the conditions under which replication will be robust to the inevitable variation in circumstances of data collection. Many Labs 5 Method Researcher Recruitment and Data Collection Sites Project leads posted a call for collaborators to the online forum of the Open Science Collaboration on February 21, 2013 and to the SPSP Discussion List on July 13, 2013. Other colleagues were contacted personally. For inclusion, each replication team had to: (1) follow local ethical procedures, (2) administer the protocol as specified, (3) collect data from at least 80 participants, (4) post a video simulation of the setting and administration procedure, and (5) document key features of recruiting, sample, and any changes to the standard protocol. In total, there were 36 samples and settings that collected data from a total of 6,344 participants (27 data collections in a laboratory and 9 conducted online; 25 from the U.S., 11 from other countries; see Table 1 for a brief description of sites and Table S1 1 for a full descriptions of sites, site characteristics, and participant characteristics by site). Selection of Replication Studies Twelve studies producing thirteen effects were chosen based on the following criteria: 1 Suitability for online presentation. Our primary concern was to give each study a \"fair\" replication that was true to the original design. By administering the study through a web browser, we were able to ensure procedural consistency across sites. 2 Length of study. We selected studies that could be administered quickly so that we could examine many of them in a single study session. 3 Simple Design: With the exception of one correlation study, we selected studies that featured a simple, two-condition design.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Investigating variation in replicability: A \"Many Labs\" Replication Project", "text": "Replication is a central tenet of science; its purpose is to confirm the accuracy of empirical findings, clarify the conditions under which an effect can be observed, and estimate the true effect size ( Brandt et al., 2013;Open Science Collaboration, 2012, 2013. Successful replication of an experiment requires the recreation of the essential conditions of the initial experiment. This is often easier said than done. There may be an enormous number of variables influencing experimental results, and yet only a few tested. In the behavioral sciences, many effects have been observed in one cultural context, but not observed in others. Likewise, individuals within the same society, or even the same individual at different times (Bodenhausen, 1990), may differ in ways that moderate any particular result. Direct replication is infrequent, resulting in a published literature that sustains spurious findings ( Ioannidis, 2005) and a lack of identification of the eliciting conditions for an effect. While there are good epistemological reasons for assuming that observed phenomena generalize across individuals and contexts in the absence of contrary evidence, the failure to directly replicate findings is problematic for theoretical and practical reasons. Failure to identify moderators and boundary conditions of an effect may result in overly broad generalizations of true effects across situations (Cesario, 2013) or across individuals (Henrich, Heine, & Norenzayan, 2010). Similarly, overgeneralization may lead observations made under laboratory observations to be inappropriately extended to ecological contexts that differ in important ways (Henry, MacLeod, Phillips, & Crawford, 2004). Practically, attempts to closely replicate research findings can reveal important differences in what is considered a direct replication (Schimdt, 2009), thus leading to refinements of the initial theory (e.g., Aronson, 1992, Greenwald et al., 1986. Close replication can also lead to Many Labs 4 the clarification of tacit methodological knowledge that is necessary to elicit the effect of interest (Collins, 1974).", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Overview of the Present Research", "text": "Little attempt has been made to assess the variation in replicability of findings across samples and research contexts. This project examines the variation in replicability of thirteen classic and contemporary psychological effects across 36 samples and settings. Some of the selected effects are known to be highly replicable; for others, replicability is unknown. Some may depend on social context or participant sample, others may not. We bundled the selected studies together into a brief, easy-to-administer experiment that was delivered to each participating sample through a single infrastructure (http://projectimplicit.net/). There are many factors that can influence the replicability of an effect such as sample, setting, statistical power, and procedural variations. The present design standardizes procedural characteristics and ensures appropriate statistical power in order to examine the effects of sample and setting on replicability. At one extreme, sample and situational characteristics might have little effect on the tested effects -variation in effect magnitudes may not exceed expected random error. At the other extreme, effects might be highly contextualized -for example, replicating only with sample and situational characteristics that are highly consistent with the original circumstances. The primary contribution of this investigation is to establish a paradigm for testing replicability across samples and settings and provide a rich data set that allows the determinants of replicability to be explored. A secondary purpose is to demonstrate support for replicability for the thirteen chosen effects. Ideally, the results will stimulate theoretical developments about the conditions under which replication will be robust to the inevitable variation in circumstances of data collection.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Researcher Recruitment and Data Collection Sites", "text": "Project leads posted a call for collaborators to the online forum of the Open Science Collaboration on February 21, 2013 and to the SPSP Discussion List on July 13, 2013. Other colleagues were contacted personally. For inclusion, each replication team had to: (1) follow local ethical procedures, (2) administer the protocol as specified, (3) collect data from at least 80 participants, (4) post a video simulation of the setting and administration procedure, and (5) document key features of recruiting, sample, and any changes to the standard protocol. In total, there were 36 samples and settings that collected data from a total of 6,344 participants (27 data collections in a laboratory and 9 conducted online; 25 from the U.S., 11 from other countries; see Table 1 for a brief description of sites and Table S1 1 for a full descriptions of sites, site characteristics, and participant characteristics by site).", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Selection of Replication Studies", "text": "Twelve studies producing thirteen effects were chosen based on the following criteria: 1 Suitability for online presentation. Our primary concern was to give each study a \"fair\" replication that was true to the original design. By administering the study through a web browser, we were able to ensure procedural consistency across sites. 2 Length of study. We selected studies that could be administered quickly so that we could examine many of them in a single study session. 3 Simple Design: With the exception of one correlation study, we selected studies that featured a simple, two-condition design. 1 Table names that begin with the prefix \"S\" (e.g., Table S1) refer to tables that can be found in the supplementary materials. Tables with no prefix are in this manuscript.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Many Labs 6", "text": "4 Diversity of effects. We sought to diversify the sample of effects by topic, time period of original investigation, and differing levels of certainty and existing impact. Justification for study inclusion is described in the registered proposal (http://osf.io/project/aBEsQ/).", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "The Replication Studies", "text": "All replication studies were translated into the dominant language of the country of data collection (N = 7 languages total; 3/6 translations from English were back-translated). Next, we provide a brief description of each experiment, original finding, and known differences between original and replication studies. Most original studies were conducted with paper and pencil, all replications were conducted via computer. Exact wording for each study, including a link to the study, can be found in the supplementary materials. The relevant findings from the original studies can be found in the original proposal. 1 Sunk costs (Oppenheimer, Meyvis, & Davidenko, 2009). Sunk costs are those that have already been incurred and cannot be recovered (Knox & Inkster, 1968). adapted from Thaler, 1985) asked participants to imagine that they have tickets to see their favorite football team play an important game, but that it is freezing cold on the day of the game. Participants rated their likelihood of attending the game on a 9-point scale (1 = definitely stay at home, 9 = definitely go to the game). Participants were marginally more likely to go to the game if they had paid for the ticket than if the ticket had been free. 2 Gain versus loss framing (Tversky & Kahneman, 1981). The original research showed that changing the focus from losses to gains decreases participants' willingness to take risks - i.e., gamble to get a better outcome rather than take a guaranteed result. Participants imagined that the U.S. was preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Participants were then asked to select a course of action to Many Labs 7 combat the disease from logically identical sets of alternatives framed in terms of gains as follows: Program A will save 200 people [400 people will die], or Program B which has a 1/3 probability that 600 people will be saved [nobody will die] and 2/3 probability that no people will be saved [600 people will die]. In the \"gain\" framing condition, participants are more likely to adopt Program A, while this effect reverses in the loss framing condition. The replication replaced the phrase \"the United States\" with the country of data collection, and the word \"Asian\" was omitted from \"an unusual Asian disease\". 3 Anchoring ( Jacowitz & Kahneman, 1995). Jacowitz and Kahneman (1995) presented a number of scenarios in which participants estimated size or distance after first receiving a number that was clearly too large or too small. In the original study, participants answered 3 questions about each of 15 topics for which they estimated a quantity. First, they indicated if the quantity was greater or less than an anchor value. Second, they estimated the quantity. Third, they indicated their confidence in their estimate. The original number served as an anchor, biasing estimates to be closer to it. For the purposes of the replication we provided anchoring information before asking just for the estimated quantity for four of the topics from the original study -distance from San Francisco to New York City, population of Chicago, height of Mt. Everest, and babies born per day in the U.S. For countries that use the metric system, we converted anchors to metric units and rounded them.  (Oppenheimer & Monin, 2009). Oppenheimer and Monin (2009) investigated whether the rarity of an independent, chance observation influenced beliefs about what occurred before that event. Participants imagined that they saw a man rolling dice in a casino. In one condition, participants imagined witnessing three dice being rolled and all came up 6's. In a second condition two came up 6's and one came up 3. In a Many Labs 8 third condition, two dice were rolled and both came up 6's. All participants then estimated, in an open-ended format, how many times the man had rolled the dice before they entered the room to watch him. Participants estimated that the man rolled dice more times when they had seen him roll three 6's than when they had seem him roll two 6's or two 6's and a 3. For the replication, the condition in which the man rolls two 6's was removed leaving two conditions. 5 Low-vs.-high category scales (Schwarz, Hippler, Deutsch, & Strack, 1985). Schwarz and colleagues (1985) demonstrated that people infer from response options what are low and high frequencies of a behavior, and self-assess accordingly. In the original demonstration, participants were asked how much TV they watch daily on a low-frequency scale ranging from \"up to half an hour\" to \"more than two and a half hours\", or a high-frequency scale ranging from \"up to two and a half hours\" to \"more than four and a half hours\". In the low- frequency condition, fewer participants reported watching TV for more than two and a half hours than in the high-frequency condition. 6 Norm of reciprocity (Hyman & Sheatsley, 1950). When confronted with a decision about allowing or denying the same behavior to an ingroup and outgroup, people may feel an obligation to reciprocity, or consistency in their evaluation of the behaviors (Hyman & Sheatsley, 1950). In the original study, American participants answered two questions: whether communist countries should allow American reporters in and allow them to report the news back to American papers and whether America should allow communist reporters into the United States and allow them to report back to their papers. Participants reported more support for allowing communist reporters into America when that question was asked after the question about allowing American reporters into the communist countries. In the Many Labs 9 replication, we changed the question slightly to ensure the \"other country\" was a suitable, modern target (North Korea). For international replication, the target country was determined by the researcher heading that replication to ensure suitability (see supplementary materials). 7 Allowed/Forbidden (Rugg, 1941). Question phrasing can influence responses. Rugg (1941) found that respondents were less likely to endorse forbidding speeches against democracy than they were to not endorse allowing speeches against democracy. Respondents in the United States were asked, in one condition, if the U.S. should allow speeches against democracy or, in another condition, whether the U.S. should forbid speeches against democracy. 62 percent of participants indicated \"No\" when asked if speeches against democracy should be allowed, but only 46 percent indicated \"Yes\" when asked if these speeches should be forbidden. In the replication, the words \"The United States\" were replaced with the name of the country the study was administered in. 8 Quote Attribution (Lorge & Curtis, 1936). The source of information has a great impact on how that information is perceived and evaluated. Lorge and Curtis (1936) examined how an identical quote would be perceived if it was attributed to a liked or disliked individual. Participants were asked to rate their agreement with a list of quotations. The quotation of interest was, \"I hold it that a little rebellion, now and then, is a good thing, and as necessary in the political world as storms are in the physical world.\" In one condition the quote was attributed to Thomas Jefferson, a liked individual, and in the other it was attributed to Vladimir Lenin, a disliked individual. More agreement was observed when the quote was attributed to Jefferson than Lenin (reported in Moskowitz, 2004). In the replication, we used Many Labs 10 a quote attributed to either George Washington (liked individual) or Osama Bin Laden (disliked individual). with four photos and asked to estimate the time of day at which they were taken. In the flag- prime condition, the American flag appeared in two of these photos. In the control condition, the same photos were presented without flags. Following the manipulation, participants completed an 8-item questionnaire assessing views toward various political issues (e.g., abortion, gun control, affirmative action). Participants in the flag-primed condition indicated significantly more conservative positions than those in the control condition. The priming stimuli used to replicate this finding were obtained from the authors and identical to those used in the original study. Because it was impractical to edit the images with unique national flags, the American flag was always used as a prime. As a consequence, the replications in the United States were the only ones considered as direct replications. For international replications, the survey questions were adapted slightly to ensure they were appropriate for the political climate of the country, as judged by the researcher heading that particular replication (see supplementary materials). Further, the original authors suggested possible moderators that they have considered since publication of the original study. We included three items at the very end of the replication study to test these moderators: (1) How much do you identify with being American? (1, not at all -11, very much), (2) To what extent do you think the typical American is a Republican or Democrat? (1, Democrat -7,", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Many Labs 11", "text": "Republican), (3) To what extent do you think the typical American is conservative or liberal? (1, Liberal -7, Conservative). 10 Currency priming (Caruso, Vohs, Baxter, & Waytz, 2013). Money is a powerful symbol. Caruso et al. (2013) provide evidence that merely exposing participants to money increases their endorsement of the current social system. Participants were first presented with demographic questions, with the background of the page manipulated between subjects. In one condition the background showed a faint picture of U.S. $100 bills; in the other condition the background was a blurred, unidentifiable version of the same picture. Next, participants completed an 8-question \"system justification scale\" (Kay & Jost, 2003). Participants in the money-prime condition scored higher on the system justification scale than those in the control condition. The authors provided the original materials allowing us to construct a near identical replication for U.S. participants. However, the stimuli were modified for international replications in two ways: First, the U.S. dollar was usually replaced with the relevant country's currency (see supplementary materials); Second, the system-justification questions were adapted to reflect the name of the relevant country.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Imagined contact (Husnu & Crisp, 2010; Study 1). Recent evidence suggests that merely", "text": "imagining contact with members of ethnic outgroups is sufficient to reduce prejudice toward those groups (Turner, Crisp, & Lambert, 2007). In Husnu and Crisp (2010), British non- Muslim participants were assigned to either imagine interacting with a British Muslim stranger or to imagine that they were walking outdoors (control condition). Participants imagined the scene for one minute, and then described their thoughts for an additional minute before indicating their interest and willingness to interact with British Muslims on a four item scale. Participants in the \"imagined contact\" group had significantly higher contact Many Labs 12 intentions than participants in the control group. In the replication, the word \"British\" was removed from all references to \"British Muslims\". Additionally, for the predominately Muslim sample from Turkey the items were adapted so Christians were the outgroup target. 12 Sex differences in implicit math attitudes (Nosek, Banaji, & Greenwald, 2002). As a possible account for the sex gap in participation in science and math, Nosek and colleagues (2002) found that women had more negative implicit attitudes toward math compared to arts than men did in two studies of Yale undergraduates. Participants completed four Implicit Association Tests (IATs) in random order, one of which measured associations of math and arts with positivity and negativity. The replication simplified the design for length to be just a single IAT. 13 Implicit math attitudes relations with self-reported attitudes (Nosek et al., 2002). In the same study as Effect 12, self-reported math attitudes were measured with a composite of feeling thermometers and semantic differential ratings, and the composite was positively related with the implicit measure. The replication used a subset of the explicit items (see supplementary materials).", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Procedure", "text": "The experiments were implemented on the Project Implicit infrastructure and all data were automatically recorded in a central database with a code identifying the sample source. After a paragraph of introduction, the studies were presented in a randomized order, except that the math IAT and associated explicit measures were always the final study. After the studies, participants completed an instructional manipulation check (IMC; ), a short demographic questionnaire, and then the moderator measures for flag priming. See Table S1 for IMC and summary demographic information by site. The IMC was not analyzed further for this Many Labs 13 report. Each replication team had a private link for their participants, and they coordinated their own data collection. Experimenters in laboratory studies were not aware of participant condition for each task, and did not interact with participants during data collection unless participants had questions. Investigators who led replications at specific sites completed a questionnaire about the experimental setting (responses summarized in Table S1), and details and videos of each setting along with the actual materials, links to run the study, supplemental tables, datasets, and original proposal are available at http://osf.io/project/WX7Ck/.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Confirmatory Analysis Plan", "text": "Prior to data collection we specified a confirmatory analysis plan. All confirmatory analyses are reported either in text or in supplementary materials. A few of the tasks produced highly erratic distributions (particularly anchoring) requiring revisions to those analysis plans. A summary of differences between the original plans and actual analysis is reported in the supplementary materials.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Summary Results", "text": "Figure 1 presents an aggregate summary of replications of the thirteen effects, presenting each of the four anchoring effects separately. Table 2 presents the original effect size, median effect size, weighted and unweighted effect size and 99% confidence intervals, and proportion of samples that rejected the null hypothesis in the expected and unexpected direction. In the aggregate, 10 of the 13 studies replicated the original results with varying distance from the original effect size. One study, imagined contact, showed a significant effect in the expected direction in just 4 of the 36 samples (and once in the wrong direction), but the confidence intervals for the aggregate effect size suggest that it is slightly different than zero. Two studies -flag priming and currency priming -did Many Labs 14 not replicate the original effects. Each of these had just one p-value < .05 and it was in the wrong direction for flag priming. The aggregate effect size was near zero whether using the median, weighted mean, or unweighted mean. All confidence intervals included zero. Figure 1 presents all 36 samples for flag priming, but only U.S. data collections were counted for the confirmatory analysis (see Table 2). International samples also did not show a flag priming effect (weighted mean = .03, 99% CI [-. 04, .10]). To rule out the possibility that the priming effects were contaminated by the contents of other experimental materials, we reexamined only those participants who completed these tasks first. Again, there was no effect (Flag Priming: t(431) = 0.33, p = .75; Currency Priming: t(605) = -0.56, p = .57). 2 When an effect size for the original study could be calculated, it is presented as an \"X\" in Figure 1. For three effects (contact, flag priming, and currency priming), the original effect is larger than for any sample in the present study, with the observed median or mean effect at or below the lower bound of the 95% confidence interval for the original effect. 3 Though the sex difference in implicit math attitudes effect was within the 95% confidence interval of the original result, the replication estimate combined with another large-scale replication (Nosek & Smyth, 2011) suggests that the original effect was an overestimate. <.001) suggests that very little in the variability of effect sizes can be attributed to the samples, and substantial variability is attributable to the effect under investigation. To illustrate, Figure 2 shows the same data as Figure 1 organized by sample rather than by effect. There is almost no variation in the average effect size across samples.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Variation Across Samples and Settings", "text": "However, it is possible that particular samples would elicit larger magnitudes for some effects and smaller magnitudes for others. That might be missed by the aggregate analyses. Table 3 presents tests of whether the heterogeneity of effect sizes for each effect exceeds what is expected by measurement error. Cochran's Q and I^2 statistics revealed that heterogeneity of effect sizes was largely observed among the very large effects -anchoring, allowed-forbidden, and relations between implicit and explicit attitudes. Only one other effect -quote attribution -showed substantial heterogeneity. This appears to be partly attributable to this effect occurring more strongly in U.S. samples and to a lesser degree in international samples. To test for moderation by key characteristics of the setting, we conducted a condition X country (US or other) X location (lab or online) ANOVA for each effect. Table 3 presents the essential condition X country and condition X location effects. Full model results are available in supplementary materials. A total of 10 of the 32 moderation tests were significant, and seven of those were among the largest effects -anchoring and allowed-forbidden. Even including those, none of the moderation effect sizes exceeded a partial eta-squared of .022. The heterogeneity in anchoring effects may be attributable to differences in knowledge of the height of Mt Everest, distance to NYC, or population of Chicago between the samples. Overall, whether the sample was collected in the U.S. or elsewhere, or whether data collection occurred on-line or in the laboratory, had little systematic effect on the observed results.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Many Labs 16", "text": "Additional possible moderators of the flag priming effect were suggested by the original authors. On the U.S. participants only (N~4670), with five hierarchical regression models, we tested whether the items moderated the effect of the manipulation. They did not (p's = .48, .80, .62, .07, .05). Details available in the online supplement.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Discussion", "text": "A large scale replication with 36 samples successfully replicated eleven of thirteen classic and contemporary effects in psychological science, some of which are well-known to be robust, and others that have been replicated infrequently or not at all. The original studies produced underestimates of some effects (e.g., anchoring-and-adjustment and allowed versus forbidden message framing), and overestimates of other effects (e.g., imagined contact producing willingness to interact with outgroups in the future). Two effects -flag priming influencing conservatism and currency priming influencing system justification -did not replicate. A primary goal of this investigation was to examine the heterogeneity of effect sizes by the wide variety of samples and settings, and to provide an example of a paradigm for testing such variation. Some studies were conducted on-line, others in the laboratory. Some studies were conducted in the United States, others elsewhere. And, a wide variety of educational institutions took part. Surprisingly, these factors did not produce highly heterogeneous effect sizes. Intraclass correlations suggested that most of the variation in effects was due to the effect under investigation and almost none to the particular sample used. Focused tests of moderating influences elicited sporadic and small effects of the setting, while tests of heterogeneity suggested that most of the variation in effects is attributable to measurement error. Further, heterogeneity was mostly restricted to the largest effects in the sample -counter to an intuition that small effects would be the most likely to be variable across sample and setting. Further, the lack of heterogeneity is particularly Many Labs 17 interesting considering that there is substantial interest and commentary about the contingency of effects on our two moderators, lab versus online (Gosling, Vazire, Srivastava & John, 2004;Paolacci, Chandler & Ipeirotis, 2010), and cultural variation across nations ( Henrich et al., 2010). All told, the main conclusion from this small sample of studies is that, to predict effect size, it is much more important to know what effect is being studied than to know the sample or setting in which it is being studied. The key virtue of the present investigation is that the study procedure was highly standardized across data collection settings, and samples were 80 participants or larger, ensuring appropriate power for detecting the effects under investigation. This minimized the likelihood that factors other than sample and setting contributed to systematic variation in effects. At the same time, this conclusion is surely constrained by the small, non-random sample of studies represented here. Additionally, the replication sites included in this project cannot capture all possible cultural variation, and most societies sampled were relatively Western, Educated, Industrialized, Rich, and Democratic (WEIRD; Henrich et al., 2010). Nonetheless, the present investigation suggests that we should not necessarily assume that there are differences between samples; indeed, even when moderation was observed in this sample, the effects were still quite robust in each setting. The present investigation provides a summary analysis of a very large, rich dataset. This dataset will be useful for additional exploratory analysis about replicability in general, and these effects in particular. The data are available for download at the Open Science Framework (http://osf.io/project/WX7Ck/).", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Conclusion", "text": "This investigation offered novel insights into variation in the replicability of psychological effects, and specific information about the replicability of 13 effects. This methodology - crowdsourcing dozens of laboratories running an identical procedure -can be adapted for a variety of investigations. It allows for increased confidence in the existence of an effect and for the investigation of an effect's dependence on the particular circumstances of data collection (Open Science Collaboration, 2013). Further, a consortium of laboratories could provide mutual support for each other by conducting similar large-scale investigations on original research questions, not just replications. Thus, collective effort could accelerate the identification and verification of extant and novel psychological effects.  Notes: All effect sizes (ES) presented in Cohen's d units. Weighted statistics are computed on the whole aggregated dataset (N>6000); Unweighted statistics are computed on the disaggregated dataset (N=36). 95% CI's for original effect sizes used cell sample sizes when available and assumed equal distribution across conditions when not available. The original anchoring article did not provide sufficient information to calculate effect sizes for individual scenarios, therefore an overall effect size is reported. The Anchoring original effect size is a mean point-biserial correlation computed across 15 different questions in a test-retest design, whereas the present replication adopted a between-subjects design with random assignments. One sample was removed from sex difference and relations between implicit and explicit math attitudes because of a systemic error in that laboratory's recording of reaction times. Flag priming includes only U.S. samples. Confidence intervals around the unweighted mean are based on the central normal distribution. Confidence intervals around the weighted effect size are based on non-central distributions. Notes: Tasks ordered from largest to smallest observed effect size (see Table 2). Heterogeneity tests conducted with R-package metafor. REML was used for estimation for all tests. One sample was removed from sex difference and relations between implicit and explicit math attitudes because of a systemic error in that laboratory's recording of reaction times. * Moderator statistics are F value of the interaction of condition and the moderator from an ANOVA with condition, country, and location as independent variables with the exception of Relations between impl. and expl. math attitudes for is reported the F value associated with the change in R squared after the product term between the independent variable and the moderator is added in a hierarchical linear regression model. Details of all analyses are available in the supplement.", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "Figure 1. Replication Results Organized By Effect", "text": "Notes: \"X\" indicates the effect size obtained in the original study. Large circles represent the aggregate effect size obtained across all participants. Error bars represent 99% noncentral confidence intervals around the effects. Small circles represent the effect sizes obtained within each site (black and white circles for US and international replications, respectively).", "title": "Investigating Variation in Replicability", "file_name": "Klein et al. - 2014 - Investigating Variation in Replicability A \u201cMany .pdf"}
{"section": "| Terms and Abbreviations", "text": "BD2K-Big Data 2 Knowledge, is a trans-NIH initiative established to enable biomedical research as a digital research enterprise, to facilitate discovery and support new knowledge, and to maximise community engagement. DOI-Digital Object Identifier; a code used to permanently and stably identify (usually digital) objects. DOIs provide a standard mechanism for retrieval of metadata about the object, and generally a means to access the data object itself. FAIR-Findable, Accessible, Interoperable, Reusable. FORCE11-The Future of Research Communications and e-Scholarship; a community of scholars, librarians, archivists, publishers and research funders that has arisen organically to help facilitate the change toward improved knowledge creation and sharing, initiated in 2011. Interoperability-the ability of data or tools from non-cooperating resources to integrate or work together with minimal effort. JDDCP-Joint Declaration of Data Citation Principles; Acknowledging data as a first-class research output, and to support good research practices around data re-use, JDDCP proposes a set of guiding principles for citation of data within scholarly literature, another dataset, or any other research object. RDF-Resource Description Framework; a globally-accepted framework for data and knowledge representation that is intended to be read and interpreted by machines. done automatically to save time and avoid copy/paste errors? Does the researcher have permission to use the data from these third-party researchers, under what license conditions, and who should be cited if a data-point is re-used? Questions such as these highlight some of the barriers to data discovery and reuse, not only for humans, but even more so for machines; yet it is precisely these kinds of deeply and broadly integrative analyses that constitute the bulk of contemporary e-Science. The reason that we often need several weeks (or months) of specialist technical effort to gather the data necessary to answer such research questions is not the lack of appropriate technology; the reason is, that we do not pay our valuable digital objects the careful attention they deserve when we create and preserve them. Overcoming these barriers, therefore, necessitates that all stakeholders-including researchers, special-purpose, and general-purpose repositories-evolve to meet the emergent challenges described above. The goal is for scholarly digital objects of all kinds to become 'first class citizens' in the scientific publication ecosystem, where the quality of the publication-and more importantly, the impact of the publication-is a function of its ability to be accurately and appropriately found, re- used, and cited over time, by all stakeholders, both human and mechanical. With this goal in-mind, a workshop was held in Leiden, Netherlands, in 2014, named 'Jointly Designing a Data Fairport'. This workshop brought together a wide group of academic and private stakeholders all of whom had an interest in overcoming data discovery and reuse obstacles. From the deliberations at the workshop the notion emerged that, through the definition of, and widespread support for, a minimal set of community-agreed guiding principles and practices, all stakeholders could more easily discover, access, appropriately integrate and re-use, and adequately cite, the vast quantities of information being generated by contemporary data-intensive science. The meeting concluded with a draft formulation of a set of foundational principles that were subsequently elaborated in greater detail-namely, that all research objects should be Findable, Accessible, Interoperable and Reusable (FAIR) both for machines and for people. These are now referred to as the FAIR Guiding Principles. Subsequently, a dedicated FAIR working group, established by several members of the FORCE11 community 10 fine-tuned and improved the Principles. The results of these efforts are reported here.", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "The significance of machines in data-rich research environments", "text": "The emphasis placed on FAIRness being applied to both human-driven and machine-driven activities, is a specific focus of the FAIR Guiding Principles that distinguishes them from many peer initiatives (discussed in the subsequent section). Humans and machines often face distinct barriers when attempting to find and process data on the Web. Humans have an intuitive sense of 'semantics' (the meaning or intent of a digital object) because we are capable of identifying and interpreting a wide variety of contextual cues, whether those take the form of structural/visual/iconic cues in the layout of a Web page, or the content of narrative notes. As such, we are less likely to make errors in the selection of appropriate data or other digital objects, although humans will face similar difficulties if sufficient contextual metadata is lacking. The primary limitation of humans, however, is that we are unable to operate at the scope, scale, and speed necessitated by the scale of contemporary scientific data and complexity of e-Science. It is for this reason that humans increasingly rely on computational agents to undertake discovery and integration tasks on their behalf. This necessitates machines to be capable of autonomously and appropriately acting when faced with the wide range of types, formats, and access-mechanisms/protocols that will be encountered during their self-guided exploration of the global data ecosystem. It also necessitates that the machines keep an exquisite record of provenance such that the data they are collecting can be accurately and adequately cited. Assisting these agents, therefore, is a critical consideration for all participants in the data management and stewardship process-from researchers and data producers to data repository hosts. Throughout this paper, we use the phrase 'machine actionable' to indicate a continuum of possible states wherein a digital object provides increasingly more detailed information to an autonomously- acting, computational data explorer. This information enables the agent-to a degree dependent on the amount of detail provided-to have the capacity, when faced with a digital object never encountered before, to: a) identify the type of object (with respect to both structure and intent), b) determine if it is useful within the context of the agent's current task by interrogating metadata and/ or data elements, c) determine if it is usable, with respect to license, consent, or other accessibility or use constraints, and d) take appropriate action, in much the same manner that a human would. For example, a machine may be capable of determining the data-type of a discovered digital object, but not capable of parsing it due to it being in an unknown format; or it may be capable of processing the contained data, but not capable of determining the licensing requirements related to the retrieval and/or use of that data. The optimal state-where machines fully 'understand' and can autonomously and correctly operate-on a digital object-may rarely be achieved. Nevertheless, the FAIR principles provide 'steps along a path' toward machine-actionability; adopting, in whole or in part, the FAIR principles, leads the resource along the continuum towards this optimal state. In addition, the idea of being machine-actionable applies in two contexts-first, when referring to the contextual metadata surrounding a digital object ('what is it?'), and second, when referring to the content of the digital object itself ('how do I process it/integrate it?'). Either, or both of these may be machine-actionable, and each forms its own continuum of actionability. Finally, we wish to draw a distinction between data that is machine-actionable as a result of specific investment in software supporting that data-type, for example, bespoke parsers that understand life science wwPDB files or space science Space Physics Archive Search and Extract (SPASE) files, and data that is machine-actionable exclusively through the utilization of general-purpose, open technologies. To reiterate the earlier point-ultimate machine-actionability occurs when a machine can make a useful decision regarding data that it has not encountered before. This distinction is important when considering both (a) the rapidly growing and evolving data environment, with new technologies and new, more complex data-types continuously being developed, and (b) the growth of general-purpose repositories, where the data-types likely to be encountered by an agent are unpredictable. Creating bespoke parsers, in all computer languages, for all data-types and all analytical tools that require those data-types, is not a sustainable activity. As such, the focus on assisting machines in their discovery and exploration of data through application of more generalized interoperability technologies and standards at the data/repository level, becomes a first-priority for good data stewardship.", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "The FAIR Guiding Principles in detail", "text": "Representatives of the interested stakeholder-groups, discussed above, coalesced around four core desiderata-the FAIR Guiding Principles-and limited elaboration of these, which have been refined (Box 2) from the meeting's original draft, available at (https://www.force11.org/node/6062). A separate document that dynamically addresses community discussion relating to clarifications and explanations of the principles, and detailed guidelines for and examples of FAIR implementations, is currently being constructed (http://datafairport.org/fair-principles-living-document-menu). The FAIR Guiding Principles describe distinct considerations for contemporary data publishing environments with respect to supporting both manual and automated deposition, exploration, sharing, and reuse. While there have been a number of recent, often domain-focused publications advocating for specific improvements in practices relating to data management and archival 1,11,12 , FAIR differs in that it describes concise, domain-independent, high-level principles that can be applied to a wide range of scholarly outputs. Throughout the Principles, we use the phrase '(meta)data' in cases where the Principle should be applied to both metadata and data. The elements of the FAIR Principles are related, but independent and separable. The Principles define characteristics that contemporary data resources, tools, vocabularies and infrastructures should exhibit to assist discovery and reuse by third-parties. By minimally defining each guiding principle, the barrier-to-entry for data producers, publishers and stewards who wish to make their data holdings FAIR is purposely maintained as low as possible. The Principles may be adhered to in any combination and incrementally, as data providers' publishing environments evolve to increasing degrees of 'FAIRness'. Moreover, the modularity of the Principles, and their distinction between data and metadata, explicitly support a wide range of special circumstances. One such example is highly sensitive or personally-identifiable data, where publication of rich metadata to facilitate discovery, including clear rules regarding the process for accessing the data, provides a high degree of 'FAIRness' even in the absence of FAIR publication of the data itself. A second example involves the publication Box ", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "| The FAIR Guiding Principles", "text": "To be Findable: F1. (meta)data are assigned a globally unique and persistent identifier F2. data are described with rich metadata (defined by R1 below) F3. metadata clearly and explicitly include the identifier of the data it describes F4. (meta)data are registered or indexed in a searchable resource To be Accessible: A1. (meta)data are retrievable by their identifier using a standardized communications protocol A1.1 the protocol is open, free, and universally implementable A1.2 the protocol allows for an authentication and authorization procedure, where necessary A2. metadata are accessible, even when the data are no longer available of non-data research objects. Analytical workflows, for example, are a critical component of the scholarly ecosystem, and their formal publication is necessary to achieve both transparency and scientific reproducibility. The FAIR principles can equally be applied to these non-data assets, which need to be identified, described, discovered, and reused in much the same manner as data. Specific exemplar efforts that provide varying levels of FAIRness are detailed later in this document. Additional issues, however, remain to be addressed. First, when community-endorsed vocabularies or other (meta)data standards do not include the attributes necessary to achieve rich annotation, there are two possible solutions: either publish an extension of an existing, closely related vocabulary, or-in the extreme case-create and explicitly publish a new vocabulary resource, following FAIR principles ('I2'). Second, to explicitly identify the standard chosen when more than one vocabulary or other (meta)data standard is available, and given that for instance in the life sciences there are over 600 content standards, the BioSharing registry (https://biosharing.org/) can be of use as it describes the standards in detail, including versions where applicable.", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "The Principles precede implementation", "text": "These high-level FAIR Guiding Principles precede implementation choices, and do not suggest any specific technology, standard, or implementation-solution; moreover, the Principles are not, themselves, a standard or a specification. They act as a guide to data publishers and stewards to assist them in evaluating whether their particular implementation choices are rendering their digital research artefacts Findable, Accessible, Interoperable, and Reusable. We anticipate that these high level principles will enable a broad range of integrative and exploratory behaviours, based on a wide range of technology choices and implementations. Indeed, many repositories are already implementing various aspects of FAIR using a variety of technology choices and several examples are detailed in the next section; examples include Scientific Data itself and how narrative data articles are anchored to a progressively FAIR structured metadata. Examples of FAIRness, and the resulting value-added Dataverse 7 : Dataverse is an open-source data repository software installed in dozens of institutions globally to support public community repositories or institutional research data repositories. Harvard Dataverse, with more than 60,000 datasets, is the largest of the current Dataverse repositories, and is open to all researchers from all research fields. Dataverse generates a formal citation for each deposit, following the standard defined by Altman and King 13 . Dataverse makes the Digital Object Identifier (DOI), or other persistent identifiers (Handles), public when the dataset is published ('F'). This resolves to a landing page, providing access to metadata, data files, dataset terms, waivers or licenses, and version information, all of which is indexed and searchable ('F', 'A', and 'R'). Deposits include metadata, data files, and any complementary files (such as documentation or code) needed to understand the data and analysis ('R'). Metadata is always public, even if the data are restricted or removed for privacy issues ('F', 'A'). This metadata is offered at three levels, extensively supporting the 'I' and 'R' FAIR principles: 1) data citation metadata, which maps to DataCite schema or Dublin Core Terms, 2) domain-specific metadata, which when possible maps to metadata standards used within a scientific domain, and 3) file-level metadata, which can be deep and extensive for tabular data files (including column-level metadata). Finally, Dataverse provides public machine-accessible interfaces to search the data, access the metadata and download the data files, using a token to grant access when data files are restricted ('A'). FAIRDOM (http://fair-dom.org/about): integrates the SEEK 14 and openBIS 15 platforms to produce a FAIR data and model management facility for Systems Biology. Individual research assets (or aggregates of data and models) are identified with unique and persistent HTTP URLs, which can be registered with DOIs for publication ('F'). Assets can be accessed over the Web in a variety of formats appropriate for individuals and/or their computers (RDF, XML) ('I'). Research assets are annotated with rich metadata, using community standards, formats and ontologies ('I'). The metadata is stored as RDF to enable interoperability and assets can be downloaded for reuse ('R'). ISA 16 : is a community-driven metadata tracking framework to facilitate standards-compliant collection, curation, management and reuse of life science datasets. ISA provides progressively FAIR structured metadata to Nature Scientific Data's Data Descriptor articles, and many GigaScience data papers, and underpins the EBI MetaboLights database among other data resources. At the heart is a general-purpose, extensible ISA model, originally only available as a tabular representation but subsequently enhanced as an RDF-based representation 17 , and JSON serializations to enable the 'I' and 'R', becoming 'FAIR' when published as linked data (http://elixir-uk.org/node-events/201cisa-as-a- fair-research-object201d-hack-the-spec-event-1) and complementing other research objects 18 . Open PHACTS 19 : Open PHACTS is a data integration platform for information pertaining to drug discovery. Access to the platform is mediated through a machine-accessible interface 20 which provides multiple representations that are both human (HTML) and machine readable (RDF, JSON XML, CSV, etc), providing the 'A' facet of FAIRness. The interface allows multiple URLs to be used to access information about a particular entity through a mappings service ('F' and 'A'). Thus, a user can provide a ChEMBL URL to retrieve information sourced from, for example, Chemspider or DrugBank. Each call provides a canonical URL in its response ('A' and 'I'). All data sources used are described using standardized dataset descriptions, following the global VoID standard, with rich provenance ('R' and 'I'). All interface features are described using RDF following the Linked Data API specification ('A'). Finally, a majority of the datasets are described using community agreed upon ontologies ('I'). wwPDB 4,21 : wwPDB is a special-purpose, intensively-curated data archive that hosts information about experimentally-determined 3D structures of proteins and nucleic acids. All wwPDB entries are stably hosted on an FTP server ('A') and represented in machine-readable formats (text and XML); the latter are machine-actionable using the metadata provided by the wwPDB conforming to the Macromolecular Information Framework (mmCIF 22 ), a data standard of the International Union of Crystallography (IUCr) ('F','I' for humans, 'F','I' for IUCr-aware machines). The wwPDB metadata contains cross-references to common identifiers such as PubMed and NCBI Taxonomy, and their wwPDB metadata are described in data dictionaries and schema documents (http://mmcif.wwpdb.org and http://pdbml.wwpdb.org) which conform to the IUCr data standard for the chemical and structural biology domains ('R'). A variety of software tools are available to interpret both wwPDB data and meta-data ('I','R' for humans, 'I','R' for machines with this software). Each entry is represented by a DOI ('F', 'A' for humans and machines). The DOI resolves to a zipped file which requires special software for further interrogation/interpretation. Other wwPDB access points [23][24][25] provide access to wwPDB records through URLs that are likely to be stable in the long-term ('F'), and all data and metadata is searchable through one or more of the wwPDB-affiliated websites ('F') UniProt 26 : UniProt is a comprehensive resource for protein sequence and annotation data. All entries are uniquely identified by a stable URL, that provides access to the record in a variety of formats including a web page, plain-text, and RDF ('F' and 'A'). The record contains rich metadata ('F') that is both human-readable (HTML) and machine-readable (text and RDF), where the RDF formatted response utilizes shared vocabularies and ontologies such as UniProt Core, FALDO, and ECO ('I'). Interlinking with more than 150 different databases, every UniProt record has extensive links into, for example, PubMed, enabling rich citation. These links are machine-actionable in the RDF representation ('R'). Finally, in the RDF representation, the UniProt Core Ontology explicitly types all records, leaving no ambiguity-neither for humans nor machines-about what the data represents ('R'), enabling fully-automated retrieval of records and cross-referencing information. In addition to, and in support of, communities and resources that are already pursuing FAIR objectives, the Data Citation Implementation Group of Force11 has published specific technical recommendations for how to implement many of the principles 27 , with a particular focus on identifiers and their resolution, persistence, and metadata accessibility especially related to citation. In addition, the 'Skunkworks' group that emerged from the Lorentz Workshop has been creating software supporting infrastructures 28 that are, end-to-end, compatible with FAIR principles, and can be implemented over existing repositories. These code modules have a particular focus on metadata publication and searchability, compatibility in cases of strict privacy considerations, and the extremely difficult problem of data and metadata interoperability (manuscript in preparation). Finally, there are several emergent projects, some listed in Box 3, for which FAIR is a key objective. These projects may provide valuable advice and guidance for those wishing to become more FAIR.", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "FAIRness is a prerequisite for proper data management and data stewardship", "text": "The ideas within the FAIR Guiding Principles reflect, combine, build upon and extend previous work by both the Concept Web Alliance (https://conceptweblog.wordpress.com/) partners, who focused on machine-actionability and harmonization of data structures and semantics, and by the scientific and scholarly organizations that developed the Joint Declaration of Data Citation Principles (JDDCP 29 ), Box 3 | Emergent community/collaborative initiatives with FAIR as a core focus or activity bioCADDIE (https://biocaddie.org): The NIH BD2K biomedical and healthCAre Data Discovery Index Ecosystem (bioCADDIE) consortium works to develop a Data Discovery Index (DDI) prototype, which is set to be as transformative and impactful for data as PubMed for the biomedical literature 30 . The DDI focuses on finding ('F') and accessing ('A') the datasets stored across different sources, and progressively works to identify relevant metadata 31 ('I') and maps them to community standards ('R'), linking to BioSharing. CEDAR 32 : The Center for Expanded Data Annotation and Retrieval (CEDAR) is an NIH BD2K funded center of excellence to develop tools and technologies that reduce the burden of authoring and enhancing metadata that meet community-based standards. CEDAR will enable the creation of metadata templates that implement community based standards for experimental metadata, from BioSharing (https://biosharing.org), and that will be uniquely identifiable and retrievable with HTTP URIs, and annotated with vocabularies and ontologies drawn from BioPortal (http://bioportal.bioontology.org) ('F','A','I','R'). These templates will guide users to create rich metadata with unique and stable HTTP identifiers ('F') that can be retrieved using HTTP ('A') and accessible in a variety of formats (JSON-LD, TURTLE, RDF/XML, CSV, etc) ('I'). These metadata will use community standards, as defined by the template, and include provenance and data usage ('R'). These two projects, among others, provide tools and or collaborative opportunities for those who wish to improve the FAIRness of their data. who focused on primary scholarly data being made citable, discoverable and available for reuse, so as to be capable of supporting more rigorous scholarship. An attempt to define the similarities and overlaps between the FAIR Principles and the JDDCP is provided at (https://www.force11.org/node/ 6062). The FAIR Principles are also complementary to the 'Data Seal of Approval' (DSA) (http:// datasealofapproval.org/media/filer_public/2013/09/27/guidelines_2014-2015.pdf) in that they share the general aim to render data re-usable for users other than those who originally generated them. While the DSA focuses primarily on the responsibilities and conduct of data producers and repositories, FAIR focuses primarily on the data itself. Clearly, the broader community of stakeholders is coalescing around a set of common, dovetailed visions spanning all facets of the scholarly data publishing ecosystem. The end result, when implemented, will be more rigorous management and stewardship of these valuable digital resources, to the benefit of the entire academic community. As stated at the outset, good data management and stewardship is not a goal in itself, but rather a pre-condition supporting knowledge discovery and innovation. Contemporary e-Science requires data to be Findable, Accessible, Interoperable, and Reusable in the long-term, and these objectives are rapidly becoming expectations of agencies and publishers. We demonstrate, therefore, that the FAIR Data Principles provide a set of mileposts for data producers and publishers. They guide the implementation of the most basic levels of good Data Management and Stewardship practice, thus helping researchers adhere to the expectations and requirements of their funding agencies. We call on all data producers and publishers to examine and implement these principles, and actively participate with the FAIR initiative by joining the Force11 working group. By working together towards shared, common goals, the valuable data produced by our community will gradually achieve the critical goals of FAIRness.", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "Acknowledgements", "text": "The original Lorentz Workshop 'Jointly Designing a Data FAIRport' was organized by Barend Mons in collaboration with and co-sponsored by the Lorentz center, The Dutch Techcenter for the Life Sciences and the Netherlands eScience Center. The principles and themes described in this manuscript represent the significant voluntary contributions and participation of the authors at, and/or subsequent to, this workshop and from the wider Force11, BD2K and ELIXIR communities. We also acknowledge and thank the organizers and backers of the NBDC/DBCLS BioHackathon 2015, where several of the authors made significant revisions to the FAIR Principles.", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "Author Contributions", "text": "M.W. was the primary author of the manuscript, and participated extensively in the drafting and editing of the FAIR Principles. M.D. was significantly involved in the drafting of the FAIR Principles. B.M. conceived of the FAIR Data Initiative, contributed extensively to the drafting of the principles, and to this manuscript text. All other authors are listed alphabetically, and contributed to the manuscript either by their participation in the initial workshop and/or by editing or commenting on the manuscript text.", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "Additional Information", "text": "Competing financial interests: M.A. is the Nature Genetics' Editor in Chief; S.A.S. is Scientific Data's Honorary Academic Editor and consultant. ", "title": "The FAIR Guiding Principles for scientific data management and stewardship", "file_name": "Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf"}
{"section": "Abstract", "text": "To answer the question in the title, this article characterizes the socially efficient organization of the market with search frictions. The efficient organization depends on the relative elasticity in the supply between the two sides of the market, the costs of participating in the market and organizing trade, and the (a)symmetry in matching. We also show that the social optimum can be implemented by a realistic market equilibrium where the organizers set up trading sites to direct the other side's search. The results provide a unified explanation for why trade has often been organized by sellers in the goods market, by buyers (firms) in the labor market, and by both sides in the asset market. The analysis also sheds light on how the efficient market organization can change with innovations such as e-commerce and just-in-time production. JEL Codes: D40, D60, D83.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "I. INTRODUCTION", "text": "Search frictions impede trade. To mitigate these frictions, some individuals actively organize trade by setting up trading sites to direct other participants' search. The trading sites can be shops, job advertisements, websites, etc. In addition to the site cost, there are costs of participating in the market. To maximize social welfare in such a frictional market, should buyers or sellers organize trade? We address this question by characterizing the social optimum constrained by search frictions. In reality, market organizers vary across markets. In the goods market, sellers set up shops and advertise to direct buy- ers' search. In the labor market, buyers of labor services (firms) organize trade by incurring the cost to post vacancies, while sell- ers (workers) search for jobs. In the asset market, buyer-organized trade and seller-organized trade coexist. How are these variations in the market organization related to search frictions and trading costs? Moreover, matching and trading technologies can change over time, as witnessed in the fast growth of e-commerce in the * We thank an editor and four referees for helpful comments that improved the article significantly. A previous version was presented at the Federal Reserve Bank of Chicago and at the Search and Matching workshop at University of Penn- sylvania. Shi acknowledges financial support by Pennsylvania State University. past two decades. How do these innovations affect the socially efficient organization of a market? Given the prevalence of search frictions, questions like the ones above should have first-order importance in economics. How- ever, they have been either made irrelevant by assumption or not addressed formally. In the Walrasian paradigm, the market orga- nization is irrelevant because trade is assumed to be frictionless. The literature on firms and organizations, pioneered by Coase (1937), Williamson (1981), and Grossman and Hart (1986), as- sumes the market to be inefficient to determine the boundaries of firms, but it takes the market as given. In this article, we put the organization of the market at the center of the analysis. To address the question who should organize trade, we focus on the social planner's choice of the market organization under search frictions. This normative focus is also helpful for shedding light on what regulations are necessary for inducing the efficient orga- nization if the market is inefficient. In the end, we show that the social optimum can be implemented by an equilibrium with price posting and directed search-a market mechanism commonly ob- served in reality. It may not be obvious why the market organization can mat- ter for social welfare, especially when search is directed. In the labor market, for example, Burdett, Shi, and Wright (2001) as- sume that firms post wages to direct workers' search, but Julien, Kennes, and King (2000) assume that workers post auctions with reserve wages to direct firms' search. 1 Given the market organiza- tion in these models, the equilibrium is socially efficient under the constraint of matching frictions. This result gives the impression that which side of the market organizes trade is immaterial for ef- ficiency, as long as search is directed. However, this impression is false. Under both market organizations, match failures arise from the lack of coordination among searchers. Some searchers may ap- ply to the same target, but only one of them is chosen to form a match. The difficulty of coordinating increases with the number of targets per searcher. Thus if the short side of the market or- ganizes trade, the number of matches is higher, which increases social welfare (see Herreiner 1999). In these papers, the role of market organizers is captured by an asymmetry in the matching 1. Burdett, Shi, and Wright (2001) examine the goods market in the main sections of their paper, but they discuss the model's implications for the labor market in the concluding section. function. The matching game generates the so-called urn-ball matching function. Switching the two sides of the market in this matching function yields different numbers of matches. This example illustrates the importance of search frictions but misses several necessary ingredients for an analysis of the efficient market organization. First, which side of the market is short should be endogenous rather than fixed. It is necessary to trace the determinants of the efficient market organization to the fundamentals. Second, social welfare depends on the trading cost, not just the number of matches emphasized in the above exam- ple. Social efficiency may call for a compromise on the number of matches to economize on the trading cost, and a market or- ganized by the long side may maximize social welfare sometimes. Third, the example uses a particular matching game that yields an asymmetric matching function. To understand the efficient mar- ket organization, it is necessary to analyze both symmetric and asymmetric matching functions. Moreover, a general matching function enables us to examine how the efficient market organi- zation changes with the matching process. The benchmark model in this article incorporates these ingre- dients, with homogeneous individuals on each side of the market. On one side, the supply of individuals is elastic and determined by competitive entry. The supply on the other side is relatively inelastic; for simplicity, we fix its measure at 1. Both sides face a cost to participate in the market. In addition, if an individual chooses to organize trade, he or she must also incur a site cost to set up a trading site. The site capacity is normalized to one per organizer. If an individual does not organize trade, he or she is a visitor. The measure of matches is given by a general matching function that allows for potentially asymmetric roles of organizers versus visitors, such as the ones in the foregoing example of di- rected search. In a match, one indivisible unit of a good or service is traded. We analyze the planner's allocation that maximizes the sum of expected net utilities of all individuals under the constraints of search frictions and individual rationality. Because individuals on each side are homogeneous, the efficient allocation is to have only one side of the market organize trade. The planner chooses which side to organize trade and how many elastic individuals to enter the market. We characterize the social optimum under symmetric matching first in Section III and then under asymmetric matching in Section IV. When the matching function is symmetric and the site cost is positive, the model generates three main predictions: (i) the short side of the market should organize trade; (ii) the elastic side is short if and only if the side's participation cost exceeds a thresh- old; (iii) a reduction in the site cost increases this threshold and, hence, increases the likelihood that the efficient organizers are on the inelastic side. Because matching is symmetric, these predic- tions have nothing to do with the earlier example that relies on asymmetric matching. Instead, they arise from the fact that an organizer incurs the site cost but a visitor does not. Because of this asymmetric effect, fewer elastic individuals enter the market if they are organizers than if they are visitors. The lower entry economizes on the total cost of trade at the expense of reducing the number of matches. When the elastic side's participation cost exceeds a threshold, economizing on the total cost of trade is the dominant consideration for efficiency. In this case, the elastic side should incur the site cost to organize trade, which puts them on the short side. When the elastic side's participation cost is below the threshold, increasing the number of matches is the dominant consideration for efficiency. In this case, the inelastic side should incur the site cost to induce more elastic individuals to enter the market, which again puts the organizers on the short side. A re- duction in the site cost reduces the importance of the total cost of trade in the efficiency consideration and, hence, increases the threshold above which the elastic side should organize trade. If the site cost is zero, social welfare is independent of which side organizes trade, despite the existence of search frictions and par- ticipation costs. The predictions accord well with observations in the goods market and the labor market. In the goods market, sellers are elastic; in the labor market, firms (buyers) are elastic. In both, an elastic individual incurs a substantial participation cost to estab- lish a business or set up production. The site cost is also positive. Given these features, trade should be organized by sellers in the goods market and by firms in the labor market. Such market or- ganizations have been prevalent in the form of shops maintained by sellers and of jobs advertised by firms. Also, because the effi- cient market organizers are on the short side, there is a welfare justification for why there are fewer job vacancies than there are unemployed workers. However, these predictions call for a dis- tinction between the site cost and the participation cost, which is blurred in the literature (see Section III.B). These predictions help us understand how the market organi- zation can evolve with technological advances such as the growth of e-commerce. As online shops replace physical shops, the site cost and the participation cost fall significantly. Another advance is the increasing use of just-in-time production, which postpones part of the production cost from the participation cost to the post- match cost. In the labor market, sectoral changes move jobs from manufacturing to services where firms are less costly to set up and jobs are more flexible. By the third prediction, all these changes increase the likelihood that more goods will be made on demand instead of being made for order and that more job-wanted instead of help-wanted announcements will be advertised. When the matching function is asymmetric, we define intu- itively whether the function favors the short or the long side (see Section II.A) and link the asymmetry to the underlying meeting process (see Section IV). The additional prediction is as follows: if matches are generated by one-to-many meetings, such as the process underlying the urn-ball matching function, the market should be organized by the short side even when the site cost is zero; if matches are generated by one-to-one meetings and if the organizers have sufficiently lower search efficiency than the visitors, then the market should be organized by the long side. In Section IV, we use this prediction to shed light on the differ- ences between the marriage market and the labor market. We also discuss how the market organization changes with technological innovations, such as online trading and trading platforms. Section V introduces heterogeneity on the inelastic side. The new result is that markets organized by different sides can coexist when the elastic side's participation cost is intermediate. Applying this result to the asset market, the model yields the following pre- diction: asset sellers who have high liquidity needs organize one market to initiate trade, and asset sellers who have low liquidity needs participate as visitors in another market organized by buy- ers. We use this prediction to explain the trading pattern and the growth of the short-term loan market of repurchase agreements (repos). In Section VI, we consider a realistic market mechanism where individuals compete to set up trading sites and post the terms of trade to direct the other side's search. We show that the market equilibrium implements the social optimum. In addition to internalizing matching externalities, competition with directed search induces the efficient organization of the market to emerge. Matching frictions play a pivotal role in our analysis. If match- ing frictions did not exist, social welfare would be independent of which side organizes trade. A contrast is Taylor (1995), who com- pares prices posted by the two sides of a market without matching frictions. All equilibria in his paper yield the same social welfare. This article is related to the literature on directed search pi- oneered by Peters (1991) and Montgomery (1991). However, this literature exogenously fixes one side of the market to direct search. For the goods market, the literature assumes that sellers direct buyers' search, for example, Peters (1991) and Burdett, Shi, and Wright (2001). For the labor market, the literature assumes that search is directed by firms in some papers (e.g., Moen 1997;Acemoglu and Shimer 1999;Burdett, Shi, and Wright 2001;Shi 2001;Galenianos and Kircher 2009), and by workers in other papers (e.g., Julien, Kennes, and King 2000). We endogenize the market organization to show that social efficiency calls for a specific side of the market to direct search. 2 In a matching game, Herreiner (1999) demonstrates that the number of matches is higher if the short side of the market directs the other side's search. She fixes the number of participants on each side and the game generates the urn-ball matching function that favors the short side. As we explained earlier, it is neces- sary to endogenize the relative supply between the two sides of a market and use a general matching function. An illustration of this necessity is the case where matching is symmetric. When the relative supply between the two sides is endogenous, the efficient market organization is determinate, provided that the site cost is positive. When the relative supply is fixed as in Herreiner's model, welfare would be independent of which side organizes trade if the game were changed to yield a symmetric matching function. More- over, with a general matching function, the efficient organizers are not always on the short side. 3 2. The literature on directed search has often assumed that the search- directing side of the market is elastic in the supply. This is a possible cause of the false impression that social efficiency is independent of which side directs search. Under this assumption, a change in the market organization changes the model environment, which makes the two market organizations not comparable. We avoid this potential confusion by assuming that which side is elastic is inde- pendent of which side organizes trade. 3. Julien, Kennes, and King (2006) briefly examine how switching the roles of the two sides of a market in directed search can affect welfare, but they do not reach a clear conclusion. The literature on undirected search has examined how an equilibrium changes with the search pattern. Burdett et al. (1995) compare the equilibrium where only one side of the market searches with the equilibrium where both sides search. Kultti et al. (2009) find that sufficiently unequal population between the two sides of a market is important for the equilibrium with one side searching to be robust to coalition deviations. These papers do not study the efficient market organization. In fact, if the supply of individuals is endogenous on at least one side, the equilibria in these models are generically inefficient because they fail to internalize matching externalities (see Hosios 1990). In operations research, Alpern (1995) studies the least ex- pected time for two individuals randomly placed in a region to find each other and shows that the symmetry in the region lengthens the expected time. Missing from this problem are the basic ingre- dients of an economic model, such as markets and the interactions among individuals. Finally, a literature on platform competition emphasizes network externalities (Rochet and Tirole 2003). We focus on search frictions instead. Throughout the analysis, we ab- stract from such externalities by assuming that the site cost is constant per site.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "II.A. Model Environment", "text": "The economy lasts for one period and is populated by homo- geneous and risk-neutral individuals on each side of the market. 4 One side of the market is elastic in the supply because the measure of individuals is determined by entry. The other side is relatively inelastic in the supply and the measure of individuals is fixed at 1. The elastic side is indexed by i = e and the inelastic side by i = n. For brevity, we refer to an individual on side e as an elastic individual and an individual on side n as an inelastic individual. In the general description, we do not tie the elastic side to the sup- ply or the demand side, because the tie can vary across markets. For specific examples, one can think of the elastic side as sellers in the goods market who compete to supply goods and as buyers (firms) in the labor market who compete to create vacancies. In both examples, the inelastic side is not completely fixed. Instead, buyers in the goods market can delay search if prices are exceed- ingly high, and workers can choose between work and leisure. However, fixing the measure of individuals on the inelastic side is without loss of generality, because only the relative elasticity between the two sides matters for the results as shown in Online Appendix G. 5 To participate in the market, an individual on side i must incur a cost c i 0. The utility of staying out of the market is zero. In a match, one unit of an indivisible good is transferred from one party to the other party for consumption. The utility of consumption net of the postmatch production cost is normalized to 1. In addition to the indivisible good, there is a divisible good that everyone can produce and consume. The marginal cost of producing and the marginal utility of consuming the divisible good are equal to 1. This good is used to transfer utilities between individuals. Any individual can become a market organizer by incurring a cost k 0 for a site. We refer to the collection of trades or- ganized by side i \u2208 {e, n} as market i. The individuals trading with the organizers are called visitors. An organizer faces a ca- pacity constraint on the number of sites, which is normalized to 1 for simplicity. 6 Thus, the measure of trading sites is equal to the measure of organizers. The ratio of elastic individuals to inelas- tic individuals, denoted \u03b8 , is determined by entry endogenously. Since the measure of inelastic individuals is normalized to 1, \u03b8 is also the measure of elastic individuals. Note that organizing trade is not necessarily the same as posting the price. Although the two actions are often related in reality, it is conceivable that market organizers can create sites but allow visitors to name the price. The matching process in a market is frictional. In general, an organizer and a visitor may contribute differently to the match formation, and the relative role switches with the market 5. There, we introduce search effort on the inelastic side to make the supply partially elastic and prove that the results do not change. We thank an editor for suggesting this extension. 6. Online Appendix F analyzes the effect of the site capacity. Also, the capacity constraint can be endogenized, although we do not carry out the exercise. If the marginal cost of a site is sufficiently increasing, the main results of this article continue to hold. On the other hand, if the marginal cost of a site is constant or decreasing, then the efficient allocation is the uninteresting outcome that only one organizer participates in the market to create all the sites needed for trade. organization. For example, the organizers may direct visitors' search. To capture the role of the organizers consistently between different market organizations, we put the measure of the orga- nizers always as the first argument in the matching function and the measure of visitors as the second argument. Thus, the mea- sure of matches is M(\u03b8 , 1) in market e and M(1, \u03b8 ) in market n, where M is a matching function with constant returns to scale. Denote F(\u03b8 ) \u2261 M(\u03b8 , 1). Then, in market e, the site-visitor ratio is \u03b8 , the matching probability is F(\u03b8 ) for a visitor (an inelastic individual) and F(\u03b8) \u03b8 for a site (an elastic individual). In market n, the site-visitor ratio is 1 \u03b8 , the matching probability is F( 1 \u03b8 ) for a visitor (an elastic individual) and \u03b8 F( 1 \u03b8 ) for a site (an inelastic individual). Elastic individuals are on the short side of the market if and only if \u03b8 < 1. We say that a matching function M is symmetric if M(\u03b8 , 1) = M(1, \u03b8 ) for all \u03b8 0, favors the short side if M(\u03b8 , 1) > M(1, \u03b8 ) is equivalent to \u03b8 \u2208 (0, 1), and favors the long side if M(\u03b8 , 1) > M(1, \u03b8 ) is equivalent to \u03b8 \u2208 (1, \u221e). 7 By the definition of F, it is clear that M(\u03b8 , 1) > M(1, \u03b8 ) if and only if F (\u03b8 ) > \u03b8 F( 1 \u03b8 ) , and so (a)symmetry of the matching function can be defined equivalently with F. Note that the site cost and the possible asymmetry in the matching function are the defining features of an organizer in this model. It is crucial to distinguish the site cost from the participation cost. All individuals need to incur the participation cost, but only the organizers incur the site cost. Thus, any trading cost that can be avoided by changing from an organizer to a visitor is a site cost. Conversely, any trading cost that must be incurred independently of whether an individual organizes trade is a participation cost. Clear examples of the site cost are the costs to maintain a shop, advertise, and maintain a vacancy, because a visitor does not incur such costs. In contrast, creating a job opening in the first place is a participation cost to a firm because the firm cannot avoid it by being a visitor instead of an organizer. Less clear is a seller's cost to set up a shop. Although it may be natural to regard this cost as a seller's participation cost, it is a site cost if the seller can avoid it by visiting shops created by buyers instead. Note that the participation cost also includes part of the search cost. For example, the literature specifies an unemployed worker's search cost to include the cost of staying in the labor force, which is a participation cost. 8 We impose the following assumption on the function F(\u03b8 ) = M(\u03b8 , 1): ASSUMPTION 1. F(\u03b8 ) \u2208 [0, 1] and F(\u03b8) \u03b8 \u2208 [0, 1] for all \u03b8 \u2208 [0, \u221e). For all \u03b8 \u2208 (0, \u221e) , F is strictly concave and twice continuously differentiable, with F > 0, F > 0, and F \u2212 \u03b8 F > 0. Moreover, A \u2261 F (0) \u2208 (0, 1], F(\u221e) = 1, F(0) = lim \u03b8\u2192\u221e \u03b8 F (\u03b8 ) = 0. This assumption is standard. In particular, F > 0 and F \u2212 \u03b8 F > 0 require that, as the number of sites per visitor increases, the matching probability should increase for a visi- tor and decrease for a site. 9 Also, all proper matching functions should have the property that the matching probabilities do not exceed 1. This property implies A \u2261 F (0) 1, which is listed in Assumption 1 to facilitate the reference. To see why, recall that the matching probability for an elastic individual is F(\u03b8) \u03b8 in mar- ket e. Because F(\u03b8) \u03b8 1 for all \u03b8 , then F (0) = lim \u03b8\u21920 F(\u03b8) \u03b8 1. Any matching function that violates F (0) 1 is improper (e.g., the Cobb-Douglas function), and it can be made proper by redefin- ing F (\u03b8 ) = min{F(\u03b8 ), \u03b8, 1}. However, the redefined function fails to be differentiable at \u03b8 0 < 1 such that F(\u03b8 0 ) = \u03b8 0 . Although the analysis can be modified to deal with such nondifferentiability, the modification is cumbersome and omitted. Moreover, note that a necessary but not sufficient condition for a matching function to be symmetric is A = 1 (see Lemma 2 in Appendix A). The three matching functions in the following example satisfy Assumption 1 and have been widely used in the literature: EXAMPLE 1. The urn-ball matching function yields F(\u03b8 ) = \u03b8 [1 \u2212 e \u2212 1 \u03b8 ] . This function has A = F (0) = 1, favors the short side, 8. Other parts of the search cost are in k instead of c. For example, some time ago sellers of vacuum cleaners carried samples to sell door to door. If sellers had stayed put to wait for buyers to visit, as they do now, they would have saved the carrying cost, but then buyers would have had to incur the search cost. Although the heterogeneity in the site cost between the two sides is interesting, we abstract from it for simplicity. It is predictable that a lower site cost gives a side an advan- tage in organizing trade. Burdett et al. (1995) explore the importance of search costs for the use of money. 9. Note that F(\u221e) = 1 implies lim , which can be derived as the outcome of a bilateral matching game, for example, Burdett et al. (1995). \u03b8\u2192\u221e [ F(\u03b8) \u03b8 ] = 0 and that lim \u03b8\u2192\u221e \u03b8 F (\u03b8 ) = 0 implies F (\u221e) = 0.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "II.B. Planner's Problem", "text": "The social planner maximizes social welfare defined as the sum of all individuals' expected net utilities, subject to individual rationality (participation) constraints. 10 The planner can make transfers between the two sides. However, the planner faces the same search frictions the market does. Precisely, the planner takes the matching function as given and must treat all identical sites or visitors symmetrically. Although the social planner allocates the individuals to the matching process directly without resorting to prices, we continue to use the term \"market\" for convenience. The subscript i indexes the variables in market i. We formulate the planner's problem for each market i under the assumption that the planner can create only market i. Then we argue that the welfare comparison between the two markets is valid even if the planner can create markets e and n simultaneously. Consider the case where the planner creates only market e. The measure of sites is equal to the measure of elastic individuals, which is \u03b8 e . The sum of elastic individuals' costs of participation and sites is (c e + k)\u03b8 e . Inelastic individuals' participation costs sum up to c n . Because the matching probability of an inelastic individual is F(\u03b8 e ), total expected utility generated by all trades is F(\u03b8 e ). Thus, social welfare is equal to w e (\u03b8 e ) where (1) w e (\u03b8 ) \u2261 F (\u03b8 ) \u2212 (ce + k) \u03b8 \u2212 c n . The planner chooses \u03b8 e to maximize w e (\u03b8 e ), subject to individ- ual rationality constraints on each side that the expected surplus of participating is nonnegative. If social welfare is positive, the planner can use transfers to ensure that individual rationality constraints do not bind. Thus, the optimal choice of \u03b8 in market e is \u03b8 e (c e + k) where 11 (2) \u03b8 e (x) \u2261 F \u22121 (x) for all x 0. Maximized social welfare in market e is (3) W e = f (ce + k) \u2212 c n , where f and h are defined as (4) h (\u03b8 ) \u2261 F (\u03b8 ) \u2212 \u03b8 F (\u03b8 ) for \u03b8 \u2208 [0, \u221e), f (x) \u2261 h F \u22121 (x) for all x 0. Similarly, if the planner creates only market n, then the mea- sure of sites is equal to the measure of inelastic individuals, which is 1. The sum of inelastic individuals' costs of participation and sites is (c n + k). The measure of elastic individuals is \u03b8 n , and their costs sum up to c e \u03b8 n . Because the matching probability of an inelastic individual is \u03b8 n F( 1 \u03b8 n ), social welfare is equal to w n (\u03b8 n ) where (5) w n (\u03b8 ) \u2261 \u03b8 F 1 \u03b8 \u2212 c e \u03b8 \u2212 (cn + k) . The function w n (\u03b8 ) is maximized at \u03b8 = \u03b8 n (c e ) where (6) \u03b8 n (x) \u2261 1 h \u22121 (x) for all x 0. Because F (h \u22121 (x)) = f \u22121 (x) for all x 0, which is proven in Lemma 1, maximized social welfare in market n is (7) W n = f \u22121 (ce) \u2212 (cn + k) . Market i is viable if W i > 0. Market e dominates market n in social welfare if W e > W n , and market n dominates market e if 11. To simplify the expressions, we extend the inverse of any monotone func- tion L(z) outside the range of L as follows. Let the domain of L be [z 1 , z 2 ] and the range be [  x 1 , x 2 ]. If L is an increasing function, define L \u22121 (x) = z 1 for all x < x 1 and L \u22121 (x) = z 2 for all x > x 2 . If L is a decreasing function, we define L \u22121 (x) = z 2 for all x < x 1 and L \u22121 (x) = z 1 for all x > x 2 . This definition of the inverse extends L \u22121 (x) from the domain [x 1 , x 2 ] to all x. In particular, F \u22121 (x) = \u221e for all x < 0 and F \u22121 (x) = 0 for all x > A. W n > W e . If W e = W n , the two markets are welfare equivalent. If only one market can be created, the planner will create the dom- inant market. For the efficient allocation to have active trading, at least one market should be viable. Also, net utility of consump- tion should be high enough to cover all trading costs. We list these assumptions below. ASSUMPTION 2. max {W e , W n } > 0 and 0 k < 1 \u2212 c e \u2212 c n . The foregoing characterization of the efficient allocation is valid even if the planner can create the two markets simultane- ously. The efficient market is still the one with higher welfare. To see this, suppose that inelastic individuals are divided between markets e and n. Reinterpret W i as social welfare per inelastic individual in market i. If W e > W n , the planner can move inelastic individuals from market n to market e and increase the measure of elastic individuals in market e to keep the ratio \u03b8 e unchanged. Because the matching technology has constant returns to scale, this move does not change the matching probabilities in market e. For each inelastic individual moved to market e, welfare increases by (W e \u2212 W n ). The planner can continue to increase social welfare this way until all inelastic individuals are moved to market e. Similarly, if W e < W n , the planner can increase social welfare by moving all inelastic individuals from market e to market n. Denote G(c e , k) = W n \u2212 W e where (8) G(c, k) \u2261 f \u22121 (c) \u2212 k \u2212 f (c + k) . Then, market n dominates market e if and only if G(c e , k) > 0. Define c d as the unique solution to: (9) f (cd + k) = c d . Note that G(c d , k) = 0, and so the two markets are welfare equiv- alent if c e = c d . ASSUMPTION 3. Regularity: G c (c, k) has the same sign as G c (cd, k) at all interior solutions of c to G(c, k) = 0; if matching is asymmetric, then G c (cd, k) = 0 for all k 0. As shown in Lemma 3 in Appendix A, the regularity condi- tion ensures c d to be the unique interior solution of c to G(c, k) = 0 under all symmetric matching functions for k > 0 and under all asymmetric matching functions for k 0. Lemma 3 also proves that the regularity assumption is satisfied under all symmetric matching functions and under the urn-ball matching function. When the regularity assumption is violated, there can be multi- ple interior solutions of c to G(c, k) = 0, which are examined in Appendix C. Matching frictions are necessary for the market organization to be relevant for social welfare in this model. To demonstrate, consider the frictionless matching function M(\u03b8 , 1) = min {\u03b8 , 1}, under which the short side of the market is matched with prob- ability 1. In both market e and market n, welfare is maximized at \u03b8 = 1, and maximized welfare is the same in the two markets. Moreover, the efficient allocation in the frictionless economy can be approached as the limit of the efficient allocation in a sequence of economies with matching frictions. To see this, consider a se- quence of economies where the matching function is the Dagum function in Example 1 with \u03c1 j \u2208 (0, \u221e) and A = 1. In the limit \u03c1 j \u2192 \u221e, the matching function approaches the frictionless matching function. For each \u03c1 j , let the efficient allocation be \u03b8 ij in market i \u2208 {e, n}. With equations (2) and (6), it can be verified that \u03b8 ej \u2192 1 and \u03b8 nj \u2192 1 as \u03c1 j \u2192 \u221e. We summarize these findings: REMARK 1. When matching is frictionless, market e and market n are welfare equivalent, despite the existence of participation costs and site costs. Moreover, the efficient allocation under frictionless matching can be approached as the limit of the efficient allocation under matching frictions as such frictions vanish. 12 ", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "III.A. Main Results and Intuition", "text": "The following theorem is proven in Appendix B: with c e = c d , the ratio of sites to visitors is less than 1 under the efficient market organization. When k = 0, the curves in Figure I coincide. Finally, when k increases, the curves shift down toward the origin but still intersect with each other on the 45-degree line, resulting in a lower threshold c d . A positive site cost tilts the favor toward the short side as market organizers despite symmetric matching. However, which side of the market is relatively short is endogenous. Elastic in- dividuals are on the short side if c e > c d , and on the long side if c e < c d . What is the explanation for these results? Because match- ing is symmetric, the intuition cannot be the one given in the introduction for the matching game studied by Herreiner (1999), Burdett, Shi, and Wright (2001), and Julien, Kennes, and King (2000). Instead, the key to the explanation is that the site cost affects the entry of elastic individuals differently under the two organizations. As a visitor, an elastic individual incurs only the participation cost c e . As an organizer, an elastic individual incurs the site cost k in addition to the participation cost. Thus, a smaller measure of elastic individuals enter the market when they are organizers than when they are visitors. The smaller amount of entry reduces the total cost of trade but also reduces the measure of matches. For social welfare, there is a trade-off between these two dimensions. If the participation cost on the elastic side is high in the sense c e > c d , the cost saving dominates the consideration of the measure of matches. To save the cost, elastic individuals should incur the site cost to organize trade so that they do not en- ter the market excessively, which puts them on the short side. If the participation cost on the elastic side is low in the sense c e < c d , increasing the measure of matches dominates the cost-saving con- sideration. In this case, inelastic individuals should incur the site cost to induce more elastic individuals to enter the market, which again puts the organizers on the short side. 13 With the above explanation, it is easy to understand why the threshold c d decreases in the site cost. A higher site cost increases the importance of economizing on the total cost. In this case, it is more likely that elastic individuals should organize trade. That is, c d is lower so that it is more likely for c e > c d to occur. However, when k = 0, the market organization is irrelevant for social wel- fare. When k = 0, the marginal cost of entry on the elastic side is equal to the participation cost; hence, it is independent of which side organizes trade. The social marginal benefit of entry is to in- crease the inelastic side's matching probability. Because matching is symmetric, this marginal benefit is also independent of which side organizes trade. Thus, when k = 0, the two market organiza- tions induce the same amount of entry of elastic individuals and yield the same welfare. Reflecting the asymmetric effect of the site cost on the two markets, there are parameter regions in which one market is vi- able but the other is not. In Figure I, market e is viable but market n is not if the parameters lie above the curve f \u22121 (c e ) \u2212 k and below 13. The trade-off between the trading volume and the total cost implies that social welfare is related ambiguously to the trading volume. If c e < c d , the efficient market (i.e., market n) increases the trading volume relative to market e. However, if c e > c d , the efficient market (i.e., market e) can reduce the trading volume relative to market n. Similarly, the relationship between social welfare and the market size depends on whether c e < c d , where the market size is the measure of individuals in the market. the curve f(c e + k) for c e > c d . Market n is viable but market e is not if the parameters lie above the curve f(c e + k) and below f \u22121 (c e ) \u2212 k for c e < c d . In the next subsection, we map the results into observations in the goods market and the labor market. We discuss the asset market in Section V. For the mapping, it is useful to list the main results in Theorem 1 as follows: \u2022 Prediction 1: Elastic individuals should organize trade if their participation cost is high. \u2022 Prediction 2: The organizers are on the short side of the market. \u2022 Prediction 3: A sufficiently large reduction in the site cost or the elastic side's participation cost can change the efficient organizers from the elastic side to the inelastic side.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "III.B. Implications for the Goods Market and the Labor Market", "text": "For the mapping between the model's predictions and obser- vations, it is important to recall the general interpretation that the inelastic side of a market is not fixed but inelastic only relative to the other side, as discussed in Section II.A. 1. The Goods Market. Sellers are usually on the elastic side to compete for buyers, and they incur substantial participation costs. To participate in the market, a seller acquires knowledge of the business and maintains the relationship with distributors and wholesalers. In addition, a seller incurs the cost to obtain some products for inventory and display. For a buyer, the search cost is the main cost of participation. In this market, a trading site can be a shop, a website for the product, or a membership in a trading platform. The site cost includes the cost to maintain a site and to advertise the product. Part of the cost of setting up a shop is also a site cost if an individual can avoid the cost by being a visitor. With this description, Predictions 1 and 2 state that sellers should organize trade and be on the short side. Both predictions accord well with observations in the goods market. Shops maintained by sellers have been the main trading form in the retail sector. Buying shops are much less common, perhaps because such shops are less efficient for trade. The dominance of seller-organized trade extends from the product market to services. Most services have been advertised by providers instead of customers. 2. The Labor Market. Firms (buyers) are usually on the elastic side to create vacancies and compete for workers (sellers). A firm faces large participation costs, such as the cost to set up its operation. The cost of creating a job opening is also a participa- tion cost, instead of a site cost, because the firm needs to create a job regardless of whether the firm organizes trade. The worker's participation cost is the search cost. A trading site consists of a job advertisement and the resources devoted to recruiting. Part of the site cost is the cost to maintain a vacancy, which, in principle, differs from the cost to create a job. With this description, Predic- tion 1 is consistent with the fact that firms maintain vacancies and advertise jobs. 14 Prediction 2 is consistent with the evidence that the ratio of vacancies to unemployed workers is less than 1. This ratio has been about 0.7 in the U.S. data on Job Openings and Labor Turnover Survey and the data on the Help-Wanted Index (see Pissarides 2009). Note that the search literature has often assumed that sellers organize trade in the goods market and firms organize trade in the labor market (e.g., Diamond 1982;Mortensen 1982;Pissarides 2000). At first glance, our analysis seems to justify this assump- tion on efficiency grounds. A closer look reveals the opposite. A typical model sets the participation cost to zero and assumes a flow cost to maintain a post or a vacancy, which is a site cost. In such a model, c e = 0 < c d (k), so trade should be organized by inelastic individuals, that is, by buyers in the goods market and by workers in the labor market. This is opposite to the market organization commonly observed in reality. To make the model consistent with efficiency, the literature should introduce a suf- ficiently high participation cost on the elastic side to generate c e > c d and distinguish this cost from the site cost. 3. The Evolution of the Efficient Market Organization. By changing c e or k, innovations can change the efficient mar- ket organization (Prediction 3). This evolution of the efficient 14. Lawyers, freelancers, and contractors actively advertise their services. However, they should be interpreted as sellers of services instead of workers. The labor market may have a tighter capacity constraint than the goods market, because a vacancy is typically filled by only one worker. Although this capacity constraint may affect the market organization, the effect is not clear. Even in busi- nesses with severe capacity constraints, such as restaurants, sellers are often the organizers of trade. Moreover, the trading pattern can change over time without obvious changes in the capacity constraint, as we alluded to in footnote 8 with the example of vacuum cleaners. organization can be traced out in Figure I. Suppose that c e is so high initially that the economy lies in the parameter region above the curve f \u22121 (c e ) \u2212 k and below the curve f(c e + k). In this region, only market e is viable. If k is fixed while c e falls to the left of the curve f \u22121 (c e ) \u2212 k, market n becomes viable but is still dominated by market e. If c e falls further to the left of c d (k), the efficient organization changes to market n. Similarly, if the site cost k is so high initially that c d (k) < c e , market e is efficient. Be- cause k decreases while c e is fixed, the curves in Figure I shift up. Their intersection moves up along the 45-degree line, resulting in a higher threshold c d (k). If the fall in k is sufficiently large so that c d (k) > c e , then market n becomes efficient. In the goods market, new technologies reduce the site cost by enabling sellers to keep inventory at a lower cost and lower depreciation than before. They can also reduce sellers' participa- tion cost by reducing the amount of goods that need to be pur- chased in advance of sales. A related but different innovation is the adoption of just-in-time production that shifts the production cost from the prematch stage to the postmatch stage, which is studied in detail in Section III.C. In addition, regulatory changes can reduce the cost of setting up a business, and new information technologies can reduce a seller's cost of learning about the trade. All these changes have the tendency to move the market from seller-organized trade to buyer-organized trade. In the labor mar- ket, sectoral changes move firms from manufacturing to services that require smaller costs to set up and maintain. For example, a job in software design is easier to set up and more flexible than a job on an assembly line. Reflecting this contrast, an assembly-line worker rarely advertises his or her labor service, but a software designer might do so. Online trade is a prominent example of the reduction in the site cost and the participation cost. Setting up and maintaining a physical shop can be very costly. In comparison, online stores are much less costly to create and monitor. As e-commerce develops, it may become increasingly common for buyers to specify their demand on websites and sellers to search to meet such demand. This implication is also relevant for the teaching service of sub- jects for which the site cost and the instructors' participation cost are low. For such subjects, learners may post their needs online while instructors search. Similarly, posting jobs online can signif- icantly reduce the vacancy cost, so we may see an increase in the advertisements for job-wanted relative to help-wanted.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "III.C. Just-in-Time Production", "text": "The technology of just-in-time production enables a seller to shift part of the participation cost from the prematch stage to the postmatch stage. How does this technology affect the efficient market organization? This question requires a separate analysis because the shift in the cost is not just a reduction in c e -it also changes the ex post match surplus. Precisely, let \u03b4 \u2208 [0, 1) be the fraction of an elastic individual's participation cost postponed to the postmatch stage so that the individual's participation cost becomes (1 \u2212 \u03b4)c e . Utility of consumption net of the postmatch production cost is U = 1 \u2212 \u03b4c e . 15 Let C i U be the participation cost of an individual on side i and KU the site cost, so that C i and K are the costs normalized by U. Then, (10) C e = (1 \u2212 \u03b4) c e 1 \u2212 \u03b4c e , C n = c n 1 \u2212 \u03b4c e , K = k 1 \u2212 \u03b4c e . After replacing (c e , c n , k) by (C e , C n , K), the analysis in Section II.B is valid for all \u03b4 \u2208 [0, 1). The efficient ratio of elastic to inelastic individuals is \u03b8 e (C e + K) in market e and \u03b8 n (C e ) in market n. Welfare per inelastic individual is W i in market i, where W e = f (Ce + K) \u2212 C n U W n = f \u22121 (Ce) \u2212 C n \u2212 K U . Market n dominates market e if and only if W n > W e , that is, if and only if G(C e , K) > 0 where G is defined in equation (8). To simplify the analysis, assume k > 0. Adapting the proof of Theorem 1, we have G(C e , K) > 0 if and only if C e \u2208 (0, C d (K)), where C d solves equation (9). Moreover, C e < C d (K) can be written as c e < c d , where c d now denotes the unique solution to (1 \u2212 \u03b4) c d 1 \u2212 \u03b4c d = C d k 1 \u2212 \u03b4c d . Denote f e (c e , \u03b4) = (1 \u2212 \u03b4c e )f(C e + K) and f n (c e , \u03b4) = (1 \u2212 \u03b4c e )[f \u22121 (C e ) \u2212 K], where C e and K are functions of (c e , k) defined by (10). Since W e = f e (c e , \u03b4) \u2212 c n and W n = f n (c e , \u03b4) \u2212 c n , then market n dominates market e if and only if f n > f e . 15. The joint surplus U is independent of which side pays the postponed cost, \u03b4c e . So are social welfare and the efficient market organization. Figure II depicts the effect of increasing \u03b4 from \u03b41 = 0 to \u03b42 = 0.5, with k = 0.015. It uses the telephone matching function in Example 1 with A = 1. After the increase in \u03b4, both curves f e and f n shift up, so each market organization becomes viable in a wider region of the parameters (c e , c n ). Social welfare increases un- der each market organization. Moreover, the curve f n shifts up by more than the curve f e . The new intersection between the curves is below the 45-degree line, and the threshold c d increases. 16 There- fore, the delay of the production cost to postmatch increases the likelihood that the market organized by the inelastic side is ef- ficient. This effect contrasts with a reduction in c e alone, which does not change c d , and with a reduction in k, which moves up the intersection between the curves along the 45-degree line. The increase in \u03b4 improves social welfare by saving the cost \u03b4c e when an elastic individual fails to match. This cost saving increases the gain to an elastic individual, induces higher entry of elastic individuals, and increases the measure of matches. To learn why an increase in \u03b4 benefits market n more than market e, recall that the trading cost to an elastic individual in market e consists of both the participation cost and the site cost. Because the site cost does not change with \u03b4, the delay in an elastic indi- vidual's participation cost reduces the individual's expected cost of trade less than one for one in market e. In contrast, in market n, the trading cost to an elastic individual consists of only the participation cost, so the delay in the individual's participation cost reduces the expected cost of trade one for one. Thus, the de- lay increases welfare by more in market n than in market e. As \u03b4 increases sufficiently, more products will be made on spot in a market organized by the inelastic side.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "IV. ASYMMETRIC MATCHING AND TRADING PLATFORMS", "text": "Given the rich results obtained so far under symmetric match- ing, why is it useful to analyze asymmetric matching? There are several reasons. First, matching is asymmetric in well-known en- vironments, such as the urn-ball matching function in Example 1. Second, as explained later, asymmetry in the matching function reflects fundamental features of the matching process, and exam- ining the asymmetry can enhance the understanding of why the market organization can differ across markets. Third, examining asymmetric matching can help us understand how the efficient organization changes over time in a given market as the relative search efficiency between the two sides changes. For example, pa- rameter A in the telephone matching function in Example 1 is the relative search efficiency of market organizers to visitors. A decrease in A from 1 changes the matching function from a sym- metric function to one that favors the long side. This change may lead to a different market organization. The analysis of asymmet- ric matching also leads to a discussion of trading platforms. The following theorem is proven in Appendix B: THEOREM 2. Assume that the matching function is asymmetric. Asymmetric matching affects the two cutoff levels k a and k b . When the matching function favors the short side or does not favor the long side strongly (i.e., when k b < k), Figure I is still valid for k 0. However, when the matching function favors the long side strongly so that k a > k, the efficient organization is depicted in Figure III, where the relative position of the two curves f(c e + k) and f \u22121 (c e ) \u2212 k is switched from Figure I. For c e > c d , the curve f(c e + k) lies below the curve f \u22121 (c e ) \u2212 k, in which case market n dominates market e. For c e < c d , the curve f(c e + k) lies above the curve f \u22121 (c e ) \u2212 k, in which case market e dominates market n. For all c e = c d , there are more sites than visitors under the efficient organization. Under asymmetric matching, the short side of the market is determined by the same consideration as under symmetric match- ing. That is, the social marginal benefit of entry by an elastic in- dividual should be equal to the social marginal cost of entry. The asymmetry in the matching function affects the threshold, c d , by affecting the marginal benefit of entry. Given c d , it is still true that the elastic side is on the short side if and only if the side's par- ticipation cost exceeds c d . In contrast to symmetric matching, the short side is not always the efficient side to organize the market. If the matching function favors the short side, the asymmetry in the matching function reinforces the site cost to favor the short side as the efficient organizers of the market. In this case, the short side should organize trade even when the site cost is zero. This result contrasts with symmetric matching under which the two organizations are welfare equivalent when k = 0. If the matching function favors the long side, the asymmetry in the matching function and the site cost have opposite effects on the efficient organization. If the matching function does not favor the long side strongly, that is, if k b < k, then the site cost dominates the asymmetry in the matching function. In this case, the short side should organize trade, and other results are qualita- tively similar to those under symmetric matching. If the matching function favors the long side strongly, that is, if k a > k, then the asymmetry in the matching function dominates the site cost. In this case, the long side should organize trade. The long side is inelastic if c e > c d and elastic if c e < c d , as in Figure III. The asymmetry in the matching function reflects fundamen- tal features of the matching process. To relate the market or- ganization to these features, it is useful to contrast the micro- foundations of the urn-ball function and the telephone matching function. For this purpose, we treat a match as a meeting pro- cess followed by selection, e.g., job applications followed by inter- views. The urn-ball matching function arises endogenously under directed search from one-to-many meetings. That is, market orga- nizers direct visitors' search, and each site can meet many visitors before choosing one to form a match (see Herreiner 1999;Julien, Kennes, and King 2000;Burdett, Shi, and Wright 2001). In such an environment, matching failures arise from the lack of coordi- nation among visitors. The difficulty to coordinate is lower and the number of matches is higher if the short side directs search than if the long side directs. In contrast, the telephone matching function arises from one-to-one meetings (see Burdett et al. 1995). If the two sides have the same search efficiency, the number of matches is the same regardless of which side organizes trade. Moreover, increasing the relative efficiency of visitors to organizers makes the matching function favor the long side. With this exposition, Theorem 2 has the following implication: \u2022 Prediction 4: If the meeting process has one-to-many meetings, the market should be organized by the short side even if the site cost is zero. If matches are generated by one-to-one meetings and if the organizers have sufficiently lower search efficiency than the visitors, then the market should be organized by the long side. In both cases, the short side is elastic if and only if the elastic side's partici- pation cost is high. In the labor market, since one-to-many meetings are com- mon, Prediction 4 implies that firms should organize trade and be on the short side. This implication strengthens Predictions 1 and 2, because it holds even when the site cost is zero. In contrast, in the marriage/dating market, one-to-one meetings are common, and so the number of matches can be approximated by the tele- phone matching function. In this market, trade can be organized by either side, depending on the site cost and the relative elas- ticity of the two sides. If the site cost and the relative elasticity of the organizers to visitors are both small, then trade should be organized by the long side. The analysis sheds light on how trading platforms affect the efficient market organization. Platforms are common in online trading, where a third party creates trading sites and charges fees for usage. Network externalities are an important feature of trading platforms, that have been emphasized by Rochet and Tirole (2003). We examine other differences between a platform and a traditional trading site (see Online Appendix F). On the surface, a significant difference of a platform from a traditional trading site is that the platform fees shift the cost from the site cost to the participation cost. However, this feature alone does not change the efficient market organization, because the planner can use transfers between the sides to neutralize the effect of the fees. For an efficient organization, the relevant changes are technological and, particularly, changes to the matching function. For example, in the marriage/dating market, the Internet can change the meeting process from one-to-one meetings to one-to- many meetings, because a posted profile can attract more than one person before one is selected. At the same time, the Internet may increase the search efficiency of a visitor relative to an organizer, since a visitor can visit many sites quickly. These changes affect the matching function in opposite directions. While the change to one-to-many meetings favors the short side, the increase in a visitor's relative search efficiency favors the long side. A trading platform can also change the cost and the capacity of sites. By reducing the site cost, a platform increases the likelihood that the market will be organized by the inelastic side, as analyzed in Section III.B. By increasing the site capacity per organizer, a platform effectively reduces the inelasticity of sites in the market organized by the inelastic side and, hence, increases the efficiency of such a market.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "V. HETEROGENEOUS INDIVIDUALS AND COEXISTENCE OF MARKETS", "text": "Markets organized by the two sides coexist for assets in re- ality. The benchmark model does not generate such coexistence generically. For the coexistence, we introduce heterogeneity on the inelastic side. 18 Suppose that net surplus in a match is u H for a fraction \u03c6 of inelastic individuals and u L for the remaining frac- tion of inelastic individuals. The type of an inelastic individual is public information. Recall that u is the utility of consumption minus the postmatch production cost. If inelastic individuals are buyers, then a type H individual is a buyer with higher utility; if inelastic individuals are sellers, then a type H individual is a seller whose product has higher quality or whose postmatch pro- duction cost is lower. Assume u H > u L > k + min {c n , c e } so that a trade is beneficial even for a low-value inelastic individual if participation costs are sufficiently low. Normalize the costs by the joint surplus in a match as C i (u) = c i u and K (u) = k u , where i \u2208 {e, n}. We shorten the notation C i (u j ) to C ij and K(u j ) to K j , where i \u2208 {e, n} and j \u2208 {L, H}. Also, let \u03c6 H = \u03c6 and \u03c6 L = 1 \u2212 \u03c6. Let market ij denote the market that is organized by side i and has type j inelastic individuals, where i \u2208 {e, n} and j \u2208 {L, H}. In market ij, let \u03b8 ij be the ratio of elastic to inelastic individuals, and W ij be social welfare per inelastic individual. Modifying the analysis in Section II.B, we have: W ej \u2261 f C ej + K j \u2212 C nj u j , W nj \u2261 f \u22121 C ej \u2212 K j \u2212 C nj u j . Market ej dominates market nj if W ej > W nj , market nj dominates market ej if W nj > W ej , and the markets are welfare equivalent 18. One may introduce heterogeneity on both sides as in Shi (2001), who has examined efficient sorting with search frictions but assumed that firms direct search. Although this extension is exciting, it would take the analysis too far afield and hence is left for future research. if W ej = W nj . The inequality W nj > W ej can be written as G(C ej , K j ) > 0, where G is defined in equation (8). As in the benchmark model, the case W ej = W nj occurs only in a measure-zero set of parameters. Thus, it is generically inefficient to have two market organizations for the same type of inelastic individuals. However, the efficient market organization can differ between the two types of inelastic individuals. As before, we assume that for each j \u2208 {L, H}, max {W ej , W nj } > 0 so that at least one market is viable for each j. Equivalently, this assumption is (11) C nj < max{ f C ej + K j , f \u22121 C ej \u2212 K j } for j \u2208 {L, H}. Maintain Assumption 3 so that G(x, y) = 0 has a unique generic interior solution of x for any given y \u2208 (0, 1 \u2212 x). To economize on space, we assume k > k b if the matching function favors the long side and k > 0 if the matching function is symmetric. Under Assumption 3, G(C ej , K j ) > 0 if and only if C ej < c d (K j ), where c d (K) is defined similarly to equation (9) by (12) f (cd + K) = c d . Using the definitions of (C ej , K j ), we express the condition C ej < c d (K j ) as (13) c e u j < c d k u j . Note that c d (K) defined by equation (12) is a strictly decreasing function, and so c d ( k u j ) strictly increases in u j for any given k. If equation (13) is satisfied for j = L, then it is also satisfied for j = H. There are parameter values with which equation (13) is satisfied for j = L and violated for j = H. We summarize these results in the following theorem and omit the proof:  If u L c d ( k u L ) < c e < u H c d ( k u H ), market nH organized by type H inelastic individuals and market eL organized by elastic individuals coexist. The intuitive explanation for these results is similar to those in Sections III and IV. In the case we focus on here, the match- ing function does not favor the long side strongly, so trade should be organized by the short side. If elastic individuals' participa- tion cost is low, it is socially desirable to have a relatively large measure of elastic individuals enter the market to be on the long side. This is achieved by having inelastic individuals incur the site cost to organize all trade in two submarkets, one by each type of inelastic individuals. If elastic individuals' participation cost is high, the social optimum asks elastic individuals to be on the short side to organize all trade in two submarkets, one for each type of inelastic individuals to visit. When elastic individuals' participa- tion cost is intermediate, high-value inelastic individuals organize market nH while low value inelastic individuals visit market eL organized by the elastic side. By incurring the site cost to orga- nize trade, high-value inelastic individuals are on the short side of market nH so that they have relatively high matching probability to realize the high value. In contrast, low-value inelastic individ- uals are on the long side of market eL so that elastic individuals in the market trade with relatively high probability. The expected surplus for an elastic individual in market eL is the same as in market nH. 1. The Asset Market. The coexistence of the two organiza- tions is common for assets and durables. For many assets, includ- ing houses and artwork, sellers are relatively less elastic than buyers. High-end sellers may pay brokers to sell their assets and low-end sellers may wait for buyers to come. For financial assets, a main determinant of a seller's type is the need for liquidity. Asset sellers who have urgent needs for liquidity may pay the cost to actively seek buyers, whereas sellers who do not have immediate liquidity needs may wait for buyers to contact them. We list this implication of Theorem 3 as follows: \u2022 Prediction 5: Asset holders with high liquidity needs are likely to organize trade while asset holders with low liquidity needs are likely to await buyers to initiate trade. An example is the market for repurchase agreements, a short- term loan market where one party sells collateral securities tem- porarily for money and agrees to buy back the collateral at a preset price and time. The amount of outstanding loans in this market is in trillions of dollars despite the large fall in the [2008][2009] recession. In this market, trading is decentralized. An owner of the securities intends to hold the securities to maturity instead of selling them outright before maturity. However, the owner may have temporary needs to borrow money, for example, to close another deal. Such an owner can sell the securities in the repo market. On the other side, a buyer of the securities may need the securities to temporarily cover a short position or simply to earn interest through lending. A repurchase agreement is called a repo to the seller of collateral securities (the borrower) and a reverse to the buyer of collateral (the lender). A repo is typically initiated by the borrower and a reverse by the lender. The cost of initiating a deal can be treated as a site cost. In this mar- ket, the supply is endogenous on both sides, but sellers are rel- atively less elastic because their needs for short-term liquidity are more pressing. In contrast, a depository institution that has unexpected idle money may or may not enter the repo market to lend. Moreover, borrowers are heterogeneous in the need for liquidity. Prediction 5 is consistent with the observation on dealers in the repo market for U.S. Treasury securities. These dealers initiate both repos and reverses. Relative to other participants, dealers are more likely to face liquidity shortage in the short term, such as overnight. As a result, dealers consistently borrow more money doing overnight and open repos than they lend doing overnight and open reverses (Stigum and Crescenzi 2007). In term repos and reverses, which have longer maturities than overnight and open repos, the liquidity needs change for a dealer relative to a nondealer. As a result, the trading pattern is reversed, where dealers consistently lend more money doing term reverses than they borrow doing term repos. Theorem 3 also helps us understand the growth of the repo market induced by innovations and regulatory reforms. A main innovation is the general collateral finance introduced in 1998 by the Fixed Income Clearing Corporation, a clearing agency for U.S. government securities. This system allows borrowers and lenders to settle their daily transactions on a net basis, instead of the gross settlement used previously. It also increases the flexibil- ity for borrowers to substitute the collateral in a contract with similar securities in case they fail to find the specified collateral in time. Partly because of these changes, outstanding loans in the repo market multiplied between 2000 and 2006. For dealers in the market for the U.S. government securities, the amount of borrowing through repos and the amount of lending through re- verses grew. The difference between the two-net repo financing by dealers-tripled over these six years (Stigum and Crescenzi 2007). To use Theorem 3 to explain this growth, note that net- ting reduces the settlement cost and hence increases the match surplus u. In contrast, the flexibility in collateral substitution re- duces the site cost to initiate a contract, since a borrower does not necessarily need to have the particular securities in place to initiate a repo. The increase in u and the reduction in k increase the gains from trade and stimulate the growth of the market. As analyzed before, the reduction in k increases the benefits for the inelastic side to be the market organizers relative to the elastic side. With dealers being interpreted as being relatively inelastic (see above), this may explain why net repo financing by dealers increased.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "VI. MARKET IMPLEMENTATION", "text": "Can a realistic market mechanism implement the social op- timum? If the answer is negative, then we can investigate what corrective policies are needed. If the answer is affirmative, it gives further support for the mapping between our results and reality. In addition, a market mechanism reveals how prices divide the match surplus. We start with the benchmark model where all individuals on each side are homogeneous. Consider a realistic mechanism in which market organizers post prices to compete for customers (e.g., Peters 1991;Montgomery 1991). Each price p is associated with a ratio \u03b8 , that is, the measure of elastic in- dividuals per inelastic individual at the particular price. When an organizer chooses p or when a visitor chooses which orga- nizer to visit, the individual takes into account the dependence of \u03b8 on p. In this sense, the terms of trade (p, \u03b8 ) direct visitors' search. To simplify the description, we refer to the group of or- ganizers who post the same p together with the visitors to such organizers as submarket (p, \u03b8 ). In a submarket, the matching function M determines the measure of matches. In the equilib- rium, the ratio \u03b8 must be consistent with individuals' optimal choices. In both market e and market n, an organizer chooses (p, \u03b8 ) to maximize an inelastic individual's payoff, subject to the constraint that an elastic individual's net expected profit of participating in the market is nonnegative. In market e, this choice solves: 19 (14) max ( p,\u03b8) F (\u03b8 ) (1 \u2212 p) \u2212 c n s.t. F (\u03b8 ) \u03b8 p c e + k. In market n, the choice of an organizer (i.e., an inelastic individ- ual) solves: 20 (15) max ( p,\u03b8) \u03b8 F 1 \u03b8 (1 \u2212 p) \u2212 c n \u2212 k s.t. F 1 \u03b8 p c e . In both markets, the quintessential feature is that an individual makes the trade-off between the price and the matching probabil- ity. Note that the surplus division is endogenous, with the elastic side's share being equal to the price. In equations (14) and (15), the constraint holds with equality because of free entry of elastic individuals. Solving the price p from such an equality, we can verify that \u03b8 e (c e + k) defined in equation (2) is the solution to equation (14) and \u03b8 n (c e ) defined in equation (6) is the solution to equation (15). Moreover, the maximized ex- pected payoff of an inelastic individual is W e = f(c e + k) \u2212 c n in market e and W n = f \u22121 (c e ) \u2212 c n \u2212 k in market n. Each inelastic individual chooses to participate in the market that yields the rel- atively higher payoff. Because W n and W e in the equilibrium are the same as in the social optimum, the market organization in the equilibrium is the same as in the social optimum. Thus, we have proven the following proposition: PROPOSITION 1. In the economy where each side of the market con- tains homogeneous individuals, the market equilibrium in which individuals compete to offer (p, \u03b8 ) to direct search im- plements the social optimum. 19. When an elastic individual chooses whether to participate in the market, the individual also chooses which submarket to enter. Thus, the participation cost is not sunk at the time of choosing the submarket. This is why c e appears in the constraint in equation (14). 20. It is immaterial which side's payoff is used as the objective function. In particular, in market n, we can formulate the choice problem as max (p, \u03b8)  [F( 1 \u03b8 ) p \u2212 c e ] s.t. \u03b8 F( 1 \u03b8 ) (1 \u2212 p) \u2212 c n \u2212 k = n , where n is an inelastic individual's payoff in the market. This problem is dual to the primal problem equation (15). In the equilibrium, free entry of elastic individuals determines n by forcing such an individual's payoff to zero. With such n , the dual problem and the primal problem have the same solution. Given the market organization, it is well known that directed search can induce efficient entry of elastic individuals by inter- nalizing matching externalities (e.g., Moen 1997;Acemoglu and Shimer 1999;Shi 2001). The equilibrium price divides the match surplus endogenously in a way that compensates each side of the market with the side's share in the matching function-a condi- tion established by Hosios (1990). What is new in Proposition 1 is that directed search also induces the efficient market organiza- tion. In both market e and market n, competitive entry of elastic individuals drives their expected gain from participation to zero. As a result, social welfare in each market is equal to an inelas- tic individual's expected gain from participation. The market that maximizes this expected gain wins the competition because indi- viduals choose which market to participate in. The implementation of the efficient allocation can be extended to the economy in Section V where there are two types of inelas- tic individuals. In this extension, the markets are indexed by ij, where i \u2208 {e, n} is the side of the organizers and j \u2208 {L, H} is inelastic individuals' type. In market ij, the terms of trade are (p ij , \u03b8 ij ) across submarkets. Elastic individuals enter the market competitively, and each earns zero net expected profit from entry. An analysis similar to the foregoing shows that the equilibrium allocation coincides with the social optimum. 1. Unequal Welfare Weights. In the analysis so far, we have used an egalitarian social welfare function. If the planner puts un- equal weights on the two sides of the market, are the results on the social optimum robust, and does the equilibrium of directed search still implement the social optimum? The answer is affirma- tive to the first question, but positive to the second question only under a qualification. The following proposition lists the results (see Online Appendix E for a proof): PROPOSITION 2. Let \u03bb \u2208 (0, \u221e) be the welfare weights for the elastic side relative to the inelastic side. The efficient al- location under unequal welfare weights \u03bb = 1 is the same as under equal welfare weights and can be implemented by the above equilibrium with directed search. However, equi- librium welfare is the same as in the social optimum only when \u03bb 1. When \u03bb > 1, the expected surplus is inefficiently low for elastic individuals and inefficiently high for inelastic individuals. When the two sides of the market have different welfare weights, the division of the match surplus matters for social welfare. The planner chooses this division, in addition to \u03b8 , to maximize social welfare under individual rationality constraints. Because utility is transferable, the planner can increase social welfare by shifting the match surplus from the side with lower welfare weights to the side with higher welfare weights. That is, the planner maximizes social welfare by setting the expected surplus of participating in the market to zero for the side that has lower welfare weights. With this efficient division of the sur- plus, social welfare depends only on the sum of expected match surpluses. Maximizing this sum, the efficient \u03b8 under unequal welfare weights is identical to the one under the egalitarian wel- fare function. So is the efficient market organization. Moreover, the foregoing equilibrium with directed search induces the same entry of elastic individuals (\u03b8 ) and the same market organization as in the social optimum. Although the equilibrium allocation is efficient, equilibrium welfare may differ from the social optimum. Because competitive entry of elastic individuals drives expected surplus to zero for these individuals, equilibrium welfare is the same as in the social optimum only when the elastic side's welfare weights are less than or equal to the inelastic side's. When the elastic side has higher welfare weights than the inelastic side, the equilibrium generates inefficiently low welfare for elastic individuals and for the economy as a whole. To restore efficient welfare, a planner can use a price subsidy to elastic individuals financed by a tax on inelastic individuals, together with permits that restrict entry of elastic individuals. A subsidy to elastic individuals alone is ineffective because it will be competed away.", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "VII. CONCLUSION", "text": "Return to the question in the title: should buyers or sellers organize trade in a frictional market? The answer depends on the relative elasticity of the supply of buyers to sellers, the partici- pation cost of the elastic side, the cost of a trading site, and the (a)symmetry in the matching function. When the site cost is posi- tive or the matching function favors the short side of the market, the market should be organized by the short side. When the site cost is sufficiently small and the matching function strongly favors the long side, the market should be organized by the long side. In both cases, the elastic side is short if and only if the elastic side's participation cost exceeds a threshold that decreases in the site cost. The efficient organizers can change from the elastic to the inelastic side if the site cost or the elastic side's participation cost falls sufficiently, or if the search efficiency of organizers relative to visitors falls sufficiently. These results provide a unified explana- tion for why trade has often been organized by sellers in the goods market and by buyers (firms) in the labor market, and how the efficient organization changes with technological advances. More- over, we introduce heterogeneity on the inelastic side to show that markets organized by the two sides can coexist, as in the asset market. Finally, we formulate a directed-search equilibrium to implement the social optimum. A useful extension of the article is to introduce private infor- mation. For the asset market where sellers have private informa- tion about the assets, Guerrieri, Shimer, and Wright (2010) and Chang (2018) assume that uninformed individuals (buyers) post the trading mechanism to direct search. Such a market faces the problem of adverse selection. In contrast, Delacroix and Shi (2013) assume that informed individuals post the trading mechanism to direct search. Such a market faces a signaling problem. In gen- eral, the decision to trade may reveal information, for example, Wolinsky (1990). Different market organizations may differ in the ability to separate heterogeneous individuals and mitigate infor- mation frictions. The current article shows that different market organizations also differ in the ability to mitigate matching fric- tions and trading costs. It is interesting to examine how these two roles of a market organization interact. Specifically, the two sides can differ in both how elastic and how informed they are. It is an open question which combination of these two characteristics makes a side the efficient market organizers.   Proof. Under Assumption 1, the properties of h and f in (i) and (ii) of Lemma 1 can be verified from equations (2)  For (iv), suppose A < 1 and temporarily denote Q(x) = A \u2212 x \u2212 f(x). Then, Q (x) < 0 for x \u2208 (0, A) and Q(x) = A \u2212 x for all x \u2208 [A, 1]. Since Q(0) = A \u2212 1 < 0, Q(A) = 0 and Q (A) < 0, there is a unique x 0 \u2208 (0, A) as defined in equation (16)  For (v), the definition of \u03b8 e (x) in equation (2) implies \u03b8 e (F (\u03b8 )) = \u03b8 for all \u03b8 \u2208 [0, \u221e). The definition of f implies f(F (\u03b8 )) = h(\u03b8 e (F (\u03b8 ))) = h(\u03b8 ), as stated in equation (17). Setting \u03b8 = h \u22121 (x), we have f(F (h \u22121 (x))) = h(h \u22121 (x)) = x for all x 0. Thus, f \u22121 (x) = F (h \u22121 (x)) for all x 0 , as stated in equation (18). , where G is defined in equation (8)  . Since h \u22121 (x) = 1 \u03b8 n( x) by equation (6), then \u03b8 e (x) = \u03b8 n (x). In addition, For x > A, f (x) = f (x) = 0. (iii) f(x) 1 \u2212 x for all x \u2208 [0, 1],", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "A.2. Properties of the Matching Function", "text": "f \u22121 (x) = F h \u22121 (x) = F 1 \u03b8 e (x) = h (\u03b8e (x)) = f (x) for all x 0. The first equality is equation (18), the second equality was just de- rived above, the third equality uses D ( ", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "A.3. A Lemma Related to Assumption 3", "text": "Examine the function G(c, k) defined in equation (8). A so- lution c to G(c, k) = 0 is generic if G c (c, k) = 0, and interior if c \u2208 (0, \u00af c), where  and G(c, k) < 0 for all (cd, \u00af c); otherwise, c d would not be the only interior solution to G(c, k) = 0. In particular, G(\u03b5, k) > 0 where \u03b5 > 0 is sufficiently small. Taking the limit \u03b5 \u2192 0 yields G(0, k) 0. Similarly, it is necessary to have G ( \u00af c \u2212 \u03b5, k) < 0 where \u03b5 > 0 is sufficiently small, which leads to the condition (v) The urn-ball matching function in Example 1 yields: where . Moreover, A = 1. With f and f \u22121 above, we have: and use the definition of \u03b8 e (c + k) to substitute To see whether the organizers should be on the short or the long side, recall that the site-visitor ratio is \u03b8 e when the elastic side organizes trade and 1 \u03b8 n when the inelastic side or- ganizes trade. Also, (iii) in Lemma 2 implies \u03b8 e (cd + k) = 1 Moreover, \u03b8 e (c + k) < 0 by equation (2) and \u03b8 n (c) < 0 by equation (6). Since G c (cd, k) < 0, then 1 If c e \u2208 (0, c d ), inelastic individuals should organize trade, in which case the site-visitor ratio is 1 \u03b8 n( c e ) c), elastic individuals should organize trade, in which case the site-visitor ratio is \u03b8 e (c e + k) < \u03b8 e (c d + k) < 1. Thus, for all c e \u2208 (0, \u00af c) with c e = c d , the organizers should be on the short side. Finally, since f is a decreasing function, it is evident from equation (9) that c d is a decreasing function of k. Proof of Theorem 2. Assume that the matching function is asymmetric, as in the theorem. Then, k = k 0 as required by As- sumption 3. (i) Suppose that the matching function favors the short side. Then, A = 1, k 0 0 and f(k) A \u2212 k (see Lemma 2). The necessary condition for Assumption 3, Since G c (cd, k) < 0, the same proof as that for Theorem 1 shows that for all c e \u2208 (0, \u00af c), the organizers should be on the short side if the matching function favors the short side. (ii) Now suppose that the matching function favors the long side. Then, A 1 and k 0 0. Consider first the necessary condition for Assumption 3, (k \u2212 k 0 )[A \u2212 k \u2212 f(k)] 0. If k > k 0 , then it is necessary to have f(k) A \u2212 k which, in turn, requires k > x 0 (see Lemma 2). If k < k 0 , then it is necessary to have f(k) A \u2212 k which, in turn, requires k < x 0 (see Lemma 2). Thus, to satisfy Assumption 3, it is necessary to restrict attention to k < k a or k > k b , where k a \u2261 min {k 0 , x 0 } and k b \u2261 max {k 0 , x 0 }. If k > k b , then k > k 0 , and so G c (cd, k) < 0. The analysis and the result in this case are the same as the above, where the matching function favors the short side. That is, trade should be organized by the elastic side if c e > c d and by the inelastic side if c e < c d . Also, the organizers should be on the short side regardless of whether c e > c d . If k < k a , then k < k 0 , and so  where y is the largest integer that does not exceed y. If k = 0 and the matching function is symmetric, welfare is independent of which side organizes trade. (i) If k > k 0 , the market should be organized by the inelastic side when c e \u2208 1 and by the elastic side when c e \u2208 2 . (ii) If k < k 0 , the market should be organized by the elastic side when c e \u2208 1 and by the inelastic side when c e \u2208 2 . In (i) and ( ", "title": "Should Buyers or Sellers Organize Trade in a Frictional Market?*", "file_name": "Shi and Delacroix - 2018 - Should Buyers or Sellers Organize Trade in a Frict.pdf"}
{"section": "Abstract", "text": "Armies, churches, organizations, and communities often engage in activities-for example, marching , singing, and dancing-that lead group members to act in synchrony with each other. Anthropologists and sociologists have speculated that rituals involving synchronous activity may produce positive emotions that weaken the psychological boundaries between the self and the group. This article explores whether synchronous activity may serve as a partial solution to the free-rider problem facing groups that need to motivate their members to contribute toward the collective good. Across three experiments, people acting in synchrony with others cooperated more in subsequent group economic exercises, even in situations requiring personal sacrifice. Our results also showed that positive emotions need not be generated for synchrony to foster cooperation. In total, the results suggest that acting in synchrony with others can increase cooperation by strengthening social attachment among group members.", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "Method", "text": "An experimenter led 30 participants (60% female; mean age 5 20, SD 5 2.0) in groups of 3 on walks around campus. In the synchronous condition, participants walked in step. In the control condition, they walked normally. After their walk, par- ticipants completed a questionnaire designed to convince par- ticipants that they had finished the experiment. In an ostensibly separate experiment, a second experimenter conducted the Weak Link Coordination Exercise, which models situations in which group productivity is a function of the lowest level of input (Weber, Camerer, & Knez, 2004;Weber, Rottenstreich, Camerer, & Knez, 2001). In this exercise, each participant chooses a number from 1 to 7 without communi- cating. As Table 1 shows, payoffs increase as a function of the smallest number chosen and decrease with the distance between the participant's choice of number and the smallest number chosen in the group. Every participant would do best if all group members chose the number 7, but if participants fear that some individual ''weak link'' may not choose a high number, they might rationally choose lower numbers. Because misperceptions are costly, the game measures expectations of cooperation. Participants played six rounds of the game and were paid based on the outcomes of a round chosen at random following the completion of the last round. Participants could not talk during the exercise. Each participant wrote down his or her selection for each round, after which the experimenter surveyed the responses, announced the minimum number selected, and instructed participants to write down a number for the next round. Afterwards, participants answered ''How connected did you feel with the other participants during the walk?'', ''How much did you trust the other participants going into the exer- cise?'', and ''How happy do you feel?'' using 7-point Likert scales (1 5 not at all, 7 5 very much).", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "Results and Discussion", "text": "Consistent with our synchrony-cooperation hypothesis, partici- pants who walked in step chose higher numbers in the first round than did those who did not walk in step (M 5 5.4, SD 5 1.6 vs. M 5 3.6, SD 5 1.1), t(24.6) 5 2.09, p rep 5 .92, d 5 1.29. Choices in subsequent rounds were not significantly different. Participants in the synchronous condition felt more connected with their counterparts than did those in the asynchronous condition (M 5 4.5, SD 5 1.4 vs. M 5 2.9, SD 5 1.9), t(28) 5 2.61, p rep 5 .97, d 5 0.96, and trusted their counterparts more (M 5 5.6, SD 5 1.3 vs. M 5 4.1, SD 5 1.1), t(28) 5 3.01, p rep 5 .97, d 5 1.25. Contrary to the mechanism of collective effervescence, participants in the synchronous condition did not feel happier than did those in the control condition (M 5 4.7, SD 5 1.5 vs. M 5 4.8, SD 5 0.8).", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "STUDY 2", "text": "In Study 2, we explored whether synchrony could boost coop- eration above and beyond the effects of two established sources of group cohesion: common identity and common fate (e.g., Brewer & Silver, 1978;Tajfel, Flament, Billig, & Bundy, 1971;Tajfel & Turner, 1986). In the manipulation phase of our ex- periment, the experimenter verbally referred to the participants as a group, and group members participated in a task together (common identity). Group members also faced a common payoff for their performance (common fate). Thus, for synchrony to be shown to affect cooperation, its effects had to reach beyond common fate and common identity.", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "Method", "text": "In groups of 3, 96 participants (56% female; mean age 5 21 years, SD 5 1.9) listened to music through headphones while performing tasks requiring differing degrees of synchrony. Each task involved handling plastic cups and listening to music. Participants were told that they would be paid between $1 and $5 based on their group performance during this ''cups and music'' task and that all members of their group would receive the same payment. The music in this study was ''O Canada,'' a song chosen to test whether synchrony can induce cooperation when the soundtrack to the group experience is an out-group anthem (our participants were residents of the United States). Groups were randomly assigned to one of four conditions: In the control condition (i.e., the no-singing, no-moving condition), participants listened to ''O Canada,'' held a plastic cup above the table, and silently read the lyrics to the anthem. In the synchronous-singing condition, participants listened to the an- Note. In this exercise, payoffs increase as a function of the smallest number chosen by a group member and decrease with the distance between the par- ticipant's choice and the minimum value chosen in the group. them, held the cup, and sang the words ''O Canada'' at the appropriate times. In the synchronous-singing-and-moving condition, participants listened to the anthem, sang the words ''O Canada,'' and moved cups from side to side in time with the music. In the asynchronous condition, participants sang and moved cups, but participants each listened to the anthem at a different tempo, causing them to move their cups at different rates and sing ''O Canada'' at different times. Participants in all conditions were told that they might hear the same or different versions of ''O Canada,'' but only participants in the asynchro- nous condition actually heard different versions. We predicted that participants in the two synchrony conditions would coop- erate more in the subsequent Weak Link Coordination Exercise described in Study 1 than would participants in the control or asynchronous conditions. While participants were told that group performance deter- mined their payment, participants received $4 for their partic- ipation in the group study. This payment placed them high in the range of possible payoffs and reinforced feelings of success. After the cups-and-music task, participants answered ''How much did you feel you were on the same team with the other participants?'', ''How much did you trust the other participants going into the exercise?'', ''How similar are you to the other participants?'', and ''How happy are you right now?'' using 7-point Likert scales (1 5 not at all, 7 5 very much). Figure 1 displays mean participant choices by condition. Counter to the muscular-bonding hypothesis, cooperation did not differ between the synchronous-singing and synchronous- singing-and-moving conditions, t(49) 5 0.16, p rep 5 .54. As we predicted, participants in these synchronous conditions chose higher numbers in Round 1, t(70) 5 2.06, p rep 5 .93, d 5 0.58, and in the final round, F(1, 29) 5 4.26, p rep 5 .92, d 5 0.74, than did those in the asynchronous condition. They also reported greater feelings of being on the same team (M 5 5.31, SD 5 1.34 vs. M 5 3.71, SD 5 1.43), t(70) 5 4.21, p rep 5 .99, d 5 1.15. Counter to a collective effervescence explanation, they did not report being any happier (M 5 5.03, SD 5 1.05 vs. M 5 4.95, SD 5 0.74), t (67)  In sum, Study 2 showed that synchronous activity can in- crease future cooperation. Although all participants had real financial incentives to cooperate, participants in the synchro- nous conditions cooperated more than did those in other con- ditions. Synchrony involving large-muscle movements did not produce significantly more cooperation than did synchronous singing alone.", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "STUDY 3", "text": "In Study 3, we explored whether moving in synchrony could boost cooperation when behaving cooperatively conflicts with personal self-interest. We tested whether, after behaving in synchrony with others, people would contribute more to a public account in a commons dilemma known as a public-goods game (Croson & Marks, 2000).", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "Method", "text": "In groups of 3, 105 participants (66% female; mean age 5 21 years, SD 5 2.0) first engaged in the cups-and-music task used in Study 2. We used the same set of synchrony manipulations as in Study 2. Participants then engaged in a public-goods game and finally completed the questionnaire used in Study 2. In the public-goods game, each of 3 participants had 10 to- kens in each of five rounds that he or she could contribute into a public account or keep in a private account. Tokens in the public account earned $0.25 for every member of the group. Tokens kept in the private account were worth $0.50 each to the person holding the token but nothing to the other two group members. In this kind of game, individuals obtain more direct value from keeping tokens in their private account, but full contribution of tokens to the public account maximizes group earnings. As in the classic prisoner's dilemma or the tragedy of the commons, the dominant economic strategy in this exercise is to behave selfishly-keeping one's own resources in one's private account while reaping the benefits of others' contributions to the public account.", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "Results and Discussion", "text": "As Figure 2 illustrates, levels of cooperation in the synchronous- singing condition paralleled those in the synchronous-singing- and-moving condition, t(52) 5 0.08, p rep 5 .52. Relative to participants in the asynchronous condition, participants in the synchronous conditions allocated marginally more tokens in Round 1, t(79) 5 1.69, p rep 5 .88, d 5 0.42, and significantly more tokens in all subsequent rounds, all p rep s > .96. Partici- pants in the synchronous conditions also cooperated marginally more in Round 1, t(79) 5 1.69, p rep 5 .88, d 5 0.42, and sig- nificantly more in Rounds 2 through 4, all p rep s > .92, than did those in the control condition. Synchrony made contributions to the public account more persistent over time. Participants in asynchronous conditions contributed significantly fewer tokens to the public account in the last round than they did in the first round, t(26) 5 3.39, p rep 5 .99, d 5 1.33, but no corresponding decline occurred in the synchronous conditions. This persistence is particularly interesting because the modal pattern in public-goods games is for contributions to fall over rounds (Andreoni, 1995). Participants in the synchronous conditions reported greater feelings of being on the same team (M 5 4.9, SD 5 1.7) than did those in the asynchronous conditions (M 5 3.6, SD 5 2.0), t(79) 5 3.19, p rep 5 .95, d 5 0.70, or control condition (M 5 4.1, SD 5 1.7), t(76) 5 1.95, p rep 5 .92, d 5 0.48. These feelings of being on the same team partially mediated the effect of condition on tokens contributed in Rounds 3 through 5, Sobel tests > 2.2, p rep s > .94. Thus, synchronous participants continued to co- operate in part because they felt they were on the same team. Participants in synchronous conditions received higher pay- offs (M 5 $6.49, SD 5 $1.12) than did those in the asynchro- nous condition (M 5 $5.79, SD 5 $0.97), F(1, 32.5) 5 11.15, p rep 5 .99, d 5 0.67, or the control condition (M 5 $5.96, SD 5 $0.89), F(1, 32.5) 5 5.84, p rep 5 .95, d 5 0.52. They also felt more similar to their counterparts than did those in the asynchronous condition (M 5 4.2, SD 5 1.2 vs. M 5 3.4, SD 5 1.4), t(79) 5 2.50, p rep 5 .95, d 5 0.61, and trusted them mar- ginally more (M 5 4.6, SD 5 1.5 vs. M 5 4.0, SD 5 1.7), t(79) 5 1.79, p rep 5 .89, d 5 0.37. They did not report being happier (M 5 4.8, SD 5 1.2) than did participants in the asynchronous (M 5 5.1, SD 5 0.9) or control (M 5 4.8, SD 5 1.1) conditions.", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "GENERAL DISCUSSION", "text": "Taken together, these studies suggest that acting in synchrony with others can lead people to cooperate with group members. While the studies do not eliminate the possibility that muscular bonding and collective effervescence may, under the right conditions, strengthen the effects of synchrony on cooperation, our results show that synchronous action need not entail mus- cular bonding or instill collective effervescence to create a willingness to cooperate. Our results suggest that cultural practices involving synchrony (e.g., music, dance, and march- ing) may enable groups to mitigate the free-rider problem and more successfully coordinate in taking potentially costly social action. Synchrony rituals may have therefore endowed some cultural groups with an advantage in societal evolution, leading some groups to survive where others have failed (Nowak, 2006;Sober & Wilson, 1998).", "title": "Synchrony and Cooperation", "file_name": "Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf"}
{"section": "Abstract", "text": "nonradiatively [calculation in the supplementary materials (21)] in a perovskite film in air on glass. This value relates to a photon recycling-assisted average excitation travel distance of 20 mm (fig. S18). The average travel distance could be enhanced at larger charge densities (for example, under high fluences) and can reach values beyond 50 mm. In terms of e-h + transport, our results suggest that the average distance a charge carrier can travel in a perovskite is not limited by the charge-carrier diffusion length, for as long as recombination is radiative and the photon stays in the film, the e-h + pair can be regenerated and can propagate over large distances. This process creates a distinction between extraction and charge diffusion lengths and allows us to solve the existing contradiction of reported high recombination rates and long diffusion lengths. What are the implications of the observations presented here for standard thin-film perovskite solar cells (3, 6)? The thin-film samples from our work provide valuable model systems for these structures. Using the model and parameters developed above, we estimate that, under open-circuit conditions, in a device with a thickness of 350 nm and nonquenching electrodes, recycling produces a doubling of the internal photon density under 1-sun illumination. These effects can be enhanced further by minimizing nonradiative decay channels and being subjected to higher fluences, such as in solar concentrators, where high bimolecular recombination rates dominate. In the ideal case of unity PLQE and a perfect back mirror, photon recycling can produce internal photon densities up to 25 suns (4n 2 with n = 2.5) (31) in perovskite solar cells under open-circuit conditions. Photon management, such as the use of highly reflective back mirrors to minimize photonic losses and texturing of the top surface, offers promising approaches for using photon recycling to improve photoconversion efficiencies of perovskite solar cells toward the Shockley-Queisser limit. Higher photon densities lead to higher internal luminescence and a build-up of excited charges, which increase the split of quasi-Fermi levels and enhance the achievable open-circuit voltage in a solar cell.", "title": "Photon recycling in lead iodide perovskite solar cells", "file_name": "Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf"}
{"section": "ACKNOWLEDGMENTS", "text": "We acknowledge financial support from the Engineering and Physical Sciences Research Council of the UK (EPSRC) and King Abdulaziz City for Science and Technology. L.M.P.-O. thanks the Cambridge Home European Scheme for financial support. L.M.P.-O. and H.J.B. also thank the Nano Doctoral Training Center (NanoDTC) of the EPSRC for financial support. M.S., M.V., and J.M.R. thank the Winton Programme for the Physics of Sustainability (University of Cambridge). M.C.-Q. would like to thank the Marie Curie Actions (FP7-PEOPLE-IEF2013) ", "title": "Photon recycling in lead iodide perovskite solar cells", "file_name": "Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf"}
{"section": "Introduction", "text": "As the legislative landscape regarding cannabis alters, potentially altering patterns of use (Hopfer, 2014;Hasin et al. 2015;Shi et al. 2015), a greater understanding of environmental and genetic influences on progression to harmful or disordered cannabis use is needed. Cannabis Use Disorder (CUD) is included in the DSM-5 (American Psychiatric Association, DSM-5 Task Force, 2013), an amalgamated update of DSM-IV cannabis abuse and cannabis dependence (American Psychiatric Association, 2000) characterised by loss of control over use, failure to fulfil social roles, recurrent use in hazardous situations, and use des- pite worsening of health problems. An estimated 10-16% of individuals who have ever used cannabis will develop dependence (Anthony, 2006) and globally 13.1 million individuals meet criteria for cannabis dependence contributing 10.3% of the illicit drug use global burden of disease (Degenhardt et al. 2014). Individuals with drug dependence pass through several intermediate stages before develop- ing a clinical condition, and many non-clinical individuals will reach earlier stages of drug use involvement without progressing to disorder. The earliest stage of involvement is having the opportunity to use (regardless of whether the individual uses the drug or not). Opportunity is required for use to occur, and forms an individual's earliest necessary condition from which they are at risk of developing dependence (Wagner & Anthony, 2002). Once initiation of use has occurred, individuals will vary in frequency of cannabis use, with the increased fre- quency associated with increased likelihood for the development of cannabis dependence (Chen et al. 1997). Considering the sources of variation in progression through the stages of cannabis use, and the extent to which influences consist across different stages can provide insight into the aetiology of CUD ( Hines et al. 2015aHines et al. , 2016. Twin modelling has identified a strong genetic contribution to CUD, with a review of six studies in the area concluding heritabil- ity estimates range from 45 to 78% (Agrawal & Lynskey, 2006). Meta-analysis estimated heritability of problematic cannabis use (having one or more of the symptoms of cannabis abuse or dependence) at 51.4 [95% confidence interval (CI) 37.9-64.9] in males and 58.5 (95% CI 44.2-72.9) in females ( Verweij et al. 2010). However, the magnitude of these influences may differ across stages of drug use. Early stages may be genetically influ- enced through personality traits such as novelty seeking (Laucht et al. 2007), whereas at subsequent stages, such as drug depend- ence and development of withdrawal, genetic influences on drug metabolism, may be more influential (Dick et al. 2014). Common genetic influences may act on multiple stages. The majority of research into the correlation of influences between ini- tiation of use and disordered use comes from the alcohol and tobacco literature, where a genetic correlation (0.15-0.88) has been consistently demonstrated between the earlier and later stages of drug use ( Broms et al. 2006;Pagan et al. 2006;Morley et al. 2007). Similarly, studies of the alcohol use disorder have identified a strong genetic correlation between age of alcohol ini- tiation and alcohol use disorder ( Sartor et al. 2009;Ystrom et al. 2014). Similar mechanisms may be acting on CUD. Only 34% of the variance in cannabis abuse/dependence is unique to this phenotype, with the rest shared with genetic influences on initi- ation ( Agrawal et al. 2005), and cannabis availability explains almost all the shared environmental risks in cannabis initiation and abuse ( Gillespie et al. 2009b). To date, research has not explored the extent to which genetic influences may correlate across more than two stages of drug use. Additionally, the heritability of the earliest stage of drug use - having the opportunity to use a drug (Wagner & Anthony, 2002) -has been somewhat overlooked. This is despite evidence of the importance of this phenotype for the design of genetic research ( Nelson et al. 2013): individuals who do not have the opportunity to use a substance are unable to express their genetic vulnerability to later stages, including use and use disorders. Not only are such individuals structurally missing in analytic terms, but excluding individuals who have no drug use opportunity to use from genetic association studies can provide superior control for environmental background and related covariates. Opportunity may be regarded as a putative environmental factor, likely subject to broader environmental modifications, such as changes in national policy, but also to individual-specific factors, including peer provision of drugs. Despite these un- derpinnings, such 'environmental' factors have been shown to have heritable variation (Kendler & Baker, 2007;Gillespie et al. 2009b). Considering this phenotype in the context of later stages of drug transitions, such as escalation to frequent use and the development of abuse/dependence will provide insight into the pathways to the development of dependence. By applying trivariate twin models to the phenotypes age of cannabis opportunity, frequency of cannabis use, and abuse/ dependence, this paper aims to determine the extent to which genetic influences on the development of cannabis abuse/depend- ence are unique to the phenotype, and the extent to which they correlate with influences on opportunity to use cannabis and the frequency of cannabis use.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Sample", "text": "The sample was drawn from the Australian Twin Registry. From a pool of pairs born 1972-1979, 3348 MZ and DZ twins completed the interview component of a study of cannabis and other drug misuse. A full description of the study methodology and of the characteristics of participants has been published previously ( Lynskey et al. 2012). The 3303 twins who provided information on whether or not they had ever had the opportunity to use can- nabis, and who had complete zygosity information, form the ana- lysis sample for this paper. This sample consisted of 975 MZ males, 481 DZ males, 734 MZ females, 371 DZ females, and 742 opposite-sex DZ twins. Of these, 808 were singletons. Mean age was 31.8 (range 27-40 years, median 32.0).", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Assessment", "text": "Participants were assessed through computer-assisted telephone interviews which collected information on socio-demographics, childhood experiences, drug use and common mental health dis- orders, including cannabis and other drug use disorders, assessed using the Semi-Structured Assessment for the Genetics of Alcoholism (SSAGA-OZ) interview ( Bucholz et al. 1994;Heath et al. 1997). The SSAGA-OZ is a validated measure of mental health using DSM-IV criteria, and includes assessment of canna- bis and other drug abuse and dependence. Specific measures used in the current analyses are described below.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Measures", "text": "Opportunity to use cannabis Participants were asked 'have you ever been offered, or had the opportunity to use cannabis, even if you didn't use it at the time? How old were you the first time?' Of 3348 twins inter- viewed, 3325 provided information on whether or not they had ever had the opportunity to use cannabis. Of these twins, infor- mation on zygosity was missing for 22, resulting in an analysis sample of 3303. For analysis, participants were categorised as having never had the opportunity to use cannabis (N = 356, 10.8%), having had later opportunity to use cannabis (first opportunity reported as happening at age 16 and over, N = 2264, 68.5%), or having had early opportunity to use cannabis (first opportunity reported as occurring at age 15 or earlier, N = 670, 20.3%). As there is no pre- cedent in the literature for what age represents an 'early' opportun- ity to use cannabis, sensitivity analyses were conducted on the cut-off age. The correlations obtained by different cut-off points indicated results were not affected by the choice of age 15 as age cut-off for the early opportunity (see online Supplementary Material).", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Cannabis use frequency", "text": "Participants were asked about lifetime frequency of use through the item 'have you used marijuana 40 or more times, 21-39 times, 11-20 times, 7-10 times, 1-6 times?', then the estimated number of times used. Participants were categorised as having used cannabis infrequently, at a level that precluded being asked about cannabis abuse/dependence (0-11 times, N = 1913), mod- erately (12-50 times, N = 476), or high frequency (50 + times, N = 554).", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Cannabis abuse/dependence", "text": "Participants were classified as meeting DSM-IV criteria for life- time cannabis abuse if they reported one or more of the following: often using cannabis in a situation where they might get hurt; arrested more than twice within a 12-month period as a result of their cannabis use; cannabis use having caused difficulty with work, study or household responsibilities; cannabis having caused social and interpersonal problems more than 3 times within a 12-month period. Participants were classified as meeting lifetime criteria for DSM-IV cannabis dependence if they reported three or more of the following symptoms occurring within the same 12-month period: using cannabis a greater number of times/greater amount than was intended, tolerance, wanting to cut down/stop use, spending so much time obtaining/using/recovering from the effects of cannabis the participant had little time for anything else, reducing important activities as a result of cannabis use, con- tinuing use despite it worsening health/emotional problems. In the sample used in this analysis, 16.4% (N = 543) reported canna- bis abuse and/or dependence.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Sex", "text": "Sex was determined through self-report (76.9% female, N = 2540).", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Zygosity", "text": "Zygosity of twin pairs was measured through standard questions about physical similarity and the extent to which twin identity was confused by parents, teachers, and strangers; methods found to give better than 95% agreement with results of genotyp- ing ( Cederlof et al. 1961;Kasriel & Eaves, 1976;Sarna et al. 1978).", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Statistical analyses", "text": "All analyses were conducted using OpenMX v2.5. 2 (Boker et al. 2011) for the statistical software R v3.1.2 (R Core Team, 2013). Analyses used full information maximum-likelihood estimation with raw data, and the optimiser SLSQP was applied to analyses. Analyses were adjusted for sex.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Staged trivariate twin model", "text": "Classical twin modelling estimates the extent to which additive genetic (A), common environment (C) and unique environment (E) influence a phenotype (Neale & Cardon, 1992). Approaches using twins reared together can be used to determine the herit- ability of, and environmental contribution to, a phenotype or trait. Identical -or monozygotic (MZ) -twin pairs share 100% of their genetic material. Fraternal -or dizygotic (DZ) -twin pairs share only 50%, on average, of the same genetic material. This means they are no more genetically alike than full siblings. However, unlike siblings DZ twins will grow up in the same envir- onment. Using this knowledge we can calculate the extent to which the variance in a phenotype is due to genetic effects, and the extent to which it is due to environmental effects (Plomin et al. 2013). If the MZ correlation is twice the DZ correlation then all twin-pair similarity can be attributed to A, whereas if the MZ correlation is greater than the DZ correlation, but not twice the DZ correlation, there is also evidence of some shared environmental influences. The extent to which the MZ twin cor- relation is less than 1.0 indicates the magnitude of non-shared environmental influences. Dominant genetic effects (D), which are non-additive interaction effects between genes, cannot be assessed simultaneously with C (Neale & Cardon, 1992). Structural equation modelling of twin data is used to obtain pre- cise estimates of A, C, and E and allows for the comparison of models and generation of CIs around estimates (Neale & Cardon, 1992). A staged twin model was fitted to assess contributions of A, C, and E to variance in age of opportunity to use cannabis, frequency of cannabis use, and lifetime cannabis abuse/dependence, and to estimate the extent to which the influences of A, C, and E on the three phenotypes were correlated ( Heath et al. 2002). The staged model is appropriate for situations where early-stage phenotypes, such as cannabis use opportunity, are necessary for the expression of later behaviours, such as the development of dependence, and is a variation of the classic bivariate model appropriate for analysis of variables with data missing at random (data are missing as a result of observations on a previous variable, as opposed to data missing completely at random) ( Kendler et al. 1999;Heath et al. 2002;Neale et al. 2006). See Heath et al. (2002) for full details. Explicitly modelling such structurally missing data also has the advantage of estimating the extent of covariation between these contingent stages of use (i.e. opportunity, frequency, abuse/ dependence) while not excluding those who do not provide in- formation on a prior stage (e.g. opportunity) from analyses of later stages (e.g. abuse/dependence). A Cholesky decomposition model was used to parse the phenotypic correlations between the three stages of cannabis use and misuse into A, C, and E sources, including those specific to each of the latter stages of frequency and abuse/dependence as well as the magnitude of overlapping influences across the three stages.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Assumption testing", "text": "The analysis assumes each threshold-selected trait has an un- derlying bivariate/multivariate normal liability distribution. Exploring this methodological issue falls beyond the scope of this paper, but such modelling techniques have been shown to be robust to breaches of this assumption ( Reinartz et al. 2009). Thresholds represent cut-off points along this unobserved continuous distribu- tion of liability. In order to test whether thresholds could be equated between MZ and DZ twins, nested models were compared against a satu- rated twin model. Differences in the fit of more parsimonious models compared with the saturated or ACE model were assessed via the Akaike Information Criterion (AIC) and the change in - 2loglikelihood (\u0394 \u2212 2LL), which can be approximated by a \u03c7 2 dis- tribution with degrees of freedom (DF) equal to the difference in DF of the nested models. Where these measures lead to different conclusions on parsimony, the p value has been prioritised. The significance of thresholds (and equality between thresholds) was determined by \u0394 \u2212 2LL and change in DF (\u0394DF) and associated \u03c7 2 distribution. The significance of variance and covariance paths was similarly determined through likelihood ratio testing.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Results", "text": "Prevalence of, and correlations between, opportunity to use cannabis, frequency of cannabis use and abuse/dependence A saturated twin model was used to estimate tetrachoric co- rrelations for the categorically-defined traits of the age of opportunity, the frequency of cannabis use, and lifetime cannabis abuse/dependence (see Table 1). The relative magnitude of MZ within-trait correlations indicates heritable influences on all of these traits. The across twin/across trait correlations and CIs indi- cate genetic factors contribute to all correlations. MZ within trait and across trait correlations are not twice the DZ correlations, suggesting some influence of C. All correlations are less than 1.0, suggesting moderate to low effects of E.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Assumption testing", "text": "MZ and DZ thresholds could not be equated (\u0394 \u2212 2LL = 15.0, \u0394DF = 5, p = 0.01), and were estimated separately in all further models.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Trivariate Cholesky model fitting", "text": "A saturated model provided fit statistics, estimates for each com- ponent of the variance for all three phenotypes, and estimates for the covariance between phenotypes. The fit statistics for this model were \u22122LL = 11 029.68 DF = 7249, AIC = \u22123468.32.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Nesting models to develop parsimonious model fit", "text": "In order to identify the most parsimonious model, nested models constrained individual variance, and covariance components to zero, when CIs on the estimate from the saturated model included 0. It was possible to drop all C parameters (\u0394 \u2212 2LL = 6.07, \u0394DF = 6, p value = 0.41) without a significant decrement in fit. In addition, there was no statistically significant covariance between opportunity and either frequency or abuse/dependence attributable to E (\u0394 \u2212 2LL = 0.58, \u0394DF = 2, p value = 0.75).", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Final model", "text": "The final most parsimonious model was an AE model (\u0394 \u2212 2LL = 7.22, \u0394DF = 8, p value = 0.51). Variance component estimates are presented in Table 2. Approximately 64-78% of the variance in each phenotype was due to additive genetic influences, with CIs indicating both frequency and abuse/dependence were modestly, but significantly, more heritable than an opportunity to use. A proportion of these genetic influences were shared across the three stages. As shown in Table 2, genetic correlations across stages ranged from 0.37 (opportunity and abuse/dependence) to 0.68 (frequency and abuse/dependence). For frequency, about 55% of the genetic influences were unique from those acting on the opportunity, while for cannabis abuse/dependence, 17% of the genetic influences were unique from those acting on oppor- tunity and frequency of use. In addition, cannabis abuse/depend- ence shared individual-specific environmental influences with frequency (but not opportunity) with 27% specific to this stage.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Discussion", "text": "Additive genetic influences determine the majority of variance in age of opportunity to use cannabis (0.64, 95% CI 0.58-0.70), fre- quency of cannabis use (0.74, 95% 0.66-0.80), and cannabis abuse/dependence (0.78, 95% 0.65-0.88). Of these influences, 55% of additive genetic influences acting on the frequency of can- nabis use are unique from those acting on the age of opportunity to use cannabis, and 17% of additive genetic influences acting on cannabis abuse/dependence are unique from those acting on opportunity and frequency. No significant effect of the shared environment was observed, but there were unique environmental influences on all phenotypes. The only correlated unique environ- mental influences were between cannabis use frequency and abuse/dependence. Previous research has not explored the correlation between influences on cannabis use opportunity and cannabis abuse or dependence, although existing studies focusing on cannabis initi- ation observed overlapping liabilities between cannabis initiation and progression to heavy use (0.88; 33% due to genetic factors) ( Fowler et al. 2007). This is a similar genetic contribution to the overlap in liabilities to that presently observed between canna- bis opportunity and frequency of use. This demonstrates the pre- sent findings are in line with existing research showing a genetic correlation between the early stages of cannabis use and later substance use disorders. Opportunity to use cannabis is the necessary first step in the progression towards problematic use, and this phenotype could be expected to be subject only to environmental influence.  However, 64% of the variance in cannabis age at opportunity was due to genetic factors. Although it may be surprising that an apparently environmental phenotype is influenced by heritable factors, this result is consistent with previous findings that canna- bis use availability ( Gillespie et al. 2009b) and other putative mea- sures of 'environment' (Kendler & Baker, 2007) are, in fact, influenced by genetic factors. Environmental measures can be heritable if there is a bidirectional relationship between an indivi- dual's behaviour and their environment if aspects of behaviour are subject to genetic influences (Kendler & Baker, 2007;Lynskey & Agrawal, 2009). A review of this area identified positive and nega- tive life events, divorce, and social support all have heritable influ- ences (Kendler & Baker, 2007). The additive genetic correlation may also indicate evocative or active interactions taking place ( Plomin et al. 2013), with genes influencing earlier age of canna- bis use opportunity contributing to individuals selecting into environments and behaviours that facilitate the development of cannabis dependence. Alternatively, genetic influences associated with other beha- viours may be influencing progression through the stages of can- nabis use. Previous research has identified conduct disorder influences transitions to cannabis use opportunity, and from the opportunity to dependence (Hines et al. 2016). This is in line with existing research demonstrating the consistent influence of conduct disorder on drug use ( Lynskey et al. 2002;Storr et al. 2011;Reboussin et al. 2015), and genes relating to conduct dis- order and involvement with deviant peers ( Gillespie et al. 2009a) are plausible candidates for the shared genetic liability between age of opportunity and the development of cannabis abuse/dependence. Additionally, personality factors associated with drug use ( Malmberg et al. 2010), such as sensation seeking, may underlie this shared genetic liability. Cannabis opportunity, the frequency of use, and abuse/ dependence show a moderate effect of the unique environment (0.35, 0.26, and 0.22, respectively), but the correlation between unique environmental influences on the opportunity and the later stages of drug use were non-significant. This may reflect measurement error ( Plomin et al. 2013) but is in line with existing research demonstrating the pattern of environmental factors asso- ciated with progression between specific stages of drug use differs between transitions ( Sartor et al. 2007;Belsky et al. 2013;Hines et al. 2016). For example, childhood and early adolescent factors have been shown to be uniquely associated with cannabis oppor- tunity, whereas escalating other drug use factors is uniquely asso- ciated with the development of cannabis dependence ( Hines et al. 2016). The present analysis indicated none of the observed variances in the opportunity to use cannabis, the frequency of use or abuse/ dependence in males was attributable to the shared environment in this sample. The shared environment is usually found to be more important at earlier stages than later ( Fowler et al. 2007), and these findings contradict findings of a high shared environ- mental correlation between cannabis availability and cannabis abuse ( Gillespie et al. 2009b). The samples differ, with the Gillespie et al. findings based on an all-male population, but these contradictory findings indicate cannabis availability (the perceived ease of obtaining cannabis) and opportunity (having been offered cannabis or being around cannabis use) represent different phenotypes. Previous research has not tested the extent to which genetic influences on cannabis initiation and cannabis abuse overlap, so comparisons cannot be made to the present findings for the Psychological Medicine 5 opportunity and abuse/dependence. However, when considered in the light of findings that variation in progression to subsequent use of cannabis is almost entirely attributable to the unique envir- onment ( Hines et al. 2015b), a picture is beginning to emerge of how different factors influence progression from the very earliest stages of cannabis to the development of dependence.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Implications", "text": "The potential for opportunity to use cannabis to be a marker for intervention has previously been discussed ( Neumark et al. 2012), and the overlap in genetic influences between age of opportunity and both frequency of cannabis use and cannabis abuse/depend- ence indicates there is potential to use this measure to indicate those at greatest risk of developing later frequent and/or problem- atic use. It has previously been suggested that prevention strategies focused on modifying beliefs, norms and behavioural patterns within close social networks may be effective at reducing drug use opportunity, and consequently drug use ( Neumark et al. 2012). The identified moderate influence of unique environmen- tal factors on all phenotypes indicates there is scope to determine further influences which may be amenable to target within inter- vention efforts. The findings of this paper have important implications for future studies of gene variants and heritability of problematic can- nabis use, and in the choice of controls in case-control studies. These results indicate only a moderate proportion of genetic influ- ences on cannabis abuse/dependence are unique from those act- ing on age of opportunity to use cannabis. These findings reflect previous research demonstrating the importance of consid- ering drug use opportunity when looking at the genetics of opiate use ( Nelson et al. 2013). Comparison of participants in treatment for opiate depend- ence with nondependent neighbourhood controls (high exposure to illicit drugs, either via use or from residing in environments with widespread drug availability) identified SNPs in ANKK1 and TTC12 as associated with heroin dependence, whereas com- parison with controls sourced from the ATR (individuals not dependent on alcohol or illicit drugs, with significantly lower illicit drug exposure) found no association with these SNPs ( Nelson et al. 2013). Until now the importance of considering cannabis use opportunity in genetic studies has not been explored, although some studies remove those who have not initiated use. Removing those who have not initiated cannabis use can reduce sample size and power, and the present results indicate excluding those without opportunity may avoid conflat- ing genetic influences whilst retaining a greater proportion of a sample. A further advantage of incorporating opportunity to use may arise in meta-analyses of genomewide association studies of cannabis use and misuse. Marked regional variation in the opportunity to use across different samples may comprise an international meta-analytic effort. Exclusion of, or accounting for, variability in exposure opportunity, even using crude indices of national policy or cannabis-related law, might reduce hetero- geneity in the extent to which genetic vulnerability to later stages of cannabis problems have been adequately expressed. Consequently, a key implication of the current findings is the necessity of taking into consideration the stage of drug use reached amongst the controls for genomic analyses. Existing research has utilised information on the extent of cannabis use in controls (e.g. excluding those who had used cannabis fewer than 6 times) ( Hartman et al. 2009), but such issues are not always taken into consideration ( Benyamina et al. 2009). This may be especially important in studies of cannabis; a drug with a high prevalence of use, but relatively low prevalence of depend- ence amongst lifetime users. As the legal status of cannabis changes ( Shi et al. 2015) availability may become to be compar- able with that of alcohol, but individual opportunity to use may remain variable. Depending on the research question, and on the development of research identifying genetic overlap between progression to other stages of cannabis use and problematic can- nabis use, screening controls not only for opportunity or initiation of cannabis use but also for the frequency of use may have utility in improving cannabis dependence SNP identification in the future. These findings have further implications for the overlap of genetic influences across drug classes. Existing research has sug- gested a proportion of the genetic factors underlying SUDs are not specific to individual drugs, and environmental influences determine the drug of misuse ( Kendler et al. 2003) However, pre- vious research in this area has not incorporated consideration of the stage sequential nature of drug dependence into their analyses. Much of the non-specificity of genetic influences on SUDs likely results from shared influences on the earlier stages of drug use, with more specific influences (such as those related to drug metabolism, for example) associated with later stages of use.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Limitations", "text": "Certain limitations must be taken into account when interpreting these results. The data are based on retrospective self-report. Retrospective recall of age onset of drug use behaviours has been shown to be reliable ( Shillington et al. 1995;Johnson & Mott, 2001;Parra et al. 2003;Ensminger et al. 2007), but the ana- lyses would benefit from replication in prospective longitudinal cohorts. Self-report has been shown to be a valid measure of data collection relating to drug use (Darke, 1998), and has been described as the gold standard for collecting data on phenotypes such as initiation and opportunity (Wagner & Anthony, 2002). Given use of cannabis was illegal at the time of data collection some participants in this study may have misreported their drug use. However, the high prevalence of self-reported lifetime cannabis use (68.5%) suggests it is unlikely this was an issue. The results are based on a twin population. Research has demonstrated twin and non-twin populations do not differ in incidence of psychiatric illness ( Kendler et al. 1996), and no asso- ciation has been found between twin environmental similarity and mental health outcomes ( Kendler et al. 1993).", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Conclusions", "text": "There are significant genetic contributions to lifetime cannabis abuse/dependence, but a proportion of this overlaps with genetic influences acting on the opportunity to use cannabis and the fre- quency of cannabis use. Individuals without drug use opportunity are uninformative, and studies of drug use disorder and frequency of use, whether focused on identifying gene variants or environ- mental factors, must incorporate consideration of drug use expos- ure amongst controls in order to accurately identify aetiological factors. Supplementary material. The supplementary material for this article can be found at https://doi.org/10.1017/S0033291718000478. Acknowledgements. This research was funded by National Institute on Drug Abuse (NIDA) grants DA18267, DA23668 & DA032573 and facilitated through access to the Australian Twin Registry. Twins Research Australia receives support from the National Health and Medical Research Council through a Centre of Research Excellence Grant, which is administered by the University of Melbourne. Declaration of interest. AA has previously received peer-reviewed funding from ABMRF/Foundation for Alcohol Research which receives partial support from the brewing industry. JS is a researcher and clinician and has worked with a range of types of treatment and rehabilitation service-providers. He has also worked with pharmaceutical companies to seek to identify new or improved treatments, and also with a range of governmental and non-governmental organisations. His employer (King's College London) is registering intellectual property on an innovative medication development with which JS is involved (not relevant to cannabis), and JS has been named in a patent registration by a Pharma company as the inventor of a potential novel overdose resuscitation product (not relevant to cannabis). There are no other declarations of interest from authors of this paper.", "title": "Overlap of heritable influences between cannabis use disorder, frequency of use and opportunity to use cannabis: trivariate twin modelling and implications for genetic design", "file_name": "Hines et al. - 2018 - Overlap of heritable influences between cannabis u.pdf"}
{"section": "Abstract", "text": "Economic geographers have recently taken up the study of markets after a long period of inattention. This growing literature has highlighted the diverse spaces, scales, and fields where markets are present, as well as the ways in which markets vary in form. However, the study of markets in economic geography still exists in tension between neoclassical and Marxist conceptions of markets as predictable and approaches like the social studies of economization/marketization which emphasize their contingency. This paper argues that work of Polyani and Gramsci can provide a way forward for the subfield through conceptualizing markets as sites of social struggle.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "I Introduction", "text": "Markets are everywhere. From buying groceries at the supermarket to speculating on securitized mortgages, markets are fundamental to the organization of everyday life. This is truer now than ever, with the triumph of market ideology so prevalent that, as Massey writes, the neoclas- sical assumption of market perfection has 'become so deeply rooted in the structure of thought . . . that even the fact that it is an assumption seems to have been lost to view ' (2013: 14). This ideology, which posits that markets are the most efficient and just method of governance (O'Neill, 1998), has led to the creation of new markets as calls for social and environmental justice are met with market- based responses such as voucher systems for education and carbon markets to reduce green- house gas emissions (Fraser, 2014). Yet despite their role in governing everyday life and as basic institutions of capitalist societies, it is only recently that economic geographers have taken up markets as worthy of study (Berndt andBoeckler, 2009, 2011a;Boeckler and Berndt, 2013;Hall, 2012;Jones, 2013;Peck, 2012;Prince, 2012;Schoenberger, 2008;Smith, 2005). While this emerging literature has done much to enhance our understanding of how mar- kets differ in form and function, many questions remain for the geography of markets; most nota- bly, how should markets be defined, how do space and place shape the functioning of markets, and what roles do markets play in broader societal processes? These challenges have remained unaddressed in part because geographers studying markets have conducted their work largely in opposition to orthodox versions of neoclassical and Marx- ist economics, both of which project a universal view of markets and their functioning. On the one hand, neoclassical economics' assumption of efficient markets presumes the predictable behaviour of market actors regardless of social context (Barber, 1995;Milonakis and Fine, 2009). On the other, to Marxists, the 'coercive laws of competition' (Marx, 1990(Marx, [1867: 433) and the need for the continual circulation of capital place strict limits on the variability of market forms (McNally, 1993). As described by Karl Polanyi (2001[1944: 89), these two approaches have long polarized political eco- nomic thought, with 'progress and perfectibility on the one hand, determinism and damnation on the other'. In contrast, recent work in geography has emphasized the contingency of market forms, using concepts from economic sociology to deconstruct markets and emphasize their complexity. For a distinctive geography of mar- kets to emerge, however, scholars in the field cannot forge their arguments solely in opposi- tion to these dominant theories but must also study the commonalities between them and how the functioning of markets relates to other socio- economic processes (Muellerleile and Akers, 2015;Peck, 2012). It is with these questions in mind that this article reviews scholarship within both the polit- ical economic tradition and the emerging geo- graphy of markets, arguing that a critical, but productive, engagement with this work can help build a nuanced understanding of how markets function and the roles they play. In particular, the paper argues that the scholarship of cultural Marxists and Polanyian scholars can help eco- nomic geographers move past the tension between new approaches to studying markets and the discipline's longstanding tradition of political economy (Braun, 2016;Christophers, 2014a;Muellerleile and Akers, 2015;Peck, 2012). Through centring markets as sites of social struggle, cultural Marxists and Polanyian scholars have rejected deterministic readings of markets while still holding potential limits to market variability within their frame of analy- sis. In doing so they provide signposts as to how the geography of markets can connect the micro-politics of markets to the macro- structures of capitalism and thereby open new understandings of the role of space and place in their functioning. To make this argument this paper engages with multiple theories of markets. This is done as a means of highlighting why the geography of markets has emerged at this moment, some of the challenges it faces, and the potential of addressing the tension between political econ- omy and the emerging geography of markets through a hybrid Polanyian/cultural Marxist framework. The paper therefore does not pres- ent an exhaustive review of all the questions facing the subfield but, instead, highlights cer- tain questions and challenges as a means of addressing this central debate. These arguments are made in the four sections that follow this introduction: (1) an outline of why the geogra- phy of markets matters and three challenges for the subfield; (2) an examination of how markets are conceptualized within different theoretical traditions; (3) an engagement with cultural Marxist and Polanyian thought as a means of addressing the challenges reviewed earlier; and (4) some concluding thoughts on ways forward for the subfield.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "II Placing markets in economic geography: Three challenges", "text": "Following the financial crisis and the subse- quent revelation of how exotic financial instru- ments had reshaped American housing markets, geographers started to turn their attention to markets as an important area of research. This engagement mirrored a movement throughout the social sciences by scholars seeking to dis- rupt the taken-for-granted understanding of markets found within neoclassical economics (Boeckler and Berndt, 2013). In economic geo- graphy this focus on markets marked a break from the long-standing disciplinary focus on production systems and supply chains, where 'the market remains a black box and is simply taken as pre-given' (Berndt and Boeckler, 2009: 538-9) and where the explicit study (and pro- blematization) of markets has therefore been limited (Jones, 2013;Peck, 2012;Sheppard, 2011;Smith, 2005). Given its emergence in the wake of the financial crisis, this turn towards markets was not simply an academic question but also an urgently political one. As Fraser (2013: 119) has poignantly argued, neoclassical conceptions of markets have enabled 'a relent- less push to extend and de-regulate markets [that] is everywhere wreaking havoc -destroy- ing the livelihoods of billions; fraying families, weakening communities and rupturing solidari- ties; trashing habitats and despoiling nature across the globe'. With the rule of markets expanding into new areas of everyday life, studying how markets function is both an important academic and political endeavour. This post-financial crisis research agenda has begun the process of opening the 'black box' of markets and highlighted the diversity of market forms and of areas under market rule. A special issue of Environment and Planning A on 'mak- ing markets' illustrates the breadth of these proj- ects. The issue includes case studies of the creation of markets for financial derivatives (Muellerleile, 2015), real estate (Akers, 2015), schooling (Cohen and Lizotte, 2015) and green- house gas emissions (Cooper, 2015). The work therein and similar projects on agro-markets in Ghana ( Ouma et al., 2013;Ouma, 2015), con- struction markets in the UK ( Lovell and Smith, 2010), and street markets in Yemen (Lauermann, 2013) highlight the different spaces, scales, and fields that markets populate as well as the diverse methods through which they can be understood. For example, Cooper (2015) and Lovell and Smith (2010) study markets through examining how market actors make disparate commodities commensurable and therefore suitable for exchange by analysing the technol- ogies used to calculate their value. Muellerleile (2015), on the other hand, takes a historical approach, charting how the legal-regulatory system governing financial derivative markets gradually evolved out of the trade for agricul- tural products in Chicago. While they take dif- ferent approaches to understand market creation, these works have a common focus on the social, sociotechnical, and spatial constitu- tion of markets that is consciously forged in opposition to the asocial market of neoclassical economics. Peck (2012) and Muellerleile and Akers (2015) note the promise of this work but argue that for the study of markets to evolve beyond a collection of individual research projects it must move past this oppositional approach and towards a constructive intellectual program that re-theorizes the place of markets in society. Akers (2015: 1784) specifi- cally warn that 'ignoring the possibility that actually existing markets may have some com- mon characteristics across space and time, whether as a price mechanism or otherwise, will leave [the geography of markets] underspeci- fied and incoherent'. By highlighting how cur- rent work has yet to fully address what ties the geography of markets together, this critique illustrates that many challenges remain for the subfield to address. While it is beyond the reach of a single paper to provide an answer to every open question in the subfield, this paper argues that a hybrid Polanyian/cultural Marxist approach can be useful in approaching at least three challenges facing geographers of markets. The first challenge is careful consideration of what geographers mean when they refer to mar- kets. While the general outlines of a research program can be seen through the cases discussed above, the specifics of what constitu- tes a market remain unclear. In their three-part review of the geography of markets, for exam- ple, Berndt andBoeckler (2009, 2011a;Boeckler and Berndt, 2013) do not explicitly problematize the definition of markets, instead focusing on 'marketization' as a process through which goods and services are assigned value, made ready for exchange, and circulated (see also Birch and Siemiatycki, 2016;Bryant, 2016;Ouma et al., 2013). Using this definition, geographers have extended the study of markets into realms of production in sectors such as agri- food (Ouma, 2015) or into the global circulation of commodities such as tomatoes (Berndt and Boeckler, 2011b). The question remains, how- ever, of what roles markets themselves -as the sites where the exchange of commodities takes place and is organized 1 -play within this broader process of marketization. And, if they have no role, or if their role is simply as a pass- through between consumption and production, why should geographers study markets at all? While definitional specificity may not be needed to describe how markets break from the neoclassical ideal, if geographers want to make the case that what happens in markets matters and understand the roles markets play in socio- economic processes, they must be clear on their object of study; without doing so, it will be dif- ficult to understand what is common across markets. The second challenge for geographers of markets is to provide a more fulsome explora- tion of the role of space and place in their func- tioning. This includes not only the sites (physical or otherwise) where exchange takes place, but also how market exchange remakes spatial relationships. To be clear, geographers have begun the process of unpacking how spa- tial relations are central to the functioning of markets. Most notably, there is a vibrant litera- ture on how the contested process of geographi- cally bounding a market renders it calculable and allows actors to make decisions (Berndt, 2013;Christophers, 2014b;Hall, 2015;Kama, 2014). Nevertheless, there is still work to be done in understanding the spatiality of markets (Christophers, 2014c;Hall, 2015;Jones, 2013) and several unaddressed issues remain, such as how to understand overlapping spatial relation- ships between markets (e.g. between local, regional, and national markets for a commodity [Jones, 2013]). Given that existing geographic work on markets has relied on adding spatial elements to theories borrowed from economic sociology (see Section III), more reflection is needed on the tension between these approaches and the spatial questions of interest to geographers. Finally, the third challenge is to understand the roles that markets play in the societies and spaces they exist within. This challenge has been the subject of much debate within the sub- field, with recent articles by Braun (2016), Christophers (2014a), Muellerleille andAkers (2015), andPeck (2012) all posing the question of how the micro-politics of markets feed back into the macro-structures of capitalism. This can be understood in part as a reaction by political economists to an alternative approach to study- ing the economy. However, the stakes are higher than a disciplinary squabble. Under- standing the role of markets in wider power structures is integral to unpacking how what happens within markets impacts broader socio- economic processes. For example, Smith (2005) argues that that the roles of markets are not fixed and that 'perhaps the politics and ethics of mar- kets can be challenged not by arguing against markets, but by making a bid for them' (p. 17). Conversely, Christophers questions the limits of focusing on market hybridity, asking 'if so much active work is involved in configuring and operating markets, and if in theory they are inherently contestable and fragile as a result, why, in reality, are they actually so incredibly resilient? ' (2015: 1861). In light of these con- flicting assertions, geographers must explore not only the role of space in the functioning of markets but also how the demands placed on markets under capitalism may limit how this hybridity changes other socioeconomic pro- cesses. Without doing so we cannot be clear on what is at stake in studying markets. Addressing these challenges and others is vital if the geography of markets is to follow Peck's (2012) and Muellerleile and Akers's (2015) calls to build a constructive program: a necessary task for geographers seeking to understand how markets impact everyday life and feed back into the broader economy. As this paper argues, there is great potential in using the work of cultural Marxists and Polanyian scho- lars to meet these challenges. Through concep- tualizing markets as not just sites where the exchange of commodities is organized but also as sites of contestation over the distribution of resources, geographers can develop a more thor- ough understanding of the roles that markets play in sociospatial relations.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "III Perfection, damnation, and deconstruction: The theoretical foundations of the geography of markets", "text": "That geographers have only recently begun to study markets is unsurprising given the vast shadow cast by neoclassical and Marxist eco- nomics. Despite holding hugely different visions of the results of market exchange, both traditions view markets as functioning in a largely predictable manner, and geographers in these traditions have therefore left the internal logics of markets mostly unexplored. Given the dominance of these theories, the geography of markets has largely emerged in opposition to them, drawing on the social studies of economi- zation/marketization and other theories from economic sociology to highlight how and why markets diverge from neoclassical and Marxist conceptions. This section reviews how markets are understood in these traditions in order to explore the complicated place of markets within economic geography and the strengths and weaknesses of current approaches.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Perfection and damnation: Markets in neoclassical and Marxist economics", "text": "Despite their oppositional programs, orthodox Marxists and neoclassical economists both hold similar conceptions of markets as consisting of the smooth exchange of commodities, diverging on the likely effects of this functioning rather than on the nature of exchange itself (Berndt and Boeckler, 2009;Christophers, 2014a;Lie, 1997). As Harvey (2004: 73) writes, 'Marx's general theory of capital accumulation is con- structed under certain crucial initial assump- tions which broadly match those of classical political economy . . . [but] the brilliance of Marx's dialectical method is to show that mar- ket liberalization . . . will not produce a harmo- nious state in which everyone is better off. It will instead produce ever greater levels of social inequality'. In both conceptions market exchange is viewed as largely predictable: for neoclassical economists markets consist of aso- cial actors exchanging solely to maximize prof- its, while for Marxist economists market actors are constrained by the social relations found in production. There are important differences in how the two traditions view the functioning of markets, however. For neoclassical economists, markets are characterized by their asocial nature; they are sites of neutral exchange where buyers and sellers find each other and, through a rivalrous process of determining a price and exchanging commodities, where supply and demand are balanced and scarce resources are allocated efficiently. The assumed neutrality of markets in the process of exchange has meant that the origins of, and the institutions that define, markets have largely gone unremarked in the discipline and that markets themselves are rarely studied ( Barber, 1995;Graeber, 2012;Jackson, 2007;Mirowski, 2007;Rosenbaum, 2000;Schoenberger, 2008). Indeed, through fashion- ing a 'scientific' discipline that uses rigid, immutable laws and mathematical modelling to explain the workings of the economy, neo- classical economists have placed their underly- ing assumptions about how markets function largely outside of the realm of debate (Lawson, 2013). 2 This does not mean that these econo- mists uniformly hold a na\u0131\u00a8vena\u0131\u00a8ve belief that every market functions in exactly this manner, but rather that doubts about these assumptions are rarely reflected in models built to simulate the economy (Mann, 2013). By contrast, in Marxist thought markets are predictable not because of a lack of social connections between actors but because mar- ket exchange is overdetermined by its role in the continuation of capitalist social relations. This is because in orthodox Marxist thought markets play an important but largely super- ficial function (Christophers, 2014a). Markets are important because they ensure that capi- talists and workers perform their roles in the continual circulation of capital as the site where the disciplining force of competition is most strongly experienced. However, these roles are determined through capitalist social relations rather than within markets them- selves, with Marx writing that 'competition executes the inner laws of capital; makes them into compulsory laws towards the indi- vidual capital, but it does not invent them. It realizes them' (1973 [1939]: 752, emphasis added). Importantly, this relation between market interactions and the underlying social relations they express is hidden by the fetishism of the commodity, whereby market actors see only the exchange taking place and not the social rela- tions that underpin it. Commodity fetishism allows for markets, and not social relations, to be seen as holding a coercive power which requires actors (both capitalist and working class) to engage in ways that allow them to compete in a market. This is what Marx means when he writes that competition 'executes' the laws of capital but 'does not invent them'. Capi- talist market exchange is how an individual worker sells their labour to a capitalist, or alter- natively how a capitalist experiences the pres- sure to cut costs, but at their root these pressures are the result of the mode of production, which requires the constant circulation of capital to increase the capture of surplus value (Clarke, 1995;Kozel, 2013;McNally, 1993). To ortho- dox Marxists, then, markets, through competi- tion, act as a coercive force that ensures that actors play by the rules of capitalist social rela- tions; relations that are determined outside of markets. In economic geography, the dominance of these theoretical traditions has meant that mar- kets have rarely been an explicit object of study. On the contrary, economic geographers have largely eschewed moments of exchange in favour of studying the circulation of capital and systems of production. This includes Marxist scholarship as well as post-structuralist analy- ses, which study how gender, race, and other axes of difference shape the functioning of eco- nomic systems and vice versa (Sheppard, 2011). 3 This work, and work that trades more closely with neoclassical economics, therefore tends to leave markets and processes of exchange untouched (Berndt and Boeckler, 2011a). There are of course exceptions: some Marxists do see a role for markets in structuring exchange since value cannot be realized until exchange occurs (see Karatani, 2003), but in these cases markets remain a second-order phe- nomenon used to better understand production and the capture of surplus value (Christophers, 2014a). This does not mean that Marxist scho- larship has nothing to offer to the geography of markets; Marx's insights into the crisis-ridden nature of capitalist expansion can provide a guide to how and why markets are created and undergo processes of change. However, differ- ent theoretical tools are needed to address important questions about how and why markets differ in form and the impacts of these variations.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Deconstruction: Economic sociology and the geography of markets", "text": "Perhaps because of the discipline's lack of attention to markets, economic geographers have turned to concepts from economic sociol- ogy for the theoretical tools needed to under- stand their social underpinnings (Berndt and Boeckler, 2011a). 4 In contrast to neoclassical and Marxist conceptions of markets as predict- able, work in the sociology of markets instead emphasizes their contingent and variable nature. Indeed, while economic sociologists have disparate views on the precise nature of markets, in general the subdiscipline holds a common conception of markets as spaces where social connections shape the nature of exchange and competition (Fligstein and Dauter, 2007;Fourcade, 2007;Ouma, 2015). For this reason, ideas from economic sociology have inspired much of the increased attention to markets in geography. There are multiple schools of thought within economic sociology on how markets can be conceptualized (Fligstein and Dauter, 2007;Fourcade, 2007). In geography, however, the social studies of economization/marketization approach (SSEM) most associated with Callon has been particularly influential. To scholars using this approach markets are neither natural nor predetermined but rather 'performed' by actors who make decisions within a dense web of relationships between actors, from buyers and sellers to non-human actors such as eco- nomic theories or mathematical models. In order to make sense of all these relations, mar- ket actors must frame their decisions within what they consider to be the boundaries of a market and by using calculative technologies that make markets appear coherent (Boeckler and Berndt, 2013;Callon, 1998). This framing is accomplished through what Callon (1998) calls 'performation', whereby markets are cal- culated, defined, and made legible so that actors can make decisions. Importantly, the ideas of neoclassical economics have shaped the ways in which actors frame market boundaries, lead- ing to the creation of markets that largely com- port with neoclassical principles, although they often 'overflow' this narrow framing, causing crisis (Callon, 1998;Ouma, 2015). While as recently as 2009 the use of SSEM in geography was referred to in this journal as being in its 'initial stages' (Berndt and Boeckler, 2009), its popularity in the discipline has grown rapidly. Using Callon's ideas and their subsequent evolutions, geographers have applied the SSEM approach to study markets as diverse as exotic animal auctions (Collard, 2014), business education (Hall and Appleyard, 2011), and emissions trading (Kama, 2014). This work has allowed geographers to depart from existing geographic scholarship which views markets as predictable and, instead, deconstruct how markets function as highly contingent assemblages which turn commodi- ties into calculable objects fit for exchange (Berndt and Boeckler, 2011a). For example, geographers have explored how the calculative technologies used to establish fishing rights as a tradeable commodity have remade the salmon industry in Alaska (H\u00e9bert, 2014) and the UK (Cardwell, 2015). Such work necessitates not only studying markets as sites of exchange but also examining the theories that guide their con- struction and the interaction of these theories with the human and non-human actors that make up a fishing community. Beyond SSEM, there are other approaches from economic sociology that have had an impact on geographic thought, albeit a more limited one. This includes the embeddedness approach associated with Granovetter (1985), which examines how the networks that form through multiple interactions between actors influence the functioning of markets (see Lai, 2011); 5 institutionalism, which examines how formal and informal institutions set the rules within which markets operate (see Hall, 2007); and the convention school of French sociology, which argues that actors make decisions through higher common principles, or conven- tions, with the neoclassical 'rational' decision maker just one of many (see Berndt and Boeckler, 2011a;Ouma, 2015;Ponte, 2016). 6 Despite differences, these approaches all illustrate how markets change in relation to the contingent social networks, institutional struc- tures, and/or technologies and ideas that consti- tute them. This work has allowed for the emergence of a geography of markets that stands in contrast to Marxist and neoclassical thought and therefore much existing scholarship in the discipline. Rather than taking markets and the process of exchange for granted, geogra- phers using these approaches have highlighted how market exchange is always shaped by social connections between actors and therefore the necessity of undertaking in-depth research on how markets function.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Strengths and tensions in the geography of markets", "text": "This grounding in economic sociology has helped develop a vital geographic literature on markets. By adopting a focus on the networks, institutions, and/or technologies and ideas that surround the process of exchange, geographers have illustrated how space and place are integral to the functioning of markets. This has included studying how China's emerging financial mar- kets have been shaped by competing knowledge networks of Western experts and Chinese bank- ers (Lai, 2011), how racial segregation has facilitated the spread of education markets in the United States (Cohen, 2016), and how mar- kets for Fair Trade goods depend on geographic imaginaries ( Doherty et al., 2015). In each case these authors have emphasized that the social/ sociotechnical constitution of markets cannot be removed from its spatial context, with the clash between circulating models of finance resulting in a distinctly Chinese financial market or the geographic imaginary of the fairly-treated, Southern farmer shaping Fair Trade markets. A central feature of this work has been study- ing how the social/sociotechnical process of geographically bounding markets is essential to how markets operate. As Christophers (2014c: 754-5) deftly points out, it is difficult to discuss a market without referring to its geo- graphical boundaries: 'the French automobile market, the New York housing market, the glo- bal financial services market, and so on: when we attempt to identify and define markets, geo- graphical scope . . . is invariably one of the key dimensions on which we do so'. As geographers have illustrated, studying how these boundaries operate is essential to understanding the nature of markets. For example, Kama's (2014) work on emissions trading highlights how carbon markets depend on the boundaries which allow for the calculation of carbon emissions and therefore for their trade. Or alternatively, Christophers (2014b) has shown how pharmaceutical corpora- tions use territorial boundaries to enable differ- ential pricing and thereby increase their market power. In both cases, geographic boundaries are used to constitute a market, shape how it func- tions, and allow certain actors to gain power. Similarly, a focus on the social/sociotechni- cal has allowed geographers to trace what Col- lard (2014) refers to as the 'spatial momentum' of markets. Collard argues that the ways that exchange is organized within a market can extend outwards to remake the relationships between sites and to create new geographies. Using the case of auctions for exotic wildlife in the United States, she writes that understand- ing the valuation and commodification of ani- mals that occurs at these sites is essential to understanding the ecological impact of the exo- tic wildlife trade on the Guatemalan landscape. Likewise, Ouma (2015) traces how the shape of Ghanian agrifood production along a just-in- time model was driven by competition in European retail markets. In both cases the spe- cific social practices through which these mar- kets assign a price remake the connections between places and reshape sites far removed from the marketplace itself. This work high- lights how markets are often central nodes in the development of spatial relations. However, while the approaches borrowed from the sociology of markets are well suited to understand what Muellerleile (2013Muellerleile ( : 1638 refers to as the 'internal logic by which markets function' and therefore many questions that geographers of markets are interested in, they are an uneasy fit with the political economic tradition in economic geography. Critiques of these approaches in both economic sociology (Krippner, 2002;Lie, 1997;Mirowski and Nik-Khah, 2007;Fourcade, 2007) and geogra- phy (Christophers, 2014a(Christophers, , 2015Muellerleile, 2013;Muellerleile and Akers, 2015;Peck, 2012) have made the point that studies decon- structing the internal functioning of markets must also address the wider frame of reference within which markets exist. These critiques highlight a tension between scholarship on the micro-politics of markets and work that is focused on the economy at a much wider scale. This critique is central to the most active debate within the geography of markets: how the contingency and spatiality of markets feeds back into the macro-structures of capitalism. While many scholars of SSEM would reject this micro/macro divide, 7 recent articles by Braun (2016), Christophers (2014a), Jones (2013), Muellerleille and Akers (2015), and Peck (2012) have raised this question and pushed geographers to integrate it into their research programs. Braun (2016: 258) sets the stage clearly, writing that 'the empirical challenge, then, is to show how market devices, market structures and forms of capitalism are interwo- ven -that is, to establish both micro-meso and meso-macro connections'. For the most part these authors suggest that geographers design research projects focusing on the common elements between markets in order to uncover these connections. For example, Peck (2012) advocates using Polanyi's concept of the 'always embedded economy' to develop a col- lective program investigating how markets are embedded in the social and spatial relations they exist within (see also Muellerleile, 2013). Alter- natively, Jones (2013) suggests following the distinctive spatialities that shape markets as a means of creating a practice-oriented approach that informs debates on market regulation. Christophers (2014a) takes a different tack, arguing that a 'weaker' version of the SSEM approach is compatible with Marxist political economy. Drawing on Harvey, he argues that 'capital will tolerate all manner of exchange (and distribution and consumption) structures, so long as they do not restrict accumulation for accumu- lation's sake ' (p. 19). In doing so he charts a way forward by emphasizing that the variability of markets may be constricted by the demands placed upon them in ensuring the continual circulation and accumulation of capital. Christophers makes the case that geographers should therefore use the SSEM approach as a method of studying how markets relate to value and the value form under capitalism. However, he does not provide a road map for doing so, instead writing that 'much more can and needs to be said about this' (p. 19) and thereby neglecting the 'meso' step that Braun (2016) calls for in linking the micro- politics of markets to the macro-structures of capitalism. Nevertheless, by drawing attention to how studying the limits of market forms can help reconcile the deconstruction of markets with Marxist political economy, Christophers provides the basis for advancing the conversa- tion beyond an uneasy tension.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "IV Markets as sites of struggle: Ways forward for economic geography", "text": "As outlined above, concepts from economic sociology have allowed geographers of markets Cohento break from a longstanding focus on produc- tion and circulation in economic geography. However, it is also clear that these concepts present challenges when placed in conversation with the discipline's tradition of political econ- omy and in understanding how what happens within markets shapes, and is shaped by, the functioning of wider socioeconomic processes. It is by integrating the work of cultural Marxists and Polanyian scholars into the existing geogra- phy of markets that geographers can best address this tension and move forward, tackling the three challenges outlined in Section II. In this section, then, cultural Marxist and Pola- nyian scholarship on markets is reviewed before the potential of using its insights in the geogra- phy of markets is outlined.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Struggle in and of markets", "text": "The work of cultural Marxists and Polanyian scholars provides a general theoretical approach through which markets can be understood as variable within the 'finitude of possibility' set by capital that Christophers highlights (2014a: 15). Scholarship in both traditions views the proliferation of markets as driven by capitalist accumulation while also conceptualizing social institutions such as markets as sites of struggle that help to contain the most brutal of capital- ism's destructive forces (Burawoy, 2003). Studying markets within these frameworks therefore presents a method of understanding markets as social/sociotechnical while keeping wider power relations within the frame. While they have important differences, both concep- tualizations have great potential in helping geo- graphers understand how the variability of markets relates to the functioning of the wider economy and, conversely, how broader socio- economic processes affect the internal function- ing of markets. Specifically, this potential lies in understand- ing markets not only as sites where the exchange of commodities is organized but also as sites of struggle over the distribution of resources. This is essential in the current moment where the expansion of market rule into new areas of everyday life has added importance to struggles over the social/sociotechnical processes through which markets organize exchange. Polanyi (2001Polanyi ( [1944), writing about an earlier era of market rule, argued that struggles over markets in such eras take the form of a 'double movement' whereby the most destructive effects of markets are met with opposition as society moves to protect itself. This occurs as those most negatively impacted by market- based governance push for interventions that will mitigate their worst tendencies. Because of this double movement, markets become a central site of class struggle as the working classes attempt to protect themselves from the dominant class's project of market expansion and the two use 'government and business, state and industry, respectively [as] their strong- holds' (Polanyi, 2001(Polanyi, [1944: 139). Updating Polanyi's ideas for the current moment, Fraser (2013) argues that the double movement fails to capture the importance of social reproduction and emancipation in strug- gles over the shape of markets. She makes the point that Polanyi's theory cannot capture the importance of the post-1960s' growth in eman- cipatory struggles against racism, imperialism, war, sexism, homophobia, ableism, and trans- phobia that do not fall neatly into the market/ anti-market divide. For those fighting against these systems of oppression, anti-market forces (as embodied in the welfare state) are often as oppressive as the market system itself. Fraser thus sets up a three-sided conflict, or a triple movement, between pro-market, anti-market, and emancipatory political forces. Notably, she argues there are no easy alliances between the three, with those pushing for emancipation often aligning with market projects promoted as opening access to resources that margina- lized groups have long been prevented from using. Burawoy (2003) sees parallels between Pola- nyi's work and the scholarship of culturally oriented Marxists such as Gramsci who posit that society forms the terrain of class struggle. Like other Marxists, Gramsci viewed the econ- omy as driven by the social mode of production, but on the terrain of the superstructure he believed that the shapes that capitalist societies take are the result of a complex interplay of economic, social, and political dynamics. Gramsci argued that, through struggle, different societal formations could emerge in ways not entirely determined by capitalist logics (Burawoy, 2003;Hart, 2002). This focus on the inde- terminacy of society can help us understand the workings of markets, since markets, as social/ sociotechnical, are also likely to be shaped by ideological struggles over their form. Further- more, Gramsci wrote of 'historical blocs' where dominant powers promote ideologies that 'cre- ate the terrain on which men move, acquire con- sciousness of their position, struggle, etc.' (2012 [1971]: 377). This is important in the current moment, where the dominant ideology is the ideology of market efficiency; in the words of Hall: In a world saturated by money exchange, and everywhere mediated by money, the 'market' experience is the most immediate, daily and uni- versal experience of the economic system for everyone . . . [and has become] the model of other social and political relations. (Hall, 1986: 38, emphasis in original) As Hall argues, the neoclassical ideal of the market has become the model for other social relations and this centrality of markets means that they have become a necessary terrain upon which hegemonic struggles must occur. As such, struggles within markets are often part of a greater hegemonic project to promote, or resist, the ideology of market rule and, as Fraser (2013) highlights, markets can also become the site of proxy battles against other hegemonic forces. Understanding markets as not only sites of struggle but as key sites in wider contests over market hegemony is what distinguishes a hybrid Polanyian/cultural Marxist approach from oth- ers that also view markets as sites of struggle. For example, while Barry (2002), Callon (2007), and Fligstein (1996) focus on the inter- nal dynamics through which market contesta- tion is pacified and Bourdieu (2005) conceives of the economy as fields of struggle between firms, these conceptions are largely geared at internal market relations. In the conception advocated here, however, because, as Marx argued, markets are sites where the compulsory laws of capital are ever present, it follows that contestation within markets is often connected to a general contestation of market rule. In essence, struggles over how markets are gov- erned, over their ideology, may cut across the particularities of specific markets and be a com- mon occurrence across many market institu- tions. Indeed, Hall (1986: 36, emphasis in original) argues that mounting an ideological struggle over markets was one of Marx's pur- poses in Capital, writing that: There is no fixed and unalterable relation between what the market is, and how it is construed within an ideological or explanatory framework. We could even say that one of the purposes of Capital is precisely to displace the discourse of bourgeois political economy . . . and to replace it. Channelling Gramsci and following Hall's thought, we can therefore understand markets as shaped by the material force of market ideol- ogy while recognizing that this relationship is not fixed and unalterable but rather is itself the site of struggle; this recognition opens an under- standing of the double or triple movement as a consciously-forged ideological struggle rather than a defensive one (Burawoy, 2003). This line of reasoning is used by Burawoy (2003) when he puts the Polanyian notion of the double movement in conversation with a Gramscian understanding of hegemony to create what he calls a 'Sociological Marxism'. Central to this project for Burawoy (p. 244) is understanding the relationship of markets to society; as he writes, 'Sociological Marxism's task is to understand under what conditions and in what form state and society will hold up the market juggernaut, throw up barriers to or rush headlong away from the commodification of land, labor, and money'. Burawoy departs from orthodox Marxism in approaching markets as the central relation defining his approach. He does this based on the idea that, while produc- tion creates the basis for hegemony, counterhe- gemony is most likely to emerge from an ideological break with the market ideology in the manner predicted by Polanyi's double movement: Gramsci makes a convincing case that accumula- tion based on capitalist relations of production is the material basis of capitalist hegemony but errs in thinking that production, or at least the experi- ence of production, can also provide the basis of counterhegemony . . . Whereas alienated and degraded labor may excite a limited alternative, it does not have the universalism of the market that touches everyone in multiple ways. It is the market, therefore, that offers possible grounds for counterhegemony. (Burawoy, 2003: 231) The 'Sociological Marxism' proffered by Burawoy thus places markets at the centre of struggles over civil society and offers a meso- level theory that can help answer the three chal- lenges outlined earlier. Drawing on Polanyi and, like Hall (1986), recognizing the commonality of the market experience, Burawoy suggests that a Gramscian understanding of hegemony and counterhegemony be focused on markets rather than production. This focus on counter- hegemonic projects brings into markets not only class dynamics but, following Fraser (2013Fraser ( , 2014, an array of other struggles. This is wit- nessed in the way that claims for justice become depoliticized through the creation of markets or the implementation of market solutions. For instance, calls for climate justice spur the cre- ation of markets for greenhouse gas emissions and the increasing inequality of wealth is chan- nelled into a demand for higher minimum wages on the labour market rather than other redistri- butive mechanisms. In this manner, markets become central to struggles over social and eco- logical justice. Burawoy's blending of the double movement with a Gramscian understanding of hegemony and counterhegemony therefore offers a bridge whereby the limits to markets can be held inside the frame while also studying the variability between markets. Importantly, this is not to say that every market will include a double or triple movement whereby destructive tendencies are curbed or demands for justice change how a market operates. Indeed, Block (2011) points out that in an era where market ideology dom- inates, counterlogics are often overridden and battles between capitalists over exchange will dominate market structure. Barry (2002) further argues that markets are often sites of a strong anti-politics where the use of expertise and tech- nologies of calculation overwhelms resistance to market rule (see also Ouma, 2015). What it does mean is that in many markets, especially markets where the advantages accrued to pow- erful actors are clearly visible or where markets are explicitly used to address calls for redistri- butive justice, struggles over the shape of mar- kets can include a counterhegemonic element that may help account for how and why markets diverge from the shapes assumed by rigid understandings of their functioning.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Markets as sites of sociospatial struggles", "text": "Conceptualizing markets as sites of struggle in this manner provides several promising direc- tions for the geography of markets. Through linking the social struggles within markets to wider power relations, the approach outlined here provides a meso-level theory between the micro-politics of markets and the macro-structures of capitalism. In doing so it also allows geographers to move forward in addres- sing the three challenges laid out in Section II: (1) defining markets, (2) understanding the role of space and place in their functioning, and (3) studying their roles in the societies and spaces they exist within. Regarding the definitional challenge, given the arguments by Polanyian and cultural Marx- ist scholars, it is clear that markets must be understood as more than just sites where the exchange of commodities is organized. This limited definition fails to capture the complex- ity of markets in an era where market rule is a defining feature of everyday life and, therefore, where contestation is an important element in many markets. In this light, a definition of mar- kets as sites of struggle over the networks, insti- tutions, and/or ideas and technologies through which the exchange of commodities is organized allows geographers to be clear about what makes a market -the organization of exchange -without tightly binding markets to the act of exchange in isolation from other social pro- cesses. Importantly, keeping exchange central in this definition provides a focal point for the subfield and allows geographers to study the commonalities that exist amid market varia- tions. Furthermore, it provides a core element from which geographers can move outwards as they study the distinct networks, institutions and/or ideas and technologies that characterize different markets, thereby providing a common thread and addressing Muellerleile and Akers's (2015) concern about the incoherency of the subfield. The insights of cultural Marxist and Pola- nyian scholars also provide a theoretical frame through which geographers can better under- stand the role of space and place in the function- ing of markets. This is because the social conflicts discussed by these scholars are inher- ently spatial, with markets functioning as cen- tral sites through which sociospatial relations are contested. Notably, this approach allows geographers to understand markets as key nodes in the 'geographies of power' which are often mobilized to remake the politics of a place or define new geographies (Prince, 2012: 140). Indeed, existing work has examined how trans- national networks of travelling expertise are used to set up particular kinds of markets in a manner that can have profound consequences for the future of a place (Cohen, 2016;Lai, 2011). More work of this nature is possible if the ways that the internal politics of markets are connected to broader sociospatial processes is integrated into research programs. By providing a theoretical framework for examining how dynamics such as the scalar politics of state restructuring or the financialization of the econ- omy are rolled out (or contested) through mar- kets, this approach helps geographers conceptualize the articulation between the often place-based politics of markets and circulating geographies of power. An approach centred on markets as sites of social struggle also has the potential to meet Jones's (2013) call to study how markets over- lap and relate across different spaces and scales. Again, work within the geography of markets hints at this potential but takes on new signifi- cance when read within the frame of markets as sites of struggle. For instance, processes of boundary drawing around markets have already been a source of scholarship (Berndt, 2013;Christophers, 2014b;Kama, 2014;Hall, 2015), but how and why groups contest and remake the scale of a market has significance beyond the politics of a particular market. Much as political ecologists have argued that the 'continuous reorganisation of spatial scales is an integral part of social strategies to combat and defend control over limited resources' (Swyngedouw and Heynen, 2003: 913), so too is this reorgani- zation a strategy for market actors who pick the scale of action which fits their mode of politics and advances their other projects. For example, Hall (2015) argues that the marketization of English higher education through the introduction of student fees depended on a depiction of the national, rather than global, labour market as the scale of action. While work studying the spatiality of markets is only in its nascent stage (Christophers, 2014c;Hall, 2015;Jones, 2013), the two examples above illustrate how a hybrid Polanyian/cultural Marxist approach can help advance the subfield's under- standing of the role of space and place in the functioning of markets. Finally, it is in understanding the roles that markets play in the societies and spaces they exist within that this approach has the most potential. Taking up where Christophers (2014a) leaves off in questioning how the varia- bility of market forms may be limited by their roles under capitalism, understanding markets as sites of struggle provides a mechanism to study both this variability and its limits. By probing how (or whether) resistance to capitalist and other hegemonic logics results in the contest of market rule and how (or whether) power is used to ensure that markets continue to function in a manner that ensures the smooth circulation of capital, geographers have a method of explor- ing the missing meso-level connections between the micro-politics of markets and the macro-structures of capitalism. This means an attention to the uneven power relations between actors struggling over markets and to how power is used to restructure markets and to pacify counterhegemonic projects. In other words, by studying how struggle occurs in mar- kets and the resulting mobilization of power, geographers can trace connections between markets and other social forces. Importantly, the potential of this approach also includes tracing how changes within mar- kets reverberate into other social relations. Geo- graphers cannot be content to understand how power is marshalled to pacify markets, but must also study how contestation within markets can impact other social and economic processes. For example, geographers could trace how con- testation over how a market functions is received by the various circuits of power invested in that market's structure, or how dif- ferent struggles against market rule are con- nected through social movements. In either case the priority must be tracing how changes in markets are linked to, and perhaps change, other arenas of social and economic life. In doing so, this approach builds on a long history of geographers studying the sociospatial con- nections that link seemingly disparate phenomena.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "V Conclusions", "text": "The growth of scholarship on markets is an important and exciting development in eco- nomic geography. Current work demonstrates the potential of studying the spatiality of mar- kets, highlighting the material effects of varia- tions in their form and the important role of space and place in how markets function. The popularity of the subfield is only likely to grow as the expansion of market rule continues una- bated despite the lessons of the financial crisis. Addressing critiques of, and challenges to, this work is an important next step as geographers studying markets engage with wider disciplin- ary debates. As recent articles have emphasized, addressing these critiques and challenges requires a focus on the commonalities between markets and on the roles they play in the spaces and societies they exist within (Braun, 2016;Christophers, 2014a;Muellerleille and Akers, 2015;Peck, 2012) -a process which requires new theoretical tools. This paper has argued that an approach to markets which highlights their role in social struggles can help geographers of markets address many of these challenges and provide a promising theoretical approach to understand- ing both market variability and its potential lim- its. Through probing how markets are tied to capitalist logics, how they become proxy sites of other hegemonic struggles, and how resis- tance is built within their institutional walls, geographers can advance their understanding of the sociospatial nature of markets. While the suggestions in this paper have been principally theoretical, it is essential for empirical work to further probe these connections given the strength of market ideology under neoliberal- ism. This is especially important if Burawoy is right and there is the potential for counterhege- monic projects within markets. If this is the case, then how do such projects arise, and what are the characteristics of the particular markets that allow them to do so? Or, if it not the case, what are the geographies of power marshalled in defence of markets and how do they work? Geo- graphers should have much to say in answer to these important questions. would also like to thank the three anonymous reviewers and editor Christian Berndt who helpfully pushed this paper in a new, productive direction. All errors are my own responsibility.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Declaration of conflicting interests", "text": "The author(s) declared no potential conflicts of inter- est with respect to the research, authorship, and/or publication of this article.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Funding", "text": "The author(s) received no financial support for the research, authorship, and/or publication of this article.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Notes", "text": "1. As Rosenbaum (2000) argues, definitions of markets almost always refer to a site (not always physical) where exchange takes place between actors. Yet, as Rosenbaum further points out, to be a distinct object markets must have a function that distinguishes them from simple exchange. For this reason, markets are best characterized as an institutionalized and routinized form of exchange. Hodgson (2008) comes to a similar conclusion, noting that the modern understanding of markets can be traced to physical sites such as the Greek agora, where exchange did not simply take place but was also organized. 2. Lawson (2013) lists three assumptions: (1) that individ- uals constitute the appropriate locus for understanding the economy; (2) that actors make decisions in some rational, usually utility-maximizing, manner; and (3) that information is either perfectly or mostly available to market actors. To this we might add the assumed homogeneity (and exchangeability) of commodities (Fine and Lapavitsas, 2000). 3. This does not mean these approaches accept the neo- classical and Marxist description of markets; the oppo- site is true, and the notion that markets are social/ sociotechnical would hardly be a surprise to feminist economic geographers. It is rather to say that markets are rarely a central concern for these approaches. 4. There are approaches outside of this review that do not hold a deterministic view of market relations but which have not been widely used by geographers studying mar- kets. The feminist attention to how gender shapes the economy, evolutionary economics' focus on continual economic transformations, or the Austrian view of the necessity of sublimating social relations to market rule diverge from deterministic understandings of markets. 5. While the embeddedness approach has been taken up in the regional economics literature, it has mostly been utilized to understand networks within production as a 'counterworld to markets' rather than to study markets directly, as Granovetter does (Jones, 2013). 6. The labels used here are not unproblematic, and there are overlaps between them. Furthermore, as Fourcade (2007) writes, each category has its own internal differentiations. 7. Rather than viewing a divide between micro-politics and macro-structures of capitalism, many using the SSEM approach would argue that the point is to find the macro-structures contained in these micro-politics. For example, Callon (2005: 5), while against a singular theory of capitalism, suggests an openness to the French school of regulation's focus on multiple types of capit- alism; however, he states that 'instead of situating it, as [the regulation school] does, in institutional macroar- rangements, it seems preferable to try to locate it in socio-technical agencements themselves'.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Acknowledgements", "text": "Thanks go to Jamie Peck and Martin Danyluk for commenting on multiple drafts of this article and Josh Akers, Rosemary Collard, and Chris Mueller-leile for their helpful feedback on the initial draft. I", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Author biography", "text": "Dan Cohen is a PhD candidate at the University of British Columbia. His dissertation project is on the creation of markets for publicly-funded education in the United States.", "title": "Between perfection and damnation: The emerging geography of markets", "file_name": "Cohen - 2018 - Between perfection and damnation The emerging geo.pdf"}
{"section": "Abstract", "text": "We attempt to replicate 67 papers published in 13 well-regarded economics journals using author-provided replication files that include both data and code. Some journals in our sample require data and code replication files, and other journals do not require such files. Aside from 6 papers that use confidential data, we obtain data and code replication files for 29 of 35 papers (83%) that are required to provide such files as a condition of publication, compared to 11 of 26 papers (42%) that are not required to provide data and code replication files. We successfully replicate the key qualitative result of 22 of 67 papers (33%) without contacting the authors. Excluding the 6 papers that use confidential data and the 2 papers that use software we do not possess, we replicate 29 of 59 papers (49%) with assistance from the authors. Because we are able to replicate less than half of the papers in our sample even with help from the authors, we assert that economics research is usually not replicable. We conclude with recommendations on improving replication of economics research.", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Chang et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Introduction", "text": "In response to McCullough and Vinod (2003)'s failed replication attempt of several articles in the American Economic Review (AER), then-editor of the AER Ben Bernanke strengthened the AER's data and code availability policy to allow for successful replication of published results by requiring authors to submit to the AER data and code replication files (Bernanke, 2004). Since the AER strengthened its policy, many of the other top journals in economics, such as Econometrica and the Journal of Political Economy, also started requiring data and code replication files. There are two main goals of these replication files: (1) to bring economics more in line with the natural sciences by embracing the scientific method's power to verify published results, and (2) to help improve and extend existing research, which presumes the original research is replicable. These benefits are illustrated by the policy-relevant debates between Krueger (1994, 2000) and Neumark and Wascher (2000) on minimum wages and employment; Hoxby (2000Hoxby ( , 2007 and Rothstein (2007) on school choice; Levitt (1997Levitt ( , 2002 and McCrary (2002) on the causal impact of police on crime; and, more recently, Reinhart and Rogoff (2010) and Herndon, Ash, and Pollin (2014) on fiscal austerity. In extreme cases, replication can also facilitate the discovery of scientific fraud, as in the case of Broockman, Kalla, and Aronow (2015)'s investigation of the retracted article by LaCour and Green (2014). This article is a cross-journal, broad analysis of the state of replication in economics. 1 We attempt to replicate articles using author-provided data and code files from 67 papers published in 13 well-regarded general interest and macroeconomics journals from July 2008 to October 2013. This sampling frame is designed to be more comprehensive across well- regarded economics journals than used by existing research. Previous work has tended to focus on a single journal, such as McCullough, McGeary, and Harrison (2006), who look at the Journal of Money, Credit and Banking (JMCB); McCullough and Vinod (2003), who attempt to replicate a single issue of the AER (but end up replicating only Shachar and Nalebuff (1999) with multiple software packages); or Glandon (2010), who replicates a selected sample of nine papers only from the AER. Using the author-provided data and code replication files, we are able to replicate 22 of 67 papers (33%) independently of the authors by following the instructions in the author- provided readme files. The most common reason we are unable to replicate the remaining 45 papers is that the authors do not provide data and code replication files. We find that some authors do not provide data and code replication files even when their article is published in a journal with a policy that requires submission of such files as a condition of publication, indicating that editorial offices do not strictly enforce these policies, although provision of replication files is more common at journals that have such a policy than at journals that do not. Excluding 6 papers that rely on confidential data for all of their results and 2 papers that provide code written for software we do not possess, we successfully replicate 29 of 59 papers (49%) with help from the authors. Because we successfully replicate less than half of the papers in our sample even with assistance from the authors, we conclude that economics research is usually not replicable. 2 Despite our finding that economics research is usually not replicable, our replication success rates are still notably higher than those reported by existing studies of replication in economics. McCullough, McGeary, and Harrison (2006) find a replication success rate for articles published in the JMCB of 14 of 186 papers (8%), conditioned on the replicators' access to appropriate software, the original article's use of non-proprietary data, and without assistance from the original article's authors. Adding a requirement that the JMCB archive contain data and code replication files the paper increases their success rate to 14 of 62 papers (23%). Our comparable success rates are 22 of 59 papers (37%), conditioned on our having appropriate software and non-proprietary data, and 22 of 38 papers (58%) when we impose the additional requirement of having data and code files. Dewald, Thursby, and Anderson (1986) successfully replicate 7 of 54 papers (13%) from the JMCB, conditioned on the replicators having data and code files, the original article's use of non-confidential data, help from the original article's authors, and appropriate software. Our comparable figure is 29 of 38 papers (76%).", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Chang et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Methodology and Sampling Frame", "text": "Our sampling frame includes papers from 13 well-regarded macroeconomics and general interest economics journals: American Economic Journal: Economic Policy, American Eco- Quarterly Journal of Economics. We choose papers from these journals because of the rela- tive likelihood that such papers will have a policy effect and also influence future research. 3 We do not select these journals to single out a particular author, methodology, institution, or ideology. From our sample of journals, we browse for original research articles published in issues from July 2008 to October 2013. 4,5 Within these issues, we identify all papers with the following three characteristics: (1) an empirical component, (2) model estimation with only US data, and (3) a key empirical result produced by inclusion of US gross domestic product (GDP), published by the Bureau of Economic Analysis (BEA), in an estimated model. 6 We choose to focus on GDP because of its status as a standard macroeconomic statistic and its widespread use in research. 7 For each paper in this set, we attempt to replicate the key empirical results. 8 We focus on the key empirical results for two reasons: (1) replicating only the key results allows us to expand the sample to more papers, and (2) the key result of the paper is presumably what drove the paper's publication; robustness checks merely serve as confirming evidence. Defining a key result is subjective and requires judgmental decisions on our part. We attribute a key result of the paper to GDP when the authors themselves refer to GDP as driving a key result, or when a discussion of GDP is featured either in the abstract or prominently in the introduction of their work (or both). We also take key results as those that appear in figures and tables. 9 We find 67 papers that fit these criteria. Of these papers, 6 papers use proprietary data for all of the key results, so we do not include them in our replication exercise (Fisher and Peters, 2010;Alexopoulos, 2011;Alexopoulos and Cohen, 2011;Hall and Sargent, 2011;Bansak, Graham, and Zebedee, 2012;Gilchrist and Zakraj\u0161ek, 2012). If a subset of the key results could be obtained using non-proprietary data, then we attempt to replicate those results. For the remaining papers that use public data and are published in journals that maintain  (2006), we find that journal data and code archives are incomplete. Of the 35 papers that use public data and are published in journals that require data and code replication files, we obtain files for 28 papers (80%) from journal archives. For papers where we are unable to obtain replication data and code files from journal archive sites, either because the mandatory files are is missing or because the paper is not subject to a data availability policy, we check the personal websites of each of the authors for replication files. If we are unable to locate replication files online, then we email each of the authors individually requesting the replication files. 10 Of the 7 papers that use public data, are subject to a data and code policy, and do not have replication files on the journal's archive site, this procedure nets us one additional set of replication files. Therefore, we are unable to locate replication files for 6 of 35 papers (17%) that are published in journals that require submission of data and code replication files. For papers published in journals without a data and code availability policy and that use public data, we are unable to obtain data and code replication files for 15 of 26 papers (58%). We do not single out any paper or author that fails to comply with a journal's mandatory data and code policy. We therefore only report these summary statistics of compliance with data availability policies and only cite papers that we either successfully replicate, that use proprietary data, or where we have what appears to be a complete set of replication files in a software we do not possess. Our intention is to highlight the general state of replication files for published economics research, not to berate any given author, methodology, institution, or ideology. To determine whether a paper was subject to a data availability policy, we check the implementation dates of the journal data policies and compare them to the publication and submission dates of the published work. If the journal's website does not allow us to extract this information, then we query the editorial office as to when their data availability policy became effective. We do not ask the editorial offices whether a particular paper was subject to a data availability policy. Aside from papers with proprietary data, we find that journal data archives do not provide lists of potentially exempt papers. Therefore, we are unable to determine whether a paper is exempt for a reason other than using proprietary data, although we are not aware of reasons why journals would grant a paper a data and code exemption other than for proprietary data. The authors we query whose papers we believe are subject to a data availability policy yet whose replication files we are unable to locate do not volunteer whether their papers are exempt from the policy, and we do not ask the authors for this information. For the papers for which we are able to obtain data and code replication files, we attempt to replicate the key results of the paper using only the instructions provided in the author readme files. If the readme files are insufficient or if the replication files are incomplete (or both) and the paper is subject to a replication policy, then we email the corresponding author (if no corresponding author, then the first author) for either clarification or to request the missing files. If we do not receive a response within a week, then we query the second author, and so on, until all authors on the paper had been contacted. 11 We define a successful replication as when the authors or journal provide data and code files that allow us to qualitatively reproduce the key results of the paper. For example, if the paper estimates a fiscal multiplier for GDP of 2.0, then any multiplier greater than 1.0 would produce the same qualitative result (i.e., there is a positive multiplier effect and 11 If we already contacted the authors to request data or code but were having difficulty executing the code, then we only queried the authors whom we did not yet contact. We initiate contact with each author a maximum of one time. that government spending is not merely a transfer or crowding out private investment). 12 We define success using this extremely loose definition to get an upper bound on what the replication success rate could potentially be. 13 We allow for minimal re-working of the provided files, following the procedure of McCullough, McGeary, and Harrison (2006). 14 One dimension where we are unable to follow the authors exactly is the software ver- sion they use. To execute the replications, we make use of the following software version- available in the readme, we attempt to run the software version-operating system combina- tion specified by the authors. When the replication files fail to execute on a given software version-operating system combination, the author readme did not specify a particular soft- ware version-operating system combination, and it appeared that the data and code were complete, we email the authors to find out which combination they use.", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Chang et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Results", "text": "This section presents summary statistics of our replication attempts. 16 12 This definition corresponds to a rating of three out of five, \"minor discrepancies,\" or better by Glandon (2010), and \"partially successful replication\" or better by McCullough, McGeary, and Harrison (2006). 13 This definition is less stringent than the definition for replication success of Harrison (2006, 2008) 14 McCullough, McGeary, and Harrison (2008), in their appendix, suggest that \"the author [whose study is being replicated] provides code such that data and code, when placed in the same subdirectory, will execute; and that the output from doing this also will be provided... and produces the results in his paper,\" which implies that replication files should contain the data and code that requires no re-working. If the code is clearly missing the ability to replicate results, then we do not attempt to re-code the procedures ourselves. 15 We check the replication results of a small sample of selected papers across different versions of Matlab for Windows and find very minor differences. None of the differences in results across different versions of Matlab are qualitatively significant. The web appendix lists the software version-operating system combination we use for each successfully replicated paper. 16 Interested readers can find detailed results for each paper we successfully replicate in the web appendix on Chang's website, https://sites.google.com/site/andrewchristopherchang/research.  Table 2 shows that our overall replication success rate is 29 of 67 papers (43%). Table 2, Panel B shows that we successfully replicate 23 of 39 papers (59%) from journals that require data and code replication files. This rate compares to 6 of 28 (21%) of the papers from journals that do not require such files, shown in Table 2, Panel C. These replication rates are similar when we only consider papers with publicly available data: we successfully replicate 23 of 35 (66%) of the papers from journals with mandatory data and code policies and 6 of 26 (23%) of the papers from journals without such policies. The presence of a mandatory data and code policy does not necessarily imply a causal relationship from the policy to successful replication. Authors select which journals to submit papers to, taking into account idiosyncratic journal policies such as mandatory submission of replication data and code. However, we find that it is significantly easier to replicate published research that comes from journals that require authors to submit their data and code. Table 3, Panel A provides explanations for why we are unable to replicate papers ac- cording to four broad classifications: \"missing public data or code,\" \"incorrect public data or code,\" \"missing software,\" or \"proprietary data.\" Panel B provides the breakdown for jour- nals that require data and code. Panel C shows the results for journals that do not require data and code. From Table 3, Panel A we find that we are unable to replicate 21 papers because of \"missing data or code,\" which constitutes the majority of our failed replications (55%). As we outline in our methodology, for each of these unsuccessful replications we attempt to secure data and code from the authors by visiting their personal websites, visiting the journal websites (when the journal requires authors to submit data or code), and sending email requests. We classify an unsuccessful replication as \"missing data or code\" when at least one of two events occur: (1) the replication code file(s) are clearly missing necessary author-written functions for a subset or all of the key results or (2) the replication data file(s) are missing at least one variable. If the replication data has a shorter data sample than reported in the paper, then we still attempt the estimation and do not necessarily classify the paper as \"missing data or code.\" We are unable to replicate 9 papers (24% of failed replications) because of \"incorrect data or code.\" We classify an unsuccessful replication as \"incorrect data or code\" when all variables are present in the dataset and the authors self-identify code for each of the key figures and tables we attempt to replicate. The author-provided code may finish executing and give different results or the code may not finish executing and still fall into this category. We believe we do not have the needed software to run two papers (Senyuz, 2011;Jermann and Quadrini, 2012) because we are unable to locate a necessary packaged function in our versions of the appropriate software, because of significant syntax changes between software versions, or because the authors declared that they use a particular software version and we are aware that our software would not be compatible. However, it is tricky differentiating be- tween an unsuccessful replication due to \"incorrect data or code\" or due to \"missing software.\" Because the implementation of packaged functions may differ across software versions even without syntax changes, we believe the number of failed replications we classify as \"missing software\" is a lower bound. It is possible that a paper we classify as \"incorrect data or code\" is actually replicable with the appropriate operating system-software combination, so some of the papers that we classify as \"incorrect data or code\" may belong in the \"missing software\" category. However, we cannot verify this statement without additional documentation. Table 4 shows our summary statistics for successful replications independent of the au- thors versus replications that were successful with the author's help. Overall, we find that contacting the authors marginally improves our success rate for replication. Of the 29 suc- cessful replications, we complete 22 without any help from the authors.", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Chang et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Conclusion and Recommendations", "text": "In this article, we attempt to replicate 67 papers from 13 well-regarded economics journals using author-provided data and code replication files. Improving on existing work evaluating the state of replication in economics, our sampling frame is broader across different journals and covers a large number of original research articles. We replicate 22 of 67 papers (33%) by using only the authors' data and code files, and an additional 7 papers (for a total of 29 papers, 43%) with assistance from the authors. The most common cause of our inability to replicate findings is that authors do not provide files to the journal replication archives, which constitutes approximately half of our failed replication attempts (21 of 38 papers, 55%). Because we are able to replicate less than half of the papers in our sample, we conclude that economics research is generally not replicable. We now turn to some recommendations that we feel would improve the ability for re- searchers to replicate and extend published articles, largely echoing the recommendations of McCullough, McGeary, and Harrison (2006). \u2022 Mandatory data and code files should be a condition of publication. Our replication success rate is significantly higher when we attempt to replicate papers from journals that have a mandatory replication data and code submission policy. We believe that replication files need to encompass both data and code. As shown in Table 2, the data-only archives at Economic Journal and Journal of Applied Econometrics only allow for replication of 4 of 20 papers (20%) that use non-confidential data, compared to the replication success rate of 23 of 35 papers (66%) that use non-confidential data from journals that require both data and code. \u2022 An entry in the journal's data and code archive should indicate whether a paper without replication files in the journal's archive is exempt from the journal's replication policy. Among papers that we believe were subject to a mandatory data and code policy, we are unable to acquire replication files for 6 of 35 papers (15%) even after emailing, and often receiving a response from, the authors. However, we are unsure whether these six papers are exempt from their respective journal's mandatory data and code policies, and the authors did not volunteer whether their papers are exempt in response to our requests for replication files. Therefore, we suggest that journals include an exemption entry in their replication archives. This note in the replication archives would have four virtues: (1) it is low-cost for the journal, (2) it would save authors who are exempt from submitting replication files from needing to respond to queries about replication files, (3) it would save would-be-replicators from searching for replication files for papers that are exempt from the journal's policy, and (4) it would identify those authors who are not compliant with the journal's mandatory data and code policy. \u2022 Readme files should indicate the operating system-software version combination used in the analysis. We attempt to use the operating system-software version combination reported by the au- thors in their readme files, but we notice that very few readmes include the operating system- software version combination used to conduct their analysis. When we ask authors about the operating system or software version they use to run their models, most authors do not recall this information. Although it is not a focus of our paper, we notice minor discrepancies for a selected subset of papers when running programs on different versions of Matlab (although the discrepancies are not large enough to change the key qualitative results). \u2022 Readme files should contain an expected model estimation time. Many macroeconomic models are estimated with Bayesian (i.e., Markov Chain Monte Carlo) methods, which can take a considerable amount of processing time to execute even under the best of circumstances. We encountered a few instances where we believed an estimation was executing, only to find out weeks later that the programs were stuck in an infinite loop and were supposed to run in much less time. In addition, frequently programs are not written to optimize computation time and also frequently written without a progress bar, so there is no way to track the expected completion time of estimation. A low-cost alternative to a progress bar is simply writing the expected estimation time in the readme file. \u2022 Code that relies on random number generators should set seeds and specify the random number generator. Optimization algorithms often rely on a set of initial conditions, which are commonly speci- fied through a random number generator. For any research that relies on a random number generator, replication requires the same set of numbers that are generated in the published article. \u2022 Readme files should clearly delineate which files should be executed in what order to produce desired results. Occasionally, we are presented with replication data and code that requires files to be exe- cuted in a particular order to furnish published results. In cases where the execution order is critical but unspecified, we spend a considerable amount of time attempting to determine the proper order of execution and, in some cases, ultimately fail to do so. We now turn to two recommendations that will improve the ability of researchers to extend published work, in addition to merely replicating it. \u2022 Authors should provide raw data in addition to transformed series. While only the transformed data are needed to conduct replication of published results, raw data facilitate potential extensions of research. For example, raw data allow for the investigation of the effect that revisions to macroeconomic data have on previously published research, as in Croushore and Stark (2003) and Chang and Li (2015). \u2022 Programs that replicate estimation results should carry out the estimation. We notice that the replication files for a few papers run smoothly and exactly furnish the results of the tables and figures as published. However, oftentimes the results in tables and figures depend on a model's parameters being estimated. Some of these replication files, instead of estimating the models, take the relevant parameters as given to produce results in tables and figures. For verification of published results, and particularly for purposes of extending research, we assert that code that actually estimates the relevant models would be far more useful.  Barro and Redlick (2011) Baumeister and Peersman (2013) Canova and Gambetti (2010) Carey and Shore (2013) Chen, Curdia, and Ferrero (2012) Clark and McCracken (2010) Corsetti, Meier, and M\u00fcller (2012  Journal of Applied Econometrics requires data only. Economic Journal currently requires data and code, but the papers in our sample were not subject to a data and code policy according to the Economic Journal's editorial office.  ", "title": "Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say \"Usually Not\"", "file_name": "Chang et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf"}
{"section": "Include the most central and important statistically significant result (as emphasized", "text": "in the original study) of this study among all within or between subjects treatment comparisons. The interpretation of which was the most central and important result in the selected study was made by us, but the original authors could provide feedback on this when they commented on the draft of the replication report sent to them for feedback (see below). Prior to conducting the replications we did not receive feedback from any of the original authors, that we had selected the \"wrong\" result. After conducting the replication and seeing the results the original authors in one study argued that we had not selected the most central and important result. This was for the replication of Janssen et al. 3 , where the original authors ex post argued that comparing the effect of communication on net earning within subjects was a more central result of their study (in our replication we replicated the result on the effect of communication on net earnings between subjects as the between subjects comparison is a stronger identification of causal effects). In some other cases the original authors of papers reporting the results of a series of studies argued that the first study selected for replication did not contain the most important result in their paper; but in these cases the result selected for replication followed directly from applying criteria 1 above. 3. If more than one equally central result remained, we randomly picked one of the results for replication. This was the case for three papers 4\u22126 . The first criterion differs from the criteria used in the Replication Project: Psychology (RPP) 7 and the Experimental Economics Replication Project (EERP) 8 . In those projects the last study, rather than the first study, was included for studies reporting a series of studies. To include the last study (or the first study) is an arbitrary rule for selecting which study to replicate, and there is little a priori reason to believe that replicability differs depending on the order studies are reported. However, it may be problematic if a rule is established that replications are always conducted on the last study, as that may lead researchers to present the study they believe the most likely to replicate, last in the paper. To counteract such possible strategic incentives we decided to include the first rather than the last study for papers reporting a series of studies. We deviated from that criteria for one study: the Rand et al. 9 paper. The Rand et al. 9 paper included 10 studies. Studies 1-5 were correlational studies not estimating treatment effects, and studies 6-7 tested the effect of time pressure on contributions to a public good; study 7 is included in a recently published Registered Replication Report study 10 (study 6 is similar to study 7, but conducted on AMT instead of in the lab). Based on criteria 1 above, study 6 would have been selected for replication as it was the first treatment effect study, but as this study was already subject to a Registered Replication Report study organized by Perspectives of Psychological Science, we chose not to select this study for replication (one of the authors of this project was also involved in one of the replications of the Registered Replication Report). A further reason for not replicating study 6 or 7 in Rand et al. 9 is that it has been noted that the reported significant finding was due to selection bias and analyzing the data appropriately as \"intention to treat\" did not yield a significant effect 11 . The intention to treat result was also the primary focus of the Registered Replication Report 10 . The Registered Replication Report found that the result did not replicate, but this was not known to us when we decided not to replicate this study. For the above reasons we decided to include study 8 in Rand et al. 9 , as it was the first treatment effect study not subject to an ongoing Registered Replication Report study and it was not subject to a controversy over the appropriate analysis of the data. Note also that for some of the other papers our criteria above implied that study 1 was not replicated, as it did not report any significant treatment effects. This was the case for Duncan et al. 12 in which we included study 1b, Gervais and Norenzayan 13 in which we included study 2, and Wilson et al. 14 in which we included study 8. In our initial screening we selected 22 studies that met the above inclusion criteria. We contacted the original authors requesting materials and asking for feedback on an initial version of the replication report for these 22 studies. However, during this process we discovered that the result selected for replication for one of these studies did not fulfill our inclusion criteria and this study was excluded. This was the study by Mani et al. 15 . Originally we intended to replicate the result that inducing thoughts about financial hard- ship reduces cognitive function more among poor individuals than rich ones (from study 1 in the paper). This was an interaction effect between a treatment that induced thinking about a hard versus an easy financial problem, and individual differences in participants' income (i.e., above or below median household income). But as the income level was not assigned to households as a randomized treatment, but rather was based on their actual income, this does not match the definition of a treatment effect; it is instead a comparison of a treatment effect between different groups (rich and poor participants) and the com- parison cannot be given a causal interpretation (unless strong assumptions are made). It is thus not consistent with our criteria for the type of experiment (a between or within subjects treatment effect) which was only realized after the initial screening. It was also a borderline case in terms of the subjects included in the experiment criteria (the exper- iment was conducted at a mall with subjects recruited from the mall). This study was thus excluded from the replication project prior to starting any data collection and our final replication sample consisted of 21 studies. If an original study included more be- tween subjects treatments than the treatments selected for replication, we in general only included the treatments used for the result selected for replication.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Replication procedure", "text": "Statistical Tests. In the replications, we used the same statistical test as was used in the original study. For one study 16 we deviated from this rule as it turned out that the exact same test as used in the original study yielded clearly implausible standard errors on the replication data, stemming from the two-way clustering (on the individual and round level) of standard errors used in the original paper; we instead modified the test slightly and used one-way clustering (on the individual level) as the main replication test. This, however, does not affect the conclusion about whether the study replicates or not. See the replication report for this study for further details. As we used the same statistical tests as used in the original studies, we did not test if the data collected in the replications met the assumptions of the statistical tests used (such as assumptions about normality or equal variances). Thus, for tests based on normal distributions the data distribution was assumed to be normal but this was not formally tested. All reported p-values in the paper and Supplementary Information are based on two-sided tests.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Replication teams. There were five replication teams: a team at CalTech and Wharton", "text": "(responsible for four replications); a team at the Center for Open Science and the Uni- versity of Virginia (responsible for five replications); a team at the Stockholm School of Economics (responsible for five replications); a team at the National University of Sin- gapore (responsible for three replications); and a team at the University of Innsbruck (responsible for four replications). All the online experiments using AMT were replicated by the same team (the team at the Stockholm School of Economics). Language. One of the original experiments was conducted in German, one in French and one in Italian; the remaining ones were conducted in English. The replication of the original study in German was conducted in German (the team at the University of Innsbruck), but the replications of the original studies in French and Italian were conducted in English. Fifteen out of the 18 original experiments in English were replicated in English and the remaining three studies were replicated in German (the team at the University of Innsbruck). Experimental software. The same software and computer programs as in the original experiments were used to conduct the replications whenever possible, but for some studies we needed to program new software to conduct the replications: \u2022 The experiment in Aviezer et al. 17 was implemented using the software package E- Prime. While the original authors were eager to provide us with the original code, the replication experiments were implemented using a custom-designed software module using oTree 18 . However, instructions used in the original experiment were provided and the final program was approved by the original authors. \u2022 The original authors were unable to retrieve the E-Prime computer program used in the original experiment by Karpicke & Blunt 19 . Specifically, in the original study, the sea otters text was presented on the computer screen, as was the answer box for the retrieval practice condition. However, all other tasks were completed using pencil and paper in the original study. For this reason, our replication used hard copies of all of the original materials used in the original study (including the sea otters text and the retrieval practice answer box originally run through the E-Prime program). The original authors agreed that the change from computer to paper responses should not be a consequential difference for the effect studied in the replication. \u2022 In Kovacs et al. 20 the original experiment was carried out in Psyscope X to mea- sure reaction times, but the original program is no longer available. Therefore we developed a computer program to measure reaction times used in the replication. \u2022 The software used in the original experiment of the study by Morewedge et al. 6 , Adobe Authorware, is no longer supported. The replication experiment was im- plemented using z-Tree 21 . However, instructions and pictures used in the original study were made available by the original authors and the final software used in the replication experiment was approved by the original authors. \u2022 The original study by Nishi et al. 16 was conducted using a software called Breadboard, but the version used by the original authors is no longer supported (and the original authors did not provide the code for the version used in the original experiment). We therefore programmed the experiment from the beginning in the most recent version of Breadboard. \u2022 The original authors of Ramirez & Beilock 22 did not provide the software used in the original study, and the experiment was therefore programmed in Qualtrics based on the materials and instructions provided by the original authors. \u2022 For the replication of Rand et al. 9 , Qualtrics was used instead of LimeSurvey using the same phrasing as in the original experiment (and this was approved by the original authors). \u2022 For the replication of Sparrow et al. 23 , the original authors did not respond to our requests for feedback and the original materials and software used, and the senior author passed away in 2013. We therefore programmed the software for this replication ourselves using oTree 18 based on the information provided in the original paper and the supplementary materials. But as we received no feedback about the design from the original authors, we could not verify that the instructions and implementation of the experiment in the replication were identical to the original study. After the replication had been conducted we received some feedback from the original authors detailing the following design differences. (i) The cognitive load manipulation differed between the original study and the replication, such that in the original study participants were asked to remember a different six-digit number for each word used in the modified Stroop task, whereas in the replication participants were asked to remember the same six-digit number while completing all trials of the modified Stroop task. (ii) The replication used 48 trials of the modified Stroop task whereas the original study used 24 trials. (iii) The replication excluded mistakes from the analysis (where participants click on the key for the wrong color), whereas this was not done in the original study. However, the conclusion about the study not replicating remains unchanged when only the first 24 trials are included in the analysis or if mistakes are not excluded from the analysis. It also turned out that the number of participants reported in the original paper was not correct, which affected the power of this replication to some degree (see below). While we consider it implausible, we cannot rule out the possibility that the difference in the cognitive load manipulation impacted the replication result. Replication reports and pre-registration. The replication team responsible for each replication wrote a replication report detailing the planned replication (with the following sections: hypothesis to replicate and bet on; power analysis and criteria for replication: first data collection; power analysis and criteria for replication: second data collection; sample; materials; procedure; analysis; and differences from the original study). A draft of each replication report was sent to the original authors for feedback, and the replica- tion reports were revised based on the comments; this process continued until the origi- nal authors approved the replication report (except for the Sparrow et al. 23 replication, where the original authors did not provide any feedback on the replication report and the Ramirez & Beilock 22 replication where the original authors after seeing the replica- tion results argued that they had not approved the replication report). This version of each replication report (the pre-replication version) was then posted at the project web- page (www.socialsciencesreplicationproject.com) and pre-registered at the Open Science Framework (OSF, https://osf.io/pfdyw/) prior to starting the data collection (we also saved all communications between the original authors and the replication teams on a dedicated e-mail account). After the replications had been conducted, the replication re- ports were updated with the results of the replication (the following four sections were added to the reports: results for the first data collection (90% power to detect 75% of the original effect size); results for the first and second data collection pooled (90% power to detect 50% of the original effect size); unplanned protocol deviations; and discussion. The replication reports with the updated results were again sent to the original authors for comments. After revision, the final versions of the replication reports were posted at www.socialsciencesreplicationproject.com and at OSF (https://osf.io/pfdyw/) (the ver- sions prior to the replications and the final versions are posted and publicly available). In Supplementary Table 2, which lists each study, we document whether the original au- thors of that study shared the materials of the original study and whether or not they approved of the pre-replication version of the Replication Report with the pre-registered design of the replication. In the table we also note if the replication used the same soft- ware as the original study. We used the same exclusion criteria as used in the original studies for excluding any observations in the analyses; any deviations from this are de- tailed in the \"Differences from the original study\" section (if the deviation was planned and pre-registered) or the \"Unplanned protocol deviations\" section (if the deviation was unforeseen and made after the data collection) in the replication reports. At the end of the Supplementary Methods we list any \"unplanned protocol deviations\" for each of the replications. We invited all original authors to post a comment about the replication alongside the replication reports at the project website and OSF. For several studies comments by the original authors have been posted at the time of writing this SI. In these comments the original authors have noted some perceived limitations on the replications of their studies. Also, we received some comments in the journal peer review process. These are briefly discussed below (the comments of the original authors of Sparrow et al. 23 have also already been discussed above). The Duncan et al. 12 study replicated in Stage 2 based on the statistical significance crite- rion, but with a smaller effect size than observed in the original study. Here the original authors observe that the memory accuracy was lower in the replication than in the original study, and argues that this implies higher measurement error in the replication and that this may have lowered the replication effect size. However, the lower memory accuracy in the replication may be due to a lower memory ability in the replication sample, and we cannot differentiate between these two explanations in the data. The original authors also note that the replication results show larger treatment effects when analyzed according to the subjective mnemonic reports (i.e. based on the subjective evaluation of whether the preceding trial was new or old instead of the objective classification). Based on the subjective mnemonic reports the result is already significant in Stage 1 (p = 0.035), and the results after Stage 2 are more significant than in the main replication test based on the objective classification (t(91) = 5.78 with the subjective classification and t(91) = 4.63 with the objective classification). The results based on the subjective classification are re- ported in the replication report for the study and are consistent with the conclusion of the original study replicating based on the pre-registered test for the objective classification. Janssen et al. 3 whose study replicated according to the statistical significance criterion, but with a smaller effect size than in the original study, argues that the result tested is not the most important statistically significant finding of their study (the result tested is that communication increase average earnings in a common-pool resources game). This feedback was not communicated to us prior to conducting the replication when we made a priori commitments to identifying the key finding. The original authors argue that the effect of communication was of main interest to their study, but that the within-subject test of the effect of communication was more central to their study than the between-subjects test of communication we replicated. We originally decided to replicate the between- subjects test rather than the within-subjects test, as the identification of the causal effect is stronger in the between subjects test. The original authors of Kidd & Castano 24 acknowledge that our replication is a direct replication of their study 1, with the methods closely following those of the original study. However, they argue that the design used in their study 1 has important limitations, and that subsequent studies have improved on the methodology through better methods of removing individuals who did not read their assigned texts (methods of excluding individ- uals with low reading times). They therefore argue that our replication is a relatively poor test of the underlying hypothesis. Here we note that we followed their original procedures closely, and the replication results fail to support the original findings and provide strong support for the null hypothesis. We are also not convinced that removing observations from the data is an improvement in methodology, but that intention to treat is in general a preferred methodology for identifying causal effects (especially in this case as the reading time may be endogenous to the treatment, causing selection bias if individuals with low reading times are excluded). Lee & Schwarz 25 argue that an unintended methodological difference between the original study and the replication may have affected the results of the replication. In the origi- nal experiment, participants examined covers of 30 music albums, picked 10 they would like to own, wrote down these 10 albums' titles and artist names, and reported their pre-manipulation rankings for these 10 albums in order of preference. Next, participants worked on filler tasks while the experimenter prepared a different form by listing the 10 albums in alphabetical order of the artist names (rather than album titles). The presen- tation order of albums did therefore differ in the pre-vs. post-manipulation evaluation and the original authors argue that this is important for the design as they argue that easy recall of one's pre-manipulation evaluation may constrain one's post-manipulation evaluation. In the replication, we did by mistake not follow this step during the data collection. Instead, the presentation order of albums in the post-manipulation evaluation was the same as in the pre-manipulation evaluation if participants copied them in that or- der. The original authors argue that this design difference compared to the original study may have reduced the chances of finding a significant effect of washing away postdecisional dissonance. The original authors argue that postdecisional dissonance is a phenomenon that involves multiple processes, and that the unintended methodological deviation in the replication may have increased reliance on one's memory of the earlier evaluation at the expense of the affective processes that motivated the work of the original authors on wash- ing away postdecisional dissonance. We do not dismiss the possibility that the protocol deviation could have had an impact on the observed effects of dissonance reduction, but we may find that possibility less plausible than do the original authors. We did replicate the dissonance effect, and we did not observe any reduction of that dissonance effect whether or not participants copied in the order of ranks or not (which we perceive as a plausible reinforcement of memory for the original ranks). Nevertheless, we cannot conclusively determine the reason for this, or any of the other, failures to replicate in the SSRP. As the original authors suggest follow-up investigation is necessary to firmly determine if the unintended protocol deviation in the replication is important or not. The original authors of Ramirez & Beilock 23 argue that we in the replication failed to create a high-pressure performance situation because there was no significant difference in performance between the high pressure and the low pressure condition and that we could therefore not test whether expressive writing can boost performance under pressure. We disagree with the interpretation of the original authors. As suggested by the original authors we preregistered tests of manipulation checks to assess whether the pressure ma- nipulation increased participant anxiety, and those manipulation checks were successful. For further discussion on this we refer to the original author comment on this replica- tion and the email correspondence between the original authors and the replication team. These appear at https://osf.io/hps2b/ and https://osf.io/n276s/ respectively. The original authors of Rand et al. 9 raise the issue that the Amazon Mechanical Turk (AMT) subject pool has changed since the data collection was carried out in their study, and in particular that the experience of economic game paradigms have increased over time in the AMT subject pool. Prior to conducting the replication they also asked us to include a question about prior experience in the data collection, which we did. But they did not ask us to pre-register any analyses or robustness test using this question. At their advice prior experience was measured on a 5-point likert scale; with 1 (nothing similar), 3 (something similar) and 5 (exactly this scenario) marked on the scale. The distribution of the n = 2, 136 responses on this scale was: 1 (n = 367), 2 (n = 362), 3 (n = 1, 033), 4 (n = 282), and 5 (n = 92); with a mean of 2.71. The experience level can not directly be compared to the replicated study in Rand et al. 9 as experience was not measured in that study, and without pre-registering how to define low and high experience it is not obvious what cut-off to use on the five-point scale. After seeing the data, the original authors did a sub-group analysis on those with the lowest experience level (1 on the scale). They found a similar effect size in this sub-group as for the overall sample in the original study, but it was non-significant (p = 0.108) and therefore provides no evidence for an effect among inexperienced subjects (the n = 367 limits the power of this test, although it is higher than the n = 343 in the original study; as the test was not pre-registered it should furthermore be interpreted cautiously). It cannot be ruled out that changes in the AMT subject pool over time affects results, but we also note that the two other studies on the public goods game based on AMT data replicated (Hauser et al. 26 and Nishi et al. 16 ). Shah et al. 27 carried out a series of five studies and we replicated study 1 based on our criteria for replicating the first study reporting a significant treatment effect. In their comments the original authors argue that study 1 is less important for their paper than their other studies, and inspired by our replication they decided to carry out a replication study of their own on all their five studies (with results posted at https:osf.io/vzm23/). They did replicate what they consider to be their most important finding in their paper, that scarcity itself leads to over-borrowing. But importantly they also failed to replicate study 1 confirming our findings for that study. to information about the prediction market results prior to the completion of the repli-cations. These two people were not involved in any replication data collection. Everyone involved in carrying out the replications was also instructed not to discuss the prediction market with any individuals who participated in the prediction markets and surveys. This was done to rule out the possibility that the persons conducting the experiments and carrying out the replications were affected by the prediction market results. Determination of replication sample sizes. We used a two-stage procedure for car- rying out the replications. In the first data collection we had 90% power to detect 75% of the original effect size at the 5% significance level in a two-sided test. If the original result replicated in the first data collection (a two-sided p-value < 0.05 and an effect in the same direction as the original study), no further data collection was carried out. If the original result did not replicate in the first data collection, we carried out a second data collection to have 90% power to detect 50% of the original effect size for the first and second data collection pooled. We then tested if the original result replicated in the pooled sample (a two-sided p-value < 0.05 and an effect in the same direction as the original study). For the Sparrow et al. 23 replication, the replication power was lower than the planned 90% power (82.0% in the first data collection and 80.7% in the first and second data collection pooled). This is due to an error in the reported sample size in the Sparrow et al. 23 paper; the paper reports a sample size of n = 46 but a sample size of n = 69 was actually used (the sample size affects the power estimation as it affects the estimated standardized effect size (r) of the original study). The original authors only provided feedback about this error in the original paper after the replication had already been conducted. Due to an initial mistake in analyzing the results of the first data collection in one study 28 , a second data collection was carried out in spite of the fact that the first data collection showed an effect in the same direction as the original result (and a two-sided p-value < 0.05). An initial incorrect analysis of the data from the first data collection showed a p-value > 0.05, and when this error was detected the second data collection was almost completed (and we therefore decided to complete the second data collection). As the result with all data collected is the most informative, we include the second data collection in the Stage 2 results for this study, although, according to the initial protocol, the second data collection should not have been carried out. Note that if a study fails to replicate in the first data collection, it is given a second chance to replicate and two tests are conducted. This increases the false positive risk somewhat compared to carrying out a single test. However, as the replication tests are directional (i.e., the effect in the replication needs to be in the same direction as the original study) and two-sided tests are used, the false positive risk in each test is only 2.5% (so the total false positive risk with our two-stage procedure does still not exceed 5%; we ran a simulation to estimate the false positive risk more exactly with our two-stage procedure and the false positive risk is 4.2%). Related to this our power estimations, of the power to detect 50% of the original effect size in the first and second data collection pooled, is somewhat conservative as it does not take into account the dependency of the Stage 1 and Stage 2 tests. We ran a simulation to estimate the power more exactly of our two-stage testing procedure and the power to detect 50% of the original effect size is 91.1% instead of 90%. In some replications the statistical power was slightly larger than 90% as the total number of observations needed to be evenly divisible by some number (e.g., subjects or groups needed to be evenly divided into treatments). In some replications the sample sizes were also slightly larger than the planned sample size (as some original studies used exclusion criteria and the number of exclusions were not known in advance, it was difficult to collect exactly the planned number of observations). The replication sample size needed was estimated in the same way for all the replications. The standardized effect size was estimated as the correlation coefficient (r) for all the original studies in the same way as done for the RPP 7 and the EERP 8 ; see below for more details on this. For the first stage data collection we estimated the sample size needed to have 90% power to detect 75% of the original effect size (expressed as the correlation coefficient r) at the 5% level in a two-sided test; in the second stage data collection we estimated the sample size needed to have 90% power in the pooled first and second data collection to detect 50% of the original effect size (expressed as the correlation coefficient r) at the 5% level in a two-sided test. Note that using some other measure of effect size such as Cohen's D can result in somewhat different sample sizes as the correlation coefficient (r) and Cohen's D are not linearly related. The replication power and sample sizes are substantially more ambitious than for the RPP 7 and the EERP 8 . Those projects were based on having 90% power to detect 100% of the original effect size, compared to having 90% power to detect 75% (the first data collection) and 50% of the original effect size in this project (for the first and second data collection pooled). Basing the power calculations on 75% of the original effect size rather than 100% approximately doubles the replication sample sizes, and basing the power calculations on 50% of the original effect size rather than 100% leads to an approximate fourfold increase in sample sizes. The reason for increasing the power is that the RPP 7 and the EERP 8 have been criticized for being underpowered as they do not take into account that original effect sizes of true positive findings may be inflated due to publication and reporting biases. We therefore took into account the possibility for inflated effect sizes in original studies with true positive results in our power estimation. The reason for basing the power estimations on having 90% power to detect 50% of the original effect size was based on the replication effect sizes in the RPP being about 50% of the original effect sizes on average 7 . Note, however, that this gives an overestimation of the inflation effect as the true average inflation rate should be estimated only for the original studies with true positive findings (but this fraction is not exactly known). If, for instance, the rate of false positives in the RPP sample would be 50% (such that the true effect size is zero for 50% of the studies), the RPP result would imply zero inflation on average in original studies of true findings. As part of the results of the SSRP we estimate the inflation rate in original effect sizes of true positive findings. The estimated relative effect size of the 13 studies that replicated according to the statistical significance criterion is 0.74 with a 95% confidence interval between 0.60 and 0.89. The corresponding estimate from the Bayesian mixture model is 0.71 with a 95% credible interval that ranges from 0.58 to 0.83. These results suggest that we are well powered to detect true positive findings in the SSRP (in Stage 2 we have 90% power to detect effect sizes below the 95% confidence intervals in the above estimations).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Estimation of standardized effect sizes and complementary replicability indicators", "text": "Relative effect size. Both the RPP 7 and the EERP 8 used the correlation coefficient (r) as a standardized effect size measure to compare effect sizes between original studies and replications. We used the same measure of standardized effect sizes and transformed effect sizes into correlation coefficients (r) in the same way as done for the RPP 7 . The standard errors of the correlation coefficients were calculated by applying the Fisher transformation, and depend only on the sample size of the study. The correlation coefficient was coded as positive for the original study regardless of the actual sign of the effect, and the replication effect size was coded as positive if it was in the same direction as in the original study, and negative if it was in the opposite direction. In Supplementary Figure 4, we show the relationship between the original and replication standardized effect sizes (r). The standardized effect sizes (r) are useful for comparing results between the original and the replication study and to estimate a measure of the relative effect size of the replication. However, it is less useful for comparing the level of effect sizes across studies if the studies are based on different levels of aggregation. Some studies use observations at the group level as the unit of observation and any aggregation reduces the variance of the data (i.e., the variance between individuals is larger than the variance between aggregated groups). A higher degree of aggregation of the data and thus lower variance generally increases the standardized effect size. This is, however, not a problem for comparing the standardized effect size between the original study and the replication, because we carry out the statistical test in an identical way for both the original study and the replication with the same level of aggregation. Due to the difficulties in comparing effect sizes between original studies, we normalized the original effect sizes to 1 in our figures (i.e., the standardized replication effect sizes (r) as well as the upper and lower bound of the 95% CI of r are divided by the standardized effect size (r) in the original study). However, in Supplementary Table 3 and Supplementary Table 4 we also report the standardized effect sizes (r) of the original studies and the replications without the normalization of the original effect size to 1. Meta-analytic effect size. We also computed a fixed-effect weighted meta-analytic effect size measure for each study pair as it was done both for the RPP 7 and the EERP 8 . The meta-analysis treats original and replicated studies equally except for the difference in sample size and gives an estimate of the pooled effect of the original study and the replication. The meta-analytic effect size measure is based on the assumption that there are no publication or reporting biases in the original studies and should thus be interpreted with great caution. The meta-analytic results are shown in Fig. 1c. 29 recently proposed another method to assess replicability. They suggest to estimate a 95% prediction interval for the original estimate and test how many of the replications fall within this prediction interval. The method takes into account the variability in both the original study and the replication study. For original studies with relatively high, but \"significant\", p-values prediction intervals can be expected to lead to higher replication rates than our primary replication indicator (a significant effect in the original direction). This is because for original studies with a p-value close to 0.05 the prediction interval will overlap with the zero effect size or be close to a zero effect size even for large replication sample sizes (the prediction market formula involves taking the square root of the sum of the variance of the estimated effect size (r) in the original study and the replication study; only including the variance of the original estimate of r leads to a lower bound of zero for the prediction interval if the p-value of the original study is 0.05; adding also the variance of the replication result widen the prediction interval further). For a false positive with an original p-value close to 0.05 the likelihood of the point estimate of the replication falling within the prediction interval is thus high as a false positive has a 50% probability of a point estimate above zero. In other words, with p-values close to 0.05 in original research, the prediction interval approach is a very liberal strategy for estimating replication success. However, there is also an effect in the opposite direction for high powered replication designs. For original studies that are highly statistically significant it is possible that a replication can show a significant effect in the same direction as the original study, but that fall below the prediction interval (and is thus classified as not replicating according to the prediction interval replication indicator). The 95% prediction interval results are shown in Fig. 2a. \"Small Telescopes\" approach. The estimated standardized effect sizes were also used to estimate replicability using the \"Small Telescopes\" approach, which was recently proposed 30 . In the Small Telescopes approach it is estimated whether the replication effect size is significantly smaller (with a one-sided test at the 5% level) than a \"small effect\" in the original study. A small effect is defined as the effect size the original study would have had 33% power to detect. If the effect size in the replication is significantly smaller than this \"small effect size\" it is considered a failed replication (and otherwise it is considered a successful replication). The Small Telescopes approach recommends using a replication sample that is always 2.5 times the original sample size, as this gives about 80% power to reject a \"small effect\" 30 . On average, the replication sample sizes in the SSRP are larger than the sample sizes proposed in the Small Telescopes approach, leading to higher power on average to reject a \"small effect\". The replication sample sizes in Stage 1 are on average about 3 times as large as the original sample sizes and replication sample sizes in Stage 2 are on average about 6 times as large as the original sample sizes. With high powered replication designs the results with the Small Telescopes approach and our primary repli- cation indicator can be expected to start converging. The Small Telescopes indicator can even lead to a lower replication rate as with a high powered replication it is possible to find a significant effect in the same direction as the original study, but that is significantly smaller than a \"small effect size\". The results for the Small Telescopes approach are shown in Fig. 2b.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Prediction intervals. Patil, Peng and Leek", "text": "Bayesian Analysis. We computed the one-sided default Bayes factors for the replica- tions 31 , as the hypotheses tested in the replications are clearly one-sided. Hence we obtain the strength of evidence in favor of the hypothesis that stipulates an effect in the direction of the original experiment (where a default prior is assigned to the size of the effect, that is, a folded Cauchy distribution with scale 0.707) versus the null hypothesis that stipulates the effect to be absent. The default Bayes factor are shown in Fig. 3. Following Marsman et al. 32 the default Bayes factors in the figure are interpreted in terms of the evidence categories proposed by Jeffreys 33 (from extreme support for the null hypothesis to extreme support for the original hypothesis). We furthermore computed the replication Bayes factor. The replication Bayes factor is similar to the default Bayes factor but uses the posterior distribution of effect size from the original experiment as the prior distribution of effect size under the alternative hypothesis for the evaluation of the replication study 34 . A replication Bayes factor above one favors the effect size observed in the original study and a replication Bayes factor below 1 favors the null hypothesis of no effect. The replication Bayes factor are shown in Supplementary   Figure 3. The Replication Bayes factor exceeds 1, showing evidence in favor of the effect size observed in the original study for 12 (57.1%) studies (all of whom replicated according to our primary replication indicator). This evidence is strong to extreme for 9 (42.9%) studies and moderate for the remaining 3 studies. The default Bayes factor is below 1 for 9 (42.9%) studies showing evidence in support of the null hypothesis of a zero effect size over the effect size observed in the original study; this evidence is strong to extreme for 7 (33.3%) studies and moderate for 2 studies. The replication Bayes factor yields similar results to the default Bayes factor with the exception of one study. For the Janssen et al. 3 study the Bayes factor shifts direction. The one-sided default Bayes factor for this study shows moderate support for an effect in the same direction as the original study, and the replication Bayes factor shows extreme support for a zero effect size over the effect size in the original study. This is consistent with an effect in the original direction for this replication, but with an effect size closer to the null effect than the effect size observed in the original study. The default Bayes factor tests the same thing as in our primary replication indicator and the test in the replication Bayes factor is similar to the test in the prediction interval approach (the prediction interval test if there is a significant difference between the replication effect size and the original effect size). The Janssen et al. 3 study has a significant effect in the original direction in our primary replication test consistent with the default Bayes factor. However, the replication result is within the 95% prediction interval, although close to the lower bound. The prediction interval indicator and the replication Bayes factor thus reach different conclusions. The Janssen et al. 3 study is the only study using a non-parametric Mann-Whitney test in our study. In a robustness test below we show that the prediction interval result for this study is sensitive towards if a t-test or a Mann-Whitney test is used as a basis of converting the original study result into a standardized effect size. With a t-test the prediction interval result is in line with the replication Bayes factor result. In addition to computing Bayes factors, we also estimated a Bayesian mixture model of the overall results 35,36 . The Bayesian mixture model provides an estimate of both the rate of true positives in the sample, and the relative effect size among true positive findings. The Bayesian mixture model assumes that each replication study originates from one of two components. The first component is the null hypothesis, according to which the expected effect size in a replication study is zero. The second component is the alternative hypothesis, according to which the expected effect size in a replication study equals a proportion of that from the original study. This is the effect size deflation factor for true positive studies -the extent to which replication studies yield effect sizes smaller than those obtained in the original studies for true positive studies. This estimate is important for determining the appropriate power to use in replication studies (i.e., the power has to take into account that effect sizes of true positive findings are likely to be inflated in original studies). All analyses were performed on the Fisher-transformed effect sizes r. Robustness Analysis. In the RPP, the meta-analysis was only carried out for the subset of studies based on t-tests and one degree of freedom F -tests (which is identical to a t-test), as no standard errors of the correlation coefficients were estimated for studies using other tests 7 . We therefore carried out a robustness test based on t-tests of the treatment effect for all the 21 studies. For the five original studies using a z-test statistic, we re-analyzed the original data and the replication data with a t-test. We then based the estimated effect sizes on the t-tests in the meta-analytic estimates, the prediction interval estimates and the Small telescopes estimates. Balafoutas and Sutter 4 and Gneezy et al. 5 used a z-test of the differences in proportions between two treatments. The data in these studies were re-analyzed using an independent samples t-test. This gives very similar results. For Balafoutas and Sutter 4 the z-value of the original study is 2.371 (p = 0.018) and the effect size is r = 0.278; with a t-test the t-value is t(70) = 2.436 (p = 0.017) and the effect size is r = 0.280. In the replication the z-value is 2.285 (p = 0.022) and the effect size is r = 0.146; with a t-test the t-value is t(241) = 2.300 (p = 0.022) and the effect size is r = 0.147. The relative effect size of the replication based on the z-test is 0.527 and the relative effect size of the replication based on the t-test is 0.524. For Gneezy et al. 5 the z-value of the original study is 3.000 (p = 0.003) and the effect size is r = 0.223; with a t-test the t-value is t(176) = 3.066 (p = 0.003) and the effect size is r = 0.225. In the replication the z-value is 3.706 (p < 0.001) and the effect size is r = 0182; with a t-test the t-value is t(405) = 3.761 (p < 0.001) and the effect size is r = 0.184. The relative effect size of the replication based on the z-test is 0.818 and the relative effect size of the replication based on the t-test is 0.816. Derex et al. 38 estimated a logistic regression with the probability of maintaining cultural diversity as a function of group size, and the group size coefficient was evaluated with a Wald test (equivalent to a z-test). The data in this study was re-analyzed with a linear probability model. The z-value of the group size variable coefficient in the original study is 4.037 (p < 0.001) and the effect size is r = 0.525; with a t-test in a linear probability model the t-value is t(49) = 4.812 (p < 0.001) and the effect size is r = 0.566. In the replication the z-value of the group size coefficient is 2.972 (p = 0.003) and the effect size is r = 0.361; with a t-test in a linear probability model the t-value is t(63) = 3.625 study the replication effect size based on a z-test is within the prediction interval (although very close to the lower bound of the interval), but based on a t-test the replications falls below the prediction interval. This is driven by the t-test being even more strongly significant than the Mann-Whitney test for the original study. Overall there is a significant effect in the original direction for this study, but the effect is significantly smaller than in the original study. This is in line with Bayes factor results for this study; the default Bayes factor showed support for the original hypothesis over the null hypothesis, and the replication Bayes factor showed that the effect size is closer to the null effect size than the original effect size. For the study by Rand et al. 9 , the replication effect size based on a z-test does not fall within the prediction interval (although very close to the lower bound of the interval), but based on a t-test the effect size is within the prediction interval. The difference in results for Rand et al. is small and it is close to the border of the prediction interval in both cases, but just outside the interval based on the z-test and just inside the interval based on the t-test. In the robustness test, 14 effects replicate (66.7%) according to the prediction interval approach. This is the same as for the main analysis. The only other change in the robustness analysis is that the Rand et al. 9 study does not replicate in the meta-analysis. This is due to a small change in the meta-analytic p-value for this study from 0.038 to 0.066. In the robustness test of the meta-analysis 15 studies (71.4%) have a significant effect in the same direction as the original study. However, if the lower p-value threshold of 0.005 for statistically significant new findings suggested by Benjamin et al. 33 is applied, the conclusion for Rand et al. 9 in the meta-analysis is not affected in the robustness test and 13 studies or 61.9% still have a p-value < 0.005 in the meta-analysis. The mean standardized effect size (correlation coefficient, r) of the replications in the robustness test is 0.252, compared to 0.473 in the original studies. The mean relative effect size of the replications is 45.7%. For the 13 studies that replicated, the mean relative effect size is 73.7%, and for the 8 studies that did not replicate, the mean relative effect size is 0.1%. These results are almost identical to the initial results.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Implementation of prediction markets and surveys", "text": "We used both surveys and prediction markets to measure peer beliefs about replicabil- ity. Prediction markets can be utilized as a mechanism to aggregate private information and beliefs and have been successfully applied to make predictions in several fields 39\u221245 . Prediction markets and surveys were used to estimate peer beliefs about replication in a subset of the studies in the RPP 46 , and for the replications in the EERP 8 . Treatments. We used two different treatments for eliciting peer beliefs. Participants were randomly assigned to these two treatments after signing up to participate in the study, and prior to filling out the survey. In both treatments we elicited beliefs with both a survey and with prediction markets for the 21 replication studies. In Treatment 1 we elicited beliefs about replicability in the first data collection (Stage 1). In Treatment 2 we elicited beliefs about replication in the first data collection (Stage 1) and in the first and second data collection pooled (Stage 2). In the prediction markets, shares could be traded whose value was determined by the actual outcome of the replication. Using two different treatments allowed us to implement a design that is as simple as possible (Treatment 1) and identical to previous prediction market settings 8, 46 , as well as a more complex design to elicit predictions specific to the two-stage approach used in SSRP (Treatment 2). Comparison of the two treatments allowed us to investigate how robust forecasts from surveys and prediction markets are. Recruitment. We sent invitations to participate in the survey and prediction markets to the Economic Science Association mailing list, the Society for Judgment and Decision Making e-mail list, the OSF e-mail list and the PsychMAP Facebook group. The invitation was also tweeted by Brian Nosek. The invitation contained a link to an online form where participants could sign up using their email address. A PhD degree or currently being a PhD student was a requirement for participating in the survey and prediction markets. The invitations to participate in the survey and prediction markets were e-mailed on October 17, 2016 and registrations closed on October 31. The survey was sent out to those who had registered by October 31 and the deadline for completing the survey was November 5. The prediction markets opened on November 7 and closed on November 21. 18 to 68 in Treatment 2. Of the participants 6.8% did not work in academia (but had a PhD), 35.0% were PhD students, 35.0% were post-docs or assistant professors, 11.2% were lecturers or associate professors, and 11.7% were full professors. The average time spent in academia after obtaining the PhD (for the 80% who answered this question) was 6.0 years. 41.3% of the participants resided in Europe and 51.0% resided in North America. The most common core field of research was psychology (51.5%) followed by economics (40.3%). Information available to participants. All participants had access to the replication reports for each replication (the version of the replication reports before the replications were conducted), and the references to the original papers. In the instructions to the survey and prediction markets, participants were also informed that the statistical power was 90% to detect 75% of the original effect size in Stage 1, and 90% power to detect 50% of the original effect size in Stage 2 (and that the criteria for replication was a p-value < 0.05 in a two-sided test and an effect in the same direction as the original study). For each replication study, participants were informed about the hypothesis to be replicated, the p-value of the original result, and the sample size of both the original study and the replication. Elicitation of peer beliefs about replicability. The pre-market survey (available at www.socialsciencesreplicationproject.com) was designed to elicit the same type of infor- mation as the prediction markets (i.e., the beliefs about replicability). Participants in the pre-market survey in Treatment 1 were asked to assess, for each replication study: (i) the likelihood that the hypothesis would be replicated in Stage 1 of the data collection; (ii) their stated expertise for the study the hypothesis was taken from. Participants in the pre-market survey in Treatment 2 were asked to assess, for each replication study: (i) the likelihood that the hypothesis would be replicated in Stage 1 of the data collection; (ii) the likelihood that the hypothesis would not be replicated in Stage 1, but would be replicated in Stage 2 with the pooled data; (iii) the likelihood that the hypothesis would not be replicated in Stage 1 or in Stage 2; (iv) their stated expertise for the study the hypothesis was taken from. Participants could also optionally answer a few demographic questions. The survey questions were not incentivized. Implementation of prediction markets. To implement the prediction markets we used the same web-based trading platform as in the EERP 8 , but adjusted the software for Treatment 2. There were two main views on the trading interface: (i) the market overview and (ii) the trading page. The market overview showed the 21 markets alongside summary information and a trade button for each market. The trading page was shown after clicking the trade button; at the trading page the participant could make investment decisions and view more detailed information about the market (see Supplementary Figure 6). Trading and market pricing. In both treatments, the prediction markets participants were endowed with 100 Tokens. Once the markets opened, these Tokens could be used to trade shares in the markets. For each share held at market closing, participants re- ceived one Token if the corresponding outcome was realized and zero otherwise. Prices for this type of share are typically interpreted as the predicted probability of the outcome to occur 47,48 ; see Sonneman et al. 49 for lab evidence that averaged beliefs are close to prediction market prices. In Treatment 1, participants could trade shares that paid one Token if a study replicated after Stage 1, as defined by a p-value < 0.05 in a two-sided test and an effect in the same direction as in the original study. Participants in this treatment could short-sell, which is equivalent to buying shares that pay one Token if the study was not replicated after See Supplementary Table 6 for data about trading volume on the prediction markets. Incentivisation. The markets were resolved after all replication experiments were com- pleted. If a replication was successful, shares held in the corresponding market were worth 1 Token. Tokens awarded as a result of holding shares were converted to USD at a 0.5 rate. Tokens that had not been invested in a market were not converted.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Comparison of prediction market beliefs, survey beliefs, and replication outcomes", "text": "To compare the survey results to the prediction markets results we based the pre-market survey measure on the sample of individuals who participated on the prediction markets (n = 114 in Treatment 1 and n = 92 in Treatment 2). Prediction market beliefs and survey beliefs for Treatment 1 and Treatment 2 are shown in Supplementary Table 5. We analyzed the results separately for Treatment 1 and Treatment 2, and we focus on the results for Treatment 2 in the main text (as this measures beliefs about replication after Stage 2 using all the data on replication). All prediction market beliefs below refer to final trading prices on the study level.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Treatment 1 results", "text": "Treatment 1 measures beliefs about replicability after Stage 1. Supplementary Figure 7 de- picts the relationship between survey beliefs and prediction market beliefs from this treat- ment and how they relate to the replication outcome. Prediction market and survey beliefs are strongly related and the Spearman correlation between the prediction market beliefs (final market prices) and the survey beliefs is 0.894 (p < 0.001, 95% CI = [0.752, 0.956], n = 21). The range of predictions is 21.3% to 79.9% with a mean of 56.9% (M dn = 62.6%) in the prediction markets and 19.0% to 70.5% with a mean of 48.9% (M dn = 49.6%) in the survey. This can be compared to the observed replication rate of 57.1% after Stage 1; the prediction market and survey beliefs do not differ significantly from the observed repli- cation rate in Stage 1 (Wilcoxon signed-ranks test, z = 0.156, p = 0.876, n = 21, for prediction market beliefs versus the observed replication rate and z = 0.643, p = 0.520, n = 21, for survey beliefs versus the observed replication rate). However, the prediction market beliefs are significantly higher than the survey beliefs (Wilcoxon signed-rank test, z = 3.076, p = 0.002, n = 21). To evaluate if market beliefs and survey beliefs contain useful information for predict- ing replication outcomes, we estimated the Spearman correlation between beliefs and replication outcomes (i.e., the binary outcome whether a replication shows a statisti- cally significant effect in the same direction as in the original study); we estimated these correlations both for the replication results after Stage 1 and the replication re-  ", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Comparison of reproducibility indicators to experimental economics and psychological sciences", "text": "Supplementary Figure 9 compares the results for our two main replicability indicators (significant effect in the same direction as the original study and the relative effect size) to the results for psychological sciences in the RPP 7 and experimental economics in the EERP 8 . This comparison is based on the replication results after Stage 2. The results for the RPP study are the same ones as presented in the EERP paper 8 and they were taken from the published replication results 7 . The RPP did not directly report the relative effect size of the replication, but instead used the \"effect size difference\" as a reproducibility indicator. The \"effect size difference\" was estimated as the absolute difference in the standardized effect size (r) between the original study and the replication study. We prefer to use the relative effect size (the ratio between the standardized effect size (r) of the replication and the standardized effect size (r) of the original study). The reason for this is the lack of comparability of the standardized effect sizes between our 21 studies as discussed in Section 2 above. We used the same relative effect size measure for the RPP as estimated by Camerer et al. 8 ; they downloaded the posted effect size data from the RPP and estimated the relative replication effect size for each study.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Results and data for the individual studies and markets", "text": "The hypotheses as described to the participants on the prediction markets in each of the 21 studies are shown in Supplementary  The number of observations and participants in each replication study are reported in Supplementary Table 3 and Supplementary Table 4. In some of the replications we also collected data about gender and age, but this was not collected in all replications (the replication data collections were based on the data collected in the original study). Below we list the number of men and women and the mean age of the participants for the replication studies where this information is available (the data is for the pooled Stage 1 and Stage 2 data for the replications that proceed to Stage 2). \u2022 Ackerman et al. 28 replication: 187 men and 408 women (4 participants refused to provide gender information); mean age = 29.2 years. \u2022 Aviezer et al. 17 replication: No data on gender or age collected. \u2022 Balafoutas and Sutter 4 replication: 243 men and 243 women; no data on age col- lected. \u2022 Derex et al. 38 replication: 482 men, mean age = 23.7 years. \u2022 Duncan et al. 12 replication: No data on gender or age collected. \u2022 Gervais and Norenzayan 13 replication:196 men and 332 women (3 participants re- fused to provide gender information); mean age = 19.0 years. \u2022 Gneezy et al. 5 replication: 160 men and 267 women, mean age = 20.3 years. \u2022 Hauser et al. 26 replication: 47 men and 63 women, mean age = 36.5 years. \u2022 Janssen et al. 3 replication: No data on gender or age collected. \u2022 Karpicke and Blunt 19 replication: 23 men and 26 women, mean age = 19.1 years. \u2022 Kidd and Castano 24 replication: 328 men and 386 women, mean age = 35.2 years. \u2022 Kovacs et al. 20 replication: 46 men and 49 women; no data on age collected. \u2022 Lee and Schwarz 25 replication: 97 men and 166 women (23 refused to provide infor- mation on gender); mean age = 19.0 years. \u2022 Morewedge et al. 6 replication: 36 men and 53 women, mean age = 23.0 years. \u2022 Nishi et al. 16 replication: No data on gender or age collected. \u2022 Pyc and Rawson 52 replication: No data on gender or age collected. \u2022 Ramirez and Beilock 22 replication: 42 men and 89 women, mean age = 19.2 years. \u2022 Rand et al. 9 replication: 1080 men and 1056 women, mean age = 36.8 years. \u2022 Shah et al. 27 replication: 280 men and 339 women, mean age = 36.7 years. \u2022 Sparrow et al. 23 replication: No data on gender or age collected. \u2022 Wilson et al. 14 replication: 10 men and 29 women, mean age = 20.3 years. Below we also list any \"unplanned protocol deviations\" for each of the replications (based on this section of the \"Replication Reports\"). For the replications not included below, no \"unplanned protocol deviations\" occurred (we do not include the somewhat larger than planned sample sizes in some replications among the \"unplanned protocol deviations\" below). Ackerman et al. 28 : An error occurred during analysis of the First Data Collection analysis (90% power to detect 75% of the original effect size), in which the preregistered analysis script was run with most, but not all, of the participants in the sample. The observed p-value in that analysis did not meet criteria for concluding data collection (p < 0.05). As such, we initiated the 2 nd round of data collection being run (90% power to detect 50% of the original effect size). The analysis error was discovered when the 2 nd round of data was nearly complete. We decided to finish the 2nd data collection and report all results for completeness. Additionally, the only exclusion criteria specified in the analysis section of our pre-data collection replication report encompassed, \"sitting down or resting the clipboard on a surface.\" However, to maintain the integrity of the sample following data collection, we have employed additional criteria to exclude participants with a reported age below 18 years, as well as participants that reported previous knowledge of the experiment or correctly guessed the purpose of the study before or during the debriefing period. When all participants excluded due to values below 18 for age are included, those assigned to  Derex et al. 38 : Due to difficulties in recruiting, the show-up fee was raised from s$5.00 to s$10.00, and the average performance-based payment was raised from s$10.00 to s$20.00. Hauser et al. 26 : In the recruitment of subjects from AMT we only recruited subjects from the US, used HIT approval rate greater or equal to 85%, and number of HITs approved greater than or equal to 100. These criteria were suggested by the original authors and decided prior to starting the data collection.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Janssen et al. 3 :", "text": "In the replication experiment, it turns out that for the NCP-C treatment the condition switch (from NCP to C ) happened after the fourth period. Due to this incorrect condition switching, the NCP-C treatment in the replication experiment includes 4 rounds of the NCP condition followed by 2 rounds of the C condition. This does not affect the findings of our replication because as planned our analysis only uses data of the first 3 rounds of the treatments and the condition switch is by design unexpected by the participants. 24 : We only used US AMT workers with a HIT approval rate of at least 95% in line with the original study, but this was not specified in the pre-replication version of this Replication Report. These criteria were decided upon together with the original authors before starting the data collection.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Kidd and Castano", "text": "Two unplanned exclusion criteria were added. First, we excluded everyone who had at least one missing answer out of the 36 questions on the RMET test (31 in first data collection, 53 in second). This is because the dependent variable RMET score can not be constructed for a subject with one or more missing values on the RMET score. Since the original authors had no missing values in this outcome variable, they did not have to consider this exclusion. This exclusion criteria was decided together with the original authors before starting the data collection. Second, we excluded all participants with a negative score on the author recognition test (2 in first data collection, 6 in the second). A negative score here indicates that the subject had more incorrect guesses of authors than correct guesses. Since the analysis uses a square root transformation of this variable, these subjects could not be used in the analysis. This exclusion criteria was implemented after starting the data collection, as we had not foreseen the possibility of a negative score on the author recognition test. Furthermore, we changed the definition of one of our exclusion criteria for our main repli- cation result. We planned to exclude all participants with less than 30 seconds reading time as this was suggested by the original authors. However, to apply this threshold of 30 seconds reading time the original authors also wanted us to standardize the page length of the reading texts (which had not been done in the original study). However, we did not standardize the page length as we in the communication with the original authors did not understand that we should standardize the page length to apply the 30 seconds reading time threshold. After the data had been collected we therefore together with the original authors decided not to exclude participants with less than 30 seconds reading time, but to exclude participants with 0 reading time. Note that this implies that this exclusion criteria is now the same as in the original study (as the original study excluded participants with 0 reading time, and did not standardize the page length). Note also that some observations where participants initiate the study are automatically excluded as some participants did not proceed further than giving consent and some participants did not proceed further than the instructions. Kovacs et al. 20 : In the original study the tests were conducted in a 3m\u00d73m sound- attenuated booth using Psyscope X on an Apple PowerBook. Due to difficulties in finding exactly the same experimental venue and materials, in the replication experiment the tests were carried out in a 3m\u00d74m breakout room with participants wearing a sound-attenuated device (earmuffs with a noise reduction rating of 31 decibels) using Psyscope X on a 13-inch Apple MacBook Air. Lee and Schwarz 25 : During our correspondence with the original authors, it was indi- cated that the original experimenter ensured that the presentation order of albums was different in the pre-vs. post-manipulation evaluation, which was necessary to minimize the likelihood that participants provided their post-manipulation rankings by simply retriev- ing their memory of pre-manipulation rankings. Specifically, in the original experiment, participants provided their pre-manipulation rankings by writing down the album titles in the order they wanted, and then during filler tasks, the experimenter (in a different room) would prepare a different form by listing the albums in alphabetical order of the artist names (rather than album titles). In effect, the presentation order of albums would differ in the pre-vs. post-manipulation evaluation. We noted this step in our \"Procedure Script\" prior to the initiation of data collection. However, we inadvertently neglected the alphabetization step when filling in the \"Sec- ondary Album Ranking\" form for participants. Instead, the presentation order was the same as what was in the pre-manipulation evaluation. We conducted some exploratory analysis on the possible influence of episodic memory on the presence of the effect of post-decisional dissonance. We separated participants who had transcribed their selected albums in ascending order by rank from those who did not. The former would be more likely to remember their album rankings by using the rank- ordering on the page as a cue. As such, if we remove these participants, we might observe some evidence for a difference between the examining and hand-washing conditions for participants that did not have this memory cue. Out of the 285 participant responses (not including the five exclusions from the pooled study sample) for the pre-manipulation, 57.5% (n = 164) of participants transcribed their 10 selected albums in the order in which they were ranked, ascending 1-10 directly down the form. The remaining 42.5% (n = 121) of participants did not. Among the 121 participants that did not list the albums in rank order, we observed a large dissonance effect (d = 1.13) of approximately the same size as for the full sample The focal hypothesis, however, was not supported by the data. The rank difference between the chosen and rejected CDs did not differ between the participants in the examining conditions (M = 1.60, SD = 1.65) and the hand-washing condition (M = 1.52, SD = 2.19), F (1, 119) = 0.0500, p = 0.8234. The direction of the effect, though insignificant, was the same as the equivalent test in the original study. Morewedge et al. 6 : As in the original study, observations more extreme than 2.5 stan- dard deviations from the overall mean were dropped from the analysis. The overall mean of consumed M&M's was M = 10.29 grams (SD = 8.99). 6 participants consumed more than M + 2.5SD = 32.76 grams of M&M's and one participant did not consume any M&M's at all such that 7 observations were excluded from the analysis. Since no partic- ipant refused to consume any M&M's in the original study, the original authors did not have to consider this exclusion. However, this exclusion criteria was decided on together with the original authors before starting the data collection. Nishi et al. 16 : In line with the original study we did not restrict the sample to only Amer- ican Turkers, but this was not specified in the pre-replication version of the Replication Report. We only used AMT workers with a HIT approval rate of at least 95%, but this was not specified in the pre-replication version of this Replication Report (and it is unclear if the original study used any restrictions on the HIT approval rate for participating in the study). One session was by mistake conducted with only 10 subjects. When the number of subjects finishing the training rounds did not reach at least 13, the attempted session was supposed to be canceled. By mistake, this was not done for one session in the visible treatment, but rather the game was played with only 10 subjects. The mean Gini over the 10 rounds was 0.1764 in this session which is quite close to the mean of 0.1690 for all the sessions in the visible treatment group. In spite of the inclusion of this session with only 10 subjects, the average group size (16.5) was the same for both the visible and the invisible treatment. We planned to carry out the statistical test of the replication result using the model with multiway clustering on session and round as this was used to test the hypothesis in the original study. However, as this model produced implausible standard errors in the replication we instead used a model with only clustering on the session level as the main replication result. This does not affect the conclusion about whether the original study replicates. It was challenging to carry out this replication. The original study was programmed in a version of the program Breadboard, which is no longer supported and the original authors did not provide the source code for the original version of the experiment. We thus had to program the experiment from the beginning using the new version of Breadboard. We experienced several problems with using Breadboard, and the program did not always function correctly. During the data collection the experimental software broke down 21 times and in these cases the session had to be restarted with new subjects. We relied on the screenshots provided in the Supplementary Information to program the experiment as closely as possible to the original study. But as we did not have access to the source code of the original experiment and the original authors provided much guidance initially but were not able to reply to all subsequent detailed queries, we had to decide on a number of issues that were not clear from the original paper or Supplementary Information (these issues are listed at the end of the replication report). Pyc and Rawson 52 : Contrary to our initial expectations, data for this replication was collected largely during the summer. While the original study used undergraduate partic- ipants exclusively, this replication used undergraduate and graduate students, as recruit- ment rates were low during the summer. A second deviation from the protocol was the additional robustness check conducted by analyzing the manually coded responses. We did not anticipate the issue of potential mis- classification of successful recalls due to typos before data-collection (it was not mentioned in the original publication). All conditions for both the first and second rounds of collection were randomly assigned using a random number generator (found at Random.org), to generate numbers 1-4, with each digit assigned to one of the 4 experimental conditions. This created a random im-balance between the two high pressure conditions over the course of the pooled first and second round collections. Additionally, prioritization was placed on reaching the target sample for the main high pressure conditions used in the focal analysis during the Spring 2017 collection period, which led to an imbalance in the manipulation-check low pressure conditions. The sample that we had already collected (79 in the high-pressure conditions, and 52 in the low-pressure conditions) gave us better than 99.9% power to detect the original manipulation-check effect size of d = 0.99, and 79% power to detect 50% of that effect size. Also, because we oversampled the high-pressure conditions, we had more power to detect the focal tests than prespecified.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Ramirez and Beilock", "text": "Rand et al. 9 : We only used AMT workers with a HIT approval rate of at least 95% in line with the original study, but this was not specified in the pre-replication version of this Replication Report. This criteria was decided upon together with the original authors before starting the data collection. Shah et al. 27 : We ran a test round with 10 observations to test that the program worked as intended before getting some more feedback from the authors. These 10 observations were not included in the analysis as we received new information about the MTurk recruit- ment criteria used in the original study after collecting these observations. We updated the HIT to only accept users from the US as was done in the original study, and we added a requirement for a HIT success rate of at least 95% (this was decided in agreement with the original authors). One observation in the first data collection and 9 in the second were incomplete and could not be used, because the program had not recorded their treatment group or subject id. It is possible that these users never finished the task, or maybe the software did not work properly on their platform. While we cannot link these observations to specific users, a few users did report minor technical problems. Wilson et al. 14 : Some of the participants (10/39) self-reported being non-psychology majors, even though the recruitment details emphasized that only psychology majors were eligible to participate in the study. Apart from that, the replication study was conducted exactly the way outlined above, without additional deviations from protocol. ", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Supplementary Tables", "text": "Supplementary Table 1. Hypotheses for the 21 replication studies.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Ackerman et al. (2010), Science", "text": "Participants that evaluate a resume while using a heavier clipboard will rate the resume as better overall compared to the participants that evaluate the resume while using a lighter clipboard. The original study used F -test for a two condition comparison, p < 0.05. Original test statistics: Heavy Condition: N = 26, M = 5.80, SD = 0.76; Light Condition: N = 28, M = 5.38, SD = 0.79. F (1, 52) = 4.08, p = 0.049. If there were no covariates in the model, we will convert the F to t for comparison with the replication tests.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Aviezer et al. (2012), Science", "text": "The body context is diagnostic for the affective valence of the situation during peak intensity moments (tests the hypothesis of a higher mean valence rating of winning bodies versus losing bodies in the 'body treatment' in Experiment 1; within subjects variation, paired t-test, t(14) = 13.07, p < 0.0001, p. 1226 and Fig. 1c).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Balafoutas and Sutter (2012), Science", "text": "With preferential treatment of women -i.e., each woman's performance is au- tomatically increased by one unit in the competition -more women will choose to compete (a comparison of the fraction of women who chose the tourna- ment scheme rather than the piece rate scheme in the 'preferential treatment one (pt1)' versus the 'control treatment (ctr)'; \u03c7 2 (1) = 5.62, p = 0.018, p. 580). (This hypothesis was picked by lottery instead of comparing pt2 to ctr; \u03c7 2 (1) = 10.89, p = 0.001, p. 580).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Derex et al. (2013), Nature", "text": "The probability of maintaining cultural diversity (that is, observing both tasks in the group) increases with group size; \u03c7 2 (1) = 16.3, the p-value < 0.0001 (exact 0.000054) (p. 389; measured at the group level with group sizes, 2, 4, 8, and 16).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Duncan et al. (2012), Science", "text": "Similar objects are more accurately identified as being similar if they are pre- ceded by new objects than if they are preceded by old objects (a comparison of the fraction of objects rated as similar in trials where they are preceded by new objects compared to trials where they are preceded by old objects in Study 1b (within-subject variation), t(14) = 3.41, p = 0.0042, p. 486).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Gervais and", "text": "Norenzayan (2012) ", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Gneezy et al. (2014), Science", "text": "The likelihood of choosing a charity is higher when potential donors know that the overhead is already paid for, than when the donors pay for overhead themselves (a comparison of the fraction choosing to donate to 'charity: wa- ter' between the '50% overhead, covered treatment' and the '50% overhead treatment', z = 3.00, p < 0.01 (exact p = 0.0027), p. 633). (This hypothesis was picked by lottery instead of comparing the 'no overhead treatment' and the '50% overhead treatment', z = 3.27, p < 0.01, p. 633.)", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Hauser et al. (2014), Nature", "text": "Choosing an extraction level for all group members using median voting leads to a higher degree of sustainability of a common pool than allowing each in- dividual to choose their own extraction amount. That is, a comparison of the average probability that the common pool was sustained by the first gener- ation between the voting treatment and the unregulated treatment (in both treatments there is an 80% probability that a new generation occurs and an extraction threshold of 50%). To evaluate this hypothesis, a linear probability model with a treatment dummy variable is used; see the 1 st generation regres- sion equation in Table S1; p = 1.427e \u221210 (reported as p < 0.001) in a t-test (t (38)  Participants automatically project agents' beliefs and store them in a way similar to that of their own representation about the environment. A compar- ison of the mean reaction time between the 'P-A-treatment' and the 'P-A+ treatment' in Study 1 (within subject variation), shows that reaction time is shorter in the P-A+ treatment; results show that t(23) = 2.42, p-value = 0.02 (exact p = 0.0238).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Lee and Schwarz (2010), Science", "text": "Hand washing will significantly reduce the need to justify one's choice by in- creasing the perceived difference between alternatives. Specifically, the mean difference between the rankings of the chosen and rejected albums before and after making the choice will be greater for the soap examining condition com- pared to the soap hand washing condition. F -test assessing the interaction between before-after and hand-washing condition, p < 0.05. In initially unequal situations, wealth visibility leads to greater inequality than when wealth is invisible (a comparison of the mean Gini coefficient between the visible and high initial inequality treatment and the invisible and high ini- tial inequality treatment; OLS regression of the session/round Gini coefficient as the dependent variable and multiway clustering of standard errors at the session and round level; regression equation (5) in Table S2, p = 0.0044 of a t-test of the treatment dummy variable coefficient, t(198) = 2.881).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Pyc and Rawson (2010), Science", "text": "Retrieval of mediators is greater with test-restudy practice than with restudy practice; a comparison of mean mediator retrieval between the test-restudy and the restudy treatments within the cmr treatment, p. 335, t(34) = 2.37 and p-value = 0.02, t-value and p-value from authors). Note that a successful retrieval in each of the final test questions is defined as correctly recalling any of the keyword mediators that had been generated during session 1.", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Ramirez and Beilock (2011), Science", "text": "In a high-pressure in-lab math test, those writing for 10 minutes about their deepest thoughts and feelings regarding the upcoming test improve more on that test compared to simply sitting quietly; an F -test, p < 0.05 using a two- tailed test. Original test statistics: N = 20 (10 in each condition); Expressive writing Mpre = 0.86 (SD = 0.09), Mpost = 0.91 (SD = 0.05), Control Mpre = 0.82 (SD = 0.09), Mpost = 0.70 (SD = 0.11); F (1, 18) = 30.53; p = 0.00003 (reported as p < 0.01, p. S11).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Rand et al. (2012), Nature", "text": "Priming intuition increases cooperation in a public goods game compared to priming reflection (a comparison of the mean contribution in a public goods game between the 'intuition-good'/'reflection-bad' treatments and the 'intuition-bad'/'reflection-good' treatments; a Tobit regression (with robust standard errors) with a treatment dummy variable, regression equation (1) in Table S11; z = 2.617, p = 0.0089 in a z-test of the treatment dummy variable coefficient).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Shah et al. (2012), Science", "text": "Low-wealth subjects, that are given fewer chances to win in repeated 'Wheel of Fortune' type word puzzle games, perform worse in a subsequent attention task (Dots-Mixed task) than do high-wealth individuals (a comparison of the mean performance on the Dots-Mixed task between the 'poor treatment' and the 'rich treatment'; ANOVA test, F (1, 54) = 4.16 and p = 0.046, p. 683).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Sparrow et al. (2011), Science", "text": "Computer terms are more accessible than general words after answering a block of hard trivia questions; measured as longer color-naming reaction times in a Modified Stroop Task after priming with computer terms compared to priming with non-computer terms (paired t-test, within subject variation; t(45) = 3.26, p = 0.0021, study 1, p. 776, and Fig. 1).", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Wilson et al. (2014), Science", "text": "An external activity from a list (e.g. watching television or reading a book) for 12 minutes is rated as being more enjoyable than a 12 minute 'thinking period' entertaining themselves with their thoughts (a higher average self-rated enjoyment (the mean of three nine-point scale items) in the 'external activities' treatment than in the 'standard thought instructions' treatment in Study 8, t(28) = 4.83, p = 0.000044, p. 76). Notes: \u2713 indicates \"yes\", \u2717 indicates \"no\", and \u2022 denotes \"not applicable\". \u2020 See section 1.2 in the Supplementary Information for details about when the original software was not used. * The original authors did not respond to our requests for materials and feedback on the replication report, prior to conducting the replication.  CIs of replication effect sizes (standardized to correlation coefficients r) after Stage 1. The standardized effect sizes are normalized so that 1 equals the original effect size. There is a significant effect in the same direction as in the original study for 12 out of 21 replications [57.1%; 95% CI = (34.1%, 80.2%)]. (B) Plotted are 95% CIs of replication effect sizes (standardized to correlation coefficients r) after Stage 2 (replications not proceeding to Stage 2 are included with their Stage 1 results). The standardized effect sizes are normalized so that 1 equals the original effect size. There is a significant effect in the same direction as in the original study for 13 out of 21 replications [61.9%; 95% CI = (39.3%, 84.6%)]. (C) Meta-analytic estimates of effect sizes combining the original and replication studies. 95% CIs of standardized effect sizes (correlation coefficient r). The standardized effect sizes are normalized so that 1 equals the original effect size. 15 out of 21 studies have a significant effect in the same direction as the original study in the meta-analysis [71.43%; 95% CI = (50.4%, 92.5%)].  F(1, df) tests. (a) Plotted are 95% prediction intervals for the standardized original effect sizes (correlation coefficient r). The standardized effect sizes are normalized so that 1 equals the original effect size. 14 out of 21 replications [66.7%; 95% CI = (44.7%, 88.6%)] are within the 95% prediction interval and replicate according to this indicator. (b) Plotted are 90% CIs of replication effect sizes in relation to small effect sizes as defined by the Small Telescopes approach (the effect size the original study would have had 33% power to detect). Effect sizes are standardized to correlation coefficients r and normalized so that 1 equals the original effect size. A study is defined as failing to replicate if the 90% confidence interval is below the small effect. According to the Small Telescopes approach 12 out of 21 [57.1%; 95% CI = (34.1%, 80.2%)] studies replicate. ", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Replication Bayes Factor", "text": "Replication Bayes factor < 1 Replication Bayes factor > 1 Supplementary Figure 3. Replication Bayes factors for the 21 replications 34 . A replication Bayes factor above one favors the effect size observed in the original study and a replication factor below 1 favors the null hypothesis of no effect. The evidence categories proposed by Jeffreys 33 are also shown in the figure (from extreme support for the null hypothesis of no effect to extreme support for the original study effect size). The mean relative effect size of all the replications is 46.2% [95% CI = (27.0%, 65.5%)]; the mean relative effect size of the replications that replicated is 74.5% [95% CI = (60.1%, 88.9%)]; and the mean relative effect size of the replications that did not replicate is 0.3% [95% CI = (\u221212.4%, 13.1%)]. The Spearman correlation between the original effect size and the replication effect size is 0.574 [p = 0.007; 95% CI = (18.9%, 80.6%)]. ", "title": "Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015", "file_name": "SUPPLEMENT - Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf"}
{"section": "Abstract", "text": "Radical innovation is an important driver of the growth, success, and wealth of firms and nations. Because of its importance, authors across various disciplines have proposed many theories about the drivers of such innovation, including government policy and labor, capital, and culture at the national level. The authors contrast these theories with one based on the corporate culture of the firm. They test their theory using survey and archival data from 759 firms across 17 major economies of the world. The results suggest the following: First, among the factors studied, corporate culture is the strongest driver of radical innovation across nations; culture consists of three attitudes and three practices. Second, the commercialization of radical innovations translates into a firm's financial performance; it is a stronger predictor of financial performance than other popular measures, such as patents. The authors discuss the implications of these findings for research and practice.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Theory", "text": "The disciplinary wellsprings of research on innovation are many (Fagerberg, Mowery, and Nelson 2005). Scholars in history, economics, law, engineering, sociology, manage- ment, marketing, international business, and public policy have all contributed to the understanding of the drivers of innovation across nations (e.g., Bartholomew 1997;Fagerberg, Mowery, and Nelson 2005;Im et al. 2003;Murtha, Lenway, and Hart 2001;Song and Parry 1997). Given how many ideas populate the subject of innovation and how dis- persed they are, unifying these ideas is a challenging task (see Kortum 2004;Nelson 1993). Within any disciplinary area, researchers tend to examine the drivers of innovation that are most salient to their own discipline. Few existing theories or frameworks integrate both firm and national dri- vers of innovation across nations. Cross-disciplinary frame- works that exist tend to be tailored to fit the unique circum- stances of individual industries (e.g., Bartholomew 1997) or individual nations (e.g., Mowery and Rosenberg 1993) and are not easily applied beyond their original contexts (see Furman, Porter, and Stern 2002;Nelson 1993). The current status of the literature is both a challenge and an opportunity for marketers interested in the drivers of radical product innovation. Now is an opportune time to integrate the far-flung ideas that are relevant to this topic. Indeed, a \"home-grown\" theory of the type advocated by Rust (2006, p. 1) has the potential to guide thinking and practice not just in the field of marketing but in associated fields as well. As a first step toward building such theory, we propose a framework that links key drivers of innovation with key innovation outcomes (see Figure 1). The next section uses this framework to build a metatheory of the drivers and fruits of radical innovation in firms across nations. We first outline some of the potential drivers pro- posed in the literature on innovation in firms across nations. The literature also indicates that these factors may intersect and interact dynamically with each other (see Bartholomew 1997;Murtha, Lenway, and Hart 2001;Nelson 1993). We test for these intersections and interactions subsequently. Next, we introduce our culture-centric theory of radical innovation.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Drivers of Radical Innovation in Firms Across Nations", "text": "A review of the literature on innovation across nations sug- gests that four factors underlie most explanations for why certain firms in certain nations are more innovative than others: skilled labor, capital, government, and culture (Bartholomew 1997;Demirg\u00fc\u00e7-Kunt and Levine 2001;Furman, Porter, and Stern 2002;Nelson 1993). The terms to describe these factors and the variables studied within them may differ by discipline (Kortum 2004). Moreover, the boundaries between these factors can be fuzzy. Never- theless, we believe that these factors incorporate most variables that are currently viewed as driving radical prod- uct innovation in firms across nations in a reasonably coher- ent way. Of these factors, all except government operate at two levels: (1) the national level, in the context of the entire economy, and (2) the firm level, in the context of the indi- vidual firm. In the subsequent paragraphs, we briefly describe the role of each factor in driving radical innova- tion. For the sake of brevity, and consistent with objectives of this research, we focus on the main effects of these fac- tors in our discussion. 1 Labor. We use the term labor to refer to the skilled workforce accessible to a particular firm in a particular country. A long tradition of research has pointed to the importance of a skilled workforce as a primary driver of innovation, both at the national and at the firm level (e.g., Committee on Science, Engineering, and Public Policy 2005;Daniels 1993;Furman, Porter, and Stern 2002;Mowery and Rosenberg 1993). In general, an educated and skilled workforce, especially in scientific and technical fields, is viewed as a prerequisite for the development and commercialization of novel products. For example, at the national level, Freeman (1992, p. 171) traces the emergence of the United States and Germany as technological powers in the nineteenth and twentieth centuries and the loss of British technological leadership during this time to these factors: \"It was above all the increasing availability of con- siderable numbers of professional engineers and other skilled people which gave the decisive advantage to Ger- man and American industry.\" As with the national level, a skilled workforce is also important at the firm level. Of special importance to firms is the availability of skilled scientific and technical talent within the firm (Zucker, Darby, and Armstrong 2002). Despite national differences, differences among firms' abil- ity to recruit and retain talented technical personnel are likely to explain differences in their innovation output and the value they capture from this output (Sorescu, Chandy, and Prabhu 2007). Capital. Capital refers to the financial resources that are available in the country as a whole as well as within firms that operate in the country. Countries with strong and vibrant financial systems are likely to provide greater access to the financial resources needed for innovation (see Edquist 2005; Huang and Xu 1999) than countries that are not so well equipped. Sources of financial inputs include banks, stock markets, and venture capital. While stock mar- kets provide access to equity for established firms, banks serve as a source of finance for private firms and small firms from established sectors ( Levine and Zervos 1998). A fair number of countries now have an active network of venture capitalists that support new innovative enterprises. Risky and emerging firms and sectors are likely to benefit from such networks in their drive toward innovation ( Kortum and Lerner 2000). At the firm level, the financial resources available within individual firms are likely to play an important role in driving innovation. Within any specific country, firms that have greater access to financial resources are, ceteris paribus, likely to be more innovative and to create greater value from their innovations (Sorescu, Chandy, and Prabhu 2003). However, the mere availability of capital, whether at the national or the firm level, will translate into innovation only if the capital is used to make the right kinds of investments. At the national level, greater investment in R&D is likely to yield greater access to new product ideas for firms in the economy; the spillover of knowledge created by such spending is likely to benefit firms operating throughout the economy (Jaffe, Trajtenberg, and Fogarty 2000). Similarly, within firms, those that spend more on R&D are likely to be more innovative and, ceteris paribus, to capture more value from innovation than firms that spend less on R&D (see Dutta, Narasimhan, and Rajiv 1999). Government. Prior literature has suggested that several aspects of government policy can help or hurt innovation within firms that operate in a country (Edquist 2005;Nelson 1993). As Nelson (1993, p. 512) notes, \"Much of the cur- rent interest in national systems of innovation reflects a belief that the innovative prowess of national firms is deter- mined to a considerable extent by government policies.\" Among the most important aspects of policy are the protec- tion the government provides for intellectual property; its involvement in technology development through its encour- agement of collaboration between universities and industry; and its involvement in the diffusion of innovation through its procurement of innovative outputs in sectors such as defense, health, and education. The case for intellectual property protection in driving innovation is made strongly by legal scholars and some economists (Gutterman and Anderson 1997;Webster and Packer 1996). The argument is that protection for the ideas behind innovations enables innovators to reap the rewards for developing innovations and undertaking risks in com- mercializing them. Some proponents suggest that the suc- cess of Europe relative to Asia in the post-Renaissance period resulted from the former's legal support of intellec- tual property rights (e.g., Landes 1999;North and Thomas 1973). Others offer the innovativeness of the United States over Europe in the past 100 years as being due to its strong patent, trademark, and copyright laws (e.g., Rosenberg and Birdzell 1986). Many scholars argue that government legislation, such as the U.S. Bayh-Dole Act (35 U.S.C. \u00a7 200-212), which encourages and facilitates collaboration between universi- ties and industry, is a likely driver of innovation within firms in the country ( Etzkowitz and Leydesdorff 2000;Mowery and Sampat 2004). Such policy may help trans- form the basic research that occurs at universities into appli- cations that firms can commercialize. In addition, it may yield graduates whose skills are closely attuned to the inno- vation tasks that firms face. By creating laws that enable universities to engage in such collaboration with firms and by providing incentives that encourage them to do so, gov- ernments can help stimulate the innovativeness of firms that operate in their countries. Governments can also support innovation in firms either indirectly through R&D tax credits or directly through the procurement of new technology (Bartholomew 1997;Hall 1993;Hall and Van Reenen 2000). Such support can poten- tially create markets for products and technologies that oth- erwise may take many years to materialize or never materi- alize at all. In recent years, R&D programs targeted at security, military, and public health needs have been a pri- mary arena for government procurement and tax credits (Nelson 1993). For example, now-ubiquitous technologies in semiconductors, telecommunications, energy, and com- puting owe their origins in part to government-sponsored research with military aims. Nevertheless, the actual impact of government procurement and R&D tax policies remains ambiguous; some scholars note that though such policies might have raised technical development or scientists' wages in certain fields, innovation outputs have been non- existent or slow to follow (Goolsbee 1998;Mansfield 1984). Culture. Culture refers to a core set of attitudes and practices that are shared by the members of a collective entity, such as a nation or a firm (Hofstede 2003;Smircich 1983). The definitions of culture are many, and \"culture, like love, is a many-splendored thing\" (Prabhu, Chandy, and Ellis 2005, p. 120). However, as Triandis (1996, p. 407) states, \"almost all researchers agree that culture is reflected in shared cognitions [and] standard operating procedures.\" Our definition of culture in terms of attitudes and practices is consistent with and analogous to definitions that view culture in other terms, such as values, rituals, and codes (see Denison 1996;Deshpand\u00e9 and Webster 1989;Gregory 1983;Jones, Jimmieson, and Griffiths 2005;Miles and Snow 1978;Rokeach 1973;Triandis 1994). As with labor and capital, culture can operate at both the national and the firm level. An extensive body of literature suggests three related aspects of national culture that may drive innovation: a nation's religion, its geographic location, and the values of its citizens (Hofstede 2003). Some analysts argue that reli- gious beliefs can influence the development and adoption of innovations (see Gorski 2003) because some faiths provide believers with a strong rationale to work in and transform their environment, while others tend to emphasize the renunciation of worldly pleasures for rewards in the after- life (DeLong 1988;Landes 1999;Weber 1930). 2 Similarly, some researchers argue that a nation's geographical loca- tion-specifically, its distance from the equator-could reflect attitudes and practices that help or hinder innovation (e.g., Landes 1999;Parker 2000). Because warm climates are more abundant in animal and vegetable life than cold ones, they could lead to easier lifestyles and fewer incen- tives for work and innovation, while cold climates, which are more hostile, require long-term planning and motivate people to action, work, and innovation (Landes 1999). Finally, Hofstede (2003) shows that nations may differ along specific cultural dimensions, such as individualism- collectivism, uncertainty avoidance, power distance, masculinity-femininity, and long-term orientation. Recently, other researchers have updated and refined these dimensions ( House et al. 2004) and have highlighted their likely impact on innovation (see Dwyer, Mesak, and Hsu 2005;Shane 1994). As with national culture, recent research indicates that corporate culture may play a role in radical innovation. Cor- porate culture refers to a core set of attitudes and practices that are shared by the members of the firm (Denison 1996;Deshpand\u00e9 and Webster 1989;Detert, Schroeder, and Mauriel 2000;Hatch 1993;Martin 2002;Schein 1999;Schultz and Hatch 1996). A culture that fosters relentless innovation may help ensure that the firm stays constantly at the leading edge of innovation ( Govindarajan and Kopalle 2004;Tellis and Golder 2001). In the next subsection, we highlight the importance of corporate culture in driving innovation and propose a culture-centric theory of radical innovation in firms. We use 3 That said, we acknowledge that the sources of available capital may vary among nations as a result of historic and systemic rea- sons. For example, although German and Japanese firms rely more on debt and bank sources, U.S. firms rely more on stock and non- bank sources (see Demirg\u00fc\u00e7-Kunt and Levine 2001). Furthermore, stock markets in some nations, such as China, are more nascent than stock markets in North America or Western Europe. the terms \"firm culture\" and \"corporate culture\" interchangeably.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "A Culture-Centric Theory of Radical Innovation", "text": "Although researchers have proposed labor, capital, govern- ment, and culture as drivers of innovation, few have for- mally examined the relative importance of these factors in contemporary firms. In the next stage of theory develop- ment, we propose that in today's capitalist economies, labor, capital, and government may not be the primary fac- tors that distinguish innovative firms from others. Nor may national culture in itself be the major factor of importance. Rather, we argue that corporate culture is likely to be an important driver of innovation in firms across nations, for three reasons. Importance of corporate culture. First, markets for labor and capital have been evolving in capitalist economies over the past 400 years (Mannie, Zhang, and Hu 2006;Wright, Pruthi, and Lockett 2005). In many capitalist countries, especially with the onset of information technology, these markets are now reasonably efficient and increasingly mature and interconnected. Thus, innovative firms now have the ability to tap these markets for labor and capital to bring their innovations to fruition. In particular, the pres- ence of markets for venture capital enables entrepreneurs and entrepreneurial firms to gain access to capital for radi- cal innovations, though at a steeper rate than in the stock market (Gompers and Lerner 2001). 3 Second, recent years have witnessed an increased con- vergence across developed and emerging nations in the extent to which labor and capital are accessible to firms (Demirg\u00fc\u00e7-Kunt and Levine 2001;Krugman, Cooper, and Srinivasan 1995) and the extent to which government poli- cies are synchronized across nations ( Gong and Keller 2003;Hussler 2004;Lemola 2002). Although far from easy, negotiations across governments have led to some agree- ments on market and capital access across borders. More- over, our discussions with policy makers in both developed and emerging nations suggest another factor that could be even more important than formal agreements in promoting policy convergence. Policy makers in many nations have learned to keep a close eye on regulatory and technological developments elsewhere and have unilaterally integrated their own nations into international markets (see Baldwin 2006;Krugman, Cooper, and Srinivasan 1995;Naim 2007). In addition, though far from frictionless, markets of a rea- sonably efficient kind exist for both labor and capital in many leading and emerging nations. Capital markets have flourished in economies in many parts of the world ( Kumar and Russell 2002). Novel and promising ideas, whether in emerging economies, such as India and China, or in estab- lished markets of the Organisation for Economic Co- operation and Development (OECD), now attract capital in a manner that is in many ways unprecedented in history. Similarly, both developing and developed nations under- stand the importance of educational and other labor-related investments. Even in cases such as India and China, where the proportion of qualified technical personnel is not cur- rently large relative to the population of these nations, the sheer number of available personnel makes it possible for firms to meet their current innovation needs. Increasingly, therefore, access to labor is also diminishing in its impor- tance as a factor that explains differentials in innovation in firms across nations. Moreover, multilateral trade agree- ments and pannational institutions, such as the World Trade Organization, have helped promote an increased conver- gence in government policies across nations on intellectual property protection, government procurement, and collabo- ration between universities and industry (Baldwin 2006). Third, culture is a uniquely human product that devel- ops slowly within firms, is tacit and not easily defined, and is not easily transported across firms (Jassawalla and Sashittal 2002;Schein 1999). Indeed, markets for culture are either nonexistent or not very efficient. Reporting requirements and the presence of firms (e.g., Dun & Brad- street) that specialize in corporate information help ensure that the size and type of labor and capital pool employed by a particular firm is often evident to (and thus open to imita- tion by) its competitors. However, corporate culture is a much more elusive factor than labor, capital, and govern- ment regulation. Thus, we posit that capital, labor, and government regu- lation may be important drivers of radical innovation in firms across nations. However, in today's converging economies, corporate culture may also be more important than labor, capital, government, and national culture in explaining innovation in firms across nations. Components of corporate culture. Following prior research, we examine corporate culture by studying the core set of attitudes and practices shared by members of the firm (Deshpand\u00e9 and Webster 1989;Henard and Szymanski 2001;Smircich 1983). We do so with the recognition that the attitudes and practices that are most relevant to the inno- vation task are unlikely to be identical to those for other tasks. For this reason, scholars of corporate culture have called for middle-range descriptions of corporate culture- descriptions that preserve the holistic aspects of the con- struct while acknowledging the particulars of the tasks or outcomes being studied (see Bourgeois 1979). For example, Homburg and Pflesser (2000) examine market-oriented cul- ture by studying the attitudes and practices that the litera- ture suggests are most relevant to market orientation. Hof- stede and colleagues (1999) examine the role of corporate culture in employee promotion and dismissal outcomes by studying attitudes and practices that the literature suggests are most relevant to those outcomes. In the same vein, we examine the role of corporate culture on radical innovation by studying attitudes and practices that the literature sug- gests are most relevant to this outcome. On the basis of prior research, we identify three firm attitudes and three firm practices that may drive innovation (see Chandy and Tellis 1998;Olson, Walker, and Ruekert 1995). The attitudes are the willingness to cannibalize assets, future orientation, and tolerance for risk. These atti- tudes are likely to be essential drivers of innovation for the following reasons: First, a great hindrance to enduring inno- vation is the stream of profits that emerge from current products and services. The firm invariably tends to marshal great resources to protect this stream of profits. Any change or innovation that might threaten it is vetoed or frozen. A willingness to cannibalize assets is an attitude that puts up for review and sacrifice current profit-generating assets, including current profitable and successful innovations, so that the firm can get ahead with the next generation of inno- vations ( Chandy and Tellis 1998). Second, a firm that is successful in one generation of technology is under pres- sure to focus on the many micro problems it faces in man- aging its success with that generation. A future orientation forces a firm to realize the limitations of the current tech- nology and the emergence of a new generation of technol- ogy that may become dominant in the future (Christensen and Bower 1996;Narver and Slater 1998;Yadav, Prabhu, and Chandy 2007). Third, trading a current, sure stream of profits for a future, uncertain stream of profits is risky and does not come naturally to managers. It is vital that a firm foster and promote a tolerance for risk to make that essen- tial trade-off ( Fiegenbaum and Thomas 1988;Gilman 1995;Kuczmarski 1996). Thus, willingness to cannibalize assets, future orientation, and tolerance for risk are three essential attitudes that constitute an innovative culture. Prior research has also led us to identify three practices that engender and sustain these attitudes. First among these is the empowerment of product champions. By this practice, a firm empowers an individual with resources to explore, research, and build on promising, but uncertain, future tech- nologies ( Howell and Higgins 1990;Markham and Griffin 1998;Shane 1994). In effect, empowerment embeds within the firm the enterprising spirit that enabled it to initiate the original innovation that brought it success. Second, the firm needs to establish incentives for enterprise (Makri, Lane, and Gomez-Mejia 2006;Zenger and Lazzarini 2004). By this practice, the firm refrains from rewarding only or pri- marily seniority or management of current products. Rather, it ensures that adequate, if not large, incentives are reserved for employees who venture to explore or build new enter- prises for the firm. A third practice is the creation and main- tenance of internal markets (Halal, Geranmayeh, and Pourdehnad 1993). This practice involves two elements: internal autonomy and internal competition (Chandy and Tellis 1998). Internal autonomy refers to the extent to which divi- sion managers enjoy decision-making authority in the firm relative to the corporate office (Aiken and Hage 1968;Olson, Walker, and Ruekert 1995). Internal competition requires that divisions or groups of employees within the firm compete among themselves to identify promising tech- nologies and build innovations on those technologies. A firm with an active internal market creates the marketplace within itself, ensuring that an innovator from outside will not upstage the firm. In summary, although many variables at the national and firm level can drive radical innovation in firms across nations, our culture-centric theory of radical innovation 4 Other firm factors, such as leadership quality and cross- functional integration, may also drive innovation. Because of resource constraints, we do not cover these factors in this research. posits that in today's converging economies, corporate cul- ture may be the most important driver among all these variables. Next, we describe the model we use to test our arguments.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Model", "text": "Our empirical model has two equations: Equation 1 tests the effects of various drivers of innovation on radical inno- vation in firms across nations, as our previously articulated arguments suggest. Equation 2 tests the effects of radical innovation (relative to traditional metrics, such as patents and R&D) on a firm's financial performance. For both equations, we have repeated measures across many indus- tries; we account for industry heterogeneity using an indus- try fixed-effects model. On the basis of our theory, we specify Equation 1 as follows: (1) Radical Innovation fci = \u03b4 0 + \u03b4 j Labor c + \u03b4 j Labor f + \u03b4 j Capital c + \u03b4 j Capital f + \u03b4 j Government c + \u03a3 k \u03b4 jk Culture ck + \u03a3 l \u03b4 jl Culture fl + \u03a3 m \u03b4 jm Controls fcim + \u03bd i + \u03b5 fci , where f, i, and c are firm, industry, and country indexes, respectively; \u03b4 j s are parameters to be estimated for the jth variable; the subscripts k, l, and m are indexes for variables in national culture, firm culture, and control variables, respectively; Radical Innovation is a measure that captures the radical innovation of a firm f; Labor is a set of variables that measure skilled labor-related drivers of innovation at the national or firm level; Capital is a set of variables that measure capital-related drivers of innovation at the national or firm level; Government is a set of variables that measure government policy-related drivers of innovation; Culture is a set of variables that captures national and firm culture; \u03bd i are industry-specific error terms, and \u03b5 fci is the remaining error term (initially assumed to independently and identi- cally follow a normal distribution). Finally, Controls is a set of variables that are also likely to influence firms' radical innovation; these include firms' citation-weighted patents and the nation's population and gross domestic product (GDP). 4 Equation 2 assesses the effect of radical innovation on financial performance: (2) Value fci = \u03b2 0 + \u03b2 j Radical Innovation fci + \u03b2 j Patents f + \u03b2 j R&D f + \u2211 l \u03b2 jl Controls fcil + \u03bd\u2032 i + \u03b5\u2032 fci , where f, i, and c are firm, industry, and country indexes, respectively; \u03b2 j s are parameters to be estimated for the jth variable; Value is the market-to-book ratio of firm f; Radical Innovation is a measure that captures the radical innovation of firm f; Patents is the citation-weighted patents owned by Radical Innovation Across Nations / 9 firm f; R&D is the percentage of sales spent on R&D by firm f; \u03bd\u2032 i are industry-specific error terms; and \u03b5\u2032 fci is the remaining error term (initially assumed to independently and identically follow a normal distribution). Controls is a set of variables that are also likely to influence firms' finan- cial performance; these include firm size and country GDP, population, inflation, and credit rating. We do not include national culture in Equation 2, because there is little litera- ture to support the effects of these variables on firms' finan- cial performance. However, including these variables in the estimation does not substantially change the results of the financial impact of radical innovation. We estimate both Equations 1 and 2 using industry-level fixed-effects regression models. Furthermore, we estimate Equations 1 and 2 separately because (1) the two equations form a recursive system-Radical Innovation influences Value, but Value (which is forward looking and measured later in time than Radical Innovation) does not influence Radical Innovation; (2) an analysis of the correlation between the error terms of each of the equations indicates that these errors are uncorrelated; and (3) recursive systems with uncorrelated errors do not require joint estimation of their constituent equations (for a formal proof, see Land 1973).", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Sampling", "text": "Resources at our disposal limit our study to 17 nations. To ensure that we capture a large fraction of the world econ- omy, we chose the 8 largest economies on the basis of pur- chasing power parity: the United States, China, Japan, India, Germany, the United Kingdom, France, and Italy. To capture the possible role of innovation in propelling nations' recent progress, we chose 4 nations that have developed rapidly: Taiwan, Hong Kong, Korea, and Singa- pore. We also chose 5 nations with known major innovative or multinational firms: Canada, Switzerland, Netherlands, Sweden, and Australia. We selected a sample of publicly listed firms only, which enables us to integrate our survey data with data from archival sources on these firms. We also chose only firms within the manufacturing sector that are primarily local to their country of origin. In other words, we excluded local subsidiaries of multinational firms because they confound the role of parent firm and local national culture (see Bartlett and Ghoshal 1995;Prahalad and Doz 1987). We sampled between 62 and 848 firms from each nation to reflect the size of these nations' economy and the extent of public listings within them (for sample sizes by nation, see Table 1). We used stratified sampling by firm size; 15% of our sample consists of the largest firms in the country, and the remaining 85% consists of a third each of large, medium, and small firms.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Procedure", "text": "Survey data. We used the following procedure to develop our questionnaires and conduct the survey: First, we developed a preliminary questionnaire based on discus- sions with managers and using scales and items from prior academic research as well as innovation surveys conducted by the OECD and the European Union (EU); for the latter, we drew on both the OECD's (2005) Oslo Manual and the EU's Community Innovation Survey (Eurostat 1997(Eurostat , 2000). Second, in the questionnaire, we were careful to provide respondents with clear definitions of our key terms, such as firm, industry, and radical product innovation. We also pro- vided examples of each of these types of innovations. We used multiple items for each construct and negatively and positively valenced items to minimize demand bias and yea- saying ( Steenkamp 2001, 2006). Third, we pretested this questionnaire by sending it to a subsample of 100 firms from four English-speaking nations in our sample (the United States, Canada, the United Kingdom, and Australia). Fourth, after checking for the validity and reliability of the scales, we translated the original English language questionnaire into the other seven languages in our sample (French, German, Italian, Japanese, Korean, Mandarin Chinese, and simplified Chinese). We used inde- pendent experts to back translate and retranslate these ques- tionnaires to ensure accuracy and consistency. In some cases, we cross-checked the translated surveys face-to-face with managers from the nation in question (Germany, Switzerland, Japan, and Korea). Fifth, we obtained the names of firms to make up our sample from the OSIRIS and Worldscope databases, tele- phoned these firms to identify the vice president for innova- tion or technology or the equivalent at each firm, and mailed the surveys to all firms in all countries over a six- month period. We sent a reminder letter to each firm 10 to 14 days after we mailed the survey. Sixth, we performed relevant validity, reliability, and cross-cultural equivalence checks (see item 6 in Appendix A) on the survey data to develop the metrics we used in our analysis. Finally, we controlled for yea-and nay-saying, midpoint bias, and extreme response bias across firms and nations by applying the correction procedure used by Triandis (1994) and House and colleagues (2004). By subtracting the mean and divid- ing by the standard deviation across all responses per firm, this method corrects for the four types of previously men- tioned biases. All survey-based items used in subsequent analyses are corrected for these biases. Archival data. In addition to survey data on firm innova- tion inputs, outputs, and drivers of innovation, we collected data from multiple archival sources (see Table 2). We col- lected two types of firm secondary data: patent data from Delphion and firm financials from OSIRIS and Worldscope. We also collected several types of national secondary data. First, we collected ratio scale data from the OECD and per- ceptual data from the World Economic Forum and the IMD World Competitiveness Report on various measures of national labor, capital, and government policy. Second, we collected data on religion from the CIA World Factbook and data on geographical location from Parker (2000) and worldatlas.com. Finally, we collected data on national cul- ture from Hofstede (2003) and the GLOBE leadership sur- vey ( House et al. 2004). We linked the survey and archival   5 We also created an alternative scale that uses all four items; our results are robust to the use of this four-item additive scale instead of the three-item scale we used for the results reported herein. Table 1). data at the firm and national level to assemble a pooled database that we use in all subsequent analyses (for response rates to the survey, see", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Dependent Variables", "text": "Radical innovation. We used a three-item scale to mea- sure radical innovation (Chandy and Tellis 1998;Nijssen, Hillebrand, and Vermeulen 2005). These items are all seven-point Likert items measuring the extent of radical innovation within the firm. We combined these items into a three-item additive scale. We found this scale to be fairly reliable (Cronbach's \u03b1 = .62). We also found it to have validity: It correlates well (\u03c1 = .38, p < .0001) with a fourth seven-point item that measures the percentage of sales based on radical innovations by the firm in the previous three years (for details, see Appendix B). 5 Financial performance. We measured financial perfor- mance using individual firms' market-to-book ratio ( Barber and Lyon 1997;Fama andFrench 1992, 1995)-that is, the ratio of a firm's stock market value to the book values of its assets. We used this measure for three reasons. First, market-to-book values represent investors' valuation of a firm based on all its activities and potential. Second, this measure is future oriented because stock prices represent the net present value of expected current and future cash flows. Third, the ratio provides a measure of the intangible value of the firm beyond its assets, due to factors such as innovation. We collected financial data for each firm at the end of 2003 from Worldscope and OSIRIS.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Independent Variables", "text": "Corporate culture. We collected data through the ques- tionnaire on several organizational drivers of innovation: willingness to cannibalize, future focus, risk tolerance, use of incentives, product champions, and internal markets. Willingness to cannibalize refers to the extent to which a firm is prepared to reduce the value of its own prior invest- ments. Future market orientation is the extent to which a firm emphasizes, in its market research activities, customers and competitors that are not currently in the markets it serves. Tolerance for risk refers to the extent to which a firm takes on risk to fulfill a desired goal. The key practices we examine are incentives, empowerment of product champi- ons, and internal markets (see Howell and Higgins 1990;Quinn and Rivoli 1991). Incentives refer to the monetary and nonmonetary rewards a firm has in place to reward innovation. Empowerment of product champions refers to the extent to which a firm promotes the activities of people who aggressively pursue new product ideas. Finally, inter- nal markets refer to the level of internal autonomy and internal competition that exists among business units in a firm. We developed items for these measures (for details, see Appendix B) from existing research (e.g., Chandy and Tellis 1998;Mols 2001). We combined these items to develop additive scales for each of the variables of interest, after controlling for response biases as discussed previously. National culture. We used two sources of data on national culture. Consistent with prior research, we used Hofstede's (2003) measures of national culture on the fol- lowing dimensions: power distance, uncertainty avoidance, individualism, masculinity, and long-term orientation (see also Mitra and Golder 2002). We also collected data from the GLOBE Study of Culture, Leadership, and Organiza- tions ( House et al. 2004) on the following dimensions: per- formance orientation, future orientation, gender egalitarian- ism, assertiveness, individualism and collectivism, power distance, humane orientation, and uncertainty avoidance. The results we report in this article use Hofstede's mea- sures. However, our results are also robust to the use of the GLOBE measures. We measured national religion by col- lecting data on the percentage of people within a nation who belong to the following religious groups: Protestant, Catholic, Jewish, Hindu, Buddhist, and Muslim. We obtained this information from the CIA World Factbook. We measured geographical location using the latitude of the capital city of the country. 6 We obtained this data from Parker (2000) and worldatlas.com. National labor, capital, and government policy. We col- lected and coded archival data on labor, capital, and govern- ment policy for each nation in our sample. To measure labor, we collected data on five variables: the availability of scientists and engineers, the quality of scientific research institutions, the quality of management schools (all from the World Economic Forum's World Competitiveness Report), R&D personnel per 1000 people nationwide, and the total public expenditure on education as a percentage of GDP (from the IMD World Competitiveness Report). All five variables load on one factor, and we used the factor score as a summary measure of national labor. To measure capital, we collected data on five variables: financial market sophistication, soundness of banks, ease of access to loans, venture capital availability (all from the World Economic Forum's World Competitiveness Report), and the nation's per capita R&D spending (from the IMD World Competitiveness Report). Again, all five variables load on one factor, and we used the factor score as a sum- mary measure of national capital. Finally, to measure government policy, we collected data on intellectual property protection, university/industry research collaboration, government subsidies and tax cred- its for firm R&D, and government procurement of advanced technology products (all from the World Economic Forum's World Competitiveness Report). Again, all four variables load on one factor, and we used the factor score as a sum- mary measure of national capital. Firm labor and capital. We also collected and coded data on skilled labor and capital for each firm in our sample. 7 We also ran our models with patent counts only and found that our results are robust to this measure. We measured skilled labor using the percentage of the total number of employees who were employed in R&D in the year before our survey. We measured capital using the firm's R&D spending as a percentage of sales in the year before our survey.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Control Variables", "text": "The control variables in Equation 1 include the national GDP and population and the firm's citation-weighted U.S. patent outputs and its size in sales revenue. Patents repre- sent codified know-how that, as intermediate outputs of the innovation process, may in turn be embodied in final out- puts, such as radical innovations. Some researchers and pol- icy makers consider the registration of patents so important that they equate patents to innovation and sometimes mea- sure the latter with the former ( Archibugi and Coco 2005;Furman, Porter, and Stern 2002;Porter and Stern 1999). This line of thinking suggests that patents would be an important driver of radical innovation. Furthermore, if mar- kets value patents as highly as many researchers do, patents should have a major influence on financial returns of a firm. We used patents granted in the United States as our metric of patents because (1) most firms that seek patents tend to patent their significant innovations in the United States; (2) patent laws vary considerably across nations, and thus patents in the United States are a reasonable standard for cross-national comparisons; and (3) U.S. patents are the most widely accepted measure of patents used in cross- national studies of patenting (see Furman, Porter, and Stern 2002). We weighted the patents by their forward citations because doing so captures the importance of the patents rather than merely their volume (Griliches 1984;Jaffe, Trajtenberg, and Henderson 1993). 7 We collected the patent and citation data from Delphion for the year before the survey. The measures of the control variables in Equation 2 include the national GDP, population, inflation, national credit rating, the firm's citation-weighted U.S. patent out- put, and the firm's size in sales revenue. We control for industry in both Equations 1 and 2 as follows: We identified the two-digit Standard Industrial Classification code for the primary industry in which each firm operates and used this information to run industry-level fixed-effects estimations of Equations 1 and 2.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Results", "text": "We describe the results in four parts: response to the survey, validity and reliability of the measures, the drivers of radi- cal innovation, and the financial impact of radical innova- tion. In addition, Appendix A explores the robustness of our results, including the potential for mediating and moderat- ing effects on the dependent variables.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Survey Response", "text": "The response rates to our survey vary between 9% and 34% across nations, with an overall average response rate of 19% (see Table 1). These rates compare well with those in other large-scale international studies (e.g., Baim 1991). Never- theless, we also checked for nonresponse bias as follows: First, we compared means on the number of employees and total assets (standardized across our sample frame) for respondents and nonrespondents. We found no significant difference between respondents and nonrespondents on these demographic measures. This pattern also holds for most comparisons at other levels of analysis (within nations, across the four strata of firm size in our sampling plan, and across strata within each nation). Second, we compared means on important variables in the survey for respondents in the first versus last quartile in terms of date of response after the mailing. We performed this analysis across nations and found no significant difference in means for most of our variables in most nations. These results pro- vide some assurance that our data do not suffer from non- response bias. Respondents to the survey have substantial experience (they average 17.4 years of experience in their industry and 12.4 years of experience in their firm) and are closely involved in innovation activities (average level of personal involvement in innovation activities is 5.8 on a seven-point Likert scale anchored by \"not at all involved\" [1] and \"highly involved\" [7]). After cleaning the survey data and accounting for missing values, we have a sample size of 772. We then integrate the archival firm and national data with the survey data and achieve a sample size of 759. To our knowledge, this is the largest sample among multiconti- nent studies of firm innovation (for descriptive statistics, see Table 3). We first present the results of the estimation of our measurement model and then present the results of our for- mal tests and additional analyses.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Reliability and Validity", "text": "Following Anderson and Gerbing (1988), we assessed the measurement model before estimating the research model. To obtain reliable and valid measures of our focal variables, we first examined the face validity, interitem, and item-to- total correlations to arrive at the scales we described previ- ously. All scales show satisfactory reliabilities, with Cron- bach's alphas above the acceptable cutoff of .60 (except willingness to cannibalize, which has an alpha of .58). Next, we used confirmatory factor analysis to examine the unidimensionality of the constructs (i.e., the extent to which a single construct underlies a set of measures [items]). The overall fit of the measurement model provides the necessary information to determine whether unidimen- sionality is satisfied ( Gerbing and Anderson 1988;Steenkamp and Van Trijp 1991). The overall fit of the full model is satisfactory (Browne and Cudeck 1993). To check for discriminant validity of the latent con- structs, we constrained the correlation between each pair of latent constructs to 1; the constrained models show evi- dence of poor fit relative to the freely estimated equivalent. Lagrange-multiplier tests indicated no significant cross- loadings, thus providing further evidence of discriminant validity. The preceding tests provide support for the psycho- metric adequacy of our measures. ", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Drivers of Radical Innovation", "text": "Recall that our theory suggests that corporate culture is a primary driver of radical innovation, in addition to the effect of government, firm, and national labor, capital, and culture. Equation 1 specifies the model that captures this theory. The results (standardized coefficients) of the test of this model appear in Table 4. To show robustness of the results to mul- ticollinearity, we present the results in six nested versions of Equation 1. We highlight the key results. Model 5 shows that most of the traditional variables from the literature have little effect on radical innova- tion after we account for corporate culture. Model 5 also shows that five of the six measures of corporate culture (all except internal markets) have effects that are significantly greater than zero. Based on standardized coefficients and t-statistics, the effects of future market orientation, willing- ness to cannibalize, and tolerance for risk are particularly strong. Although the effects of incentives and product champions are relatively weaker, they are still significantly larger than zero. A reason for this difference may be because attitudes are a more proximate driver of innovation than practices. Other important results that emerge from Table 4 are that citation-weighted patents do not affect radi- cal innovations. Conversely, firms' R&D activities, mea- sured as the percentage of R&D employees to all employ- ees, have a significant, positive effect on radical innovation. Indeed, R&D may be a measure of a firm's commitment to innovation, at least in technology-driven markets. ", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "TABLE 4 Estimates of Independent Variables on Radical Innovation (Equation 1)", "text": "Note that none of the measures of religion have an impact on radical innovation (Model 4 and 5). How- ever, the measures of religion correlate with national cul- ture, as measured by Hofstede's (2003) variables, causing multicollinearity between these two sets of variables. To examine whether this multicollinearity affects the other results, we drop the religion variables from the full model (see Model 6) and find that significance levels and standard- ized coefficients of the other variables remain essentially unchanged.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Radical Innovation Across Nations / 15", "text": "Thus, for our sample of firms and nations, few of the national variables are significant drivers of radical innova- tion. This result holds across a plethora of robustness tests, including tests of mediation and moderation (see Appendix A). However, two broad drivers of radical innovation emerge as important: firms' corporate culture and their investments in skilled labor.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Financial Impact of Radical Innovation", "text": "An additional objective of this research is to establish the financial value of radical innovation. Table 5 presents the results of the impact of radical innovation on financial per- formance, as measured by firms' market-to-book ratio. Model 1 in Table 5 reveals that radical innovation has a sig- nificant effect on the market-to-book ratio, even after we control for industry fixed effects. Furthermore, Model 2 in Table 5 reveals that radical innovation continues to have a significant effect on the market-to-book ratio in the full model, which includes other firm and national variables. This model also indicates that at the firm level, skilled labor, which we measure as a ratio of R&D employees to all employees, is a strong predictor of market-to-book ratios, while patents, a commonly used measure of innovation, are not. At the national level, we find that national capital and national population have a significant impact on the market- to-book ratio. In summary, our results suggest that (1) mar- kets strongly reward radical innovation beyond any returns to patents, R&D, and other control variables and (2) radical innovation is a more powerful metric of commercial value/ performance than patents.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Conclusions", "text": "This study leads to two important conclusions. First, several factors do not seem to be as important drivers of radical innovation in firms across nations as many researchers believe. Among these are some frequently emphasized met- rics of national labor, capital, government regulation, and culture. In contrast, internal corporate culture is an impor- tant driver of radical innovation. The reasons we propose for this pattern of results are as follows: The current eco- nomic environment is characterized by increasing global- ization and the lowering of barriers to mobility of labor and capital. This \"flattening\" is often accompanied by the rapid adoption of best practice and policy by governments. The rapid progress of India and China presents some evidence of these factors in operation. In such an environment, national drivers are unlikely to be major discriminators of firms' performance, at least in the 17 nations we sample. In addition, firm and national factors have different levels of sensitivity; this is because firm factors reflect the unique features of each firm, while national factors are common across all firms in the country. Indeed, corporate culture is a factor that is unique, intangible, sticky, and difficult to change. Moreover, success in one generation of technology can breed attitudes of complacency and invulnerability with  a focus on managing current products and protecting cur- rent profits that brought that success. These cultural traits can blind a firm to radical innovations on the frontier. Thus, maintaining a culture of relentless innovation is difficult. Some examples illustrate the main results we obtain. A traditionally innovative nation, such as the United States, can be home to innovative firms, such as Apple or FedEx, and lumbering ones, such as Kodak and Kmart. Innovative firms such as Samsung (Korea) and Infosys (India) in tradi- tionally lagging economies can leapfrog ahead of slumber- ing giants in traditionally advanced economies. Indeed, the corporate culture in some of these innovative firms develops precisely to overcome aspects of their home economies that would otherwise hinder them. Thus, national factors, such as government, culture, labor, and capital, are not unimpor- tant. Rather, in the current environment among the 17 economies in our sample, corporate culture seems to be more important than these traditional country drivers in pre- dicting radical innovation in firms across nations. Second, we find that radical innovations translate into financial value to the firm. The importance of this finding lies in our measurement of radical innovations with a survey but financial value with archival, publicly available data. Thus, the result underscores the validity and impor- tance of our measure of radical innovation: Such innovation significantly increases market-to-book value, even after we control for patents, R&D, and other variables. Significantly, patents-a measure of innovation widely used in previous research-are not as important in influencing financial value as radical innovations in firms.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Implications", "text": "The prior analysis and our findings lead to some important implications for managers, researchers, and policy makers. First, our study suggests that firms are special forms of organizations that increasingly transcend national bound- aries, constraints, and systems. Innovative firms, it would seem, are similar: They share the same cultural practices and attitudes despite differences in location. However, this culture is difficult to observe, measure, and develop. We provide a diagnostic tool that can assess the relevant dimen- sions of culture and enable firms to benchmark themselves against others of their size, history, or industry. Thus, man- agers can be attuned to these cultural factors, can measure them, and can foster them to maintain a culture of relentless innovation. Such a focus may bear more tangible fruit than one that relies on government to invest in or protect mar- kets. Indeed, the appeal for government relief and interven- tion by firms may well be a cover for cultural deficiencies in their organizations that they may have previously overlooked. Second, we identify specific attitudes and practices within innovative firms that make them special and foster a culture that helps drive radical innovation. The attitudes include a willingness to cannibalize, future market orienta- tion, and risk tolerance, while the practices include empow- ering product champions and providing incentives for enter- prise. Firms, such as Apple, that have such cultural attitudes and practices are distinct and excel at radical innovation (Tellis and Golder 2001). They do not need to count patents and engineers. Indeed, Apple's \"best feat may be the culture that helps generate so many folks who've gone on to create great products elsewhere\" (BusinessWeek Online 2005). Third, our study highlights the usefulness in a cross- national context of an output-based measure of radical inno- vation (i.e., radically new products) over surrogates, such as patents and national scientific talent, used in the past (Acs and Audretsch 1987;Von Hippel 2005). Measuring radical innovation directly enables firms and nations to gauge where they really stand on the ultimate outputs that count rather than on inputs or intermediate outputs that reflect costs. This measure can enable firms and governments to channel resources toward drivers that matter and to ensure that the entire process of innovation is efficient and productive. Fourth, patents, a measure often used as a surrogate for radical innovation, turn out not to significantly affect firm capitalization and radical innovation. These results are robust and not due to model design or multicollinearity. Firms and policy makers have probably used patents because they are easily measured, seem to be a precondition for innovation, or seem to offer protection to intellectual property. However, many high-tech firms now realize that patents provide only partial protection for their inventions, and firms can be highly innovative without patenting. In this context, as a senior vice president of research at a For- tune 50 firm in the United States noted, \"We have many technologies and patents sitting on the shelf. Our problem is getting them out to market!\" So, firms and nations need to focus primarily on outputs such as radical innovation rather than on inputs such as patents, or they should at least focus on converting inputs into outputs. This conversion is not obvious, as our results and many contemporary examples show. For example, innovative Apple, with a little more than a hundred patents, stole Sony's market for mobile music, while Sony, with thousands of patents, refrained from cannibalizing its successful Walkman and music businesses. In summary, our results question some long-held premises about radical innovation, suggest a direct measure for the construct, and outline the attitudes and practices within a firm that support innovation. Authors who debate national labor, capital, culture, or government policy may be underappreciating the innovative revolution within firms. Policy makers who rely exclusively on the plausible metrics of scientific talent, patents, and intellectual property protec- tion may be missing the real battle taking place. The battle is within. It is a cultural one: between glorifying the past or being paranoid about the future (Grove 1996), between pro- tecting successes or cannibalizing them (Chandy and Tellis 1998), between averting risk or embracing it. The battle is for the soul of the firm. Innovative firms are those that clearly understand this battle and adopt decisive practices to win it.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Limitations and Further Research", "text": "Limitations of this study suggest three major research themes that could prove groundbreaking. First, we do not explore whether radical innovation leads to national wealth in addition to firm value. Current studies of nations' wealth attribute such wealth to natural resources, economic capital, talent, or favorable regulation. Further research could ascer- tain whether innovation is an important driver of national wealth in addition to the previously mentioned factors. Such research would have significant policy implications in addi- tion to implications for managers of firms. Second, consumer innovativeness and firm innovation have been topics of extensive research in marketing and strategy. To date, research has not attempted to join these two fields of inquiry. Further research could ascertain whether the innovativeness of consumers in a country drives or is influenced by the innovation of firms in that country. The findings of that research would have significant impli- cations for our understanding of whether and when radical innovation is a market-driven versus a market-driving phenomenon. Third, we do not study subsidiaries of multinational firms. A worthwhile extension of this study would be to explore whether corporate culture is strong enough to hold across various subsidiaries located thousands of miles apart in widely different nations and cultures. This topic is of spe- cial importance as firms rush to establish innovation centers in distant locales. In addition to the major research themes we have dis- cussed, our research also suggests some methodological extensions. First, our measures of corporate culture are from self-reports, while those for country variables are from secondary sources. A major challenge for researchers is to develop and gather hard measures of culture, such as those we use for patents, R&D expenditures, and financial returns ( Rindfleisch et al. 2007). Such measures would obviate the concern that some of our results may be affected by mea- surement error or differences in the level of measurement. Second, recent research (Wong, Rindfleisch, and Bur- roughs 2003) has shown that mixed-worded scales can cause problems in reliability and factor loading. In this research, we use mixed-worded scales to reduce the system- atic bias due to the use of a particular response style within a particular nation ( Baumgartner and Steenkamp 2001). Use of such scales might account for why the Cronbach's alphas of our scales are sometimes lower than the more desirable cutoff of .70. Third, we restrict our sample to 17 nations. These results may not hold for nations wrecked by political insta- bility or riddled with corruption, nepotism, or war. Con- versely, small, highly innovative nations, such as Israel, or highly populous nations, such as Indonesia and Brazil, may hold special lessons that further research can uncover. In particular, research on these nations might shed more light on the ways firm and national factors are likely to interact and intersect in driving innovation. Finally, our sample of firms is restricted to publicly listed firms in each nation. In nations such as China, where the population of publicly listed firms is not very large, our sample frame is necessarily small. Generalizations beyond such firms should be made with caution and care and after much further research.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Are Our Results Driven by Common Method Bias?", "text": "Common method bias is irrelevant when testing Equation 2 because the dependent variable in this equation (market-to- book ratio) uses data from a set of secondary sources that are entirely independent of the sources of data for virtually all the independent variables in this equation (see Podsakoff et al. 2003). Indeed, the results for this equation help vali- date our survey-based measures because they demonstrate that our survey-based measure of radical innovation is a powerful predictor of financial outcomes measured in a future period by independent sources. To check whether common method poses problems in Equation 1, we conduct two additional analyses. First, we employ Harmon's one-factor test for common method bias. Specifically, we perform a confirmatory factor analysis (see Podsakoff et al. 2003) in which we link all items in our cor- porate culture (independent) variables as well as our radical innovation (dependent) variable to a single latent factor. The results of this analysis strongly reject the single-factor hypothesis, thus alleviating some concerns about common method bias. Second, to further alleviate concerns regarding common methods in Equation 1, we examine the correlations between two nomologically related constructs-radical innovation and patents. In our survey, we ask respondents to report on both radical innovation in their firm and the num- ber of U.S. patents granted to their firm in the previous year (2003). If common methods bias is pervasive in our data (e.g., as a result of social desirability), firms would respond similarly on both radical innovation and patents. We find that this is not the case. None of the items that make up our measure of radical innovation correlate with our measure of patents granted. Moreover, the information from the survey- based measure of patents matches closely with that from Delphion, an independent (secondary) source of patent data. All these analyses indicate that our results are not driven by common method bias.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Do the Results Change if Country Factors Are Modeled as Fixed Effects?", "text": "An alternative approach to the theory-based approach we use to model country-level factors is simply to use country- specific dummy variables to account for the unobserved heterogeneity that is due to each nation in our sample (see Baltagi 2005). The results from such an analysis (which are available from the authors on request) indicate that, in gen- eral, the effects of corporate culture are consistent with those in the results we presented previously.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Does Corporate Culture Mediate the Effect of National Culture on Radical Innovation?", "text": "A key question is whether national culture affects corporate culture or whether the latter mediates the effect of the former on radical innovation (Kirca, Jayachandran, and Bearden 2005). To address these questions, we carry out a formal test of mediation. Baron and Kenny (1986) list three conditions that must hold for mediation to exist. We test for the exis- tence of each of these conditions in our data and find that none are satisfied. As such, the effects of national culture on innovation are not mediated by internal (firm) culture. Specifically, Condition 1 states that the exogenous variables and the proposed mediator must each be signifi- cantly related to the dependent variable when considered separately. In our case, this condition requires that the national and corporate culture variables, taken separately, should each have a significant impact on innovation. We find that this condition does not hold. The coefficients of corpo- rate culture (in the regression that does not include national culture) are significant (see Model 2 in Table 4). However, the coefficients of the national culture variables (in the regression that does not include corporate culture) are not significantly different from zero (see Model 1 in Table 4). Similarly, Condition 2 states that the exogenous variables should be significantly related to the proposed mediator. In our case, this would suggest that the national culture variables should have a significant impact on corpo- rate culture. This condition, which is crucial for mediation, does not hold. The results of a regression of corporate cul- ture on the national culture variables indicate that none of the national culture variables have a significant impact on corporate culture. Finally, Condition 3 states that the relationship between the exogenous variables and the dependent variable should be weaker (for partial mediation) or nonsignificant (for full mediation) when the proposed mediator is included in the model than when the mediator is not included. In our case, this condition requires that the coefficients of the national culture variables on innovation should be significantly smaller when corporate culture is included in the model than when it is not included. This condition for mediation also does not hold. The coefficients of the national culture variables in the regression that does not include corporate culture are identical to those in the regression that does include corporate culture (both sets of coefficients are not significantly different from zero). Thus, not only does corporate culture not mediate the effect of national culture on radical innovation, but its effect also does not arise from national culture. In our sample of publicly held firms in 17 leading economies, the latter does not affect radical innovation at all.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Do Firm and Nation Factors Interact to Drive Radical Innovation?", "text": "Some authors have implied that firm and national factors may interact to drive innovation (see Bartholomew 1997;Murtha, Lenway, and Hart 2001;Nelson 1993). To test for this possibility, we ran several models that included first- order interactions between the key firm and national factors in our model in addition to including all the main effects about which we theorize. We find that most of the inter- action coefficients are not significantly different from zero; moreover, the pattern of results is not consistent with any systematic effects that would be predicted by the literature. Given these results, we conclude that further study of the interaction between national and firm drivers of innovation would be a promising avenue for theoretical and empirical work.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Do the Effects of Corporate Culture Vary by Nation?", "text": "To examine this question, we first run random coefficient regression models (Raudenbush and Bryk 2002) that allow the coefficients of each corporate culture variable to vary across nations. We find that the full model, in which all cor- porate culture coefficients vary by nations, does not con- verge. We then run separate random coefficient regressions, such that a single corporate culture coefficient varies ran- domly across nations in each regression equation. In each of these equations, we find that none of the corporate culture coefficients vary significantly across nations. We also run a random-intercept regression, in which random variations from the intercept reflect national unobserved heterogene- ity. We find that the intercept for the radical innovation equation does not vary significantly across nations and that all effects of the corporate culture variables remain substan- tively similar to those we reported previously. Finally, a model that incorporates heterogeneity through latent class analysis also yields similar results. These analyses indicate consistency in the effects of corporate culture on radical innovation across the nations in our sample.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Do Our Survey Measures Have Cross-Cultural Equivalence?", "text": "Our survey measures rely on respondents interpreting key constructs, such as radical innovation, in a consistent man- ner across nations. To ensure this, we take several steps (see Harkness, Van de Vijver, and Mohler 2003): (1) translation and back translation to ensure consistency in the meaning of constructs and items across different languages, (2) a clear definition of radical innovation while anchoring it in two universal examples (the compact disc player relative to cassette tapes and record players and the microwave oven relative to conventional ovens), and (3) in-depth, face-to- face interviews with managers from different cultural con- texts (Germany, the United Kingdom, Korea, India, Japan, and China). In addition, we test our data for metric equivalence across nations. Because testing for metric invariance in eight constructs across 17 nations would require a far larger sample size than ours, we create four groups of nations that share a similar cultural and economic background. The group \"English speaking\" includes Canada, the United States, the United Kingdom, and Australia. All European nations (Italy, Germany, the Netherlands, France, Switzer- land, and Sweden) are grouped together. The group \"emerg- ing nations\" includes China, India, Taiwan, Korea, Singa- pore, and Hong Kong. Japan is a fourth group because it does not fit with any other group. Following Steenkamp and Baumgartner (1998), we first test the hypothesis of full met- ric invariance by constraining the matrix of factor loadings to be invariant across groups. Because there is a significant increase between the configural model and the full metric model invariance (\u0394\u03c7 2 (42) = 76.966, p = .001), we do not find support for full metric invariance. However, prior research has suggested that \"full mea- surement invariance is particularly unlikely\" (Steenkamp and Baumgartner, 1998, p. 81) and should be considered scientifically unrealistic (Horn, McArdle, and Mason 1983), while partial metric invariance can be regarded as sufficient to establish cross-cultural equivalence (Byrne, Shavelson, and Muthen 1989). To identify the source of lack of full metric invariance, we examine the difference between each pair of factor loadings. We find that only 23% (13 of 57) of the pairs of factor loadings are significantly different across nations; moreover, there are no clear patterns in these differences. These findings are largely consistent with Steenkamp and Baumgartner's (1998). To test for partial metric invariance, we free up the constraints on the model parameters and find that the partial metric invariance model is not significantly different from the configural model (\u0394\u03c7 2 (34) = 32.192, p = .566). Furthermore, the partial met- ric invariance model shows better fit than the configural model because the root mean square error of approximation and the consistent Akaike information criterion, which take into account both goodness of fit and model parsimony, are lower. Thus, we find support for partial metric invariance in our data. Overall, these results provide some evidence of cross-cultural equivalence in our survey measures. .74 12.94", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Standardized", "text": "3. We have no difficulty in introducing products that are radically different from existing products in the industry.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": ".37 8.37", "text": "Alternative Measure % of total sales revenue from radical product innovations introduced by our firm in the last three years (2001)(2002)(2003) (1 = 0%, 2 = 0%-1%, 3 = 1%-5%, 4 = 5%-10%, 5 = 10%-20%, 6 = 20%-30%, and 7 = over 30%) Willingness to Cannibalize .65 1. We are very willing to sacrifice sales of our existing products to improve sales of our new products. .71 2. We tend to oppose new projects that could take away from sales of our existing products. (R) .32 8.17 3. We will not aggressively pursue a new technology that causes existing investments to lose value. (R) .56 8.13 Future Market Focus .66 1. Our firm gives more emphasis to customers of the future relative to current customers. .38 7.13 2. Market research efforts in our firm are aimed at obtaining information about customers' needs in the future, relative to their current needs. .50 8.49 3. We are slow to detect fundamental shifts in our industry (e.g., competition, technology, regulation). (R) .52 9.01 4. Our firm is oriented more toward the future than the present. .57", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Risk Tolerance", "text": ".72 1. Managers in our firm rarely take risky decisions. (R) .71 11.29 2. Relative to other firms, we tend to favor higher-risk, higher-return investments. .57 10.47 3. We are reluctant to engage in untested business ventures. (R) .54 4. We believe it is often necessary to take calculated risks. .44 8.82 Product Champions .56 1. Employees with new product ideas receive no support in our firm. (R) .70 6.77 2. Top managers in our firm strongly support champions of ideas for new products.", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": ".39", "text": "Incentives for Enterprise .77 1. We provide generous monetary rewards to innovative employees. .66 9.25 2. We provide many non-monetary rewards (e.g., recognition, autonomy) to innovative employees. .82", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}
{"section": "Autonomy", "text": ".74 1. All new product and process decisions in our firm require the approval of the corporate office. (R) .88 9.23 2. Few strategic actions can be taken in divisions in our firm until the corporate office approves these actions. (R) .72 Internal Competition .68 1. Divisions in our firm frequently enter markets served by other divisions. .64 6.54 2. Divisions in our firm actively compete with each other to gain new markets .84", "title": "(electronic) Radical Innovation Across Nations: The Preeminence of Corporate Culture", "file_name": "Tellis et al. - Radical Innovation across Nations The Preeminence.pdf"}